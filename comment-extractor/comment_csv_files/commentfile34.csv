 SPDX-License-Identifier: GPL-2.0-only

 Copyright(c) 2017-2019 Intel Corporation.

 create the debugfs master-N */

 DP0 non-banked registers */

 DP0 Bank 0 registers */

 DP0 Bank 1 registers */

 SCP registers */

	/*

	 * SCP Bank 0/1 registers are read-only and cannot be

	 * retrieved from the Slave. The Master typically keeps track

	 * of the current frame size so the information can be found

	 * in other places

 DP1..14 registers */

 DPi registers */

 DPi Bank0 registers */

 DPi Bank1 registers */

 create the debugfs slave-name */

 SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)

 Copyright(c) 2021 Intel Corporation.

/*

 * Soundwire DMI quirks

/*

 * Some TigerLake devices based on an initial Intel BIOS do not expose

 * the correct _ADR in the DSDT.

 * Remap the bad _ADR values to the ones reported by hardware

/*

 * The initial version of the Dell SKU 0A3E did not expose the devices

 * on the correct links.

 rt715 on link0 */

 rt711 on link1 */

 rt1308 on link2 */

 quirk used for NUC15 'Bishop County' LAPBC510 and LAPBC710 skews */

 check if any address remap quirk applies */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright(c) 2019-2020 Intel Corporation.

/*

 * The 3s value for autosuspend will only be used if there are no

 * devices physically attached on a bus segment. In practice enabling

 * the bus operation will result in children devices become active and

 * the master device will only suspend when all its children are no

 * longer active.

/*

 * The sysfs for properties reflects the MIPI description as given

 * in the MIPI DisCo spec

 *

 * Base file is:

 *	sdw-master-N

 *      |---- revision

 *      |---- clk_stop_modes

 *      |---- max_clk_freq

 *      |---- clk_freq

 *      |---- clk_gears

 *      |---- default_row

 *      |---- default_col

 *      |---- dynamic_shape

 *      |---- err_threshold

/**

 * sdw_master_device_add() - create a Linux Master Device representation.

 * @bus: SDW bus instance

 * @parent: parent device

 * @fwnode: firmware node handle

		/*

		 * On err, don't free but drop ref as this will be freed

		 * when release method is invoked.

 add shortcuts to improve code readability/compactness */

/**

 * sdw_master_device_del() - delete a Linux Master Device representation.

 * @bus: bus handle

 *

 * This function is the dual of sdw_master_device_add()

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2019, Linaro Limited

 Port alloc/free lock */

 pg register + offset */

 pg register + offset */

 write address register */

 Check for fifo underflow during read */

 Check if read data is available in read fifo */

 Check for fifo overflow during write */

 Check for space in write fifo before writing */

 Its assumed that write is okay as we do not get any status back */

 version 1.3 or less */

		/*

		 * sleep for 10ms for MSM soundwire variant to allow broadcast

		 * command to complete.

 wait for FIFO RD to complete to avoid overflow */

 wait for FIFO RD CMD complete to avoid overflow */

 wait 500 us before retry on fifo read failure */

SCP_Devid5 - Devid 4*/

SCP_Devid3 - DevId 2 Devid 1 Devid 0*/

 Now compare with entries */

 Clear Rows and Cols */

 Enable Auto enumeration */

 Mask soundwire interrupts */

 Configure No pings */

 Configure number of retries of a read/write cmd */

 Only for versions >= 1.5.1 */

 Set IRQ to PULSE */

 enable CPU IRQs */

 port config starts at offset 0 so -1 from actual port number */

 Port numbers start from 1 - 14*/

 hw parameters wil be ignored as we only support PDM */

 PDM dais are only tested for now */

 Valid port numbers are from 1-14, so mask out port 0 explicitly */

 Valid port number range is from 1-14 */

 sentinel */},

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2015-17 Intel Corporation.

/**

 * sdw_get_device_id - find the matching SoundWire device id

 * @slave: SoundWire Slave Device

 * @drv: SoundWire Slave Driver

 *

 * The match is done by comparing the mfg_id and part_id from the

 * struct sdw_device_id.

 modalias is sdw:m<mfg_id>p<part_id>v<version>c<class_id> */

	/*

	 * fw description is mandatory to bind

	/*

	 * attach to power domain but don't turn on (last arg)

 device is probed so let's read the properties now */

 init the sysfs as we have properties now */

	/*

	 * Check for valid clk_stop_timeout, use DisCo worst case value of

	 * 300ms

	 *

	 * TODO: check the timeouts and driver removal case

/**

 * __sdw_register_driver() - register a SoundWire Slave driver

 * @drv: driver to register

 * @owner: owning module/driver

 *

 * Return: zero on success, else a negative error code.

/**

 * sdw_unregister_driver() - unregisters the SoundWire Slave driver

 * @drv: driver to unregister

 SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)

 Copyright(c) 2015-17 Intel Corporation.

/*

 * Soundwire Intel Master Driver

/*

 * debug/config flags for the Intel SoundWire Master.

 *

 * Since we may have multiple masters active, we can have up to 8

 * flags reused in each byte, with master0 using the ls-byte, etc.

/*

 * Read, write helpers for HW registers

/*

 * debugfs

		/*

		 * the value 10 is the number of PDIs. We will need a

		 * cleanup to remove hard-coded Intel configurations

		 * from cadence_master.c

 Userspace changed the hardware state behind the kernel's back */

 Userspace changed the hardware state behind the kernel's back */

 CONFIG_DEBUG_FS */

/*

 * shim ops

	/*

	 * The hardware relies on an internal counter, typically 4kHz,

	 * to generate the SoundWire SSP - which defines a 'safe'

	 * synchronization point between commands and audio transport

	 * and allows for multi link synchronization. The SYNCPRD value

	 * is only dependent on the oscillator clock provided to

	 * the IP, so adjust based on _DSD properties reported in DSDT

	 * tables. The values reported are based on either 24MHz

	 * (CNL/CML) or 38.4 MHz (ICL/TGL+).

 we first need to program the SyncPRD/CPU registers */

 set SyncPRD period */

 Set SyncCPU bit */

 Link power up sequence */

 only power-up enabled links */

 SyncCPU will change once link is active */

 this needs to be called with shim_lock */

 Switch to MIP from Glue logic */

 at this point Master IP has full control of the I/Os */

 this needs to be called with shim_lock */

 Glue logic */

 at this point Integration Glue has full control of the I/Os */

 Initialize Shim */

 Enable the wakeup */

 Disable the wake up interrupt */

 Clear wake status */

 Link power down sequence */

 only power-down enabled links */

			/*

			 * we leave the sdw->cdns.link_up flag as false since we've disabled

			 * the link at this point and cannot handle interrupts any longer.

 update SYNC register */

 Read SYNC register */

	/*

	 * Set SyncGO bit to synchronously trigger a bank switch for

	 * all the masters. A write to SYNCGO bit clears CMDSYNC bit for all

	 * the Masters.

/*

 * PDI routines

 PCM Stream Capability */

 PDM Stream Capability */

		/*

		 * WORKAROUND: on all existing Intel controllers, pdi

		 * number 2 reports channel count as 1 even though it

		 * supports 8 channels. Performing hardcoding for pdi

		 * number 2.

 zero based values for channel count in register */

 First update PCM streams followed by PDM streams */

 the Bulk and PCM streams are not contiguous */

	/*

	 * Program stream parameters to stream SHIM register

	 * This is applicable for PCM stream only.

 the Bulk and PCM streams are not contiguous */

 Program Stream config ALH register */

/*

 * bank switch routines

 Write to register only for multi-link */

 Write to register only for multi-link */

 Read SYNC register */

	/*

	 * post_bank_switch() ops is called from the bus in loop for

	 * all the Masters in the steam with the expectation that

	 * we trigger the bankswitch for the only first Master in the list

	 * and do nothing for the other Masters

	 *

	 * So, set the SYNCGO bit only if CMDSYNC bit is set for any Master.

/*

 * DAI routines

 do run-time configurations for SHIM, ALH and PDI/PORT */

 store pdi and hw_params, may be needed in prepare step */

 Inform DSP about PDI stream number */

 Port configuration */

		/*

		 * .prepare() is called after system resume, where we

		 * need to reinitialize the SHIM/ALH/Cadence IP.

		 * .prepare() is also called to deal with underflows,

		 * but in those cases we cannot touch ALH/SHIM

		 * registers

 configure stream */

 Inform DSP about PDI stream number */

	/*

	 * The sdw stream state will transition to RELEASED when stream->

	 * master_list is empty. So the stream state will transition to

	 * DEPREPARED for the first cpu-dai and to RELEASED for the last

	 * cpu-dai.

		/*

		 * we don't have a .suspend dai_ops, and we don't have access

		 * to the substream, so let's mark both capture and playback

		 * DMA contexts as suspended

 TODO: Read supported rates/formats from hardware */

 DAIs are created based on total number of PDIs supported */

 Create PCM DAIs */

 Create PDM DAIs */

 Find master handle */

 the values reported by BIOS are the 2x clock, not the bus clock */

 Initialize with default handler to read all DisCo properties */

 read Intel-specific properties */

 Initialize shim and controller */

/*

 * probe and init (aux_dev_id argument is required by function prototype but not used)

 Set property read ops */

 set driver data, accessed by snd_soc_dai_get_drvdata() */

 use generic bandwidth allocation algorithm */

	/*

	 * Ignore BIOS err_threshold, it's a really bad idea when dealing

	 * with multiple hardware synchronized links

		/*

		 * hardware-based synchronization is required regardless

		 * of the number of segments used by a stream: SSP-based

		 * synchronization is gated by gsync when the multi-master

		 * mode is set.

 Initialize shim, controller */

 Read the PDI config and initialize cadence PDI */

	/*

	 * follow recommended programming flows to avoid timeouts when

	 * gsync is enabled

 Register DAIs */

 Enable runtime PM */

		/*

		 * To keep the clock running we need to prevent

		 * pm_runtime suspend from happening by increasing the

		 * reference count.

		 * This quirk is specified by the parent PCI device in

		 * case of specific latency requirements. It will have

		 * no effect if pm_runtime is disabled by the user via

		 * a module parameter for testing purposes.

	/*

	 * The runtime PM status of Slave devices is "Unsupported"

	 * until they report as ATTACHED. If they don't, e.g. because

	 * there are no Slave devices populated or if the power-on is

	 * delayed or dependent on a power switch, the Master will

	 * remain active and prevent its parent from suspending.

	 *

	 * Conditionally force the pm_runtime core to re-evaluate the

	 * Master status in the absence of any Slave activity. A quirk

	 * is provided to e.g. deal with Slaves that may be powered on

	 * with a delay. A more complete solution would require the

	 * definition of Master properties.

	/*

	 * Since pm_runtime is already disabled, we don't decrease

	 * the refcount when the clock_stop_quirk is

	 * SDW_INTEL_CLK_STOP_NOT_ALLOWED

 disable WAKEEN interrupt ASAP to prevent interrupt flood */

	/*

	 * resume the Master, which will generate a bus reset and result in

	 * Slaves re-attaching and be re-enumerated. The SoundWire physical

	 * device which generated the wake will trigger an interrupt, which

	 * will in turn cause the corresponding Linux Slave device to be

	 * resumed and the Slave codec driver to check the status.

/*

 * PM calls

		/*

		 * if we've enabled clock stop, and the parent is suspended, the SHIM registers

		 * are not accessible and the shim wake cannot be disabled.

		 * The only solution is to resume the entire bus to full power

		/*

		 * If any operation in this block fails, we keep going since we don't want

		 * to prevent system suspend from happening and errors should be recoverable

		 * on resume.

		/*

		 * first resume the device for this link. This will also by construction

		 * resume the PCI parent device.

		/*

		 * Continue resuming the entire bus (parent + child devices) to exit

		 * the clock stop mode. If there are no devices connected on this link

		 * this is a no-op.

		 * The resume to full power could have been implemented with a .prepare

		 * step in SoundWire codec drivers. This would however require a lot

		 * of code to handle an Intel-specific corner case. It is simpler in

		 * practice to add a loop at the link level.

				/*

				 * paranoia check: this should not happen with the .prepare

				 * resume to full power

 follow required sequence from runtime_pm.rst */

	/*

	 * make sure all Slaves are tagged as UNATTACHED and provide

	 * reason for reinitialization

	/*

	 * follow recommended programming flows to avoid timeouts when

	 * gsync is enabled

	/*

	 * after system resume, the pm_runtime suspend() may kick in

	 * during the enumeration, before any children device force the

	 * master device to remain active.  Using pm_runtime_get()

	 * routines is not really possible, since it'd prevent the

	 * master from suspending.

	 * A reasonable compromise is to update the pm_runtime

	 * counters and delay the pm_runtime suspend by several

	 * seconds, by when all enumeration should be complete.

		/*

		 * make sure all Slaves are tagged as UNATTACHED and provide

		 * reason for reinitialization

		/*

		 * follow recommended programming flows to avoid

		 * timeouts when gsync is enabled

		/*

		 * An exception condition occurs for the CLK_STOP_BUS_RESET

		 * case if one or more masters remain active. In this condition,

		 * all the masters are powered on for they are in the same power

		 * domain. Master can preserve its context for clock stop0, so

		 * there is no need to clear slave status and reset bus.

			/*

			 * make sure all Slaves are tagged as UNATTACHED and

			 * provide reason for reinitialization

			/*

			 * follow recommended programming flows to avoid

			 * timeouts when gsync is enabled

			/*

			 * Re-initialize the IP since it was powered-off

 auxiliary_driver_register() sets .name to be the modname */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright(c) 2015-2020 Intel Corporation.

/*

 * Since we can't use ARRAY_SIZE, hard-code number of dpN attributes.

 * This needs to be updated when adding new attributes - an error will be

 * flagged on a mismatch.

 allocate attributes, last one is NULL */

 paranoia check for editing mistakes */

 SPDX-License-Identifier: GPL-2.0

/*

 * Virtio-based remote processor messaging bus

 *

 * Copyright (C) 2011 Texas Instruments, Inc.

 * Copyright (C) 2011 Google, Inc.

 *

 * Ohad Ben-Cohen <ohad@wizery.com>

 * Brian Swetland <swetland@google.com>

/**

 * struct virtproc_info - virtual remote processor state

 * @vdev:	the virtio device

 * @rvq:	rx virtqueue

 * @svq:	tx virtqueue

 * @rbufs:	kernel address of rx buffers

 * @sbufs:	kernel address of tx buffers

 * @num_bufs:	total number of buffers for rx and tx

 * @buf_size:   size of one rx or tx buffer

 * @last_sbuf:	index of last tx buffer used

 * @bufs_dma:	dma base addr of the buffers

 * @tx_lock:	protects svq, sbufs and sleepers, to allow concurrent senders.

 *		sending a message might require waking up a dozing remote

 *		processor, which involves sleeping, hence the mutex.

 * @endpoints:	idr of local endpoints, allows fast retrieval

 * @endpoints_lock: lock of the endpoints set

 * @sendq:	wait queue of sending contexts waiting for a tx buffers

 * @sleepers:	number of senders that are waiting for a tx buffer

 *

 * This structure stores the rpmsg state of a given virtio remote processor

 * device (there might be several virtio proc devices for each physical

 * remote processor).

 The feature bitmap for virtio rpmsg */

 RP supports name service notifications */

/**

 * struct rpmsg_hdr - common header for all rpmsg messages

 * @src: source address

 * @dst: destination address

 * @reserved: reserved for future use

 * @len: length of payload (in bytes)

 * @flags: message flags

 * @data: @len bytes of message payload data

 *

 * Every message sent(/received) on the rpmsg bus begins with this header.

/**

 * struct virtio_rpmsg_channel - rpmsg channel descriptor

 * @rpdev: the rpmsg channel device

 * @vrp: the virtio remote processor device this channel belongs to

 *

 * This structure stores the channel that links the rpmsg device to the virtio

 * remote processor device.

/*

 * We're allocating buffers of 512 bytes each for communications. The

 * number of buffers will be computed from the number of buffers supported

 * by the vring, upto a maximum of 512 buffers (256 in each direction).

 *

 * Each buffer will have 16 bytes for the msg header and 496 bytes for

 * the payload.

 *

 * This will utilize a maximum total space of 256KB for the buffers.

 *

 * We might also want to add support for user-provided buffers in time.

 * This will allow bigger buffer size flexibility, and can also be used

 * to achieve zero-copy messaging.

 *

 * Note that these numbers are purely a decision of this driver - we

 * can change this without changing anything in the firmware of the remote

 * processor.

/*

 * Local addresses are dynamically allocated on-demand.

 * We do not dynamically assign addresses from the low 1024 range,

 * in order to reserve that address range for predefined services.

/**

 * rpmsg_sg_init - initialize scatterlist according to cpu address location

 * @sg: scatterlist to fill

 * @cpu_addr: virtual address of the buffer

 * @len: buffer length

 *

 * An internal function filling scatterlist according to virtual address

 * location (in vmalloc or in kernel).

/**

 * __ept_release() - deallocate an rpmsg endpoint

 * @kref: the ept's reference count

 *

 * This function deallocates an ept, and is invoked when its @kref refcount

 * drops to zero.

 *

 * Never invoke this function directly!

	/*

	 * At this point no one holds a reference to ept anymore,

	 * so we can directly free it

 for more info, see below documentation of rpmsg_create_ept() */

 do we need to allocate a local address ? */

 bind the endpoint to an rpmsg address (and allocate one if needed) */

/**

 * __rpmsg_destroy_ept() - destroy an existing rpmsg endpoint

 * @vrp: virtproc which owns this ept

 * @ept: endpoing to destroy

 *

 * An internal function which destroy an ept without assuming it is

 * bound to an rpmsg channel. This is needed for handling the internal

 * name service endpoint, which isn't bound to an rpmsg channel.

 * See also __rpmsg_create_ept().

 make sure new inbound messages can't find this ept anymore */

 make sure in-flight inbound messages won't invoke cb anymore */

 need to tell remote processor's name service about this channel ? */

 tell remote processor's name service we're removing this channel */

/*

 * create an rpmsg channel using its name and address info.

 * this function will be used to create both static and dynamic

 * channels.

 make sure a similar channel doesn't already exist */

 decrement the matched device's refcount back */

 Link the channel to our vrp */

 Assign public information to the rpmsg_device */

	/*

	 * rpmsg server channels has predefined local address (for now),

	 * and their existence needs to be announced remotely

 super simple buffer "allocator" that is just enough for now */

 support multiple concurrent senders */

	/*

	 * either pick the next unused tx buffer

	 * (half of our buffers are used for sending messages)

 or recycle a used one */

/**

 * rpmsg_upref_sleepers() - enable "tx-complete" interrupts, if needed

 * @vrp: virtual remote processor state

 *

 * This function is called before a sender is blocked, waiting for

 * a tx buffer to become available.

 *

 * If we already have blocking senders, this function merely increases

 * the "sleepers" reference count, and exits.

 *

 * Otherwise, if this is the first sender to block, we also enable

 * virtio's tx callbacks, so we'd be immediately notified when a tx

 * buffer is consumed (we rely on virtio's tx callback in order

 * to wake up sleeping senders as soon as a tx buffer is used by the

 * remote processor).

 support multiple concurrent senders */

 are we the first sleeping context waiting for tx buffers ? */

 enable "tx-complete" interrupts before dozing off */

/**

 * rpmsg_downref_sleepers() - disable "tx-complete" interrupts, if needed

 * @vrp: virtual remote processor state

 *

 * This function is called after a sender, that waited for a tx buffer

 * to become available, is unblocked.

 *

 * If we still have blocking senders, this function merely decreases

 * the "sleepers" reference count, and exits.

 *

 * Otherwise, if there are no more blocking senders, we also disable

 * virtio's tx callbacks, to avoid the overhead incurred with handling

 * those (now redundant) interrupts.

 support multiple concurrent senders */

 are we the last sleeping context waiting for tx buffers ? */

 disable "tx-complete" interrupts */

/**

 * rpmsg_send_offchannel_raw() - send a message across to the remote processor

 * @rpdev: the rpmsg channel

 * @src: source address

 * @dst: destination address

 * @data: payload of message

 * @len: length of payload

 * @wait: indicates whether caller should block in case no TX buffers available

 *

 * This function is the base implementation for all of the rpmsg sending API.

 *

 * It will send @data of length @len to @dst, and say it's from @src. The

 * message will be sent to the remote processor which the @rpdev channel

 * belongs to.

 *

 * The message is sent using one of the TX buffers that are available for

 * communication with this remote processor.

 *

 * If @wait is true, the caller will be blocked until either a TX buffer is

 * available, or 15 seconds elapses (we don't want callers to

 * sleep indefinitely due to misbehaving remote processors), and in that

 * case -ERESTARTSYS is returned. The number '15' itself was picked

 * arbitrarily; there's little point in asking drivers to provide a timeout

 * value themselves.

 *

 * Otherwise, if @wait is false, and there are no TX buffers available,

 * the function will immediately fail, and -ENOMEM will be returned.

 *

 * Normally drivers shouldn't use this function directly; instead, drivers

 * should use the appropriate rpmsg_{try}send{to, _offchannel} API

 * (see include/linux/rpmsg.h).

 *

 * Returns 0 on success and an appropriate error value on failure.

 bcasting isn't allowed */

	/*

	 * We currently use fixed-sized buffers, and therefore the payload

	 * length is limited.

	 *

	 * One of the possible improvements here is either to support

	 * user-provided buffers (and then we can also support zero-copy

	 * messaging), or to improve the buffer allocator, to support

	 * variable-length buffer sizes.

 grab a buffer */

 no free buffer ? wait for one (but bail after 15 seconds) */

 enable "tx-complete" interrupts, if not already enabled */

		/*

		 * sleep until a free buffer is available or 15 secs elapse.

		 * the timeout period is not configurable because there's

		 * little point in asking drivers to specify that.

		 * if later this happens to be required, it'd be easy to add.

 disable "tx-complete" interrupts if we're the last sleeper */

 timeout ? */

 add message to the remote processor's virtqueue */

		/*

		 * need to reclaim the buffer here, otherwise it's lost

		 * (memory won't leak, but rpmsg won't use it again for TX).

		 * this will wait for a buffer management overhaul.

 tell the remote processor it has a pending message to read */

	/*

	 * We currently use fixed-sized buffers, so trivially sanitize

	 * the reported payload length.

 use the dst addr to fetch the callback of the appropriate user */

 let's make sure no one deallocates ept while we use it */

 make sure ept->cb doesn't go away while we use it */

 farewell, ept, we don't need you anymore */

 publish the real size of the buffer */

 add the buffer back to the remote processor's virtqueue */

 called when an rx buffer is used, and it's time to digest a message */

 tell the remote processor we added another available rx buffer */

/*

 * This is invoked whenever the remote processor completed processing

 * a TX msg we just sent it, and the buffer is put back to the used ring.

 *

 * Normally, though, we suppress this "tx complete" interrupt in order to

 * avoid the incurred overhead.

 wake up potential senders that are waiting for a tx buffer */

/*

 * Called to expose to user a /dev/rpmsg_ctrlX interface allowing to

 * create endpoint-to-endpoint communication without associated RPMsg channel.

 * The endpoints are rattached to the ctrldev RPMsg device.

 Link the channel to the vrp */

 Assign public information to the rpmsg_device */

 We expect two virtqueues, rx and tx (and in this order) */

 we expect symmetric tx/rx vrings */

 we need less buffers if vrings are small */

 allocate coherent memory for the buffers */

 half of the buffers is dedicated for RX */

 and half is dedicated for TX */

 set up the receive buffers */

 sanity check; this can't really happen */

 suppress "tx-complete" interrupts */

 if supported by the remote processor, enable the name service */

 Link the channel to our vrp */

 Assign public information to the rpmsg_device */

	/*

	 * Prepare to kick but don't notify yet - we can't do this before

	 * device is ready.

 From this point on, we can notify and get callbacks. */

 tell the remote processor it can start sending messages */

	/*

	 * this might be concurrent with callbacks, but we are only

	 * doing notify, not a full kick here, so that's ok.

 SPDX-License-Identifier: GPL-2.0



 Copyright 2019 Google LLC.

/**

 * struct rpmsg_ns_msg - dynamic name service announcement message

 * @name: name of remote service that is published

 * @addr: address of remote service that is published

 *

 * This message is sent across to publish a new service. When we receive these

 * messages, an appropriate rpmsg channel (i.e device) is created. In turn, the

 * ->probe() handler of the appropriate rpmsg driver will be invoked

 *  (if/as-soon-as one is registered).

	/*

	 * TODO: This currently is same as mtk_rpmsg_send, and wait until SCP

	 * received the last command.

	/*

	 * the name service ept does _not_ belong to a real rpmsg channel,

	 * and is handled by the rpmsg bus itself.

	 * for sanity reasons, make sure a valid rpdev has _not_ sneaked

	 * in somehow.

 don't trust the remote processor for null terminating the name */

 a dedicated endpoint handles the name service msgs */

	/*

	 * Destroy the name service endpoint here, to avoid new channel being

	 * created after the rpmsg_unregister_device loop below.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STMicroelectronics 2020 - All Rights Reserved

/**

 * rpmsg_ns_register_device() - register name service device based on rpdev

 * @rpdev: prepared rpdev to be used for creating endpoints

 *

 * This function wraps rpmsg_register_device() preparing the rpdev for use as

 * basis for the rpmsg name service device.

 invoked when a name service announcement arrives */

 don't trust the remote processor for null terminating the name */

	/*

	 * Create the NS announcement service endpoint associated to the RPMsg

	 * device. The endpoint will be automatically destroyed when the RPMsg

	 * device will be deleted.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014-2017, The Linux Foundation. All rights reserved.

 * Copyright (c) 2017, Linaro Ltd.

/**

 * struct do_cleanup_msg - The data structure for an SSR do_cleanup message

 * @version:	The G-Link SSR protocol version

 * @command:	The G-Link SSR command - do_cleanup

 * @seq_num:	Sequence number

 * @name_len:	Length of the name of the subsystem being restarted

 * @name:	G-Link edge name of the subsystem being restarted

/**

 * struct cleanup_done_msg - The data structure for an SSR cleanup_done message

 * @version:	The G-Link SSR protocol version

 * @response:	The G-Link SSR response to a do_cleanup command, cleanup_done

 * @seq_num:	Sequence number

/**

 * G-Link SSR protocol commands

 Notifier list for all registered glink_ssr instances */

/**

 * qcom_glink_ssr_notify() - notify GLINK SSR about stopped remoteproc

 * @ssr_name:	name of the remoteproc that has been stopped

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015, Sony Mobile Communications AB.

 * Copyright (c) 2012-2013, The Linux Foundation. All rights reserved.

/*

 * The Qualcomm Shared Memory communication solution provides point-to-point

 * channels for clients to send and receive streaming or packet based data.

 *

 * Each channel consists of a control item (channel info) and a ring buffer

 * pair. The channel info carry information related to channel state, flow

 * control and the offsets within the ring buffer.

 *

 * All allocated channels are listed in an allocation table, identifying the

 * pair of items by name, type and remote processor.

 *

 * Upon creating a new channel the remote processor allocates channel info and

 * ring buffer items from the smem heap and populate the allocation table. An

 * interrupt is sent to the other end of the channel and a scan for new

 * channels should be done. A channel never goes away, it will only change

 * state.

 *

 * The remote processor signals it intent for bring up the communication

 * channel by setting the state of its end of the channel to "opening" and

 * sends out an interrupt. We detect this change and register a smd device to

 * consume the channel. Upon finding a consumer we finish the handshake and the

 * channel is up.

 *

 * Upon closing a channel, the remote processor will update the state of its

 * end of the channel and signal us, we will then unregister any attached

 * device and close our end of the channel.

 *

 * Devices attached to a channel can use the qcom_smd_send function to push

 * data to the channel, this is done by copying the data into the tx ring

 * buffer, updating the pointers in the channel info and signaling the remote

 * processor.

 *

 * The remote processor does the equivalent when it transfer data and upon

 * receiving the interrupt we check the channel info for new data and delivers

 * this to the attached device. If the device is not ready to receive the data

 * we leave it in the ring buffer for now.

/*

 * This lists the various smem heap items relevant for the allocation table and

 * smd channel entries.

/**

 * struct qcom_smd_edge - representing a remote processor

 * @dev:		device associated with this edge

 * @name:		name of this edge

 * @of_node:		of_node handle for information related to this edge

 * @edge_id:		identifier of this edge

 * @remote_pid:		identifier of remote processor

 * @irq:		interrupt for signals on this edge

 * @ipc_regmap:		regmap handle holding the outgoing ipc register

 * @ipc_offset:		offset within @ipc_regmap of the register for ipc

 * @ipc_bit:		bit in the register at @ipc_offset of @ipc_regmap

 * @mbox_client:	mailbox client handle

 * @mbox_chan:		apcs ipc mailbox channel handle

 * @channels:		list of all channels detected on this edge

 * @channels_lock:	guard for modifications of @channels

 * @allocated:		array of bitmaps representing already allocated channels

 * @smem_available:	last available amount of smem triggering a channel scan

 * @new_channel_event:	wait queue for new channel events

 * @scan_work:		work item for discovering new channels

 * @state_work:		work item for edge state changes

/*

 * SMD channel states.

/**

 * struct qcom_smd_channel - smd channel struct

 * @edge:		qcom_smd_edge this channel is living on

 * @qsept:		reference to a associated smd endpoint

 * @registered:		flag to indicate if the channel is registered

 * @name:		name of the channel

 * @state:		local state of the channel

 * @remote_state:	remote state of the channel

 * @state_change_event:	state change event

 * @info:		byte aligned outgoing/incoming channel info

 * @info_word:		word aligned outgoing/incoming channel info

 * @tx_lock:		lock to make writes to the channel mutually exclusive

 * @fblockread_event:	wakeup event tied to tx fBLOCKREADINTR

 * @tx_fifo:		pointer to the outgoing ring buffer

 * @rx_fifo:		pointer to the incoming ring buffer

 * @fifo_size:		size of each ring buffer

 * @bounce_buffer:	bounce buffer for reading wrapped packets

 * @cb:			callback function registered for this channel

 * @recv_lock:		guard for rx info modifications and cb pointer

 * @pkt_size:		size of the currently handled packet

 * @drvdata:		driver private data

 * @list:		lite entry for @channels in qcom_smd_edge

/*

 * Format of the smd_info smem items, for byte aligned channels.

/*

 * Format of the smd_info smem items, for word aligned channels.

/**

 * struct qcom_smd_alloc_entry - channel allocation entry

 * @name:	channel name

 * @cid:	channel index

 * @flags:	channel flags and edge id

 * @ref_count:	reference count of the channel

/*

 * Each smd packet contains a 20 byte header, with the first 4 being the length

 * of the packet.

/*

 * Signal the remote processor associated with 'channel'.

		/*

		 * We can ignore a failing mbox_send_message() as the only

		 * possible cause is that the FIFO in the framework is full of

		 * other writes to the same bit.

/*

 * Initialize the tx channel info

/*

 * Set the callback for a channel, with appropriate locking

/*

 * Calculate the amount of data available in the rx fifo

/*

 * Set tx channel state and inform the remote processor

/*

 * Copy count bytes of data using 32bit accesses, if that's required.

/*

 * Copy count bytes of data using 32bit accesses, if that is required.

/*

 * Read count bytes of data from the rx fifo into buf, but don't advance the

 * tail.

/*

 * Advance the rx tail by count bytes.

/*

 * Read out a single packet from the rx fifo and deliver it to the device

 Use bounce buffer if the data wraps */

 Only forward the tail if the client consumed the data */

/*

 * Per channel interrupt handling

 Handle state changes */

 Indicate that we have seen any state change */

 Signal waiting qcom_smd_send() about the interrupt */

 Don't consume any data until we've opened the channel */

 Indicate that we've seen the new data */

 Consume data */

 Indicate that we have seen and updated tail */

 Signal the remote that we've consumed the data (if requested) */

 Ensure ordering of channel info updates */

/*

 * The edge interrupts are triggered by the remote processor on state changes,

 * channel info updates or when new channels are created.

	/*

	 * Handle state changes or data on each of the channels on this edge

	/*

	 * Creating a new channel requires allocating an smem entry, so we only

	 * have to scan if the amount of available space in smem have changed

	 * since last scan.

/*

 * Calculate how much space is available in the tx fifo.

/*

 * Write count bytes of data into channel, possibly wrapping in the ring buffer

/**

 * qcom_smd_send - write data to smd channel

 * @channel:	channel handle

 * @data:	buffer of data to write

 * @len:	number of bytes to write

 * @wait:	flag to indicate if write has ca wait

 *

 * This is a blocking write of len bytes into the channel's tx ring buffer and

 * signal the remote end. It will sleep until there is enough space available

 * in the tx buffer, utilizing the fBLOCKREADINTR signaling mechanism to avoid

 * polling.

 Word aligned channels only accept word size aligned data */

 Reject packets that are too big */

 Highlight the fact that if we enter the loop below we might sleep */

 Wait without holding the tx_lock */

 Fail if the channel was closed */

 Ensure ordering of channel info updates */

/*

 * Helper for opening a channel

	/*

	 * Packets are maximum 4k, but reduce if the fifo is smaller

 Wait for remote to enter opening or opened */

 Wait for remote to enter opened */

/*

 * Helper for closing and resetting a channel

 Wait up to HZ for the channel to appear */

/*

 * Finds the device_node for the smd child interested in this channel.

/*

 * Create a smd client device for channel that is being opened.

 Link qsdev to our SMD edge */

 Assign callbacks for rpmsg_device */

 Assign public information to the rpmsg_device */

/*

 * Allocate the qcom_smd_channel object for a newly found smd channel,

 * retrieving and validating the smem items involved.

	/*

	 * Use the size of the item to figure out which channel info struct to

	 * use.

 The channel consist of a rx and tx fifo of equal size */

/*

 * Scans the allocation table for any newly allocated channels, calls

 * qcom_smd_create_channel() to create representations of these and add

 * them to the edge's list of channels.

/*

 * This per edge worker scans smem for any new channels and register these. It

 * then scans all registered channels for state changes that should be handled

 * by creating or destroying smd client devices for the registered channels.

 *

 * LOCKING: edge->channels_lock only needs to cover the list operations, as the

 * worker is killed before any channels are deallocated

	/*

	 * Register a device for any closed channel where the remote processor

	 * is showing interest in opening the channel.

	/*

	 * Unregister the device for any channel that is opened where the

	 * remote processor is closing the channel.

/*

 * Parses an of_node describing an edge.

/*

 * Release function for an edge.

  * Reset the state of each associated channel and free the edge context.

/**

 * qcom_smd_register_edge() - register an edge based on an device_node

 * @parent:    parent device for the edge

 * @node:      device_node describing the edge

 *

 * Returns an edge reference, or negative ERR_PTR() on failure.

/**

 * qcom_smd_unregister_edge() - release an edge and its children

 * @edge:      edge reference acquired from qcom_smd_register_edge

 Wait for smem */

/*

 * Shut down all smd clients by making sure that each edge stops processing

 * events and scanning for new channels, then call destroy on the devices.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016, Linaro Ltd.

 * Copyright (c) 2012, Michal Simek <monstr@monstr.eu>

 * Copyright (c) 2012, PetaLogix

 * Copyright (c) 2011, Texas Instruments, Inc.

 * Copyright (c) 2011, Google, Inc.

 *

 * Based on rpmsg performance statistics driver by Michal Simek, which in turn

 * was based on TI & Google OMX rpmsg driver.

/**

 * struct rpmsg_ctrldev - control device for instantiating endpoint devices

 * @rpdev:	underlaying rpmsg device

 * @cdev:	cdev for the ctrl device

 * @dev:	device for the ctrl device

/**

 * struct rpmsg_eptdev - endpoint device context

 * @dev:	endpoint device

 * @cdev:	cdev for the endpoint device

 * @rpdev:	underlaying rpmsg device

 * @chinfo:	info used to open the endpoint

 * @ept_lock:	synchronization of @ept modifications

 * @ept:	rpmsg endpoint reference, when open

 * @queue_lock:	synchronization of @queue operations

 * @queue:	incoming message queue

 * @readq:	wait object for incoming queue

 wake up any blocked readers */

 wake up any blocking processes, waiting for new data */

 Close the endpoint, if it's not already destroyed by the parent */

 Discard all SKBs */

 Wait for data in the queue */

 Wait until we get data or the endpoint goes away */

 We lost the endpoint while waiting */

 We can now rely on the release function for cleanup */

 We can now rely on the release function for cleanup */

 Destroy all endpoints */

 SPDX-License-Identifier: GPL-2.0

/*

 * remote processor messaging bus

 *

 * Copyright (C) 2011 Texas Instruments, Inc.

 * Copyright (C) 2011 Google, Inc.

 *

 * Ohad Ben-Cohen <ohad@wizery.com>

 * Brian Swetland <swetland@google.com>

/**

 * rpmsg_create_channel() - create a new rpmsg channel

 * using its name and address info.

 * @rpdev: rpmsg device

 * @chinfo: channel_info to bind

 *

 * Returns a pointer to the new rpmsg device on success, or NULL on error.

/**

 * rpmsg_release_channel() - release a rpmsg channel

 * using its name and address info.

 * @rpdev: rpmsg device

 * @chinfo: channel_info to bind

 *

 * Returns 0 on success or an appropriate error value.

/**

 * rpmsg_create_ept() - create a new rpmsg_endpoint

 * @rpdev: rpmsg channel device

 * @cb: rx callback handler

 * @priv: private data for the driver's use

 * @chinfo: channel_info with the local rpmsg address to bind with @cb

 *

 * Every rpmsg address in the system is bound to an rx callback (so when

 * inbound messages arrive, they are dispatched by the rpmsg bus using the

 * appropriate callback handler) by means of an rpmsg_endpoint struct.

 *

 * This function allows drivers to create such an endpoint, and by that,

 * bind a callback, and possibly some private data too, to an rpmsg address

 * (either one that is known in advance, or one that will be dynamically

 * assigned for them).

 *

 * Simple rpmsg drivers need not call rpmsg_create_ept, because an endpoint

 * is already created for them when they are probed by the rpmsg bus

 * (using the rx callback provided when they registered to the rpmsg bus).

 *

 * So things should just work for simple drivers: they already have an

 * endpoint, their rx callback is bound to their rpmsg address, and when

 * relevant inbound messages arrive (i.e. messages which their dst address

 * equals to the src address of their rpmsg channel), the driver's handler

 * is invoked to process it.

 *

 * That said, more complicated drivers might need to allocate

 * additional rpmsg addresses, and bind them to different rx callbacks.

 * To accomplish that, those drivers need to call this function.

 *

 * Drivers should provide their @rpdev channel (so the new endpoint would belong

 * to the same remote processor their channel belongs to), an rx callback

 * function, an optional private data (which is provided back when the

 * rx callback is invoked), and an address they want to bind with the

 * callback. If @addr is RPMSG_ADDR_ANY, then rpmsg_create_ept will

 * dynamically assign them an available rpmsg address (drivers should have

 * a very good reason why not to always use RPMSG_ADDR_ANY here).

 *

 * Returns a pointer to the endpoint on success, or NULL on error.

/**

 * rpmsg_destroy_ept() - destroy an existing rpmsg endpoint

 * @ept: endpoing to destroy

 *

 * Should be used by drivers to destroy an rpmsg endpoint previously

 * created with rpmsg_create_ept(). As with other types of "free" NULL

 * is a valid parameter.

/**

 * rpmsg_send() - send a message across to the remote processor

 * @ept: the rpmsg endpoint

 * @data: payload of message

 * @len: length of payload

 *

 * This function sends @data of length @len on the @ept endpoint.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to, using @ept's address and its associated rpmsg

 * device destination addresses.

 * In case there are no TX buffers available, the function will block until

 * one becomes available, or a timeout of 15 seconds elapses. When the latter

 * happens, -ERESTARTSYS is returned.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_sendto() - send a message across to the remote processor, specify dst

 * @ept: the rpmsg endpoint

 * @data: payload of message

 * @len: length of payload

 * @dst: destination address

 *

 * This function sends @data of length @len to the remote @dst address.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to, using @ept's address as source.

 * In case there are no TX buffers available, the function will block until

 * one becomes available, or a timeout of 15 seconds elapses. When the latter

 * happens, -ERESTARTSYS is returned.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_send_offchannel() - send a message using explicit src/dst addresses

 * @ept: the rpmsg endpoint

 * @src: source address

 * @dst: destination address

 * @data: payload of message

 * @len: length of payload

 *

 * This function sends @data of length @len to the remote @dst address,

 * and uses @src as the source address.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to.

 * In case there are no TX buffers available, the function will block until

 * one becomes available, or a timeout of 15 seconds elapses. When the latter

 * happens, -ERESTARTSYS is returned.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_trysend() - send a message across to the remote processor

 * @ept: the rpmsg endpoint

 * @data: payload of message

 * @len: length of payload

 *

 * This function sends @data of length @len on the @ept endpoint.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to, using @ept's address as source and its associated

 * rpdev's address as destination.

 * In case there are no TX buffers available, the function will immediately

 * return -ENOMEM without waiting until one becomes available.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_trysendto() - send a message across to the remote processor, specify dst

 * @ept: the rpmsg endpoint

 * @data: payload of message

 * @len: length of payload

 * @dst: destination address

 *

 * This function sends @data of length @len to the remote @dst address.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to, using @ept's address as source.

 * In case there are no TX buffers available, the function will immediately

 * return -ENOMEM without waiting until one becomes available.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_poll() - poll the endpoint's send buffers

 * @ept:	the rpmsg endpoint

 * @filp:	file for poll_wait()

 * @wait:	poll_table for poll_wait()

 *

 * Returns mask representing the current state of the endpoint's send buffers

/**

 * rpmsg_trysend_offchannel() - send a message using explicit src/dst addresses

 * @ept: the rpmsg endpoint

 * @src: source address

 * @dst: destination address

 * @data: payload of message

 * @len: length of payload

 *

 * This function sends @data of length @len to the remote @dst address,

 * and uses @src as the source address.

 * The message will be sent to the remote processor which the @ept

 * endpoint belongs to.

 * In case there are no TX buffers available, the function will immediately

 * return -ENOMEM without waiting until one becomes available.

 *

 * Can only be called from process context (for now).

 *

 * Returns 0 on success and an appropriate error value on failure.

/**

 * rpmsg_get_mtu() - get maximum transmission buffer size for sending message.

 * @ept: the rpmsg endpoint

 *

 * This function returns maximum buffer size available for a single outgoing message.

 *

 * Return: the maximum transmission size on success and an appropriate error

 * value on failure.

/*

 * match a rpmsg channel with a channel info struct.

 * this is used to make sure we're not creating rpmsg devices for channels

 * that already exist.

 found a match ! */

 sysfs show configuration fields */

 for more info, see Documentation/ABI/testing/sysfs-bus-rpmsg */

 rpmsg devices and drivers are matched using the service name */

 match rpmsg channel and rpmsg driver */

/*

 * when an rpmsg driver is probed with a channel, we seamlessly create

 * it an endpoint, binding its rx callback to a unique local rpmsg

 * address.

 *

 * if we need to, we also announce about this channel to the remote

 * processor (needed in case the driver is exposing an rpmsg service).

/*

 * find an existing channel using its name + address properties,

 * and destroy it

/**

 * __register_rpmsg_driver() - register an rpmsg driver with the rpmsg bus

 * @rpdrv: pointer to a struct rpmsg_driver

 * @owner: owning module/driver

 *

 * Returns 0 on success, and an appropriate error value on failure.

/**

 * unregister_rpmsg_driver() - unregister an rpmsg driver from the rpmsg bus

 * @rpdrv: pointer to a struct rpmsg_driver

 *

 * Returns 0 on success, and an appropriate error value on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016, Linaro Ltd

 size of struct read_notif_request */

 Ensure head is always aligned to 8 bytes */

 Ensure ordering of fifo and head update */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-2017, Linaro Ltd

 grt0 */

 ap2r */

 r2ap */

 Header length comes from glink native and is always 4 byte aligned */

	/*

	 * Move the unaligned tail of the message to the padding chunk, to

	 * ensure word aligned accesses

 Pipe specific accessors */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-2017, Linaro Ltd

/**

 * struct glink_defer_cmd - deferred incoming control message

 * @node:	list node

 * @msg:	message header

 * @data:	payload of the message

 *

 * Copy of a received control message, to be added to @rx_queue and processed

 * by @rx_work of @qcom_glink.

/**

 * struct glink_core_rx_intent - RX intent

 * RX intent

 *

 * @data: pointer to the data (may be NULL for zero-copy)

 * @id: remote or local intent ID

 * @size: size of the original intent (do not modify)

 * @reuse: To mark if the intent can be reused after first use

 * @in_use: To mark if intent is already in use for the channel

 * @offset: next write offset (initially 0)

 * @node:	list node

/**

 * struct qcom_glink - driver context, relates to one remote subsystem

 * @dev:	reference to the associated struct device

 * @mbox_client: mailbox client

 * @mbox_chan:  mailbox channel

 * @rx_pipe:	pipe object for receive FIFO

 * @tx_pipe:	pipe object for transmit FIFO

 * @irq:	IRQ for signaling incoming events

 * @rx_work:	worker for handling received control messages

 * @rx_lock:	protects the @rx_queue

 * @rx_queue:	queue of received control messages to be processed in @rx_work

 * @tx_lock:	synchronizes operations on the tx fifo

 * @idr_lock:	synchronizes @lcids and @rcids modifications

 * @lcids:	idr of all channels with a known local channel id

 * @rcids:	idr of all channels with a known remote channel id

 * @features:	remote features

 * @intentless:	flag to indicate that there is no intent

 * @tx_avail_notify: Waitqueue for pending tx tasks

 * @sent_read_notify: flag to check cmd sent or not

/**

 * struct glink_channel - internal representation of a channel

 * @rpdev:	rpdev reference, only used for primary endpoints

 * @ept:	rpmsg endpoint this channel is associated with

 * @glink:	qcom_glink context handle

 * @refcount:	refcount for the channel object

 * @recv_lock:	guard for @ept.cb

 * @name:	unique channel name/identifier

 * @lcid:	channel id, in local space

 * @rcid:	channel id, in remote space

 * @intent_lock: lock for protection of @liids, @riids

 * @liids:	idr of all local intents

 * @riids:	idr of all remote intents

 * @intent_work: worker responsible for transmitting rx_done packets

 * @done_intents: list of intents that needs to be announced rx_done

 * @buf:	receive buffer, for gathering fragments

 * @buf_offset:	write offset in @buf

 * @buf_size:	size of current @buf

 * @open_ack:	completed once remote has acked the open-request

 * @open_req:	completed once open-request has been received

 * @intent_req_lock: Synchronises multiple intent requests

 * @intent_req_result: Result of intent request

 * @intent_req_comp: Completion for intent_req signalling

 Setup glink internal glink_channel data */

 cancel pending rx_done work */

 Free all non-reuse intents pending rx_done work */

 Reject packets that are too big */

 Wait without holding the tx_lock */

/**

 * qcom_glink_send_open_req() - send a RPM_CMD_OPEN request to the remote

 * @glink: Ptr to the glink edge

 * @channel: Ptr to the channel that the open req is sent

 *

 * Allocates a local channel id and sends a RPM_CMD_OPEN message to the remote.

 * Will return with refcount held, regardless of outcome.

 *

 * Returns 0 on success, negative errno otherwise.

 We don't send RX_DONE to intentless systems */

 Take it off the tree of receive intents */

 Schedule the sending of a rx_done indication */

/**

 * qcom_glink_receive_version() - receive version/features from remote system

 *

 * @glink:	pointer to transport interface

 * @version:	remote version

 * @features:	remote features

 *

 * This function is called in response to a remote-initiated version/feature

 * negotiation sequence.

/**

 * qcom_glink_receive_version_ack() - receive negotiation ack from remote system

 *

 * @glink:	pointer to transport interface

 * @version:	remote version response

 * @features:	remote features response

 *

 * This function is called in response to a local-initiated version/feature

 * negotiation sequence and is the counter-offer from the remote side based

 * upon the initial version and feature set requested.

 Version negotiation failed */

/**

 * qcom_glink_send_intent_req_ack() - convert an rx intent request ack cmd to

 * 	wire format and transmit

 * @glink:	The transport to transmit on.

 * @channel:	The glink channel

 * @granted:	The request response to encode.

 *

 * Return: 0 on success or standard Linux error code.

/**

 * qcom_glink_advertise_intent - convert an rx intent cmd to wire format and

 *			   transmit

 * @glink:	The transport to transmit on.

 * @channel:	The local channel

 * @intent:	The intent to pass on to remote.

 *

 * Return: 0 on success or standard Linux error code.

/**

 * qcom_glink_handle_intent_req() - Receive a request for rx_intent

 *					    from remote side

 * @glink:      Pointer to the transport interface

 * @cid:	Remote channel ID

 * @size:	size of the intent

 *

 * The function searches for the local channel to which the request for

 * rx_intent has arrived and allocates and notifies the remote back

 Drop the message */

 Might have an ongoing, fragmented, message to append */

 The packet header lied, drop payload */

 Handle message when no fragments remain to be received */

 To wakeup any blocking writers */

 Locally initiated rpmsg_create_ept */

 qcom_glink_send_open_req() did register the channel in lcids*/

 Release qcom_glink_send_open_req() reference */

 Release qcom_glink_alloc_channel() reference */

 Remote initiated rpmsg_create_ept */

	/*

	 * Send a close request to "undo" our open-ack. The close-ack will

	 * release qcom_glink_send_open_req() reference and the last reference

	 * will be relesed after receiving remote_close or transport unregister

	 * by calling qcom_glink_native_remove().

 Channel is now open, advertise base set of intents */

 Decouple the potential rpdev from the channel */

 We found an available intent */

 Mark intent available if we failed */

 Mark intent available if we failed */

/*

 * Finds the device_node for the glink child interested in this channel.

 The opening dance was initiated by the remote */

 Release the reference, iff we took it */

 cancel pending rx_done work */

 To wakeup any blocking writers */

 Decouple the potential rpdev from the channel */

 cancel any pending deferred rx_work */

 Release qcom_glink_alloc_channel() reference */

 Release any defunct local channels, waiting for close-ack */

 Release any defunct local channels, waiting for close-req */

 SPDX-License-Identifier: GPL-2.0

/**

 * of_icc_bulk_get() - get interconnect paths

 * @dev: the device requesting the path

 * @num_paths: the number of icc_bulk_data

 * @paths: the table with the paths we want to get

 *

 * Returns 0 on success or negative errno otherwise.

/**

 * icc_bulk_put() - put a list of interconnect paths

 * @num_paths: the number of icc_bulk_data

 * @paths: the icc_bulk_data table with the paths being put

/**

 * icc_bulk_set_bw() - set bandwidth to a set of paths

 * @num_paths: the number of icc_bulk_data

 * @paths: the icc_bulk_data table containing the paths and bandwidth

 *

 * Returns 0 on success or negative errno otherwise.

/**

 * icc_bulk_enable() - enable a previously disabled set of paths

 * @num_paths: the number of icc_bulk_data

 * @paths: the icc_bulk_data table containing the paths and bandwidth

 *

 * Returns 0 on success or negative errno otherwise.

/**

 * icc_bulk_disable() - disable a set of interconnect paths

 * @num_paths: the number of icc_bulk_data

 * @paths: the icc_bulk_data table containing the paths and bandwidth

 SPDX-License-Identifier: GPL-2.0

/*

 * Interconnect framework core driver

 *

 * Copyright (c) 2017-2019, Linaro Ltd.

 * Author: Georgi Djakov <georgi.djakov@linaro.org>

 draw providers as cluster subgraphs */

 draw nodes */

 draw internal links */

 draw external links */

 reference to previous node was saved during path traversal */

 count the hops including the source */

 reset the traversed state */

/*

 * We want the path to honor all bandwidth requests, so the average and peak

 * bandwidth requirements from each consumer are aggregated at each node.

 * The aggregation is platform specific, so each platform can customize it by

 * implementing its own aggregate() function.

 during boot use the initial bandwidth as a floor value */

 both endpoints should be valid master-slave pairs */

 set the constraints */

/* of_icc_xlate_onecell() - Translate function using a single index.

 * @spec: OF phandle args to map into an interconnect node.

 * @data: private data (pointer to struct icc_onecell_data)

 *

 * This is a generic translate function that can be used to model simple

 * interconnect providers that have one device tree node and provide

 * multiple interconnect nodes. A single cell is used as an index into

 * an array of icc nodes specified in the icc_onecell_data struct when

 * registering the provider.

/**

 * of_icc_get_from_provider() - Look-up interconnect node

 * @spec: OF phandle args to use for look-up

 *

 * Looks for interconnect provider under the node specified by @spec and if

 * found, uses xlate function of the provider to map phandle args to node.

 *

 * Returns a valid pointer to struct icc_node_data on success or ERR_PTR()

 * on failure.

/**

 * of_icc_get_by_index() - get a path handle from a DT node based on index

 * @dev: device pointer for the consumer device

 * @idx: interconnect path index

 *

 * This function will search for a path between two endpoints and return an

 * icc_path handle on success. Use icc_put() to release constraints when they

 * are not needed anymore.

 * If the interconnect API is disabled, NULL is returned and the consumer

 * drivers will still build. Drivers are free to handle this specifically,

 * but they don't have to.

 *

 * Return: icc_path pointer on success or ERR_PTR() on error. NULL is returned

 * when the API is disabled or the "interconnects" DT property is missing.

	/*

	 * When the consumer DT node do not have "interconnects" property

	 * return a NULL path to skip setting constraints.

	/*

	 * We use a combination of phandle and specifier for endpoint. For now

	 * lets support only global ids and extend this in the future if needed

	 * without breaking DT compatibility.

/**

 * of_icc_get() - get a path handle from a DT node based on name

 * @dev: device pointer for the consumer device

 * @name: interconnect path name

 *

 * This function will search for a path between two endpoints and return an

 * icc_path handle on success. Use icc_put() to release constraints when they

 * are not needed anymore.

 * If the interconnect API is disabled, NULL is returned and the consumer

 * drivers will still build. Drivers are free to handle this specifically,

 * but they don't have to.

 *

 * Return: icc_path pointer on success or ERR_PTR() on error. NULL is returned

 * when the API is disabled or the "interconnects" DT property is missing.

	/*

	 * When the consumer DT node do not have "interconnects" property

	 * return a NULL path to skip setting constraints.

	/*

	 * We use a combination of phandle and specifier for endpoint. For now

	 * lets support only global ids and extend this in the future if needed

	 * without breaking DT compatibility.

/**

 * icc_set_tag() - set an optional tag on a path

 * @path: the path we want to tag

 * @tag: the tag value

 *

 * This function allows consumers to append a tag to the requests associated

 * with a path, so that a different aggregation could be done based on this tag.

/**

 * icc_get_name() - Get name of the icc path

 * @path: reference to the path returned by icc_get()

 *

 * This function is used by an interconnect consumer to get the name of the icc

 * path.

 *

 * Returns a valid pointer on success, or NULL otherwise.

/**

 * icc_set_bw() - set bandwidth constraints on an interconnect path

 * @path: reference to the path returned by icc_get()

 * @avg_bw: average bandwidth in kilobytes per second

 * @peak_bw: peak bandwidth in kilobytes per second

 *

 * This function is used by an interconnect consumer to express its own needs

 * in terms of bandwidth for a previously requested path between two endpoints.

 * The requests are aggregated and each node is updated accordingly. The entire

 * path is locked by a mutex to ensure that the set() is completed.

 * The @path can be NULL when the "interconnects" DT properties is missing,

 * which will mean that no constraints will be set.

 *

 * Returns 0 on success, or an appropriate error code otherwise.

 update the consumer request for this path */

 aggregate requests for this node */

/**

 * icc_get() - return a handle for path between two endpoints

 * @dev: the device requesting the path

 * @src_id: source device port id

 * @dst_id: destination device port id

 *

 * This function will search for a path between two endpoints and return an

 * icc_path handle on success. Use icc_put() to release

 * constraints when they are not needed anymore.

 * If the interconnect API is disabled, NULL is returned and the consumer

 * drivers will still build. Drivers are free to handle this specifically,

 * but they don't have to.

 *

 * Return: icc_path pointer on success, ERR_PTR() on error or NULL if the

 * interconnect API is disabled.

/**

 * icc_put() - release the reference to the icc_path

 * @path: interconnect path

 *

 * Use this function to release the constraints on a path when the path is

 * no longer needed. The constraints will be re-aggregated.

 check if node already exists */

/**

 * icc_node_create() - create a node

 * @id: node id

 *

 * Return: icc_node pointer on success, or ERR_PTR() on error

/**

 * icc_node_destroy() - destroy a node

 * @id: node id

/**

 * icc_link_create() - create a link between two nodes

 * @node: source node id

 * @dst_id: destination node id

 *

 * Create a link between two nodes. The nodes might belong to different

 * interconnect providers and the @dst_id node might not exist (if the

 * provider driver has not probed yet). So just create the @dst_id node

 * and when the actual provider driver is probed, the rest of the node

 * data is filled.

 *

 * Return: 0 on success, or an error code otherwise

/**

 * icc_link_destroy() - destroy a link between two nodes

 * @src: pointer to source node

 * @dst: pointer to destination node

 *

 * Return: 0 on success, or an error code otherwise

/**

 * icc_node_add() - add interconnect node to interconnect provider

 * @node: pointer to the interconnect node

 * @provider: pointer to the interconnect provider

 get the initial bandwidth values and sync them with hardware */

/**

 * icc_node_del() - delete interconnect node from interconnect provider

 * @node: pointer to the interconnect node

/**

 * icc_nodes_remove() - remove all previously added nodes from provider

 * @provider: the interconnect provider we are removing nodes from

 *

 * Return: 0 on success, or an error code otherwise

/**

 * icc_provider_add() - add a new interconnect provider

 * @provider: the interconnect provider that will be added into topology

 *

 * Return: 0 on success, or an error code otherwise

/**

 * icc_provider_del() - delete previously added interconnect provider

 * @provider: the interconnect provider that will be removed from topology

 *

 * Return: 0 on success, or an error code otherwise

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Brian Masney <masneyb@onstation.org>

 *

 * Based on MSM bus code from downstream MSM kernel sources.

 * Copyright (c) 2012-2013 The Linux Foundation. All rights reserved.

 *

 * Based on qcs404.c

 * Copyright (C) 2019 Linaro Ltd

 *

 * Here's a rough representation that shows the various buses that form the

 * Network On Chip (NOC) for the msm8974:

 *

 *                         Multimedia Subsystem (MMSS)

 *         |----------+-----------------------------------+-----------|

 *                    |                                   |

 *                    |                                   |

 *        Config      |                     Bus Interface | Memory Controller

 *       |------------+-+-----------|        |------------+-+-----------|

 *                      |                                   |

 *                      |                                   |

 *                      |             System                |

 *     |--------------+-+---------------------------------+-+-------------|

 *                    |                                   |

 *                    |                                   |

 *        Peripheral  |                           On Chip | Memory (OCMEM)

 *       |------------+-------------|        |------------+-------------|

/**

 * struct msm8974_icc_provider - Qualcomm specific interconnect provider

 * @provider: generic interconnect provider

 * @bus_clks: the clk_bulk_data table of bus clocks

 * @num_clks: the total number of clk_bulk_data entries

/**

 * struct msm8974_icc_node - Qualcomm specific interconnect nodes

 * @name: the node name used in debugfs

 * @id: a unique node identifier

 * @links: an array of nodes where we can go next while traversing

 * @num_links: the total number of @links

 * @buswidth: width of the interconnect between a node and the bus (bytes)

 * @mas_rpm_id:	RPM ID for devices that are bus masters

 * @slv_rpm_id:	RPM ID for devices that are bus slaves

 * @rate: current bus clock rate in Hz

 Virtual NoC is needed for connection to OCMEM */

	/*

	 * Setting the bandwidth requests for some nodes fails and this same

	 * behavior occurs on the downstream MSM 3.4 kernel sources based on

	 * errors like this in that kernel:

	 *

	 *   msm_rpm_get_error_from_ack(): RPM NACK Unsupported resource

	 *   AXI: msm_bus_rpm_req(): RPM: Ack failed

	 *   AXI: msm_bus_rpm_commit_arb(): RPM: Req fail: mas:32, bw:240000000

	 *

	 * Since there's no publicly available documentation for this hardware,

	 * and the bandwidth for some nodes in the path can be set properly,

	 * let's not return an error.

 Set bandwidth on source node */

 Set bandwidth on destination node */

 wait for the RPM proxy */

 populate links */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 OSM Register offsets */

 EPSS Register offsets */

/**

 * struct qcom_osm_l3_node - Qualcomm specific interconnect nodes

 * @name: the node name used in debugfs

 * @links: an array of nodes where we can go next while traversing

 * @id: a unique node identifier

 * @num_links: the total number of @links

 * @buswidth: width of the interconnect between a node and the bus

 HW should be in enabled state to proceed */

 Two of the same frequencies signify end of table */

 Cast away const and add it back in qcom_osm_l3_set() */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2019-2020, The Linux Foundation. All rights reserved.

 * Copyright (c) 2021, Linaro Limited

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2021, The Linux Foundation. All rights reserved.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Qualcomm SDX55 interconnect driver

 * Author: Manivannan Sadhasivam <manivannan.sadhasivam@linaro.org>

 *

 * Copyright (c) 2021, Linaro Ltd.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Linaro Ltd

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018-2020, The Linux Foundation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Linaro Ltd

 BIMC QoS */

 NoC QoS */

 LIMITCMDS is not present on M_BKE_HEALTH_3 */

	/* QoS Priority: The QoS Health parameters are getting considered

	 * only if we are NOT in Bypass Mode.

 Set BKE_EN to 1 when Fixed, Regulator or Limiter Mode */

 Must be updated one at a time, P1 first, P0 last */

 send bandwidth request message to the RPM processor */

 set bandwidth directly from the AP */

 wait for the RPM proxy */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018-2020 Linaro Ltd

 * Author: Georgi Djakov <georgi.djakov@linaro.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020-2021, The Linux Foundation. All rights reserved.

/**

 * struct bcm_voter - Bus Clock Manager voter

 * @dev: reference to the device that communicates with the BCM

 * @np: reference to the device node to match bcm voters

 * @lock: mutex to protect commit and wake/sleep lists in the voter

 * @commit_list: list containing bcms to be committed to hardware

 * @ws_list: list containing bcms that have different wake/sleep votes

 * @voter_node: list of bcm voters

 * @tcs_wait: mask for which buckets require TCS completion

 Ensure that small votes aren't lost. */

	/*

	 * Set the wait for completion flag on command that need to be completed

	 * before the next command.

		/*

		 * Batch the BCMs in such a way that we do not split them in

		 * multiple payloads when they are under the same VCD. This is

		 * to ensure that every BCM is committed since we only set the

		 * commit bit on the last BCM request of every VCD.

/**

 * of_bcm_voter_get - gets a bcm voter handle from DT node

 * @dev: device pointer for the consumer device

 * @name: name for the bcm voter device

 *

 * This function will match a device_node pointer for the phandle

 * specified in the device DT and return a bcm_voter handle on success.

 *

 * Returns bcm_voter pointer or ERR_PTR() on error. EPROBE_DEFER is returned

 * when matching bcm voter is yet to be found.

/**

 * qcom_icc_bcm_voter_add - queues up the bcm nodes that require updates

 * @voter: voter that the bcms are being added to

 * @bcm: bcm to add to the commit and wake sleep list

/**

 * qcom_icc_bcm_voter_commit - generates and commits tcs cmds based on bcms

 * @voter: voter that needs flushing

 *

 * This function generates a set of AMC commands and flushes to the BCM device

 * associated with the voter. It conditionally generate WAKE and SLEEP commands

 * based on deltas between WAKE/SLEEP requirements. The ws_list persists

 * through multiple commit requests and bcm nodes are removed only when the

 * requirements for WAKE matches SLEEP.

 *

 * Returns 0 on success, or an appropriate error code otherwise.

	/*

	 * Pre sort the BCMs based on VCD for ease of generating a command list

	 * that groups the BCMs with the same VCD together. VCDs are numbered

	 * with lowest being the most expensive time wise, ensuring that

	 * those commands are being sent the earliest in the queue. This needs

	 * to be sorted every commit since we can't guarantee the order in which

	 * the BCMs are added to the list.

	/*

	 * Construct the command list based on a pre ordered list of BCMs

	 * based on VCD.

		/*

		 * Only generate WAKE and SLEEP commands if a resource's

		 * requirements change as the execution environment transitions

		 * between different power states.

 SPDX-License-Identifier: GPL-2.0

/*

 * RPM over SMD communication wrapper for interconnects

 *

 * Copyright (C) 2019 Linaro Ltd

 * Author: Georgi Djakov <georgi.djakov@linaro.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 * Copyright (c) 2021, Linaro Ltd.

 SPDX-License-Identifier: GPL-2.0

/*

 * Qualcomm SDM630/SDM636/SDM660 Network-on-Chip (NoC) QoS driver

 * Copyright (C) 2020, AngeloGioacchino Del Regno <kholk11@gmail.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Linaro Ltd

 * Author: Jun Nie <jun.nie@linaro.org>

 * With reference of msm8916 interconnect driver of Georgi Djakov.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

/**

 * qcom_icc_pre_aggregate - cleans up stale values from prior icc_set

 * @node: icc node to operate on

/**

 * qcom_icc_aggregate - aggregate bw for buckets indicated by tag

 * @node: node to aggregate

 * @tag: tag to indicate which buckets to aggregate

 * @avg_bw: new bw to sum aggregate

 * @peak_bw: new bw to max aggregate

 * @agg_avg: existing aggregate avg bw val

 * @agg_peak: existing aggregate peak bw val

/**

 * qcom_icc_set - set the constraints based on path

 * @src: source node for the path to set constraints on

 * @dst: destination node for the path to set constraints on

 *

 * Return: 0 on success, or an error code otherwise

/**

 * qcom_icc_bcm_init - populates bcm aux data and connect qnodes

 * @bcm: bcm to be initialized

 * @dev: associated provider device

 *

 * Return: 0 on success, or an error code otherwise

 BCM is already initialised*/

 Link Qnodes to their respective BCMs */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Exynos generic interconnect provider driver

 *

 * Copyright (c) 2020 Samsung Electronics Co., Ltd.

 *

 * Authors: Artur Świgoń <a.swigon@samsung.com>

 *          Sylwester Nawrocki <s.nawrocki@samsung.com>

 One interconnect node per provider */

 parent nodes are optional */

 Get the interconnect target node */

	/*

	 * Register a PM QoS request for the parent (devfreq) device.

 SPDX-License-Identifier: GPL-2.0

/*

 * Interconnect framework driver for i.MX SoC

 *

 * Copyright (c) 2019, BayLibre

 * Copyright (c) 2019-2020, NXP

 * Author: Alexandre Bailon <abailon@baylibre.com>

 * Author: Leonard Crestez <leonard.crestez@nxp.com>

 private icc_node data */

 imx_icc_node_destroy() - Destroy an imx icc_node, including private data */

 Allow scaling to be disabled on a per-node basis */

 icc_onecell_data is indexed by node_id, unlike nodes param */

 SPDX-License-Identifier: GPL-2.0

/*

 * Interconnect framework driver for i.MX8MM SoC

 *

 * Copyright (c) 2019, BayLibre

 * Copyright (c) 2019-2020, NXP

 * Author: Alexandre Bailon <abailon@baylibre.com>

 * Author: Leonard Crestez <leonard.crestez@nxp.com>

/*

 * Describe bus masters, slaves and connections between them

 *

 * This is a simplified subset of the bus diagram, there are several other

 * PL301 nics which are skipped/merged into PL301_MAIN

 VPUMIX */

 GPUMIX */

 DISPLAYMIX */

 HSIO */

 Audio */

 Ethernet */

 Other */

 SPDX-License-Identifier: GPL-2.0

/*

 * Interconnect framework driver for i.MX8MQ SoC

 *

 * Copyright (c) 2019-2020, NXP

/*

 * Describe bus masters, slaves and connections between them

 *

 * This is a simplified subset of the bus diagram, there are several other

 * PL301 nics which are skipped/merged into PL301_MAIN

 VPUMIX */

 GPUMIX */

 DISPMIX (only for DCSS) */

 USBMIX */

 PL301_DISPLAY (IPs other than DCSS, inside SUPERMIX) */

 AUDIO */

 ENET */

 OTHER */

 SPDX-License-Identifier: GPL-2.0

/*

 * Interconnect framework driver for i.MX8MN SoC

 *

 * Copyright (c) 2019-2020, NXP

/*

 * Describe bus masters, slaves and connections between them

 *

 * This is a simplified subset of the bus diagram, there are several other

 * PL301 nics which are skipped/merged into PL301_MAIN

 GPUMIX */

 DISPLAYMIX */

 USB goes straight to NOC */

 Audio */

 Ethernet */

 Other */

/*

 * Timer device implementation for SGI UV platform.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 2009 Silicon Graphics, Inc.  All rights reserved.

 *

 name of the device, usually in /dev */

/*

 * Period in femtoseconds (10^-15 s)

/**

 * uv_mmtimer_ioctl - ioctl interface for /dev/uv_mmtimer

 * @file: file structure for the device

 * @cmd: command to execute

 * @arg: optional argument to command

 *

 * Executes the command specified by @cmd.  Returns 0 for success, < 0 for

 * failure.

 *

 * Valid commands:

 *

 * %MMTIMER_GETOFFSET - Should return the offset (relative to the start

 * of the page where the registers are mapped) for the counter in question.

 *

 * %MMTIMER_GETRES - Returns the resolution of the clock in femto (10^-15)

 * seconds

 *

 * %MMTIMER_GETFREQ - Copies the frequency of the clock in Hz to the address

 * specified by @arg

 *

 * %MMTIMER_GETBITS - Returns the number of bits in the clock's counter

 *

 * %MMTIMER_MMAPAVAIL - Returns 1 if registers can be mmap'd into userspace

 *

 * %MMTIMER_GETCOUNTER - Gets the current value in the counter and places it

 * in the address specified by @arg.

 offset of the counter */

		/*

		 * Starting with HUB rev 2.0, the UV RTC register is

		 * replicated across all cachelines of it's own page.

		 * This allows faster simultaneous reads from a given socket.

		 *

		 * The offset returned is in 64 bit units.

 resolution of the clock in 10^-15 s */

 frequency in Hz */

 number of bits in the clock */

/**

 * uv_mmtimer_mmap - maps the clock's registers into userspace

 * @file: file structure for the device

 * @vma: VMA to map the registers into

 *

 * Calls remap_pfn_range() to map the clock's registers into

 * the calling process' address space.

/**

 * uv_mmtimer_init - device initialization routine

 *

 * Does initial setup for the uv_mmtimer device.

	/*

	 * Sanity check the cycles/sec variable

 SPDX-License-Identifier: GPL-2.0-only

/*

 * CMOS/NV-RAM driver for Linux

 *

 * Copyright (C) 1997 Roman Hodek <Roman.Hodek@informatik.uni-erlangen.de>

 * idea by and with help from Richard Jelinek <rj@suse.de>

 * Portions copyright (c) 2001,2002 Sun Microsystems (thockin@sun.com)

 *

 * This driver allows you to access the contents of the non-volatile memory in

 * the mc146818rtc.h real-time clock. This chip is built into all PCs and into

 * many Atari machines. In the former it's called "CMOS-RAM", in the latter

 * "NVRAM" (NV stands for non-volatile).

 *

 * The data are supplied as a (seekable) character device, /dev/nvram. The

 * size of this file is dependent on the controller.  The usual size is 114,

 * the number of freely available bytes in the memory (i.e., not used by the

 * RTC itself).

 *

 * Checksums over the NVRAM contents are managed by this driver. In case of a

 * bad checksum, reads and writes return -EIO. The checksum can be initialized

 * to a sane state either by ioctl(NVRAM_INIT) (clear whole NVRAM) or

 * ioctl(NVRAM_SETCKS) (doesn't change contents, just makes checksum valid

 * again; use with care!)

 *

 * 	1.1	Cesar Barros: SMP locking fixes

 * 		added changelog

 * 	1.2	Erik Gilling: Cobalt Networks support

 * 		Tim Hockin: general cleanup, Cobalt support

 * 	1.3	Wim Van Sebroeck: convert PRINT_PROC to seq_file

 #times opened */

 special open modes */

 opened for writing (exclusive) */

 opened with O_EXCL */

/*

 * These functions are provided to be called internally or by other parts of

 * the kernel. It's up to the caller to ensure correct checksum before reading

 * or after writing (needs to be done only once).

 *

 * It is worth noting that these functions all access bytes of general

 * purpose memory in the NVRAM - that is to say, they all add the

 * NVRAM_FIRST_BYTE offset.  Pass them offsets into NVRAM as if you did not

 * know about the RTC cruft.

/* Note that *all* calls to CMOS_READ and CMOS_WRITE must be done with

 * rtc_lock held. Due to the index-port/data-port design of the RTC, we

 * don't want two different things trying to get to it at once. (e.g. the

 * periodic 11 min sync from kernel/time/ntp.c vs. this driver.)

 This races nicely with trying to read with checksum checking (nvram_read) */

 On PCs, the checksum is built only over bytes 2..31 */

 CONFIG_X86 */

/*

 * The are the file operation function for user access to /dev/nvram

 initialize NVRAM contents and checksum */

		/* just set checksum, contents unchanged (maybe useful after

 CONFIG_X86 || CONFIG_M68K */

 Prevent multiple readers/writers if desired. */

 Prevent multiple writers if the set_checksum ioctl is implemented. */

 if only one instance is open, clear the EXCL bit */

 CONFIG_X86 && CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0-or-later

/* toshiba.c -- Linux driver for accessing the SMM on Toshiba laptops

 *

 * Copyright (c) 1996-2001  Jonathan A. Buzzard (jonathan@buzzard.org.uk)

 *

 * Valuable assistance and patches from:

 *     Tom May <tom@you-bastards.com>

 *     Rob Napier <rnapier@employees.org>

 *

 * Fn status port numbers for machine ID's courtesy of

 *     0xfc02: Scott Eisert <scott.e@sky-eye.com>

 *     0xfc04: Steve VanDevender <stevev@efn.org>

 *     0xfc08: Garth Berry <garth@itsbruce.net>

 *     0xfc0a: Egbert Eich <eich@xfree86.org>

 *     0xfc10: Andrew Lofthouse <Andrew.Lofthouse@robins.af.mil>

 *     0xfc11: Spencer Olson <solson@novell.com>

 *     0xfc13: Claudius Frankewitz <kryp@gmx.de>

 *     0xfc15: Tom May <tom@you-bastards.com>

 *     0xfc17: Dave Konrad <konrad@xenia.it>

 *     0xfc1a: George Betzos <betzos@engr.colostate.edu>

 *     0xfc1b: Munemasa Wada <munemasa@jnovel.co.jp>

 *     0xfc1d: Arthur Liu <armie@slap.mine.nu>

 *     0xfc5a: Jacques L'helgoualc'h <lhh@free.fr>

 *     0xfcd1: Mr. Dave Konrad <konrad@xenia.it>

 *

 * WARNING WARNING WARNING WARNING WARNING WARNING WARNING WARNING WARNING

 *

 *   This code is covered by the GNU GPL and you are free to make any

 *   changes you wish to it under the terms of the license. However the

 *   code has the potential to render your computer and/or someone else's

 *   unusable. Please proceed with care when modifying the code.

 *

 * Note: Unfortunately the laptop hardware can close the System Configuration

 *       Interface on it's own accord. It is therefore necessary for *all*

 *       programs using this driver to be aware that *any* SCI call can fail at

 *       *any* time. It is up to any program to be aware of this eventuality

 *       and take appropriate steps.

 *

 * The information used to write this driver has been obtained by reverse

 * engineering the software supplied by Toshiba for their portable computers in

 * strict accordance with the European Council Directive 92/250/EEC on the legal

 * protection of computer programs, and it's implementation into English Law by

 * the Copyright (Computer Programs) Regulations 1992 (S.I. 1992 No.3233).

/*

 * Read the Fn key status

/*

 * For the Portage 610CT and the Tecra 700CS/700CDT emulate the HCI fan function

 Portage 610CT */

 fan status */

 fan off */

 fan on */

 Tecra 700CS/CDT */

 fan status */

 fan off */

 fan on */

/*

 * Put the laptop into System Management Mode

 block HCI calls to read/write memory & PCI devices */

 do we need to emulate the fan ? */

/*

 * Print the information for /proc/toshiba

	/* Arguments

	     0) Linux driver version (this will change if format changes)

	     1) Machine ID

	     2) SCI version

	     3) BIOS version (major, minor)

	     4) BIOS date (in SCI date format)

	     5) Fn Key status

/*

 * Determine which port to use for the Fn key status

/*

 * Get the machine identification number of the current model

 do we have a SCTTable machine identication number on our hands */

 start by getting a pointer into the BIOS */

		/* At this point in the Toshiba routines under MS Windows

		   the bx register holds 0xe6f5. However my code is producing

		   a different value! For the time being I will just fudge the

		   value. This has been verified on a Satellite Pro 430CDT,

 now twiddle with our pointer a bit */

 now construct our machine identification number */

/*

 * Probe for the presence of a Toshiba laptop

 *

 *   returns and non-zero if unable to detect the presence of a Toshiba

 *   laptop, otherwise zero and determines the Machine ID, BIOS version and

 *   date, and SCI version.

	/* extra sanity check for the string "TOSHIBA" in the BIOS because

 call the Toshiba SCI support check routine */

 if this is not a Toshiba laptop carry flag is set and ah=0x86 */

 if we get this far then we are running on a Toshiba (probably)! */

 next get the machine ID of the current laptop */

 get the BIOS version */

 get the BIOS date */

	/* in theory we should check the ports we are going to use for the

	   fn key detection (and the fan on the Portage 610/Tecra700), and

	   then request them to stop other drivers using them. However as

	   the keyboard driver grabs 0x60-0x6f and the pic driver grabs

	   0xa0-0xbf we can't. We just have to live dangerously and use the

 do we need to emulate the fan? */

 are we running on a Toshiba laptop */

 set the port to use for Fn status if not specified as a parameter */

 register the device file */

 SPDX-License-Identifier: GPL-2.0-only

/* linux/drivers/char/nsc_gpio.c



   National Semiconductor common GPIO device-file/VFS methods.

   Allows a user space process to control the GPIO pins.



   Copyright (c) 2001,2002 Christer Weinigel <wingel@nano-system.com>

   Copyright (c) 2005      Jim Cromie <jim.cromie@gmail.com>

 retrieve current config w/o changing it */

 user requested via 'v' command, so its INFO */

 output-enabled/tristate */

 push pull / open drain */

 pull up enabled/disabled */

 locked / unlocked */

 level/edge input */

 trigger on rise/fall edge */

 debounce */

 View Current pin settings */

 end of settings string, do nothing */

 full string handled, report error */

 common file-ops routines for both scx200_gpio and pc87360_gpio */

 SPDX-License-Identifier: GPL-2.0-only

/* linux/drivers/char/pc8736x_gpio.c



   National Semiconductor PC8736x GPIO driver.  Allows a user space

   process to play with the GPIO pins.



   Copyright (c) 2005,2006 Jim Cromie <jim.cromie@gmail.com>



   adapted from linux/drivers/char/scx200_gpio.c

   Copyright (c) 2001,2002 Christer Weinigel <wingel@nano-system.com>,

 default to dynamic major */

 1st command-reg to check */

 alt command-reg to check */

 SuperI/O ID Register */

 Expected value in ID Register for PC87365 */

 Expected value in ID Register for PC87366 */

 chip config, bit0 is chip enable */

 ioaddr range */

 minors matching 4 8 bit ports */

 unit select reg */

 unit enable */

 unit number of GPIO */

 config-space addrs to read/write each unit's runtime addr */

 GPIO config-space pin-control addresses */

 bogus start val */

 GPIO port runtime access, functionality */

 non-uniform offsets ! */

 static int event_capable[] = { 1, 1, 0, 0 };   ports 2,3 are hobbled */

 use in dev_*() */

 try the 2 possible values, read a hardware reg to verify */

 select GPIO port/pin from device minor number */

 read current config value */

 set new config */

 read the current values driven on the GPIO signals */

	/* Verify that chip and it's GPIO unit are both enabled.

	   My BIOS does this, so I take minimum action here

 read the GPIO unit base addr that chip responds to */

 ignore minor errs, and succeed */

 SPDX-License-Identifier: GPL-2.0-only

 Derived from Applicom driver ac.c for SCO Unix                            */

 Ported by David Woodhouse, Axiom (Cambridge) Ltd.                         */

 dwmw2@infradead.org 30/8/98                                               */

 $Id: ac.c,v 1.30 2000/03/22 16:03:57 dwmw2 Exp $			     */

 This module is for Linux 2.1 and 2.2 series kernels.                      */

****************************************************************************/

 J PAGET 18/02/94 passage V2.4.2 ioctl avec code 2 reset to les interrupt  */

 ceci pour reseter correctement apres une sortie sauvage                   */

 J PAGET 02/05/94 passage V2.4.3 dans le traitement de d'interruption,     */

 LoopCount n'etait pas initialise a 0.                                     */

 F LAFORSE 04/07/95 version V2.6.0 lecture bidon apres acces a une carte   */

           pour liberer le bus                                             */

 J.PAGET 19/11/95 version V2.6.1 Nombre, addresse,irq n'est plus configure */

 et passe en argument a acinit, mais est scrute sur le bus pour s'adapter  */

 au nombre de cartes presentes sur le bus. IOCL code 6 affichait V2.4.3    */

 F.LAFORSE 28/11/95 creation de fichiers acXX.o avec les differentes       */

 addresses de base des cartes, IOCTL 6 plus complet                         */

 J.PAGET le 19/08/96 copie de la version V2.6 en V2.8.0 sans modification  */

 de code autre que le texte V2.6.1 en V2.8.0                               */

****************************************************************************/

/* NOTE: We use for loops with {write,read}b() instead of 

   memcpy_{from,to}io throughout this driver. This is because

   the board doesn't correctly handle word accesses - only

   bytes. 

 maximum of pc board possible */

 interrupt number IRQ       */

 physical segment of board  */

 number of installed boards */

 number of write error      */

 number of read error       */

 number of device error     */

 dev_id for request_irq() */

 No mem and irq given - check for a PCI card */

 Enable interrupts. */

	/* Finished with PCI cards. If none registered, 

 Now try the specified ISA cards */

 Board number 1 -> 8           */

 Index board number 0 -> 7     */

 Board TIC to send             */

 Current priority              */

 board number to send          */

 tic number to send            */

 Test octet ready correct */

 Place ourselves on the wait queue */

 Check whether the card is ready for us */

 It's busy. Sleep. */

 We may not have actually slept */

	/* Which is best - lock down the pages with rawio and then

	   copy directly, or use bounce buffers? For now we do the latter 

 No need to ratelimit this. Only root can trigger it anyway */

 Stick ourself on the wait queue */

 Scan each board, looking for one which has a packet for us */

 Got a packet for us */

 Got an error */

 Nothing for us. Try the next board */

 per board */

 OK - No boards had data for us. Sleep now */

    printk("Applicom interrupt on IRQ %d occurred\n", vec);

 Skip if this board doesn't exist */

 Skip if this board doesn't want attention */

 mailbox sent by the card ?   */

 ram i/o free for write by pc ? */

 process sleep during read ?    */

 There's another int waiting on this card */

 @ ADG ou ATO selon le cas */

	/* In general, the device is only openable by root anyway, so we're not

 SPDX-License-Identifier: GPL-2.0-only

/*                                              -*- linux-c -*-

 * dtlk.c - DoubleTalk PC driver for Linux

 *

 * Original author: Chris Pallotta <chris@allmedia.com>

 * Current maintainer: Jim Van Zandt <jrv@vanzandt.mv.com>

 * 

 * 2000-03-18 Jim Van Zandt: Fix polling.

 *  Eliminate dtlk_timer_active flag and separate dtlk_stop_timer

 *  function.  Don't restart timer in dtlk_timer_tick.  Restart timer

 *  in dtlk_poll after every poll.  dtlk_poll returns mask (duh).

 *  Eliminate unused function dtlk_write_byte.  Misc. code cleanups.

/* This driver is for the DoubleTalk PC, a speech synthesizer

   manufactured by RC Systems (http://www.rcsys.com/).  It was written

   based on documentation in their User's Manual file and Developer's

   Tools disk.



   The DoubleTalk PC contains four voice synthesizers: text-to-speech

   (TTS), linear predictive coding (LPC), PCM/ADPCM, and CVSD.  It

   also has a tone generator.  Output data for LPC are written to the

   LPC port, and output data for the other modes are written to the

   TTS port.



   Two kinds of data can be read from the DoubleTalk: status

   information (in response to the "\001?" interrogation command) is

   read from the TTS port, and index markers (which mark the progress

   of the speech) are read from the LPC port.  Not all models of the

   DoubleTalk PC implement index markers.  Both the TTS and LPC ports

   can also display status flags.



   The DoubleTalk PC generates no interrupts.



   These characteristics are mapped into the Unix stream I/O model as

   follows:



   "write" sends bytes to the TTS port.  It is the responsibility of

   the user program to switch modes among TTS, PCM/ADPCM, and CVSD.

   This driver was written for use with the text-to-speech

   synthesizer.  If LPC output is needed some day, other minor device

   numbers can be used to select among output modes.



   "read" gets index markers from the LPC port.  If the device does

   not implement index markers, the read will fail with error EINVAL.



   Status information is available using the DTLK_INTERROGATE ioctl.



 for -EBUSY */

 for request_region */

 for loops_per_jiffy */

 for inb_p, outb_p, inb, outb, etc. */

 for get_user, etc. */

 for wait_queue */

 for __init, module_{init,exit} */

 for EPOLLIN, etc. */

 local header file for DoubleTalk values */

 !TRACING */

 TRACING */

 prototypes for file_operations struct */

 local prototypes */

/*

   static void dtlk_handle_error(char, char, unsigned int);

  printk("DoubleTalk PC - dtlk_read()\n"); */

        printk("dtlk_read() reads 0x%02x\n", ch); */

				/* We yield our time until scheduled

				   again.  This reduces the transfer

				   rate to 500 bytes/sec, but that's

				   still enough to keep up with the

				/* the RDY bit goes zero 2-3 usec

				   after writing, and goes 1 again

				   180-190 usec later.  Here, we wait

				   up to 250 usec for the RDY bit to

		if (++retries > 10 * HZ) { /* wait no more than 10 sec

	/*

	   static long int j;

	   printk(".");

	   printk("<%ld>", jiffies-j);

	   j=jiffies;

 there are no exception conditions */

 There won't be any interrupts, so we set a timer instead. */

 Note that nobody ever sets dtlk_busy... */

	msleep_interruptible(500);		/* nap 0.50 sec but

						   could be awakened

						   earlier by

 ------------------------------------------------------------------------ */

 TRACE_TEXT(" dtlk_writeable"); */

                        /* put LPC port into known state, so

 INIT string and index marker */

			/* posting an index takes 18 msec.  Here, we

			   wait up to 100 msec to see whether it

 This macro records ten samples read from the LPC port, for later display */

 INSCOPE */

 This macro records ten samples read from the TTS port, for later display */

 1 us */ \

 10 ms */

 OUTSCOPE */

/*

   static void dtlk_handle_error(char op, char rc, unsigned int minor)

   {

   printk(KERN_INFO"\nDoubleTalk PC - MINOR: %d, OPCODE: %d, ERROR: %d\n", 

   minor, op, rc);

   return;

   }

 interrogate the DoubleTalk PC and return its settings */

	/*

	   if (i==50) printk("interrogate() read overrun\n");

	   for (i=0; i<sizeof(buf); i++)

	   printk(" %02x", buf[i]);

	   printk("\n");

	status.serial_number = t[0] + t[1] * 256; /* serial number is

 verify DT is ready, read char, wait for ACK */

 input from TTS port */

 no need to test -- this is only called when the port is readable */

 input from LPC port */

	/* acknowledging a read takes 3-4

	   usec.  Here, we wait up to 20 usec

 write n bytes to tts port */

  printk("dtlk_write_bytes(\"%-*s\", %d)\n", n, buf, n); */

 no flow control for CLEAR command */

 DT ready? */

 output to TTS port */

	/* the RDY bit goes zero 2-3 usec after writing, and goes

	   1 again 180-190 usec later.  Here, we wait up to 10

 SPDX-License-Identifier: GPL-2.0-only

/*

 * hangcheck-timer.c

 *

 * Driver for a little io fencing timer.

 *

 * Copyright (C) 2002, 2003 Oracle.  All rights reserved.

 *

 * Author: Joel Becker <joel.becker@oracle.com>

/*

 * The hangcheck-timer driver uses the TSC to catch delays that

 * jiffies does not notice.  A timer is set.  When the timer fires, it

 * checks whether it was delayed and if that delay exceeds a given

 * margin of error.  The hangcheck_tick module parameter takes the timer

 * duration in seconds.  The hangcheck_margin parameter defines the

 * margin of error, in seconds.  The defaults are 60 seconds for the

 * timer and 180 seconds for the margin of error.  IOW, a timer is set

 * for 60 seconds.  When the timer fires, the callback checks the

 * actual duration that the timer waited.  If the duration exceeds the

 * allotted time and margin (here 60 + 180, or 240 seconds), the machine

 * is restarted.  A healthy machine will have the duration match the

 * expected timeout very closely.

 Default fudge factor, in seconds */

 Default timer timeout, in seconds */

 Defaults to not reboot */

 Defaults to not dumping SysRQ T */

 options - modular */

 options - nonmodular */

 not MODULE */

 Last time scheduled */

 or something */

 CONFIG_MAGIC_SYSRQ */

	/*

	 * Enable to investigate delays in detail

 SPDX-License-Identifier: GPL-2.0-only

/*

 * 	NetWinder Button Driver-

 *	Copyright (C) Alex Holden <alex@linuxhacker.org> 1998, 1999.

 *

 Tell the header file who we are */

 The count of button presses */

 Times for the end of a sequence */

 Used for blocking read */

 Stores data to write out of device */

 The number of bytes in the buffer */

 The delay, in jiffies */

 The callback list */

 The number of callbacks registered */

 Number of presses to reboot */

/*

 * This function is called by other drivers to register a callback function

 * to be called when a particular number of button presses occurs.

 * The callback list is a static array of 32 entries (I somehow doubt many

 * people are ever going to want to register more than 32 different actions

 * to be performed by the kernel on different numbers of button presses ;).

 * However, if an attempt to register a 33rd entry (perhaps a stuck loop

 * somewhere registering the same entry over and over?) it will fail to

 * do so and return -ENOMEM. If an attempt is made to register a null pointer,

 * it will fail to do so and return -EINVAL.

 * Because callbacks can be unregistered at random the list can become

 * fragmented, so we need to search through the list until we find the first

 * free entry.

 *

 * FIXME: Has anyone spotted any locking functions int his code recently ??

/*

 * This function is called by other drivers to deregister a callback function.

 * If you attempt to unregister a callback which does not exist, it will fail

 * with -EINVAL. If there is more than one entry with the same address,

 * because it searches the list from end to beginning, it will unregister the

 * last one to be registered first (FILO- First In Last Out).

 * Note that this is not necessarily true if the entries are not submitted

 * at the same time, because another driver could have unregistered a callback

 * between the submissions creating a gap earlier in the list, which would

 * be filled first at submission time.

/*

 * This function is called by button_sequence_finished to search through the

 * list of callback functions, and call any of them whose count argument

 * matches the current count of button presses. It starts at the beginning

 * of the list and works up to the end. It will refuse to follow a null

 * pointer (which should never happen anyway).

/* 

 * This function is called when the button_timer times out.

 * ie. When you don't press the button for bdelay jiffies, this is taken to

 * mean you have ended the sequence of key presses, and this function is

 * called to wind things up (write the press_count out to /dev/button, call

 * any matching registered function callbacks, initiate reboot, etc.).

 Ask init to reboot us */

 Reset the button press counter */

/* 

 *  This handler is called when the orange button is pressed (GPIO 10 of the

 *  SuperIO chip, which maps to logical IRQ 26). If the press_count is 0,

 *  this is the first press, so it starts a timer and increments the counter.

 *  If it is higher than 0, it deletes the old timer, starts a new one, and

 *  increments the counter.

/*

 * This function is called when a user space program attempts to read

 * /dev/nwbutton. It puts the device to sleep on the wait queue until

 * button_sequence_finished writes some data to the buffer and flushes

 * the queue, at which point it writes the data out to the device and

 * returns the number of characters it has written. This function is

 * reentrant, so that many processes can be attempting to read from the

 * device at any one time.

/* 

 * This structure is the file operations structure, which specifies what

 * callbacks functions the kernel should call when a user mode process

 * attempts to perform these operations on the device.

/* 

 * This structure is the misc device structure, which specifies the minor

 * device number (158 in this case), the name of the device (for /proc/misc),

 * and the address of the above file operations structure.

/*

 * This function is called to initialise the driver, either from misc.c at

 * bootup if the driver is compiled into the kernel, or from init_module

 * below at module insert time. It attempts to register the device node

 * and the IRQ and fails with a warning message if either fails, though

 * neither ever should because the device number and IRQ are unique to

 * this driver.

 SPDX-License-Identifier: GPL-2.0

/*

 * Privileged ADI driver for sparc64

 *

 * Author: Tom Hromatka <tom.hromatka@oracle.com>

 unsupported */

/*

 * The DSP56001 Device Driver, saviour of the Free World(tm)

 *

 * Authors: Fredrik Noring   <noring@nocrew.org>

 *          lars brinkhoff   <lars@nocrew.org>

 *          Tomas Berndtsson <tomas@nocrew.org>

 *

 * First version May 1996

 *

 * History:

 *  97-01-29   Tomas Berndtsson,

 *               Integrated with Linux 2.1.21 kernel sources.

 *  97-02-15   Tomas Berndtsson,

 *               Fixed for kernel 2.1.26

 *

 * BUGS:

 *  Hmm... there must be something here :)

 *

 * Copyright (C) 1996,1997 Fredrik Noring, lars brinkhoff & Tomas Berndtsson

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file COPYING in the main directory of this archive

 * for more details.

 guess what */

 For put_user and get_user */

 minor devices */

 The only device so far */

 Host port timeout in number of tries */

 Maximum number of words before sleep */

 Power down the DSP */

 Power up the DSP */

 tx_wait(10); */

 tx_wait(10); */

 Magic execute */

 Don't do anything if nothing is to be done */

 8 bit */

 16 bit */

 24 bit */

 32 bit */

 Don't do anything if nothing is to be done */

 8 bit */

 16 bit */

 24 bit */

 32 bit */

 nothing to upload?!? */

/* As of 2.1.26 this should be dsp56k_poll,

 * but how do I then check device minor number?

 * Do I need this function at all???

 poll_wait(file, ???, wait); */

 Zero host flags */

***** Init and module functions ******/

 SPDX-License-Identifier: GPL-2.0-only

/* linux/drivers/char/scx200_gpio.c



   National Semiconductor SCx200 GPIO driver.  Allows a user space

   process to play with the GPIO pins.



 default to dynamic major */

 64 later, when known ok */

 use 1 cdev for all pins */

 support dev_dbg() with pdev->dev */

 nsc_gpio uses dev_dbg(), so needs this */

 succeed */

 cdev_put(&scx200_gpio_cdev); */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * OPAL Operator Panel Display Driver

 *

 * Copyright 2016, Suraj Jitindar Singh, IBM Corporation.

/*

 * This driver creates a character device (/dev/op_panel) which exposes the

 * operator panel (character LCD display) on IBM Power Systems machines

 * with FSPs.

 * A character buffer written to the device will be displayed on the

 * operator panel.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Sony Programmable I/O Control Device driver for VAIO

 *

 * Copyright (C) 2007 Mattia Dongili <malattia@linux.it>

 *

 * Copyright (C) 2001-2005 Stelian Pop <stelian@popies.net>

 *

 * Copyright (C) 2005 Narayanan R S <nars@kadamba.org>

 *

 * Copyright (C) 2001-2002 Alcôve <www.alcove.com>

 *

 * Copyright (C) 2001 Michael Ashley <m.ashley@unsw.edu.au>

 *

 * Copyright (C) 2001 Junichi Morita <jun1m@mars.dti.ne.jp>

 *

 * Copyright (C) 2000 Takaya Kinjo <t-kinjo@tc4.so-net.ne.jp>

 *

 * Copyright (C) 2000 Andrew Tridgell <tridge@valinux.com>

 *

 * Earlier work by Werner Almesberger, Paul `Rusty' Russell and Paul Mackerras.

 = 0 */

 = 0 */

 = 0 */

 = 0 */

 type1 models use those */

 type2 series specifics */

 type3 series specifics */

 16 bits */

 8 bits  */

 battery / brightness addresses */

 FAN0 information (reverse engineered from ACPI tables) */

 ioports used for brightness and type2 events */

 The set of possible ioports */

 looks like the default on C1Vx */

 same as in type 2 models */

 The set of possible interrupts */

 IRQ 11, GO22=0,GO23=1 in AML */

 IRQ 10, GO22=1,GO23=0 in AML */

 IRQ  5, GO22=0,GO23=0 in AML */

 no IRQ, GO22=1,GO23=1 in AML */

 IRQ 11, 0x80 in SIRQ in AML */

 IRQ 10, 0x40 in SIRQ in AML */

 IRQ  9, 0x20 in SIRQ in AML */

 IRQ  6, 0x10 in SIRQ in AML */

 no IRQ, 0x00 in SIRQ in AML */

 same as in type2 models */

 the rest don't need a loop until not 0xff */

 Event masks */

 The set of possible button release events */

 The set of possible jogger events  */

 The set of possible capture button events */

 The set of possible fnkeys events */

 The set of possible program key events */

 The set of possible bluetooth events */

 The set of possible wireless events */

 The set of possible back button events */

 The set of possible help button events */

 The set of possible lid events */

 The set of possible zoom events */

 The set of possible thumbphrase events */

 The set of possible motioneye camera events */

 The set of possible memorystick events */

 The set of possible battery events */

 Correspondance table between sonypi events and input layer events */

 CONFIG_ACPI */

 Initializes the device - this comes from the AML code in the ACPI bios */

	/* This model type uses the same initialization of

 Initialization of PCI config space of the LPC interface bridge. */

 Disables the device - this comes from the AML code in the ACPI bios */

 Get brightness, hue etc. Unreliable... */

 Set brightness, hue etc */

 Tests if the camera is ready */

 Turns the camera off */

 Turns the camera on */

 sets the bluetooth subsystem power state */

 Nothing, not all VAIOs generate this event */

 Interrupt handler: some event is available */

	/* We need to return IRQ_HANDLED here because there *are*

	 * events belonging to the sonypi device we don't know about,

 Flush input queue on first open */

 FAN Controls */

 GET Temperature (useful under APM) */

 Enable ACPI mode to get Fn key events */

 make sure we don't get any more events */

 disable ACPI mode */

 Initialize the Input Drivers: special keys */

 Set to NULL so we don't free it again below */

	/* try to detect if sony-laptop is being used and thus

	 * has already requested one of the known ioports.

	 * As in the deprecated check_region this is racy has we have

	 * multiple ioports available and one of them can be requested

	 * between this check and the subsequent request. Anyway, as an

	 * attempt to be some more user-friendly as we currently are,

	 * this is enough.

www.linux.it/~malattia/wiki/index.php/Sony_drivers\n");

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/drivers/char/misc.c

 *

 * Generic misc open routine by Johan Myreen

 *

 * Based on code from Linus

 *

 * Teemu Rantanen's Microsoft Busmouse support and Derrick Cole's

 *   changes incorporated into 0.97pl4

 *   by Peter Cervasio (pete%q106fm.uucp@wupost.wustl.edu) (08SEP92)

 *   See busmouse.c for particulars.

 *

 * Made things a lot mode modular - easy to compile in just one or two

 * of the misc drivers, as they are now completely independent. Linus.

 *

 * Support for loadable modules. 8-Sep-95 Philip Blundell <pjb27@cam.ac.uk>

 *

 * Fixed a failing symbol register to free the device registration

 *		Alan Cox <alan@lxorguk.ukuu.org.uk> 21-Jan-96

 *

 * Dynamic minors and /proc/mice by Alessandro Rubini. 26-Mar-96

 *

 * Renamed to misc and miscdevice to be more accurate. Alan Cox 26-Mar-96

 *

 * Handling of mouse minor numbers for kerneld:

 *  Idea by Jacques Gelinas <jack@solucorp.qc.ca>,

 *  adapted by Bjorn Ekwall <bj0rn@blox.se>

 *  corrected by Alan Cox <alan@lxorguk.ukuu.org.uk>

 *

 * Changes for kmod (from kerneld):

 *	Cyrus Durgin <cider@speakeasy.org>

 *

 * Added devfs support. Richard Gooch <rgooch@atnf.csiro.au>  10-Jan-1998

/*

 * Head entry for the doubly linked miscdevice list

/*

 * Assigned numbers, used for dynamic minors

 like dynamic majors */

	/*

	 * Place the miscdevice in the file's

	 * private_data so it can be used by the

	 * file operations, including f_op->open below

/**

 *	misc_register	-	register a miscellaneous device

 *	@misc: device structure

 *

 *	Register a miscellaneous device with the kernel. If the minor

 *	number is set to %MISC_DYNAMIC_MINOR a minor number is assigned

 *	and placed in the minor field of the structure. For other cases

 *	the minor number requested is used.

 *

 *	The structure passed is linked into the kernel and may not be

 *	destroyed until it has been unregistered. By default, an open()

 *	syscall to the device sets file->private_data to point to the

 *	structure. Drivers don't need open in fops for this.

 *

 *	A zero is returned on success and a negative errno code for

 *	failure.

	/*

	 * Add it to the front, so that later devices can "override"

	 * earlier defaults

/**

 *	misc_deregister - unregister a miscellaneous device

 *	@misc: device to unregister

 *

 *	Unregister a miscellaneous device that was previously

 *	successfully registered with misc_register().

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * linux/drivers/char/ppdev.c

 *

 * This is the code behind /dev/parport* -- it allows a user-space

 * application to use the parport subsystem.

 *

 * Copyright (C) 1998-2000, 2002 Tim Waugh <tim@cyberelk.net>

 *

 * A /dev/parportx device node represents an arbitrary device

 * on port 'x'.  The following operations are possible:

 *

 * open		do nothing, set up default IEEE 1284 protocol to be COMPAT

 * close	release port and unregister device (if necessary)

 * ioctl

 *   EXCL	register device exclusively (may fail)

 *   CLAIM	(register device first time) parport_claim_or_block

 *   RELEASE	parport_release

 *   SETMODE	set the IEEE 1284 protocol to use for read/write

 *   SETPHASE	set the IEEE 1284 phase of a particular mode.  Not to be

 *              confused with ioctl(fd, SETPHASER, &stun). ;-)

 *   DATADIR	data_forward / data_reverse

 *   WDATA	write_data

 *   RDATA	read_data

 *   WCONTROL	write_control

 *   RCONTROL	read_control

 *   FCONTROL	frob_control

 *   RSTATUS	read_status

 *   NEGOT	parport_negotiate

 *   YIELD	parport_yield_blocking

 *   WCTLONIRQ	on interrupt, set control lines

 *   CLRIRQ	clear (and return) interrupt count

 *   SETTIME	sets device timeout (struct timeval)

 *   GETTIME	gets device timeout (struct timeval)

 *   GETMODES	gets hardware supported modes (unsigned int)

 *   GETMODE	gets the current IEEE1284 mode

 *   GETPHASE   gets the current IEEE1284 phase

 *   GETFLAGS   gets current (user-visible) flags

 *   SETFLAGS   sets current (user-visible) flags

 * read/write	read or write in current IEEE 1284 protocol

 * select	wait for interrupt (in readfds)

 *

 * Changes:

 * Added SETTIME/GETTIME ioctl, Fred Barnes, 1999.

 *

 * Arnaldo Carvalho de Melo <acme@conectiva.com.br> 2000/08/25

 * - On error, copy_from_user and copy_to_user do not return -EFAULT,

 *   They return the positive number of bytes *not* copied due to address

 *   space errors.

 *

 * Added GETMODES/GETMODE/GETPHASE ioctls, Fred Barnes <frmb2@ukc.ac.uk>, 03/01/2001.

 * Added GETFLAGS/SETFLAGS ioctls, Fred Barnes, 04/2001

 should we use PARDEVICE_MAX here? */

 pp_struct.flags bitfields */

 Other constants */

 10s */

 define fixed sized ioctl cmd for y2038 migration */

 Don't have the port claimed */

 Trivial case. */

 various specials for EPP mode */

 Don't have the port claimed */

 do a fast EPP write */

 First handle the cases that don't take arguments. */

 Deferred device registration. */

		/* For interrupt-reporting to work, we need to be

 We may need to fix up the state machine. */

 But it's not really an error. */

 There's no chance of making the driver happy. */

		/* Just remember to register the device exclusively

 FIXME: validate mode */

 FIXME: validate phase */

 end switch() */

	/* Everything else requires the port to be claimed, so check

 Save the state machine's state. */

 handshake failed, peripheral not IEEE 1284 */

 handshake succeeded, peripheral rejected mode */

		/* Remember what to set the control lines to, for next

 Keep the compiler happy */

	/* Defer the actual device registration until the first claim.

	 * That way, we know whether or not the driver wants to have

	 * exclusive access to the port (PPEXCL).

 parport released, but not in compatibility mode */

 No kernel lock held - fine */

 Clean up all parport stuff */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/drivers/char/ttyprintk.c

 *

 *  Copyright (C) 2010  Samo Pogacnik

/*

 * This pseudo device allows user to make printk messages. It is possible

 * to store "console" messages inline with kernel messages for better analyses

 * of the boot process, for example.

/*

 * Our simple preformatting supports transparent output of (time-stamped)

 * printk messages (also suitable for logging service):

 * - any cr is replaced by nl

 * - adds a ttyprintk source tag in front of each line

 * - too long message is fragmented, with '\'nl between fragments

 * - TPK_STR_SIZE isn't really the write_room limiting factor, because

 *   it is emptied on the fly during preformatting.

 should be bigger then max expected line length */

 we could assume 4K for instance */

 end of tmp buffer reached: cut the message in two */

/*

 * TTY operations open function.

/*

 * TTY operations close function.

/*

 * TTY operations write function.

 exclusive use of tpk_printk within this tty */

/*

 * TTY operations write_room function.

/*

 * TTY operations hangup function.

/*

 * TTY port operations shutdown function.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Driver for TANBAC TB0219 base board.

 *

 *  Copyright (C) 2005  Yoichi Yuasa <yuasa@linux-mips.org>

 default is dynamic major device number */

/*

 * Minor device number

 *	 0 = 7 segment LED

 *

 *	16 = GPIO IN 0

 *	17 = GPIO IN 1

 *	18 = GPIO IN 2

 *	19 = GPIO IN 3

 *	20 = GPIO IN 4

 *	21 = GPIO IN 5

 *	22 = GPIO IN 6

 *	23 = GPIO IN 7

 *

 *	32 = GPIO OUT 0

 *	33 = GPIO OUT 1

 *	34 = GPIO OUT 2

 *	35 = GPIO OUT 3

 *	36 = GPIO OUT 4

 *	37 = GPIO OUT 5

 *	38 = GPIO OUT 6

 *	39 = GPIO OUT 7

 *

 *	48 = DIP switch 1

 *	49 = DIP switch 2

 *	50 = DIP switch 3

 *	51 = DIP switch 4

 *	52 = DIP switch 5

 *	53 = DIP switch 6

 *	54 = DIP switch 7

 *	55 = DIP switch 8

 PCI Slot 1 */

 PCI Slot 2 */

 PCI Slot 3 */

/*

 * random.c -- A strong random number generator

 *

 * Copyright (C) 2017 Jason A. Donenfeld <Jason@zx2c4.com>. All

 * Rights Reserved.

 *

 * Copyright Matt Mackall <mpm@selenic.com>, 2003, 2004, 2005

 *

 * Copyright Theodore Ts'o, 1994, 1995, 1996, 1997, 1998, 1999.  All

 * rights reserved.

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 * 1. Redistributions of source code must retain the above copyright

 *    notice, and the entire permission notice in its entirety,

 *    including the disclaimer of warranties.

 * 2. Redistributions in binary form must reproduce the above copyright

 *    notice, this list of conditions and the following disclaimer in the

 *    documentation and/or other materials provided with the distribution.

 * 3. The name of the author may not be used to endorse or promote

 *    products derived from this software without specific prior

 *    written permission.

 *

 * ALTERNATIVELY, this product may be distributed under the terms of

 * the GNU General Public License, in which case the provisions of the GPL are

 * required INSTEAD OF the above restrictions.  (This clause is

 * necessary due to a potential bad interaction between the GPL and

 * the restrictions contained in a BSD-style copyright.)

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES

 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, ALL OF

 * WHICH ARE HEREBY DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE

 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR

 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT

 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR

 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF

 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 * USE OF THIS SOFTWARE, EVEN IF NOT ADVISED OF THE POSSIBILITY OF SUCH

 * DAMAGE.

/*

 * (now, with legal B.S. out of the way.....)

 *

 * This routine gathers environmental noise from device drivers, etc.,

 * and returns good random numbers, suitable for cryptographic use.

 * Besides the obvious cryptographic uses, these numbers are also good

 * for seeding TCP sequence numbers, and other places where it is

 * desirable to have numbers which are not only random, but hard to

 * predict by an attacker.

 *

 * Theory of operation

 * ===================

 *

 * Computers are very predictable devices.  Hence it is extremely hard

 * to produce truly random numbers on a computer --- as opposed to

 * pseudo-random numbers, which can easily generated by using a

 * algorithm.  Unfortunately, it is very easy for attackers to guess

 * the sequence of pseudo-random number generators, and for some

 * applications this is not acceptable.  So instead, we must try to

 * gather "environmental noise" from the computer's environment, which

 * must be hard for outside attackers to observe, and use that to

 * generate random numbers.  In a Unix environment, this is best done

 * from inside the kernel.

 *

 * Sources of randomness from the environment include inter-keyboard

 * timings, inter-interrupt timings from some interrupts, and other

 * events which are both (a) non-deterministic and (b) hard for an

 * outside observer to measure.  Randomness from these sources are

 * added to an "entropy pool", which is mixed using a CRC-like function.

 * This is not cryptographically strong, but it is adequate assuming

 * the randomness is not chosen maliciously, and it is fast enough that

 * the overhead of doing it on every interrupt is very reasonable.

 * As random bytes are mixed into the entropy pool, the routines keep

 * an *estimate* of how many bits of randomness have been stored into

 * the random number generator's internal state.

 *

 * When random bytes are desired, they are obtained by taking the SHA

 * hash of the contents of the "entropy pool".  The SHA hash avoids

 * exposing the internal state of the entropy pool.  It is believed to

 * be computationally infeasible to derive any useful information

 * about the input of SHA from its output.  Even if it is possible to

 * analyze SHA in some clever way, as long as the amount of data

 * returned from the generator is less than the inherent entropy in

 * the pool, the output data is totally unpredictable.  For this

 * reason, the routine decreases its internal estimate of how many

 * bits of "true randomness" are contained in the entropy pool as it

 * outputs random numbers.

 *

 * If this estimate goes to zero, the routine can still generate

 * random numbers; however, an attacker may (at least in theory) be

 * able to infer the future output of the generator from prior

 * outputs.  This requires successful cryptanalysis of SHA, which is

 * not believed to be feasible, but there is a remote possibility.

 * Nonetheless, these numbers should be useful for the vast majority

 * of purposes.

 *

 * Exported interfaces ---- output

 * ===============================

 *

 * There are four exported interfaces; two for use within the kernel,

 * and two or use from userspace.

 *

 * Exported interfaces ---- userspace output

 * -----------------------------------------

 *

 * The userspace interfaces are two character devices /dev/random and

 * /dev/urandom.  /dev/random is suitable for use when very high

 * quality randomness is desired (for example, for key generation or

 * one-time pads), as it will only return a maximum of the number of

 * bits of randomness (as estimated by the random number generator)

 * contained in the entropy pool.

 *

 * The /dev/urandom device does not have this limit, and will return

 * as many bytes as are requested.  As more and more random bytes are

 * requested without giving time for the entropy pool to recharge,

 * this will result in random numbers that are merely cryptographically

 * strong.  For many applications, however, this is acceptable.

 *

 * Exported interfaces ---- kernel output

 * --------------------------------------

 *

 * The primary kernel interface is

 *

 * 	void get_random_bytes(void *buf, int nbytes);

 *

 * This interface will return the requested number of random bytes,

 * and place it in the requested buffer.  This is equivalent to a

 * read from /dev/urandom.

 *

 * For less critical applications, there are the functions:

 *

 * 	u32 get_random_u32()

 * 	u64 get_random_u64()

 * 	unsigned int get_random_int()

 * 	unsigned long get_random_long()

 *

 * These are produced by a cryptographic RNG seeded from get_random_bytes,

 * and so do not deplete the entropy pool as much.  These are recommended

 * for most in-kernel operations *if the result is going to be stored in

 * the kernel*.

 *

 * Specifically, the get_random_int() family do not attempt to do

 * "anti-backtracking".  If you capture the state of the kernel (e.g.

 * by snapshotting the VM), you can figure out previous get_random_int()

 * return values.  But if the value is stored in the kernel anyway,

 * this is not a problem.

 *

 * It *is* safe to expose get_random_int() output to attackers (e.g. as

 * network cookies); given outputs 1..n, it's not feasible to predict

 * outputs 0 or n+1.  The only concern is an attacker who breaks into

 * the kernel later; the get_random_int() engine is not reseeded as

 * often as the get_random_bytes() one.

 *

 * get_random_bytes() is needed for keys that need to stay secret after

 * they are erased from the kernel.  For example, any key that will

 * be wrapped and stored encrypted.  And session encryption keys: we'd

 * like to know that after the session is closed and the keys erased,

 * the plaintext is unrecoverable to someone who recorded the ciphertext.

 *

 * But for network ports/cookies, stack canaries, PRNG seeds, address

 * space layout randomization, session *authentication* keys, or other

 * applications where the sensitive data is stored in the kernel in

 * plaintext for as long as it's sensitive, the get_random_int() family

 * is just fine.

 *

 * Consider ASLR.  We want to keep the address space secret from an

 * outside attacker while the process is running, but once the address

 * space is torn down, it's of no use to an attacker any more.  And it's

 * stored in kernel data structures as long as it's alive, so worrying

 * about an attacker's ability to extrapolate it from the get_random_int()

 * CRNG is silly.

 *

 * Even some cryptographic keys are safe to generate with get_random_int().

 * In particular, keys for SipHash are generally fine.  Here, knowledge

 * of the key authorizes you to do something to a kernel object (inject

 * packets to a network connection, or flood a hash table), and the

 * key is stored with the object being protected.  Once it goes away,

 * we no longer care if anyone knows the key.

 *

 * prandom_u32()

 * -------------

 *

 * For even weaker applications, see the pseudorandom generator

 * prandom_u32(), prandom_max(), and prandom_bytes().  If the random

 * numbers aren't security-critical at all, these are *far* cheaper.

 * Useful for self-tests, random error simulation, randomized backoffs,

 * and any other application where you trust that nobody is trying to

 * maliciously mess with you by guessing the "random" numbers.

 *

 * Exported interfaces ---- input

 * ==============================

 *

 * The current exported interfaces for gathering environmental noise

 * from the devices are:

 *

 *	void add_device_randomness(const void *buf, unsigned int size);

 * 	void add_input_randomness(unsigned int type, unsigned int code,

 *                                unsigned int value);

 *	void add_interrupt_randomness(int irq, int irq_flags);

 * 	void add_disk_randomness(struct gendisk *disk);

 *

 * add_device_randomness() is for adding data to the random pool that

 * is likely to differ between two devices (or possibly even per boot).

 * This would be things like MAC addresses or serial numbers, or the

 * read-out of the RTC. This does *not* add any actual entropy to the

 * pool, but it initializes the pool to different values for devices

 * that might otherwise be identical and have very little entropy

 * available to them (particularly common in the embedded world).

 *

 * add_input_randomness() uses the input layer interrupt timing, as well as

 * the event type information from the hardware.

 *

 * add_interrupt_randomness() uses the interrupt timing as random

 * inputs to the entropy pool. Using the cycle counters and the irq source

 * as inputs, it feeds the randomness roughly once a second.

 *

 * add_disk_randomness() uses what amounts to the seek time of block

 * layer request events, on a per-disk_devt basis, as input to the

 * entropy pool. Note that high-speed solid state drives with very low

 * seek times do not make for good sources of entropy, as their seek

 * times are usually fairly consistent.

 *

 * All of these routines try to estimate how many bits of randomness a

 * particular randomness source.  They do this by keeping track of the

 * first and second order deltas of the event timings.

 *

 * Ensuring unpredictability at system startup

 * ============================================

 *

 * When any operating system starts up, it will go through a sequence

 * of actions that are fairly predictable by an adversary, especially

 * if the start-up does not involve interaction with a human operator.

 * This reduces the actual number of bits of unpredictability in the

 * entropy pool below the value in entropy_count.  In order to

 * counteract this effect, it helps to carry information in the

 * entropy pool across shut-downs and start-ups.  To do this, put the

 * following lines an appropriate script which is run during the boot

 * sequence:

 *

 *	echo "Initializing random number generator..."

 *	random_seed=/var/run/random-seed

 *	# Carry a random seed from start-up to start-up

 *	# Load and then save the whole entropy pool

 *	if [ -f $random_seed ]; then

 *		cat $random_seed >/dev/urandom

 *	else

 *		touch $random_seed

 *	fi

 *	chmod 600 $random_seed

 *	dd if=/dev/urandom of=$random_seed count=1 bs=512

 *

 * and the following lines in an appropriate script which is run as

 * the system is shutdown:

 *

 *	# Carry a random seed from shut-down to start-up

 *	# Save the whole entropy pool

 *	echo "Saving random seed..."

 *	random_seed=/var/run/random-seed

 *	touch $random_seed

 *	chmod 600 $random_seed

 *	dd if=/dev/urandom of=$random_seed count=1 bs=512

 *

 * For example, on most modern systems using the System V init

 * scripts, such code fragments would be found in

 * /etc/rc.d/init.d/random.  On older Linux systems, the correct script

 * location might be in /etc/rcb.d/rc.local or /etc/rc.d/rc.0.

 *

 * Effectively, these commands cause the contents of the entropy pool

 * to be saved at shut-down time and reloaded into the entropy pool at

 * start-up.  (The 'dd' in the addition to the bootup script is to

 * make sure that /etc/random-seed is different for every start-up,

 * even if the system crashes without executing rc.0.)  Even with

 * complete knowledge of the start-up activities, predicting the state

 * of the entropy pool requires knowledge of the previous history of

 * the system.

 *

 * Configuring the /dev/random driver under Linux

 * ==============================================

 *

 * The /dev/random driver under Linux uses minor numbers 8 and 9 of

 * the /dev/mem major number (#1).  So if your system does not have

 * /dev/random and /dev/urandom created already, they can be created

 * by using the commands:

 *

 * 	mknod /dev/random c 1 8

 * 	mknod /dev/urandom c 1 9

 *

 * Acknowledgements:

 * =================

 *

 * Ideas for constructing this random number generator were derived

 * from Pretty Good Privacy's random number generator, and from private

 * discussions with Phil Karn.  Colin Plumb provided a faster random

 * number generator, which speed up the mixing function of the entropy

 * pool, taken from PGPfone.  Dale Worley has also contributed many

 * useful ideas and suggestions to improve this driver.

 *

 * Any flaws in the design are solely my responsibility, and should

 * not be attributed to the Phil, Colin, or any of authors of PGP.

 *

 * Further background information on this topic may be obtained from

 * RFC 1750, "Randomness Recommendations for Security", by Donald

 * Eastlake, Steve Crocker, and Jeff Schiller.

 #define ADD_INTERRUPT_BENCH */

/*

 * Configuration information

/*

 * To allow fractional bits to be tracked, the entropy_count field is

 * denominated in units of 1/8th bits.

 *

 * 2*(ENTROPY_SHIFT + poolbitshift) must <= 31, or the multiply in

 * credit_entropy_bits() needs to be 64 bits wide.

/*

 * If the entropy count falls under this number of bits, then we

 * should wake up processes which are selecting or polling on write

 * access to /dev/random.

/*

 * Originally, we used a primitive polynomial of degree .poolwords

 * over GF(2).  The taps for various sizes are defined below.  They

 * were chosen to be evenly spaced except for the last tap, which is 1

 * to get the twisting happening as fast as possible.

 *

 * For the purposes of better mixing, we use the CRC-32 polynomial as

 * well to make a (modified) twisted Generalized Feedback Shift

 * Register.  (See M. Matsumoto & Y. Kurita, 1992.  Twisted GFSR

 * generators.  ACM Transactions on Modeling and Computer Simulation

 * 2(3):179-194.  Also see M. Matsumoto & Y. Kurita, 1994.  Twisted

 * GFSR generators II.  ACM Transactions on Modeling and Computer

 * Simulation 4:254-266)

 *

 * Thanks to Colin Plumb for suggesting this.

 *

 * The mixing operation is much less sensitive than the output hash,

 * where we use SHA-1.  All that we want of mixing operation is that

 * it be a good non-cryptographic hash; i.e. it not produce collisions

 * when fed "random" data of the sort we expect to see.  As long as

 * the pool state differs for different inputs, we have preserved the

 * input entropy and done a good job.  The fact that an intelligent

 * attacker can construct inputs that will produce controlled

 * alterations to the pool's state is not important because we don't

 * consider such inputs to contribute any randomness.  The only

 * property we need with respect to them is that the attacker can't

 * increase his/her knowledge of the pool's state.  Since all

 * additions are reversible (knowing the final state and the input,

 * you can reconstruct the initial state), if an attacker has any

 * uncertainty about the initial state, he/she can only shuffle that

 * uncertainty about, but never cause any collisions (which would

 * decrease the uncertainty).

 *

 * Our mixing functions were analyzed by Lacharme, Roeck, Strubel, and

 * Videau in their paper, "The Linux Pseudorandom Number Generator

 * Revisited" (see: http://eprint.iacr.org/2012/251.pdf).  In their

 * paper, they point out that we are not using a true Twisted GFSR,

 * since Matsumoto & Kurita used a trinomial feedback polynomial (that

 * is, with only three taps, instead of the six that we are using).

 * As a result, the resulting polynomial is neither primitive nor

 * irreducible, and hence does not have a maximal period over

 * GF(2**32).  They suggest a slight change to the generator

 * polynomial which improves the resulting TGFSR polynomial to be

 * irreducible, which we have made here.

 was: x^128 + x^103 + x^76 + x^51 +x^25 + x + 1 */

 x^128 + x^104 + x^76 + x^51 +x^25 + x + 1 */

/*

 * Static global variables

/*

 * crng_init =  0 --> Uninitialized

 *		1 --> Initialized

 *		2 --> Initialized from input_pool

 *

 * crng_init is protected by primary_crng->lock, and only increases

 * its value (from 0->1->2).

/**********************************************************************

 *

 * OS independent entropy store.   Here are the functions which handle

 * storing entropy in an entropy pool.

 *

 read-only data: */

 read-write data: */

/*

 * This function adds bytes into the entropy "pool".  It does not

 * update the entropy estimate.  The caller should call

 * credit_entropy_bits if this is appropriate.

 *

 * The pool is stirred with a primitive polynomial of the appropriate

 * degree, and then twisted.  We twist by three bits at a time because

 * it's cheap to do so and helps slightly in the expected case where

 * the entropy is concentrated in the low-order bits.

 mix one byte at a time to simplify size handling and churn faster */

 XOR in the various taps */

 Mix the result back in with a twist */

		/*

		 * Normally, we add 7 bits of rotation to the pool.

		 * At the beginning of the pool, add an extra 7 bits

		 * rotation, so that successive passes spread the

		 * input bits across the pool evenly.

/*

 * This is a fast mixing routine used by the interrupt randomness

 * collector.  It's hardcoded for an 128 bit pool and assumes that any

 * locks that might be needed are taken by the caller.

/*

 * Credit (or debit) the entropy store with n bits of entropy.

 * Use credit_entropy_bits_safe() if the value comes from userspace

 * or otherwise should be checked for extreme values.

 Debit */

		/*

		 * Credit: we have to account for the possibility of

		 * overwriting already present entropy.	 Even in the

		 * ideal case of pure Shannon entropy, new contributions

		 * approach the full value asymptotically:

		 *

		 * entropy <- entropy + (pool_size - entropy) *

		 *	(1 - exp(-add_entropy/pool_size))

		 *

		 * For add_entropy <= pool_size/2 then

		 * (1 - exp(-add_entropy/pool_size)) >=

		 *    (add_entropy/pool_size)*0.7869...

		 * so we can approximate the exponential with

		 * 3/4*add_entropy/pool_size and still be on the

		 * safe side by adding at most pool_size/2 at a time.

		 *

		 * The use of pool_size-2 in the while statement is to

		 * prevent rounding artifacts from making the loop

		 * arbitrarily long; this limits the loop to log2(pool_size)*2

		 * turns no matter how large nbits is.

 The +2 corresponds to the /4 in the denominator */

 Cap the value to avoid overflows */

/*********************************************************************

 *

 * CRNG using CHACHA20

 *

/*

 * Hack to deal with crazy userspace progams when they are all trying

 * to access /dev/urandom in parallel.  The programs are almost

 * certainly doing something terribly wrong, but we'll work around

 * their brain damage.

/*

 * crng_fast_load() can be called by code in the interrupt service

 * path.  So we can't afford to dilly-dally.

/*

 * crng_slow_load() is called by add_device_randomness, which has two

 * attributes.  (1) We can't trust the buffer passed to it is

 * guaranteed to be unpredictable (so it might not have any entropy at

 * all), and (2) it doesn't have the performance constraints of

 * crng_fast_load().

 *

 * So we do something more comprehensive which is guaranteed to touch

 * all of the primary_crng's state, and which uses a LFSR with a

 * period of 255 as part of the mixing algorithm.  Finally, we do

 * *not* advance crng_init_cnt since buffer we may get may be something

 * like a fixed DMI table (for example), which might very well be

 * unique to the machine, but is otherwise unvarying.

/*

 * Use the leftover bytes from the CRNG block output (if there is

 * enough) to mutate the CRNG key to provide backtracking protection.

 Wipe data just written to memory */

/*********************************************************************

 *

 * Entropy input management

 *

 There is one of these per entropy source */

/*

 * Add device- or boot-specific data to the input pool to help

 * initialize it.

 *

 * None of this adds any entropy; it is meant to avoid the problem of

 * the entropy pool having similar initial state across largely

 * identical devices.

/*

 * This function adds entropy to the entropy "pool" by using timing

 * delays.  It uses the timer_rand_state structure to make an estimate

 * of how many bits of entropy this call has added to the pool.

 *

 * The number "num" is also added to the pool - it should somehow describe

 * the type of event which just happened.  This is currently 0-255 for

 * keyboard scan codes, and 256 upwards for interrupts.

 *

	/*

	 * Calculate number of bits of randomness we probably added.

	 * We take into account the first, second and third-order deltas

	 * in order to make our estimate.

	/*

	 * delta is now minimum absolute delta.

	 * Round down by 1 bit on general principles,

	 * and limit entropy estimate to 12 bits.

 ignore autorepeat and the like */

 Exponential average factor k=1/256 */

 Use a weighted moving average */

 And average deviation */

 award one bit for the contents of the fast pool */

 first major is 1, so we get >= 0x200 here */

/*********************************************************************

 *

 * Entropy extraction routines

 *

/*

 * This function decides how many bytes to actually take from the

 * given pool, and also debits the entropy count accordingly.

 Can we pull enough? */

 never pull more than available */

/*

 * This function does the actual extraction for extract_entropy.

 *

 * Note: we assume that .poolwords is a multiple of 16 words.

	/*

	 * If we have an architectural hardware random number

	 * generator, use it for SHA's initial vector

 Generate a hash across the pool, 16 words (512 bits) at a time */

	/*

	 * We mix the hash back into the pool to prevent backtracking

	 * attacks (where the attacker knows the state of the pool

	 * plus the current outputs, and attempts to find previous

	 * ouputs), unless the hash function can be inverted. By

	 * mixing at least a SHA1 worth of hash data back, we make

	 * brute-forcing the feedback as hard as brute-forcing the

	 * hash.

	/*

	 * In case the hash function has some recognizable output

	 * pattern, we fold it in half. Thus, we always feed back

	 * twice as much data as we output.

 Wipe data just returned from memory */

/*

 * This function extracts randomness from the "entropy pool", and

 * returns it in a buffer.

 *

 * The min parameter specifies the minimum amount we can pull before

 * failing to avoid races that defeat catastrophic reseeding while the

 * reserved parameter indicates how much entropy we must leave in the

 * pool after each pull to avoid starving other readers.

 if last_data isn't primed, we need EXTRACT_SIZE extra bytes */

/*

 * This function is the exported kernel interface.  It returns some

 * number of good random numbers, suitable for key generation, seeding

 * TCP sequence numbers, etc.  It does not rely on the hardware random

 * number generator.  For random bytes direct from the hardware RNG

 * (when available), use get_random_bytes_arch(). In order to ensure

 * that the randomness provided by this function is okay, the function

 * wait_for_random_bytes() should be called and return 0 at least once

 * at any point prior.

/*

 * Each time the timer fires, we expect that we got an unpredictable

 * jump in the cycle counter. Even if the timer is running on another

 * CPU, the timer activity will be touching the stack of the CPU that is

 * generating entropy..

 *

 * Note that we don't re-arm the timer in the timer itself - we are

 * happy to be scheduled away, since that just makes the load more

 * complex, but we do not want the timer to keep ticking unless the

 * entropy loop is running.

 *

 * So the re-arming always happens in the entropy loop itself.

/*

 * If we have an actual cycle counter, see if we can

 * generate enough entropy with timing noise

 Slow counter - or none. Don't even bother */

/*

 * Wait for the urandom pool to be seeded and thus guaranteed to supply

 * cryptographically secure random numbers. This applies to: the /dev/urandom

 * device, the get_random_bytes function, and the get_random_{u32,u64,int,long}

 * family of functions. Using any of these functions without first calling

 * this function forfeits the guarantee of security.

 *

 * Returns: 0 if the urandom pool has been seeded.

 *          -ERESTARTSYS if the function was interrupted by a signal.

/*

 * Returns whether or not the urandom pool has been seeded and thus guaranteed

 * to supply cryptographically secure random numbers. This applies to: the

 * /dev/urandom device, the get_random_bytes function, and the get_random_{u32,

 * ,u64,int,long} family of functions.

 *

 * Returns: true if the urandom pool has been seeded.

 *          false if the urandom pool has not been seeded.

/*

 * Add a callback function that will be invoked when the nonblocking

 * pool is initialised.

 *

 * returns: 0 if callback is successfully added

 *	    -EALREADY if pool is already initialised (callback not called)

 *	    -ENOENT if module for callback is not alive

/*

 * Delete a previously registered readiness callback function.

/*

 * This function will use the architecture-specific hardware random

 * number generator if it is available.  The arch-specific hw RNG will

 * almost certainly be faster than what we can do in software, but it

 * is impossible to verify that it is implemented securely (as

 * opposed, to, say, the AES encryption of a sequence number using a

 * key known by the NSA).  So it's useful if we need the speed, but

 * only if we're willing to trust the hardware manufacturer not to

 * have put in a back door.

 *

 * Return number of bytes filled in.

/*

 * init_std_data - initialize pool with system data

 *

 * @r: pool to initialize

 *

 * This function clears the pool's entropy count and mixes some system

 * data into the pool to prepare it for use. The pool is not cleared

 * as that can only decrease the entropy in the pool.

/*

 * Note that setup_arch() may call add_device_randomness()

 * long before we get here. This allows seeding of the pools

 * with some platform dependent data very early in the boot

 * process. But it limits our options here. We must use

 * statically allocated structures that already have all

 * initializations complete at compile time. We should also

 * take care not to overwrite the precious per platform data

 * we were given.

	/*

	 * If kzalloc returns null, we just won't use that entropy

	 * source.

 inherently racy, no point locking */

		/*

		 * Clear the entropy pool counters. We no longer clear

		 * the entropy pool, as that's silly.

	/*

	 * Requesting insecure and blocking randomness at the same time makes

	 * no sense.

/********************************************************************

 *

 * Sysctl interface

 *

/*

 * This function is used to return both the bootid UUID, and random

 * UUID.  The difference is in whether table->data is NULL; if it is,

 * then a new UUID is generated and returned to the user.

 *

 * If the user accesses this via the proc interface, the UUID will be

 * returned as an ASCII string in the standard UUID format; if via the

 * sysctl system call, as 16 bytes of binary data.

/*

 * Return entropy available scaled to integral bits

 CONFIG_SYSCTL */

/*

 * Get a random word for internal kernel use only. The quality of the random

 * number is good as /dev/urandom, but there is no backtrack protection, with

 * the goal of being quite fast and not depleting entropy. In order to ensure

 * that the randomness provided by this function is okay, the function

 * wait_for_random_bytes() should be called and return 0 at least once at any

 * point prior.

/* It's important to invalidate all potential batched entropy that might

 * be stored before the crng is initialized, which we can do lazily by

 * simply resetting the counter to zero so that it's re-extracted on the

/**

 * randomize_page - Generate a random, page aligned address

 * @start:	The smallest acceptable address the caller will take.

 * @range:	The size of the area, starting at @start, within which the

 *		random address must fall.

 *

 * If @start + @range would overflow, @range is capped.

 *

 * NOTE: Historical use of randomize_range, which this replaces, presumed that

 * @start was already page aligned.  We now align it regardless.

 *

 * Return: A page aligned address within [start, start + range).  On error,

 * @start is returned.

/* Interface for in-kernel drivers of true hardware RNGs.

 * Those devices may produce endless random bits and will be throttled

 * when our pool is full.

	/* Suspend writing if we're above the trickle threshold.

	 * We'll be woken up again once below random_write_wakeup_thresh,

	 * or when the calling thread is about to terminate.

/* Handle random seed passed by bootloader.

 * If the seed is trustworthy, it would be regarded as hardware RNGs. Otherwise

 * it would be regarded as device data.

 * The decision is controlled by CONFIG_RANDOM_TRUST_BOOTLOADER.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Flash memory interface rev.5 driver for the Intel

 * Flash chips used on the NetWinder.

 *

 * 20/08/2000	RMK	use __ioremap to map flash into virtual memory

 *			make a few more places use "volatile"

 * 22/05/2001	RMK	- Lock read against write

 *			- merge printk level changes (with mods) from Alan Cox.

 *			- use *ppos as the file position, not file->f_pos.

 *			- fix check for out of range pos and r/w size

 *

 * Please note that we are tampering with the only flash chip in the

 * machine, which contains the bootup code.  We therefore have the

 * power to convert these machines into doorstops...

****************************************************************************/

1 Meg

4 Meg

Intel flash

Intel flash 4Meg

if set - we will display progress msgs

	/*

	 * try to get flash chip ID

	/*

	 * on 4 Meg flash the second byte is actually at offset 2...

	/*

	 * set it back to read mode

	/*

	 * We now lock against reads and writes. --rmk

	/*

	 * check for out of range pos or count

	/*

	 * We now lock against reads and writes. --rmk

block # of 64K bytes

	/*

	 * # of 64K blocks to erase and write

	/*

	 * write ends at exactly 64k boundary?

		/*

		 * first we have to erase the block(s), where we will write...

		/*

		 * write_block will limit write to space left in this block

		/*

		 * if somehow write verify failed? Can't happen??

			/*

			 * retry up to 10 times

				/*

				 * else quit with error...

/*

 * The memory devices use the full 32/64 bits of the offset, and so we cannot

 * check against negative addresses: they are ok. The return value is weird,

 * though, in that case (0).

 *

 * also note that seeking relative to the "end of file" isn't supported:

 * it has no meaning, so it returns -EINVAL.

/*

 * assume that main Write routine did the parameter checking...

 * so just go ahead and erase, what requested!

	/*

	 * reset footbridge to the correct offset 0 (...0..3)

	/*

	 * dummy ROM read

	/*

	 * reset status if old errors

	/*

	 * erase a block...

	 * aim at the middle of a current block...

	/*

	 * dummy read

	/*

	 * erase

	/*

	 * confirm

	/*

	 * wait 10 ms

	/*

	 * wait while erasing in process (up to 10 sec)

		/*

		 * read any address

              printk("Flash_erase: status=%X.\n",c1);

	/*

	 * set flash for normal read access

      *(volatile unsigned char*)(FLASH_BASE+0x8000) = 0xFF;

back to normal operation

	/*

	 * check if erase errors were reported

		/*

		 * reset error

	/*

	 * just to make sure - verify if erased OK...

/*

 * write_block will limit number of bytes written to the space in this block

	/*

	 * check if write will end in this block....

	/*

	 * wait up to 30 sec for this block

	  	/*

	  	 * dummy read

		/*

		 * kick open the write gate

		/*

		 * program footbridge to the correct offset...0..3

		/*

		 * write cmd

		/*

		 * data to write

		/*

		 * get status

		/*

		 * wait up to 1 sec for this byte

		/*

		 * while not ready...

		/*

		 * if timeout getting status

			/*

			 * reset err

		/*

		 * switch on read access, as a default flash operation mode

		/*

		 * read access

		/*

		 * if hardware reports an error writing, and not timeout - 

		 * reset the chip and retry

			/*

			 * reset err

			/*

			 * before timeout?

				/*

				 * wait couple ms

				/*

				 * return error -2

	/*

	 * we want to write a bit pattern XXX1 to Xilinx to enable

	 * the write gate, which will be open for about the next 2ms.

	/*

	 * let the ISA bus to catch on...

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2006, 2007, 2009 Rusty Russell, IBM Corporation

 * Copyright (C) 2009, 2010, 2011 Red Hat, Inc.

 * Copyright (C) 2009, 2010, 2011 Amit Shah <amit.shah@redhat.com>

/*

 * This is a global struct for storing common data for all the devices

 * this driver handles.

 *

 * Mainly, it has a linked list for all the consoles in one place so

 * that callbacks from hvc for get_chars(), put_chars() work properly

 * across multiple devices and multiple ports per device.

 Used for registering chardevs */

 Used for exporting per-port information to debugfs */

 List of all the devices we're handling */

	/*

	 * This is used to keep track of the number of hvc consoles

	 * spawned by this driver.  This number is given as the first

	 * argument to hvc_alloc().  To correctly map an initial

	 * console spawned via hvc_instantiate to the console being

	 * hooked up via hvc_alloc, we need to pass the same vtermno.

	 *

	 * We also just assume the first console being initialised was

	 * the first one that got used as the initial console.

 All the console devices handled by this driver */

 This struct holds information that's relevant only for console ports */

 We'll place all consoles in a list in the pdrvdata struct */

 The hvc device associated with this console port */

 The size of the console */

	/*

	 * This number identifies the number that we used to register

	 * with hvc in hvc_instantiate() and hvc_alloc(); this is the

	 * number passed on by the hvc callbacks to us to

	 * differentiate between the other console ports handled by

	 * this driver

 size of the buffer in *buf above */

 used length of the buffer */

 offset in the buf from which to consume data */

 DMA address of buffer */

 Device we got DMA memory from */

 List of pending dma buffers to free */

 If sgpages == 0 then buf is used */

 sg is used if spages > 0. sg must be the last in is struct */

/*

 * This is a per-device struct that stores data common to all the

 * ports for that device (vdev->priv).

 Next portdev in the list, head is in the pdrvdata struct */

	/*

	 * Workqueue handlers where we process deferred work after

	 * notification

 To protect the list of ports */

 To protect the vq operations for the control channel */

 max. number of ports this device can hold */

 The virtio device we're associated with */

	/*

	 * A couple of virtqueues for the control channel: one for

	 * guest->host transfers, one for host->guest transfers

	/*

	 * A control packet buffer for guest->host requests, protected

	 * by c_ovq_lock.

 Array of per-port IO virtqueues */

 Major number for this device.  Ports will be created as minors. */

 This struct holds the per-port data */

 Next port in the list, head is in the ports_device */

 Pointer to the parent virtio_console device */

 The current buffer from which data has to be fed to readers */

	/*

	 * To protect the operations on the in_vq associated with this

	 * port.  Has to be a spinlock because it can be called from

	 * interrupt context (get_char()).

 Protect the operations on the out_vq. */

 The IO vqs for this port */

 File in the debugfs directory that exposes this port's information */

	/*

	 * Keep count of the bytes sent, received and discarded for

	 * this port for accounting and debugging purposes.  These

	 * counts are not reset across port open / close events.

	/*

	 * The entries in this struct will be valid if this port is

	 * hooked up to an hvc console

 Each port associates with a separate char device */

 Reference-counting to handle port hot-unplugs and file operations */

 A waitqueue for poll() or blocking read operations */

 The 'name' of the port that we expose via sysfs properties */

 We can notify apps of host connect / disconnect events via SIGIO */

 The 'id' to identify the port with the Host */

 Is the host device open */

 We should allow only one process to open a port */

 This is the very early arch-specified put chars function. */

	/*

	 * This condition can be true when put_chars is called from

	 * early_init

 dma_free_coherent requires interrupts to be enabled. */

 queue up dma-buffers to be freed later */

 Release device refcnt and allow it to be freed */

 Create a copy of the pending_free_dma_bufs while holding the lock */

 Release the dma buffers, without irqs enabled */

	/*

	 * Allocate buffer and the sg list. The sg list array is allocated

	 * directly after the port_buffer struct.

		/*

		 * Allocate DMA memory from ancestor. When a virtio

		 * device is created by remoteproc, the DMA memory is

		 * associated with the parent device:

		 * virtioY => remoteprocX#vdevYbuffer.

 Increase device refcnt to avoid freeing it */

 Callers should take appropriate locks */

/*

 * Create a scatter-gather list representing our input buffer and put

 * it in the queue.

 *

 * Callers should take appropriate locks.

 Discard any unread data this port has. Callers lockers. */

 Device has been unplugged.  vqs are already gone. */

 Did the port get unplugged before userspace closed it? */

 Callers must take the port->outvq_lock */

 Device has been unplugged.  vqs are already gone. */

 Tell Host to go! */

	/*

	 * Wait till the host acknowledges it pushed out the data we

	 * sent.  This is done for data from the hvc_console; the tty

	 * operations are performed with spinlocks held so we can't

	 * sleep here.  An alternative would be to copy the data to a

	 * buffer and relax the spinning requirement.  The downside is

	 * we need to kmalloc a GFP_ATOMIC buffer each time the

	 * console driver writes something out.

	/*

	 * We're expected to return the amount of data we wrote -- all

	 * of it

/*

 * Give out the data that's requested from the buffer that we have

 * queued up.

		/*

		 * We're done using all the data in this buffer.

		 * Re-queue so that the Host can send us more data.

 Return the number of bytes actually copied */

 The condition that must be true for polling to end */

 Port got hot-unplugged. Let's exit. */

 Port got hot-unplugged. Let's exit. */

	/*

	 * Check if the Host has consumed any buffers since we last

	 * sent data (this is only applicable for nonblocking ports).

 Port is hot-unplugged. */

		/*

		 * If nothing's connected on the host just return 0 in

		 * case of list_empty; this tells the userspace app

		 * that there's no connection

 Port got hot-unplugged while we were waiting above. */

	/*

	 * We could've received a disconnection message while we were

	 * waiting for more data.

	 *

	 * This check is not clubbed in the if() statement above as we

	 * might receive some data as well as the host could get

	 * disconnected after we got woken up from our wait.  So we

	 * really want to give off whatever data we have and only then

	 * check for host_connected.

 Port got hot-unplugged. */

 Userspace could be out to fool us */

	/*

	 * We now ask send_buf() to not spin for generic ports -- we

	 * can re-use the same code path that non-blocking file

	 * descriptors take for blocking file descriptors since the

	 * wait is already done and we're certain the write will go

	 * through to the host.

 Try lock this page */

 Get reference and unlock page for moving */

 Failback to copying a page */

 Faster zero-copy write by splicing */

	/*

	 * Rproc_serial does not yet support splice. To support splice

	 * pipe_to_sg() must allocate dma-buffers and copy content from

	 * regular pages to dma pages. And alloc_buf and free_buf must

	 * support allocating and freeing such a list of dma-buffers.

 Port got unplugged */

 Notify host of port being closed */

	/*

	 * Locks aren't necessary here as a port can't be opened after

	 * unplug, and if a port isn't unplugged, a kref would already

	 * exist for the port.  Plus, taking ports_lock here would

	 * create a dependency on other locks taken by functions

	 * inside remove_port if we're the last holder of the port,

	 * creating many problems.

 We get the port with a kref here */

 Port was unplugged before we could proceed */

	/*

	 * Don't allow opening of console port devices -- that's done

	 * via /dev/hvc

 Allow only one process to open a particular port at a time */

	/*

	 * There might be a chance that we missed reclaiming a few

	 * buffers in the window of the port getting previously closed

	 * and opening now.

 Notify host of port being opened */

/*

 * The file operations that we support: programs in the guest can open

 * a console device, read from it, write to it, poll for data and

 * close it.  The devices are at

 *   /dev/vport<device number>p<port number>

/*

 * The put_chars() callback is pretty straightforward.

 *

 * We turn the characters into a scatter-gather list, add it to the

 * output queue and then kick the Host.  Then we sit here waiting for

 * it to finish: inefficient in theory, but in practice

 * implementations will do it immediately.

/*

 * get_chars() is the callback from the hvc_console infrastructure

 * when an interrupt is received.

 *

 * We call out to fill_readbuf that gets us the required data from the

 * buffers that are queued up.

 If we've not set up the port yet, we have no input to give. */

 If we don't have an input queue yet, we can't get input. */

 The port could have been hot-unplugged */

 Don't test F_SIZE at all if we're rproc: not a valid feature! */

 We set the configuration at this point, since we now have a tty */

 The operations for console ports. */

/*

 * Console drivers are initialized very early so boot messages can go

 * out, so we do things slightly differently from the generic virtio

 * initialization of the net and block drivers.

 *

 * At this stage, the console is output-only.  It's too early to set

 * up a virtqueue, so we let the drivers do some boutique early-output

 * thing.

	/*

	 * The Host's telling us this port is a console port.  Hook it

	 * up with an hvc console.

	 *

	 * To set up and manage our virtual console, we call

	 * hvc_alloc().

	 *

	 * The first argument of hvc_alloc() is the virtual console

	 * number.  The second argument is the parameter for the

	 * notification mechanism (like irq number).  We currently

	 * leave this as zero, virtqueues have implicit notifications.

	 *

	 * The third argument is a "struct hv_ops" containing the

	 * put_chars() get_chars(), notifier_add() and notifier_del()

	 * pointers.  The final argument is the output buffer size: we

	 * can do any size, so we put PAGE_SIZE here.

	/*

	 * Start using the new console output if this is the first

	 * console to come up.

 Notify host of port being opened */

 put in device directory */

	/* We can safely ignore ENOSPC because it means

	 * the queue already has buffers. Buffers are removed

	 * only by virtcons_remove(), not by unplug_port()

		/*

		 * For rproc_serial assume remote processor is connected.

		 * rproc_serial does not want the console port, only

		 * the generic port implementation.

		/*

		 * If we're not using multiport support,

		 * this has to be a console port.

	/*

	 * Tell the Host we're set so that it can send us various

	 * configuration parameters for this port (eg, port name,

	 * caching, whether this is a console port, etc.)

	/*

	 * Finally, create the debugfs file that we can use to

	 * inspect a port's state at any time

 The host might want to notify management sw about port add failure */

 No users remain, remove all port-specific data. */

 Remove unused data this port might have received. */

/*

 * Port got unplugged.  Remove port from portdev's list and drop the

 * kref reference.  If no userspace has this port opened, it will

 * result in immediate removal the port.

 Let the app know the port is going down. */

 Do this after sigio is actually sent */

	/*

	 * We should just assume the device itself has gone off --

	 * else a close on an open port later will try to send out a

	 * control message.

	/*

	 * Locks around here are not necessary - a port can't be

	 * opened after we removed the port struct from ports_list

	 * above.

 Any private messages that the Host and Guest want to share */

 No valid header at start of buffer.  Drop it. */

		/*

		 * Could remove the port here in case init fails - but

		 * have to notify the host first.

		/*

		 * If the host port got closed and the host had any

		 * unconsumed buffers, we'll be able to reclaim them

		 * now.

		/*

		 * If the guest is connected, it'll be interested in

		 * knowing the host connection state changed.

		/*

		 * If we woke up after hibernation, we can get this

		 * again.  Skip it in that case.

		/*

		 * Skip the size of the header and the cpkt to get the size

		 * of the name that was sent

		/*

		 * Since we only have one sysfs attribute, 'name',

		 * create it only if we have a name for the port.

			/*

			 * Generate a udev event so that appropriate

			 * symlinks can be created based on udev

			 * rules.

	/*

	 * Normally the port should not accept data when the port is

	 * closed. For generic serial ports, the host won't (shouldn't)

	 * send data till the guest is connected. But this condition

	 * can be reached when a console port is not yet connected (no

	 * tty is spawned) and the other side sends out data over the

	 * vring, or when a remote devices start sending data before

	 * the ports are opened.

	 *

	 * A generic serial port will discard data if not connected,

	 * while console ports and rproc-serial ports accepts data at

	 * any time. rproc-serial is initiated with guest_connected to

	 * false because port_fops_open expects this. Console ports are

	 * hooked up with an HVC console and is initialized with

	 * guest_connected to true.

 Send a SIGIO indicating new data in case the process asked for it */

		/*

		 * We'll use this way of resizing only for legacy

		 * support.  For newer userspace

		 * (VIRTIO_CONSOLE_F_MULTPORT+), use control messages

		 * to indicate console size changes so that it can be

		 * done per-port.

	/*

	 * For backward compat (newer host but older guest), the host

	 * spawns a console port first and also inits the vqs for port

	 * 0 before others.

 Find the queues. */

 Disable interrupts for vqs */

 Finish up work that's lined up */

	/*

	 * When yanking out a device, we immediately lose the

	 * (device-side) queues.  So there's no point in keeping the

	 * guest side around till we drop our final reference.  This

	 * also means that any ports which are in an open state will

	 * have to just stop using the port, as the vqs are going

	 * away.

/*

 * Once we're further in boot, we get probed like any other virtio

 * device.

 *

 * If the host also supports multiple console ports, we check the

 * config space to see how many ports the host has spawned.  We

 * initialize each port found.

 We only need a config space if features are offered */

 Ensure to read early_put_chars now */

 Attach this portdev to this virtio_device, and vice-versa. */

 Don't test MULTIPORT at all if we're rproc: not a valid feature! */

			/*

			 * The host might want to notify mgmt sw about device

			 * add failure.

 Device was functional: we need full cleanup. */

		/*

		 * For backward compatibility: Create a console port

		 * if we're running on older host.

	/*

	 * If there was an early virtio console, assume that there are no

	 * other consoles. We need to wait until the hvc_alloc matches the

	 * hvc_instantiate, otherwise tty_open will complain, resulting in

	 * a "Warning: unable to open an initial console" boot failure.

	 * Without multiport this is done in add_port above. With multiport

	 * this might take some host<->guest communication - thus we have to

	 * wait.

	/*

	 * Once more: if control_work_handler() was running, it would

	 * enable the cb as the last step.

		/*

		 * We'll ask the host later if the new invocation has

		 * the port opened or closed.

 Get port open/close status on the host */

		/*

		 * If a port was open at the time of suspending, we

		 * have to let the host know that it's still open.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2001-2006 Silicon Graphics, Inc.  All rights

 * reserved.

/*

 * SN Platform Special Memory (mspec) Support

 *

 * This driver exports the SN special memory (mspec) facility to user

 * processes.

 * There are two types of memory made available thru this driver:

 * uncached and cached.

 *

 * Uncached are used for memory write combining feature of the ia64

 * cpu.

 *

 * Cached are used for areas of memory that are used as cached addresses

 * on our partition and used as uncached addresses from other partitions.

 * Due to a design constraint of the SN2 Shub, you can not have processors

 * on the same FSB perform both a cached and uncached reference to the

 * same cache line.  These special memory cached regions prevent the

 * kernel from ever dropping in a TLB entry and therefore prevent the

 * processor from ever speculating a cache line from this page.

/*

 * Page types allocated by the device.

/*

 * One of these structures is allocated when an mspec region is mmaped. The

 * structure is pointed to by the vma->vm_private_data field in the vma struct.

 * This structure is used to record the addresses of the mspec pages.

 * This structure is shared by all vma's that are split off from the

 * original vma when split_vma()'s are done.

 *

 * The refcnt is incremented atomically because mm->mmap_lock does not

 * protect in fork case where multiple tasks share the vma_data.

 Number of vmas sharing the data. */

 Serialize access to this structure. */

 Number of pages allocated. */

 Type of pages allocated. */

 Original (unsplit) base. */

 Original (unsplit) end. */

 Array of MSPEC addresses. */

/*

 * mspec_open

 *

 * Called when a device mapping is created by a means other than mmap

 * (via fork, munmap, etc.).  Increments the reference count on the

 * underlying mspec data so it is not freed prematurely.

/*

 * mspec_close

 *

 * Called when unmapping a device mapping. Frees all mspec pages

 * belonging to all the vma's sharing this vma_data structure.

		/*

		 * Clear the page before sticking it back

		 * into the pool.

/*

 * mspec_fault

 *

 * Creates a mspec page and maps it to user space.

/*

 * mspec_mmap

 *

 * Called when mmapping the device.  Initializes the vma with a fault handler

 * and private data structure necessary to allocate, track, and free the

 * underlying pages.

/*

 * mspec_init

 *

 * Called at boot time to initialize the mspec facility.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel & MS High Precision Event Timer Implementation.

 *

 * Copyright (C) 2003 Intel Corporation

 *	Venki Pallipadi

 * (c) Copyright 2004 Hewlett-Packard Development Company, L.P.

 *	Bob Picco <robert.picco@hp.com>

/*

 * The High Precision Event Timer driver.

 * This driver is closely modelled after the rtc.c driver.

 * See HPET spec revision 1.

 from HPET spec */

/* WARNING -- don't get confused.  These macros are never used

 * to write the (single) counter, and rarely to read it.

 * They're badly named; to fix, someday.

 replaces BKL */

 This clocksource driver currently only works on ia64 */

 A lock for concurrent access by app and isr hpet activity. */

 interrupt enabled */

	/*

	 * For non-periodic timers, increment the accumulator.

	 * This has the effect of treating non-periodic like periodic.

		/* The time for the next interrupt would logically be t + m,

		 * however, if we are very unlucky and the interrupt is delayed

		 * for longer than t then we will completely miss the next

		 * interrupt if we set t + m and an application will hang.

		 * Therefore we need to make a more complex computation assuming

		 * that there exists a k for which the following is true:

		 * k * t + base < mc + delta

		 * (k + 1) * t + base > mc + delta

		 * where t is the interval in hpet ticks for the given freq,

		 * base is the theoretical start value 0 < base < t,

		 * mc is the main counter value at the time of the interrupt,

		 * delta is the time it takes to write the a value to the

		 * comparator.

		 * k may then be computed as (mc - base + delta) / t .

 we prefer level triggered mode */

	/*

	 * In PIC mode, skip IRQ0-4, IRQ6-9, IRQ12-15 which is always used by

	 * legacy device. In IO APIC mode, we skip all the legacy IRQS.

 FIXME: Setup interrupt source table */

			/*

			 * To prevent the interrupt handler from seeing an

			 * unwanted interrupt status bit, program the timer

			 * so that it will not fire in the near future ...

 ... and clear any left-over status. */

	/* 64-bit comparators are not yet supported through the ioctls,

	 * so force this into 32-bit mode if it supports both modes

		/*

		 * NOTE: First we modify the hidden accumulator

		 * register supported by periodic-capable comparators.

		 * We never want to modify the (single) counter; that

		 * would affect all the comparators. The value written

		 * is the counter value when the first interrupt is due.

		/*

		 * Then we modify the comparator, indicating the period

		 * for subsequent interrupt.

 converts Hz to number of timer ticks */

 Hz */

 information */

/*

 * Adjustment for when arming the timer with

 * initial conditions.  That is, main counter

 * ticks expired before interrupts are enabled.

	/*

	 * Try to calibrate until return value becomes stable small value.

	 * If SMI interruption occurs in calibration loop, the return value

	 * will be big. This avoids its impact.

	/*

	 * hpet_alloc can be called by platform dependent code.

	 * If platform dependent code has allocated the hpet that

	 * ACPI has also reported, then we catch it here.

 fs, 10^-15 */

 10^15 femtoseconds per second */

 round */

 ticks per second */

		/*

		 * If the timer was reserved by platform code,

		 * then make timer unavailable for opens.

 This clocksource driver currently only works on ia64 */

/*

MODULE_AUTHOR("Bob Picco <Robert.Picco@hp.com>");

MODULE_LICENSE("GPL");

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PS3 FLASH ROM Storage Driver

 *

 * Copyright (C) 2007 Sony Computer Entertainment Inc.

 * Copyright 2007 Sony Corp.

 Bounce buffer mutex */

 Start sector of buffer, -1 if invalid */

 Make kernel writes synchronous */

 use static buffer, kmalloc cannot allocate 256 KiB */

/*

 * Telecom Clock driver for Intel NetStructure(tm) MPCBL0010

 *

 * Copyright (C) 2005 Kontron Canada

 *

 * All rights reserved.

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or (at

 * your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

 *

 * Send feedback to <sebastien.bouchard@ca.kontron.com> and the current

 * Maintainer  <mark.gross@intel.com>

 *

 * Description : This is the TELECOM CLOCK module driver for the ATCA

 * MPCBL0010 ATCA computer.

 printk() */

 everything... */

 error codes */

 inb/outb */

Hardware Reset of the PLL */

 MODE SELECT */

 FILTER SELECT */

 SELECT REFERENCE FREQUENCY */

 Select primary or secondary redundant clock */

 CLOCK TRANSMISSION DEFINE */

 RECEIVED REFERENCE */

 HARDWARE SWITCHING DEFINE */

 HARDWARE SWITCHING MODE DEFINE */

 ALARMS DEFINE */

 INTERRUPT CAUSE DEFINE */

 Telecom clock I/O register definition */

 0 = Dynamic allocation of the major device number */

/* sysfs interface definition:

Upon loading the driver will create a sysfs directory under

/sys/devices/platform/telco_clock.



This directory exports the following interfaces.  There operation is

documented in the MCPBL0010 TPS under the Telecom Clock API section, 11.4.

alarms				:

current_ref			:

received_ref_clk3a		:

received_ref_clk3b		:

enable_clk3a_output		:

enable_clk3b_output		:

enable_clka0_output		:

enable_clka1_output		:

enable_clkb0_output		:

enable_clkb1_output		:

filter_select			:

hardware_switching		:

hardware_switching_mode		:

telclock_version		:

mode_select			:

refalign			:

reset				:

select_amcb1_transmit_clock	:

select_amcb2_transmit_clock	:

select_redundant_clock		:

select_ref_frequency		:



All sysfs interfaces are integers in hex format, i.e echo 99 > refalign

has the same effect as echo 0x99 > refalign.

 Event that generate a interrupt */

 if events processing have been done */

		/* this legacy device is always one per system and it doesn't

		 * know how to handle multiple concurrent clients.

	/* Make sure there is no interrupt pending while

	/* This device is wired through the FPGA IO space of the ATCA blade

 Clear interrupt events */

 put in device directory */

 Read telecom clock IRQ number (Set by BIOS) */

 not MCPBL0010 ? */

 Alarm processing is done, wake up read task */

 Read and clear interrupt events */

 Primary_Los changed from 0 to 1 ? */

 Primary_Los changed from 1 to 0 ? */

 Secondary_Los changed from 0 to 1 ? */

 Secondary_Los changed from 1 to 0 ? */

 Holdover changed from 0 to 1 ? */

 TIMEOUT in ~10ms */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/drivers/char/mem.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  Added devfs support.

 *    Jan-11-1998, C. Scott Ananian <cananian@alumni.princeton.edu>

 *  Shared /dev/zero mmapping support, Feb 2000, Kanoj Sarcar <kanoj@sgi.com>

/*

 * This funcion reads the *physical* memory. The f_pos points directly to the

 * memory location.

 we don't have page 0 mapped on sparc and m68k.. */

 Show zeros for restricted memory. */

			/*

			 * On ia64 if a page has been mapped somewhere as

			 * uncached, then it must also be accessed uncached

			 * by the kernel or data corruption may occur.

 we don't have page 0 mapped on sparc and m68k.. */

 Hmm. Do something? */

 Skip actual writing when a page is marked as restricted. */

			/*

			 * On ia64 if a page has been mapped somewhere as

			 * uncached, then it must also be accessed uncached

			 * by the kernel or data corruption may occur.

/*

 * Architectures vary in how they handle caching for addresses

 * outside of main memory.

 *

	/*

	 * On ia64, we ignore O_DSYNC because we cannot tolerate memory

	 * attribute aliases.

	/*

	 * Accessing memory above the top the kernel knows about or through a

	 * file pointer

	 * that was marked O_DSYNC will be done non-cached.

 permit direct mmap, for read, write or exec */

 can't do an in-place private mapping if there's no MMU */

 Does it even fit in phys_addr_t? */

 It's illegal to wrap around the end of the physical address space. */

 Remap-pfn-range will mark the range VM_IO */

 Just for latency reasons */

		/*

		 * mmap_zero() will call shmem_zero_setup() to create a file,

		 * so use shmem's get_unmapped_area in case it can be huge;

		 * and pass NULL for file as in mmap.c's get_unmapped_area(),

		 * so as not to confuse shmem with our handle on "/dev/zero".

 Otherwise flags & MAP_PRIVATE: with no shmem object beneath it */

/*

 * Special lseek() function for /dev/null and /dev/zero.  Most notably, you

 * can fopen() both devices with "a" now.  This was previously impossible.

 * -- SRB.

/*

 * The memory devices use the full 32/64 bits of the offset, and so we cannot

 * check against negative addresses: they are ok. The return value is weird,

 * though, in that case (0).

 *

 * also note that seeking relative to the "end of file" isn't supported:

 * it has no meaning, so it returns -EINVAL.

 to avoid userland mistaking f_pos=-9 as -EBADF=-9 */

	/*

	 * Use a unified address space to have a single point to manage

	 * revocations when drivers want to take over a /dev/mem mapped

	 * range.

		/*

		 * Create /dev/port?

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic parallel printer driver

 *

 * Copyright (C) 1992 by Jim Weigand and Linus Torvalds

 * Copyright (C) 1992,1993 by Michael K. Johnson

 * - Thanks much to Gunter Windau for pointing out to me where the error

 *   checking ought to be.

 * Copyright (C) 1993 by Nigel Gamble (added interrupt code)

 * Copyright (C) 1994 by Alan Cox (Modularised it)

 * LPCAREFUL, LPABORT, LPGETSTATUS added by Chris Metcalf, metcalf@lcs.mit.edu

 * Statistics and support for slow printers by Rob Janssen, rob@knoware.nl

 * "lp=" command line parameters added by Grant Guenther, grant@torque.net

 * lp_read (Status readback) support added by Carsten Gross,

 *                                             carsten@sol.wohnheim.uni-ulm.de

 * Support for parport by Philip Blundell <philb@gnu.org>

 * Parport sharing hacking by Andrea Arcangeli

 * Fixed kernel_(to/from)_user memory copy to check for errors

 * 				by Riccardo Facchetti <fizban@tin.it>

 * 22-JAN-1998  Added support for devfs  Richard Gooch <rgooch@atnf.csiro.au>

 * Redesigned interrupt handling for handle printers with buggy handshake

 *				by Andrea Arcangeli, 11 May 1998

 * Full efficient handling of printer with buggy irq handshake (now I have

 * understood the meaning of the strange handshake). This is done sending new

 * characters if the interrupt is just happened, even if the printer say to

 * be still BUSY. This is needed at least with Epson Stylus Color. To enable

 * the new TRUST_IRQ mode read the `LP OPTIMIZATION' section below...

 * Fixed the irq on the rising edge of the strobe case.

 * Obsoleted the CAREFUL flag since a printer that doesn' t work with

 * CAREFUL will block a bit after in lp_check_status().

 *				Andrea Arcangeli, 15 Oct 1998

 * Obsoleted and removed all the lowlevel stuff implemented in the last

 * month to use the IEEE1284 functions (that handle the _new_ compatibilty

 * mode fine).

/* This driver should, in theory, work with any parallel port that has an

 * appropriate low-level driver; all I/O is done through the parport

 * abstraction layer.

 *

 * If this driver is built into the kernel, you can configure it using the

 * kernel command-line.  For example:

 *

 *	lp=parport1,none,parport2	(bind lp0 to parport1, disable lp1 and

 *					 bind lp2 to parport2)

 *

 *	lp=auto				(assign lp devices to all ports that

 *				         have printers attached, as determined

 *					 by the IEEE-1284 autoprobe)

 *

 *	lp=reset			(reset the printer during

 *					 initialisation)

 *

 *	lp=off				(disable the printer driver entirely)

 *

 * If the driver is loaded as a module, similar functionality is available

 * using module parameters.  The equivalent of the above commands would be:

 *

 *	# insmod lp.o parport=1,none,2

 *

 *	# insmod lp.o parport=auto

 *

 *	# insmod lp.o reset=1

/* COMPATIBILITY WITH OLD KERNELS

 *

 * Under Linux 2.0 and previous versions, lp devices were bound to ports at

 * particular I/O addresses, as follows:

 *

 *	lp0		0x3bc

 *	lp1		0x378

 *	lp2		0x278

 *

 * The new driver, by default, binds lp devices to parport devices as it

 * finds them.  This means that if you only have one port, it will be bound

 * to lp0 regardless of its I/O address.  If you need the old behaviour, you

 * can force it using the parameters described above.

/*

 * The new interrupt handling code take care of the buggy handshake

 * of some HP and Epson printer:

 * ___

 * ACK    _______________    ___________

 *                       |__|

 * ____

 * BUSY   _________              _______

 *                 |____________|

 *

 * I discovered this using the printer scanner that you can find at:

 *

 *	ftp://e-mind.com/pub/linux/pscan/

 *

 *					11 May 98, Andrea Arcangeli

 *

 * My printer scanner run on an Epson Stylus Color show that such printer

 * generates the irq on the _rising_ edge of the STROBE. Now lp handle

 * this case fine too.

 *

 *					15 Oct 1998, Andrea Arcangeli

 *

 * The so called `buggy' handshake is really the well documented

 * compatibility mode IEEE1284 handshake. They changed the well known

 * Centronics handshake acking in the middle of busy expecting to not

 * break drivers or legacy application, while they broken linux lp

 * until I fixed it reverse engineering the protocol by hand some

 * month ago...

 *

 *                                     14 Dec 1998, Andrea Arcangeli

 *

 * Copyright (C) 2000 by Tim Waugh (added LPSETTIMEOUT ioctl)

 if you have more than 8 printers, remember to increase LP_NO */

 CONFIG_LP_CONSOLE */

 Bits used to manage claiming the parport device */

 --- low-level port access ----------------------------------- */

 Claim the parport or block trying unless we've already claimed it */

 Claim the parport or block trying unless we've already claimed it */

/*

 * Try to negotiate to a new mode; if unsuccessful negotiate to

 * compatibility mode.  Return the mode we ended up in.

 No error. */

		last = 0; /* Come here if LP_CAREFUL is set and no

 If we're not in compatibility mode, we're ready now! */

 Need to copy the data from user-space. */

	/* Claim Parport or sleep until it becomes available

 Go to the proper mode. */

 Write the data. */

 incomplete write -> check error ! */

 Status readback conforming to ieee1284 */

 Wait for data. */

 IEEE 1284 support */

	/* If ABORTOPEN is set and the printer is offline or out of paper,

	   we may still want to open it to perform ioctl()s.  Therefore we

	   have commandeered O_NONBLOCK, even though it is being used in

	   a non-standard manner.  This is strictly a Linux hack, and

 Determine if the peripheral supports ECP mode */

 Leave peripheral in compatibility mode */

 Convert to jiffies, place in lp_table */

	/*

	 * we used to not check, so let's not make this fatal,

	 * but deal with user space passing a 32-bit tv_nsec in

	 * a 64-bit field, capping the timeout to 1 second

	 * worth of microseconds, and capping the total at

	 * MAX_JIFFY_OFFSET.

 sparc64 suseconds_t is 32-bit only */

 for 64-bit */

 for x32 mode */

 FIXME: add an implementation if you set LP_STATS */

 --- support for console on the line printer ----------------- */

/* If the printer is out of paper, we can either lose the messages or

 * stall until the printer is happy again.  Define CONSOLE_LP_STRICT

 The console must be locked when we get here. */

 Nothing we can do. */

 Go to compatibility mode. */

 Write the data, converting LF->CRLF as we go. */

 Dodge the original '\n', and put '\r\n' instead. */

 console on line printer */

 --- initialisation code ------------------------------------- */

 disable driver on "lp=" or "lp=0" */

 Write this some day. */

 CONFIG_LP_CONSOLE */

 The user gave some parameters.  Let's see what they were.  */

 SPDX-License-Identifier: GPL-2.0-or-later

/* IBM POWER Barrier Synchronization Register Driver

 *

 * Copyright IBM Corporation 2008

 *

 * Author: Sonny Rao <sonnyrao@us.ibm.com>

/*

 This driver exposes a special register which can be used for fast

 synchronization across a large SMP machine.  The hardware is exposed

 as an array of bytes where each process will write to one of the bytes to

 indicate it has finished the current stage and this update is broadcast to

 all processors without having to bounce a cacheline between them. In

 POWER5 and POWER6 there is one of these registers per SMP,  but it is

 presented in two forms; first, it is given as a whole and then as a number

 of smaller registers which alias to parts of the single whole register.

 This can potentially allow multiple groups of processes to each have their

 own private synchronization device.



 Note that this hardware *must* be written to using *only* single byte writes.

 It may be read using 1, 2, 4, or 8 byte loads which must be aligned since

 this region is treated as cache-inhibited  processes should also use a

 full sync before and after writing to the BSR to ensure all stores and

 the BSR update have made it to all chips in the system

 This is arbitrary number, up to Power6 it's been 17 or fewer  */

 Real address */

 length of mem region we can map */

 size of the BSR reg itself */

 interval at which BSR repeats in the page */

 maps to enum below */

 bsr id number for its type */

 check for the case of a small BSR device and map one 4k page for it*/

 if we have a bsr_len of > 4k and less then PAGE_SIZE (64k pages) */

 we can only map 4k of it, so only advertise the 4k in sysfs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/drivers/char/ds1620.c: Dallas Semiconductors DS1620

 *   thermometer driver (as used in the Rebel.com NetWinder)

 define for /proc interface */

 Definitions for DS1620 chip */

/*

 * Start of NetWinder specifics

 *  Note!  We have to hold the gpio lock with IRQs disabled over the

 *  whole of our transaction to the Dallas chip, since there is a

 *  chance that the WaveArtist driver could touch these bits to

 *  enable or disable the speaker.

/*

 * End of NetWinder specifics

 convert to Fahrenheit, as per wdt.c */

	/*

	 * Trigger the fan to start by setting

	 * temperature high point low.  This kicks

	 * the fan into action.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * bios-less APM driver for ARM Linux

 *  Jamey Hicks <jamey@crl.dec.com>

 *  adapted from the APM BIOS driver for Linux by Stephen Rothwell (sfr@linuxcare.com)

 *

 * APM 1.2 Reference:

 *   Intel Corporation, Microsoft Corporation. Advanced Power Management

 *   (APM) BIOS Interface Specification, Revision 1.2, February 1996.

 *

 * This document is available from Microsoft at:

 *    http://www.microsoft.com/whdc/archive/amp_12.mspx

/*

 * One option can be changed at boot time as follows:

 *	apm=on/off			enable/disable APM

/*

 * Maximum number of events stored

/*

 * thread states (for threads using a writable /dev/apm_bios fd):

 *

 * SUSPEND_NONE:	nothing happening

 * SUSPEND_PENDING:	suspend event queued for thread and pending to be read

 * SUSPEND_READ:	suspend event read, pending acknowledgement

 * SUSPEND_ACKED:	acknowledgement received from thread (via ioctl),

 *			waiting for resume

 * SUSPEND_ACKTO:	acknowledgement timeout

 * SUSPEND_DONE:	thread had acked suspend and is now notified of

 *			resume

 *

 * SUSPEND_WAIT:	this thread invoked suspend and is waiting for resume

 *

 * A thread migrates in one of three paths:

 *	NONE -1-> PENDING -2-> READ -3-> ACKED -4-> DONE -5-> NONE

 *				    -6-> ACKTO -7-> NONE

 *	NONE -8-> WAIT -9-> NONE

 *

 * While in PENDING or READ, the thread is accounted for in the

 * suspend_acks_pending counter.

 *

 * The transitions are invoked as follows:

 *	1: suspend event is signalled from the core PM code

 *	2: the suspend event is read from the fd by the userspace thread

 *	3: userspace thread issues the APM_IOC_SUSPEND ioctl (as ack)

 *	4: core PM code signals that we have resumed

 *	5: APM_IOC_SUSPEND ioctl returns

 *

 *	6: the notifier invoked from the core PM code timed out waiting

 *	   for all relevant threds to enter ACKED state and puts those

 *	   that haven't into ACKTO

 *	7: those threads issue APM_IOC_SUSPEND ioctl too late,

 *	   get an error

 *

 *	8: userspace thread issues the APM_IOC_SUSPEND ioctl (to suspend),

 *	   ioctl code invokes pm_suspend()

 *	9: pm_suspend() returns indicating resume

/*

 * The per-file APM data

/*

 * Local variables

/*

 * This is a list of everyone who has opened /dev/apm_bios

/*

 * kapmd info.  kapmd provides us a process context to handle

 * "APM" events within - specifically necessary if we're going

 * to be suspending the system.

 no spaces */

/*

 * Compatibility cruft until the IPAQ people move over to the new

 * interface.

/*

 * This allows machines to provide their own "apm get power status" function.

/*

 * APM event queue management.

/*

 * apm_ioctl - handle APM ioctl

 *

 * APM_IOC_SUSPEND

 *   This IOCTL is overloaded, and performs two functions.  It is used to:

 *     - initiate a suspend

 *     - acknowledge a suspend read from /dev/apm_bios.

 *   Only when everyone who has opened /dev/apm_bios with write permission

 *   has acknowledge does the actual suspend happen.

			/*

			 * If we read a suspend command from /dev/apm_bios,

			 * then the corresponding APM_IOC_SUSPEND ioctl is

			 * interpreted as an acknowledge.

			/*

			 * suspend_acks_pending changed, the notifier needs to

			 * be woken up for this

			/*

			 * Wait for the suspend/resume to complete.  If there

			 * are pending acknowledges, we wait here for them.

			 * wait_event_freezable() is interruptible and pending

			 * signal can cause busy looping.  We aren't doing

			 * anything critical, chill a bit on each iteration.

			/*

			 * Otherwise it is a request to suspend the system.

			 * Just invoke pm_suspend(), we'll handle it from

			 * there via the notifier.

	/*

	 * We are now unhooked from the chain.  As far as new

	 * events are concerned, we no longer exist.

		/*

		 * XXX - this is a tiny bit broken, when we consider BSD

		 * process accounting. If the device is opened by root, we

		 * instantly flag that we used superuser privs. Who knows,

		 * we might close the device immediately without doing a

		 * privileged operation -- cevans

/*

 * Arguments, with symbols from linux/apm_bios.h.

 *

 *   0) Linux driver version (this will change if format changes)

 *   1) APM BIOS Version.  Usually 1.0, 1.1 or 1.2.

 *   2) APM flags from APM Installation Check (0x00):

 *	bit 0: APM_16_BIT_SUPPORT

 *	bit 1: APM_32_BIT_SUPPORT

 *	bit 2: APM_IDLE_SLOWS_CLOCK

 *	bit 3: APM_BIOS_DISABLED

 *	bit 4: APM_BIOS_DISENGAGED

 *   3) AC line status

 *	0x00: Off-line

 *	0x01: On-line

 *	0x02: On backup power (BIOS >= 1.1 only)

 *	0xff: Unknown

 *   4) Battery status

 *	0x00: High

 *	0x01: Low

 *	0x02: Critical

 *	0x03: Charging

 *	0x04: Selected battery not present (BIOS >= 1.2 only)

 *	0xff: Unknown

 *   5) Battery flag

 *	bit 0: High

 *	bit 1: Low

 *	bit 2: Critical

 *	bit 3: Charging

 *	bit 7: No system battery

 *	0xff: Unknown

 *   6) Remaining battery life (percentage of charge):

 *	0-100: valid

 *	-1: Unknown

 *   7) Remaining battery life (time units):

 *	Number of remaining minutes or seconds

 *	-1: Unknown

 *   8) min = minutes; sec = seconds

 short-cut emergency suspends */

		/*

		 * Queue an event to all "writer" users that we want

		 * to suspend and need their ack.

		/*

		 * Wait for the the suspend_acks_pending variable to drop to

		 * zero, meaning everybody acked the suspend event (or the

		 * process was killed.)

		 *

		 * If the app won't answer within a short while we assume it

		 * locked up and ignore it.

 timed out */

			/*

			 * Move anybody who timed out to "ack timeout" state.

			 *

			 * We could time out and the userspace does the ACK

			 * right after we time out but before we enter the

			 * locked section here, but that's fine.

 let suspend proceed */

 interrupted by signal */

		/*

		 * Anyone on the APM queues will think we're still suspended.

		 * Send a message so everyone knows we're now awake again.

		/*

		 * Finally, wake up anyone who is sleeping on the suspend.

				/*

				 * TODO: maybe grab error code, needs core

				 * changes to push the error to the notifier

				 * chain (could use the second parameter if

				 * implemented)

/**

 * apm_queue_event - queue an APM event for kapmd

 * @event: APM event

 *

 * Queue an APM event for kapmd to process and ultimately take the

 * appropriate action.  Only a subset of events are handled:

 *   %APM_LOW_BATTERY

 *   %APM_POWER_STATUS_CHANGE

 *   %APM_USER_SUSPEND

 *   %APM_SYS_SUSPEND

 *   %APM_CRITICAL_SUSPEND

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/drivers/misc/xillybus_pcie.c

 *

 * Copyright 2011 Xillybus Ltd, http://xillybus.com

 *

 * Driver for the Xillybus FPGA/host framework using PCI Express.

 End: all zeroes */ }

 L0s has caused packet drops. No power saving, thank you. */

 Set up a single MSI interrupt */

	/*

	 * Some (old and buggy?) hardware drops 64-bit addressed PCIe packets,

	 * even when the PCIe driver claims that a 64-bit mask is OK. On the

	 * other hand, on some architectures, 64-bit addressing is mandatory.

	 * So go for the 64-bit mask only when failing is the other option.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2021 Xillybus Ltd, http://xillybus.com

 *

 * Driver for the Xillybus class

 kobject_put() is normally done by cdev_del() */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2020 Xillybus Ltd, http://xillybus.com

 *

 * Driver for the XillyUSB FPGA/host framework.

 *

 * This driver interfaces with a special IP core in an FPGA, setting up

 * a pipe between a hardware FIFO in the programmable logic and a device

 * file in the host. The number of such pipes and their attributes are

 * set up on the logic. This driver detects these automatically and

 * creates the device files accordingly.

 In bytes, always a power of 2 */

 Lazy: Equals bufsize * bufnum */

 Number of bytes in the FIFO */

 serialize operations on endpoint */

 protect these two lists */

 protect @out_ep, @in_fifo, bit fields below */

 serialize fops on FPGA to host stream */

 serialize fops on host to FPGA stream */

 EOF not arrived (yet) */

 Bit fields protected by @lock except for initialization */

 For dev_err() and such */

 protect @error */

 serialize opcode transmission */

 synchronize wakeup_all() */

 FPGA to host opcodes */

 Host to FPGA opcodes */

/*

 * fifo_write() and fifo_read() are NOT reentrant (i.e. concurrent multiple

 * calls to each on the same FIFO is not allowed) however it's OK to have

 * threads calling each of the two functions once on the same FIFO, and

 * at the same time.

	/*

	 * The spinlock here is necessary, because otherwise fifo->fill

	 * could have been increased by fifo_write() after writing data

	 * to the buffer, but this data would potentially not have been

	 * visible on this thread at the time the updated fifo->fill was.

	 * That could lead to reading invalid data.

/*

 * These three wrapper functions are used as the @copier argument to

 * fifo_write() and fifo_read(), so that they can work directly with

 * user memory as well.

/*

 * When endpoint_quiesce() returns, the endpoint has no URBs submitted,

 * won't accept any new URB submissions, and its related work item doesn't

 * and won't run anymore.

/*

 * Note that endpoint_dealloc() also frees fifo memory (if allocated), even

 * though endpoint_alloc doesn't allocate that memory.

 Join @filled_buffers with @buffers to free these entries too */

 Argument may be NULL, and that's fine */

/*

 * @process_in_mutex is taken to ensure that bulk_in_work() won't call

 * process_bulk_in() after wakeup_all()'s execution: The latter zeroes all

 * @read_data_ok entries, which will make process_bulk_in() report false

 * errors if executed. The mechanism relies on that xdev->error is assigned

 * a non-zero value by report_io_error() prior to queueing wakeup_all(),

 * which prevents bulk_in_work() from calling process_bulk_in().

 *

 * The fact that wakeup_all() and bulk_in_work() are queued on the same

 * workqueue makes their concurrent execution very unlikely, however the

 * kernel's API doesn't seem to ensure this strictly.

			/*

			 * Fake an EOF: Even if such arrives, it won't be

			 * processed.

 xdev is used by work item */

/*

 * safely_assign_in_fifo() changes the value of chan->in_fifo and ensures

 * the previous pointer is never used after its return.

 This just decrements reference count */

		/*

		 * Race conditions might have the FIFO filled while the

		 * endpoint is marked as drained here. That doesn't matter,

		 * because the sole purpose of @drained is to ensure that

		 * certain data has been sent on the USB channel before

		 * shutting it down. Hence knowing that the FIFO appears

		 * to be empty with no outstanding URBs at some moment

		 * is good enough.

		/*

		 * xilly_memcpy always returns 0 => fifo_read can't fail =>

		 * count > 0

 This just decrements reference count */

		/*

		 * A write memory barrier ensures that the FIFO's fill level

		 * is visible before read_data_ok turns zero, so the data in

		 * the FIFO isn't missed by the consumer.

 We got really unexpected data */

	/*

	 * The wait queue is woken with the interruptible variant, so the

	 * wait function matches, however returning because of an interrupt

	 * will mess things up considerably, in particular when the caller is

	 * the release method. And the xdev->error part prevents being stuck

	 * forever in the event of a bizarre hardware bug: Pull the USB plug.

 Empty loop */

/*

 * Note that flush_downstream() merely waits for the data to arrive to

 * the application logic at the FPGA -- unlike PCIe Xillybus' counterpart,

 * it does nothing to make it happen (and neither is it necessary).

 *

 * This function is not reentrant for the same @chan, but this is covered

 * by the fact that for any given @chan, it's called either by the open,

 * write, llseek and flush fops methods, which can't run in parallel (and the

 * write + flush and llseek method handlers are protected with out_mutex).

 *

 * chan->flushed is there to avoid multiple flushes at the same position,

 * in particular as a result of programs that close the file descriptor

 * e.g. after a dup2() for redirection.

 Only real error, never -EINTR */

 Ignoring interrupts. Cancellation must be handled */

	/*

	 * The checkpoint is given in terms of data elements, not bytes. As

	 * a result, if less than an element's worth of data is stored in the

	 * FIFO, it's not flushed, including the flush before closing, which

	 * means that such data is lost. This is consistent with PCIe Xillybus.

 Only real error, never -EINTR */

 request_read_anything(): Ask the FPGA for any little amount of data */

		/*

		 * Sending a flush request to a previously closed stream

		 * effectively opens it, and also waits until the command is

		 * confirmed by the FPGA. The latter is necessary because the

		 * data is sent through a separate BULK OUT endpoint, and the

		 * xHCI controller is free to reorder transmissions.

		 *

		 * This can't go wrong unless there's a serious hardware error

		 * (or the computer is stuck for 500 ms?)

 Failure guarantees that opcode wasn't sent */

		/*

		 * In non-blocking mode, request the FPGA to send any data it

		 * has right away. Otherwise, the first read() will always

		 * return -EAGAIN, which is OK strictly speaking, but ugly.

		 * Checking and unrolling if this fails isn't worth the

		 * effort -- the error is propagated to the first read()

		 * anyhow.

		/*

		 * Some 32-bit arithmetic that may wrap. Note that

		 * complete_checkpoint is rounded up to the closest element

		 * boundary, because the read() can't be completed otherwise.

		 * fifo_checkpoint_bytes is rounded down, because it protects

		 * in_fifo from overflowing.

		/*

		 * To prevent flooding of OPCODE_SET_CHECKPOINT commands as

		 * data is consumed, it's issued only if it moves the

		 * checkpoint by at least an 8th of the FIFO's size, or if

		 * it's necessary to complete the number of bytes requested by

		 * the read() call.

		 *

		 * chan->read_data_ok is checked to spare an unnecessary

		 * submission after receiving EOF, however it's harmless if

		 * such slips away.

		/*

		 * Reaching here means that the FIFO was empty when

		 * fifo_read() returned, but not necessarily right now. Error

		 * and EOF are checked and reported only now, so that no data

		 * that managed its way to the FIFO is lost.

 FPGA has sent EOF */

 Has data slipped into the FIFO since fifo_read()? */

			/*

			 * Note that when xdev->error is set (e.g. when the

			 * device is unplugged), read_data_ok turns zero and

			 * fifo->waitq is awaken.

			 * Therefore no special attention to xdev->error.

 bytes_done == 0 */

 Tell FPGA to send anything it has */

	/*

	 * One second's timeout on flushing. Interrupts are ignored, because if

	 * the user pressed CTRL-C, that interrupt will still be in flight by

	 * the time we reach here, and the opportunity to flush is lost.

 The things you do to use dev_warn() and not pr_warn() */

		/*

		 * If rc_read is nonzero, xdev->error indicates a global

		 * device error. The error is reported later, so that

		 * resources are freed.

		 *

		 * Looping on wait_event_interruptible() kinda breaks the idea

		 * of being interruptible, and this should have been

		 * wait_event(). Only it's being waken with

		 * wake_up_interruptible() for the sake of other uses. If

		 * there's a global device error, chan->read_data_ok is

		 * deasserted and the wait queue is awaken, so this is covered.

 Empty loop */

		/*

		 * chan->flushing isn't zeroed. If the pre-release flush timed

		 * out, a cancel request will be sent before the next

		 * OPCODE_SET_CHECKPOINT (i.e. when the file is opened again).

		 * This is despite that the FPGA forgets about the checkpoint

		 * request as the file closes. Still, in an exceptional race

		 * condition, the FPGA could send an OPCODE_REACHED_CHECKPOINT

		 * just before closing that would reach the host after the

		 * file has re-opened.

 See comments on rc_read above */

/*

 * Xillybus' API allows device nodes to be seekable, giving the user

 * application access to a RAM array on the FPGA (or logic emulating it).

	/*

	 * Take both mutexes not allowing interrupts, since it seems like

	 * common applications don't expect an -EINTR here. Besides, multiple

	 * access to a single file descriptor on seekable devices is a mess

	 * anyhow.

 Going to the end => to the beginning */

 In any case, we must finish on an element boundary */

 Return error after releasing mutexes */

	/*

	 * If this is the first time poll() is called, and the file is

	 * readable, set the relevant flag. Also tell the FPGA to send all it

	 * has, to kickstart the mechanism that ensures there's always some

	 * data in in_fifo unless the stream is dry end-to-end. Note that the

	 * first poll() may not return a EPOLLIN, even if there's data on the

	 * FPGA. Rather, the data will arrive soon, and trigger the relevant

	 * wait queue.

	/*

	 * poll() won't play ball regarding read() channels which

	 * are synchronous. Allowing that will create situations where data has

	 * been delivered at the FPGA, and users expecting select() to wake up,

	 * which it may not. So make it never work.

 8 kiB */

 8 bytes granularity */

 Also frees FIFO mem if allocated */

 Entry is valid */

		/*

		 * A downstream channel should never exist above index 13,

		 * as it would request a nonexistent BULK endpoint > 15.

		 * In the peculiar case that it does, it's ignored silently.

 Entry is valid */

 Phase I: Set up one fake upstream channel and obtain IDT */

 Set up a fake IDT with one async IN stream */

 Interrupt on probe method? Interesting. */

 Phase II: Set up the streams as defined in IDT */

 Exclude CRC */

	/*

	 * Except for wildly misbehaving hardware, or if it was disconnected

	 * just after responding with the IDT, there is no reason for any

	 * work item to be running now. To be sure that xdev->channels

	 * is updated on anything that might run in parallel, flush the

	 * workqueue, which rarely does anything.

	/*

	 * Try to send OPCODE_QUIESCE, which will fail silently if the device

	 * was disconnected, but makes sense on module unload.

	/*

	 * If the device has been disconnected, sending the opcode causes

	 * a global device error with xdev->error, if such error didn't

	 * occur earlier. Hence timing out means that the USB link is fine,

	 * but somehow the message wasn't sent. Should never happen.

 Discourage further activity */

	/*

	 * This device driver is declared with soft_unbind set, or else

	 * sending OPCODE_QUIESCE above would always fail. The price is

	 * that the USB framework didn't kill outstanding URBs, so it has

	 * to be done explicitly before returning from this call.

		/*

		 * Lock taken to prevent chan->out_ep from changing. It also

		 * ensures xillyusb_open() and xillyusb_flush() don't access

		 * xdev->dev after being nullified below.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/drivers/misc/xillybus_of.c

 *

 * Copyright 2011 Xillybus Ltd, http://xillybus.com

 *

 * Driver for the Xillybus FPGA/host framework using Open Firmware.

 Match table for of_platform binding */

 Deprecated */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/drivers/misc/xillybus_core.c

 *

 * Copyright 2011 Xillybus Ltd, http://xillybus.com

 *

 * Driver for the Xillybus FPGA/host framework.

 *

 * This driver interfaces with a special IP core in an FPGA, setting up

 * a pipe between a hardware FIFO in the programmable logic and a device

 * file in the host. The number of such pipes and their attributes are

 * set up on the logic. This driver detects these automatically and

 * creates the device files accordingly.

 General timeout is 100 ms, rx timeout is 10 ms */

/*

 * Locking scheme: Mutexes protect invocations of character device methods.

 * If both locks are taken, wr_mutex is taken first, rd_mutex second.

 *

 * wr_spinlock protects wr_*_buf_idx, wr_empty, wr_sleepy, wr_ready and the

 * buffers' end_offset fields against changes made by IRQ handler (and in

 * theory, other file request handlers, but the mutex handles that). Nothing

 * else.

 * They are held for short direct memory manipulations. Needless to say,

 * no mutex locking is allowed when a spinlock is held.

 *

 * rd_spinlock does the same with rd_*_buf_idx, rd_empty and end_offset.

 *

 * register_mutex is endpoint-specific, and is held when non-atomic

 * register operations are performed. wr_mutex and rd_mutex may be

 * held when register_mutex is taken, but none of the spinlocks. Note that

 * register_mutex doesn't protect against sporadic buf_ctrl_reg writes

 * which are unrelated to buf_offset_reg, since they are harmless.

 *

 * Blocking on the wait queues is allowed with mutexes held, but not with

 * spinlocks.

 *

 * Only interruptible blocking is allowed on mutexes and wait queues.

 *

 * All in all, the locking order goes (with skips allowed, of course):

 * wr_mutex -> rd_mutex -> register_mutex -> wr_spinlock -> rd_spinlock

/*

 * xillybus_isr assumes the interrupt is allocated exclusively to it,

 * which is the natural case MSI and several other hardware-oriented

 * interrupts. Sharing is not allowed.

 Message NACK */

 Last message */

 Scan through messages */

 Write channel */

 Read channel */

 For select() */

 Message ACK */

/*

 * A few trivial memory management functions.

 * NOTE: These functions are used only on probe and remove, and therefore

 * no locks are applied!

 Init to silence warning */

 Not the message buffer */

		/*

		 * Buffers are expected in descending size order, so there

		 * is either enough space for this buffer or none at all.

 Not the message buffer */

 Channel 0 is message buf. */

 Initialize all channels with defaults */

 NULL for msg channel */

 Do nothing, just scan thru string */;

 This should be generated ad-hoc */

 Opcode 3 for channel 0 = Send IDT */

 Check version number. Reject anything above 0x82. */

 Initializations are there only to silence warnings */

 Note that we may drop mutex within this loop */

 Update wr_host_* to its post-operation state */

		/*

		 * Marking our situation after the possible changes above,

		 * for use after releasing the spinlock.

		 *

		 * empty = empty before change

		 * exhasted = empty after possible change

 Go on, now without the spinlock */

 Position zero means it's virgin */

				/*

				 * Tell FPGA the buffer is done with. It's an

				 * atomic operation to the FPGA, so what

				 * happens with other channels doesn't matter,

				 * and the certain channel is protected with

				 * the channel-specific mutex.

 This includes a zero-count return = EOF */

 More in RAM buffer(s)? Just go on. */

		/*

		 * Nonblocking read: The "ready" flag tells us that the FPGA

		 * has data to send. In non-blocking mode, if it isn't on,

		 * just return. But if there is, we jump directly to the point

		 * where we ask for the FPGA to send all it has, and wait

		 * until that data arrives. So in a sense, we *do* block in

		 * nonblocking mode, but only for a very short time.

			/*

			 * Note that in case of an element-misaligned read

			 * request, offsetlimit will include the last element,

			 * which will be partially read from.

			/*

			 * In synchronous mode, always send an offset limit.

			 * Just don't send a value too big.

 Don't request more than one buffer */

 Don't request more than all buffers */

			/*

			 * In asynchronous mode, force early flush of a buffer

			 * only if that will allow returning a full count. The

			 * "offsetlimit < ( ... )" rather than "<=" excludes

			 * requesting a full buffer, which would obviously

			 * cause a buffer transmission anyhow

 2 = offset limit */

		/*

		 * If partial completion is disallowed, there is no point in

		 * timeout sleeping. Neither if no_time_left is set and

		 * there's no data.

			/*

			 * This do-loop will run more than once if another

			 * thread reasserted wr_sleepy before we got the mutex

			 * back, so we try again.

 Mutex is not held if got here */

 Don't admit snoozing */

		/*

		 * If our time is out, skip the waiting. We may miss wr_sleepy

		 * being deasserted but hey, almost missing the train is like

		 * missing it.

 wr_sleepy deasserted */

 Interrupt */

 We're out of sleeping time. Desperate! */

			/*

			 * Reaching here means that we allow partial return,

			 * that we've run out of time, and that we have

			 * nothing to return.

			 * So tell the FPGA to send anything it has or gets.

 Opcode 3, flush it all! */

		/*

		 * Reaching here means that we *do* have data in the buffer,

		 * but the "partial" flag disallows returning less than

		 * required. And we don't have as much. So loop again,

		 * which is likely to end up blocking indefinitely until

		 * enough data has arrived.

/*

 * The timeout argument takes values as follows:

 *  >0 : Flush with timeout

 * ==0 : Flush, and wait idefinitely for the flush to complete

 *  <0 : Autoflush: Flush only if there's a single buffer occupied

	/*

	 * Don't flush a closed channel. This can happen when the work queued

	 * autoflush thread fires off after the file has closed. This is not

	 * an error, just something to dismiss.

 Submit the current buffer if it's nonempty */

 Copy  unflushed data, so we can put it in next buffer */

 Autoflush only if a single buffer is occupied */

			/*

			 * A new work item may be queued by the ISR exactly

			 * now, since the execution of a work item allows the

			 * queuing of a new one while it's running.

 The 4th element is never needed for data, so it's a flag */

 Set up rd_full to reflect a certain moment's state */

 Channel ID */

 Opcode 2, submit buffer */

 Autoflush */

	/*

	 * bufidx is now the last buffer written to (or equal to

	 * rd_fpga_buf_idx if buffer was never written to), and

	 * channel->rd_host_buf_idx the one after it.

	 *

	 * If bufidx == channel->rd_fpga_buf_idx we're either empty or full.

 Loop waiting for draining of buffers */

			channel->rd_full = 1; /*

					       * Not really full,

					       * but needs waiting.

		/*

		 * Indefinite sleep with mutex taken. With data waiting for

		 * flushing user should not be surprised if open() for write

		 * sleeps.

 1 second timeout */

 Initializations are there only to silence warnings */

			/*

			 * Update rd_host_* to its state after this operation.

			 * count=0 means committing the buffer immediately,

			 * which is like flushing, but not necessarily block.

		/*

		 * Marking our situation after the possible changes above,

		 * for use  after releasing the spinlock.

		 *

		 * full = full before change

		 * exhasted = full after possible change

 Go on, now without the spinlock */

 Zero means it's virgin */

 Virgin, but leftovers are due */

 Clear flag */

 2 = submit buffer */

 If there's more space, just go on */

		/*

		 * Indefinite sleep with mutex taken. With data waiting for

		 * flushing, user should not be surprised if open() for write

		 * sleeps.

 No timeout */

	/*

	 * It gets complicated because:

	 * 1. We don't want to take a mutex we don't have to

	 * 2. We don't want to open one direction if the other will fail.

	/*

	 * Note: open() may block on getting mutexes despite O_NONBLOCK.

	 * This shouldn't occur normally, since multiple open of the same

	 * file descriptor is almost always prohibited anyhow

	 * (*_exclusive_open is normally set in real-life systems).

 First open of file */

 Move the host to first buffer */

 Opcode 4, open channel */

 First open of file */

 Move the host to first buffer */

 No leftovers. */

 Opcode 4, open channel */

			/*

			 * We rely on the kernel calling flush()

			 * before we get here.

 Channel ID */

 Opcode 5, close channel */

 Opcode 5, close channel */

			/*

			 * This is crazily cautious: We make sure that not

			 * only that we got an EOF (be it because we closed

			 * the channel or because of a user's EOF), but verify

			 * that it's one beyond the last buffer arrived, so

			 * we have no leftover buffers pending before wrapping

			 * up (which can only happen in asynchronous channels,

			 * BTW)

				/*

				 * Check if eof points at the buffer after

				 * the last one the FPGA submitted. Note that

				 * no EOF is marked by negative eof.

				/*

				 * Steal extra 100 ms if awaken by interrupt.

				 * This is a simple workaround for an

				 * interrupt pending when entering, which would

				 * otherwise result in declaring the hardware

				 * non-responsive.

	/*

	 * Take both mutexes not allowing interrupts, since it seems like

	 * common applications don't expect an -EINTR here. Besides, multiple

	 * access to a single file descriptor on seekable devices is a mess

	 * anyhow.

 Going to the end => to the beginning */

 In any case, we must finish on an element boundary */

 Opcode 6, set address */

 Return error after releasing mutexes */

	/*

	 * Since seekable devices are allowed only when the channel is

	 * synchronous, we assume that there is no data pending in either

	 * direction (which holds true as long as no concurrent access on the

	 * file descriptor takes place).

	 * The only thing we may need to throw away is leftovers from partial

	 * write() flush.

	/*

	 * poll() won't play ball regarding read() channels which

	 * aren't asynchronous and support the nonempty message. Allowing

	 * that will create situations where data has been delivered at

	 * the FPGA, and users expecting select() to wake up, which it may

	 * not.

			/*

			 * Not EPOLLHUP, because its behavior is in the

			 * mist, and EPOLLIN does what we want: Wake up

			 * the read file descriptor so it sees EOF.

	/*

	 * If partial data write is disallowed on a write() channel,

	 * it's pointless to ever signal OK to write, because is could

	 * block despite some space being available.

	/*

	 * The bogus IDT is used during bootstrap for allocating the initial

	 * message buffer, and then the message buffer and space for the IDT

	 * itself. The initial message buffer is of a single page's size, but

	 * it's soon replaced with a more modest one (and memory is freed).

	/*

	 * Writing the value 0x00000001 to Endianness register signals which

	 * endianness this processor is using, so the FPGA can swap words as

	 * necessary.

 Bootstrap phase I: Allocate temporary message buffer */

 Clear the message subsystem (and counter in particular) */

	/*

	 * Set DMA 32/64 bit mode, quiesce the device (?!) and get IDT

	 * buffer size.

 Enable DMA */

 Bootstrap phase II: Allocate buffer for IDT and obtain it */

 Bootstrap phase III: Allocate buffers according to IDT */

	/*

	 * Flushing is done upon endpoint release to prevent access to memory

	 * just about to be released. This makes the quiesce complete.

 flush_workqueue() was called for each endpoint released */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014, 2015 Intel Corporation

 *

 * Authors:

 * Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * This file contains TPM2 protocol implementations of the commands

 * used by the kernel internally.

 Fixed timeouts for TPM2 */

 PTP spec timeouts */

 Key creation commands long timeouts */

/**

 * tpm2_ordinal_duration_index() - returns an index to the chip duration table

 * @ordinal: TPM command ordinal.

 *

 * The function returns an index to the chip duration table

 * (enum tpm_duration), that describes the maximum amount of

 * time the chip could take to return the result for a  particular ordinal.

 *

 * The values of the MEDIUM, and LONG durations are taken

 * from the PC Client Profile (PTP) specification (750, 2000 msec)

 *

 * LONG_LONG is for commands that generates keys which empirically takes

 * a longer time on some systems.

 *

 * Return:

 * * TPM_MEDIUM

 * * TPM_LONG

 * * TPM_LONG_LONG

 * * TPM_UNDEFINED

 Startup */

 144 */

 143 */

 17B */

 15C */

 13E */

 185 */

 186 */

 177 */

 182 */

 121 */

 129 */

 17A */

 14E */

 131 */

 153 */

 191 */

/**

 * tpm2_calc_ordinal_duration() - calculate the maximum command duration

 * @chip:    TPM chip to use.

 * @ordinal: TPM command ordinal.

 *

 * The function returns the maximum amount of time the chip could take

 * to return the result for a particular ordinal in jiffies.

 *

 * Return: A maximal duration time for an ordinal in jiffies.

/**

 * tpm2_pcr_read() - read a PCR value

 * @chip:	TPM chip to use.

 * @pcr_idx:	index of the PCR to read.

 * @digest:	PCR bank and buffer current PCR value is written to.

 * @digest_size_ptr:	pointer to variable that stores the digest size.

 *

 * Return: Same as with tpm_transmit_cmd.

/**

 * tpm2_pcr_extend() - extend a PCR value

 *

 * @chip:	TPM chip to use.

 * @pcr_idx:	index of the PCR.

 * @digests:	list of pcr banks and corresponding digest values to extend.

 *

 * Return: Same as with tpm_transmit_cmd.

/**

 * tpm2_get_random() - get random bytes from the TPM RNG

 *

 * @chip:	a &tpm_chip instance

 * @dest:	destination buffer

 * @max:	the max number of random bytes to pull

 *

 * Return:

 *   size of the buffer on success,

 *   -errno otherwise (positive TPM return codes are masked to -EIO)

/**

 * tpm2_flush_context() - execute a TPM2_FlushContext command

 * @chip:	TPM chip to use

 * @handle:	context handle

/**

 * tpm2_get_tpm_pt() - get value of a TPM_CAP_TPM_PROPERTIES type property

 * @chip:		a &tpm_chip instance

 * @property_id:	property ID.

 * @value:		output variable.

 * @desc:		passed to tpm_transmit_cmd()

 *

 * Return:

 *   0 on success,

 *   -errno or a TPM return code otherwise

/**

 * tpm2_shutdown() - send a TPM shutdown command

 *

 * Sends a TPM shutdown command. The shutdown command is used in call

 * sites where the system is going down. If it fails, there is not much

 * that can be done except print an error message.

 *

 * @chip:		a &tpm_chip instance

 * @shutdown_type:	TPM_SU_CLEAR or TPM_SU_STATE.

/**

 * tpm2_do_selftest() - ensure that all self tests have passed

 *

 * @chip: TPM chip to use

 *

 * Return: Same as with tpm_transmit_cmd.

 *

 * The TPM can either run all self tests synchronously and then return

 * RC_SUCCESS once all tests were successful. Or it can choose to run the tests

 * asynchronously and return RC_TESTING immediately while the self tests still

 * execute in the background. This function handles both cases and waits until

 * all tests have completed.

/**

 * tpm2_probe() - probe for the TPM 2.0 protocol

 * @chip:	a &tpm_chip instance

 *

 * Send an idempotent TPM 2.0 command and see whether there is TPM2 chip in the

 * other end based on the response tag. The flag TPM_CHIP_FLAG_TPM2 is set by

 * this function if this is the case.

 *

 * Return:

 *   0 on success,

 *   -errno otherwise

 We ignore TPM return codes on purpose. */

	/*

	 * Avoid unnecessary PCR read operations to reduce overhead

	 * and obtain identifiers of the crypto subsystem.

/**

 * tpm2_startup - turn on the TPM

 * @chip: TPM chip to use

 *

 * Normally the firmware should start the TPM. This function is provided as a

 * workaround if this does not happen. A legal case for this could be for

 * example when a TPM emulator is used.

 *

 * Return: same as tpm_transmit_cmd()

/**

 * tpm2_auto_startup - Perform the standard automatic TPM initialization

 *                     sequence

 * @chip: TPM chip to use

 *

 * Returns 0 on success, < 0 in case of fatal error.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 Intel Corporation

 *

 * Authors:

 * Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * This device driver implements the TPM interface as defined in

 * the TCG CRB 2.0 TPM specification.

/**

 * __crb_go_idle - request tpm crb device to go the idle state

 *

 * @dev:  crb device

 * @priv: crb private data

 *

 * Write CRB_CTRL_REQ_GO_IDLE to TPM_CRB_CTRL_REQ

 * The device should respond within TIMEOUT_C by clearing the bit.

 * Anyhow, we do not wait here as a consequent CMD_READY request

 * will be handled correctly even if idle was not completed.

 *

 * The function does nothing for devices with ACPI-start method

 * or SMC-start method.

 *

 * Return: 0 always

 mask */,

 value */

/**

 * __crb_cmd_ready - request tpm crb device to enter ready state

 *

 * @dev:  crb device

 * @priv: crb private data

 *

 * Write CRB_CTRL_REQ_CMD_READY to TPM_CRB_CTRL_REQ

 * and poll till the device acknowledge it by clearing the bit.

 * The device should respond within TIMEOUT_C.

 *

 * The function does nothing for devices with ACPI-start method

 * or SMC-start method.

 *

 * Return: 0 on success -ETIME on timeout;

 mask */,

 value */

	/* A sanity check that the upper layer wants to get at least the header

	 * as that is the minimum size for any TPM response.

	/* If this bit is set, according to the spec, the TPM is in

	 * unrecoverable condition.

	/* Read the first 8 bytes in order to get the length of the response.

	 * We read exactly a quad word in order to make sure that the remaining

	 * reads will be aligned.

/*

 * This is a TPM Command Response Buffer start method that invokes a

 * Secure Monitor Call to requrest the firmware to execute or cancel

 * a TPM 2.0 command.

	/* Zero the cancel register so that the next command will not get

	 * canceled.

 Make sure that cmd is populated before issuing start. */

	/* The reason for the extra quirk is that the PTT in 4th Gen Core CPUs

	 * report only ACPI start but in practice seems to require both

	 * CRB start, hence invoking CRB start method if hid == MSFT0101.

 Detect a 64 bit address on a 32 bit system */

/*

 * Work around broken BIOSs that return inconsistent values from the ACPI

 * region vs the registers. Trust the ACPI region. Such broken systems

 * probably cannot send large TPM commands since the buffer will be truncated.

	/* The ACPI IO region starts at the head area and continues to include

	 * the control area, as one nice sane region except for some older

	 * stuff that puts the control area outside the ACPI IO region.

	/*

	 * PTT HW bug w/a: wake up the device to access

	 * possibly not retained registers.

	/* According to the PTP specification, overlapping command and response

	 * buffer sizes must be identical.

 Should the FIFO driver handle this? */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Intel Corporation

 *

 * Authors:

 * Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * This file contains TPM2 protocol implementations of the commands

 * used by the kernel internally.

 Prevent caller getting a dangling pointer. */

		/*

		 * TPM_RC_HANDLE means that the session context can't

		 * be loaded because of an internal counter mismatch

		 * that makes the TPM think there might have been a

		 * replay.  This might happen if the context was saved

		 * and loaded outside the space.

		 *

		 * TPM_RC_REFERENCE_H0 means the session has been

		 * flushed outside the space

 sanity check, should never happen */

 load failed, just forget session */

 sanity check, should never happen */

 handle error saving session, just forget it */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 * Copyright (C) 2014 Intel Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

 *

 * Note, the TPM chip is not interrupt driven (only polling)

 * and can have very long timeouts (minutes!). Hence the unusual

 * calls to msleep.

/*

 * Bug workaround - some TPM's don't flush the most

 * recently changed pcr on suspend, so force the flush

 * with an extend to the selected _unused_ non-volatile pcr.

/**

 * tpm_calc_ordinal_duration() - calculate the maximum command duration

 * @chip:    TPM chip to use.

 * @ordinal: TPM command ordinal.

 *

 * The function returns the maximum amount of time the chip could take

 * to return the result for a particular ordinal in jiffies.

 *

 * Return: A maximal duration time for an ordinal in jiffies.

	/* A sanity check. send() should just return zero on success e.g.

	 * not the command length.

/**

 * tpm_transmit - Internal kernel interface to transmit TPM commands.

 * @chip:	a TPM chip to use

 * @buf:	a TPM command buffer

 * @bufsiz:	length of the TPM command buffer

 *

 * A wrapper around tpm_try_transmit() that handles TPM2_RC_RETRY returns from

 * the TPM and retransmits the command after a delay up to a maximum wait of

 * TPM2_DURATION_LONG.

 *

 * Note that TPM 1.x never returns TPM2_RC_RETRY so the retry logic is TPM 2.0

 * only.

 *

 * Return:

 * * The response length	- OK

 * * -errno			- A system error

 space for header and handles */

 the command code is where the return code will be */

	/*

	 * Subtlety here: if we have a space, the handles will be

	 * transformed, so when we restore the header we also have to

	 * restore the handles.

		/*

		 * return immediately if self test returns test

		 * still running to shorten boot time.

/**

 * tpm_transmit_cmd - send a tpm command to the device

 * @chip:			a TPM chip to use

 * @buf:			a TPM command buffer

 * @min_rsp_body_length:	minimum expected length of response body

 * @desc:			command description used in the error message

 *

 * Return:

 * * 0		- OK

 * * -errno	- A system error

 * * TPM_RC	- A TPM error

/**

 * tpm_is_tpm2 - do we a have a TPM2 chip?

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 *

 * Return:

 * 1 if we have a TPM2 chip.

 * 0 if we don't have a TPM2 chip.

 * A negative number for system errors (errno).

/**

 * tpm_pcr_read - read a PCR value from SHA1 bank

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 * @pcr_idx:	the PCR to be retrieved

 * @digest:	the PCR bank and buffer current PCR value is written to

 *

 * Return: same as with tpm_transmit_cmd()

/**

 * tpm_pcr_extend - extend a PCR value in SHA1 bank.

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 * @pcr_idx:	the PCR to be retrieved

 * @digests:	array of tpm_digest structures used to extend PCRs

 *

 * Note: callers must pass a digest for every allocated PCR bank, in the same

 * order of the banks in chip->allocated_banks.

 *

 * Return: same as with tpm_transmit_cmd()

/**

 * tpm_send - send a TPM command

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 * @cmd:	a TPM command buffer

 * @buflen:	the length of the TPM command buffer

 *

 * Return: same as with tpm_transmit_cmd()

/*

 * We are about to suspend. Save the TPM state

 * so that it can be restored.

/*

 * Resume from a power safe. The BIOS already restored

 * the TPM state.

/**

 * tpm_get_random() - get random bytes from the TPM's RNG

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 * @out:	destination buffer for the random bytes

 * @max:	the max number of bytes to write to @out

 *

 * Return: number of random bytes read or a negative error value.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 James.Bottomley@HansenPartnership.com

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Copyright (C) 2013 Obsidian Research Corp

 * Jason Gunthorpe <jgunthorpe@obsidianresearch.com>

 *

 * Device file system interface to the TPM

	/* If the command is not implemented by the TPM, synthesize a

	 * response with a TPM2_RC_COMMAND_CODE return for user-space.

	/* Cannot perform a write until the read has cleared either via

	 * tpm_read or a user_read_timer timeout. This also prevents split

	 * buffered writes from blocking here.

	/*

	 * If in nonblocking mode schedule an async job to send

	 * the command return the size.

	 * In case of error the err code will be returned in

	 * the subsequent read call.

	/* atomic tpm command send and result receive. We only hold the ops

	 * lock during this period so that the tpm can be unregistered even if

	 * the char dev is held open.

	/*

	 * The response_length indicates if there is still response

	 * (or part of it) to be consumed. Partial reads decrease it

	 * by the number of bytes read, and write resets it the zero.

/*

 * Called on file close

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org	 

 National definitions */

 status bits */

 output buffer full */

 input buffer full */

 F0 */

 A2 */

 ready to receive command */

 ready to receive data */

 command bits */

 normal mode */

/*

 * Wait for a certain status to appear

 status immediately available check */

 wait for status */

 status immediately available check */

 wait for status */

 read the whole packet */

	/*

	 * If we hit the chip with back to back commands it locks up

	 * and never set IBF. Hitting it with this "hammer" seems to

	 * fix it. Not sure why this is needed, we followed the flow

	 * chart in the manual to the letter.

 verify that it is a National part (SID) */

 enable the DPM module */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Copyright (C) 2013 Obsidian Research Corp

 * Jason Gunthorpe <jgunthorpe@obsidianresearch.com>

 *

 * sysfs filesystem inspection interface to the TPM

 TPM 1.2 */

 TPM 1.1 */

 should never happen */

/*

 * The following set of defines represents all the magic to build

 * the per hash attribute groups for displaying each bank of PCRs.

 * The only slight problem with this approach is that every PCR is

 * hard coded to be present, so you don't know if an PCR is missing

 * until a cat of the file returns -EINVAL

 *

 * Also note you must ignore checkpatch warnings in this macro

 * code. This is deep macro magic that checkpatch.pl doesn't

 * understand.

 Note, this must match TPM2_PLATFORM_PCR which is fixed at 24. */

 ignore checkpatch warning about trailing ; in macro. */

 ignore checkpatch warning about trailing , in macro. */

/*

 * End of macro structure to build an attribute group containing 24

 * PCR value files for each supported hash algorithm

/*

 * The next set of macros implements the cleverness for each hash to

 * build a static attribute group called pcr_group_<hash> which can be

 * added to chip->groups[].

 *

 * The first argument is the TPM algorithm id and the second is the

 * hash used as both the suffix and the group name.  Note: the group

 * name is a directory in the top level tpm class with the name

 * pcr-<hash>, so it must not clash with any other names already

 * in the sysfs directory.

 add one group for each bank hash */

			/*

			 * If triggers, send a patch to add both a

			 * PCR_ATTR_BUILD() macro above for the

			 * missing algorithm as well as an additional

			 * case in this switch statement.

	/*

	 * This will only trigger if someone has added an additional

	 * hash to the tpm_algorithms enum without incrementing

	 * TPM_MAX_HASHES.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Copyright (C) 2013 Obsidian Research Corp

 * Jason Gunthorpe <jgunthorpe@obsidianresearch.com>

 *

 * Device file system interface to the TPM

	/* It's assured that the chip will be opened just once,

	 * by the check of is_open variable, which is protected

/*

 * Called on file close

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * ATMEL I2C TPM AT97SC3204T

 *

 * Copyright (C) 2012 V Lab Technologies

 *  Teddy Reed <teddy@prosauce.org>

 * Copyright (C) 2013, Obsidian Research Corp.

 *  Jason Gunthorpe <jgunthorpe@obsidianresearch.com>

 * Device driver for ATMEL I2C TPMs.

 *

 * Teddy Reed determined the basic I2C command flow, unlike other I2C TPM

 * devices the raw TCG formatted TPM command data is written via I2C and then

 * raw TCG formatted TPM command data is returned via I2C.

 *

 * TGC status/locality/etc functions seen in the LPC implementation do not

 * seem to be present.

 ms */

 2 sec */

	/* This is the amount we read on the first try. 25 was chosen to fit a

	 * fair number of read responses in the buffer so a 2nd retry can be

 The upper layer does not support incomplete sends. */

	/* Get the message size from the message header, if we didn't get the

	 * whole message in read_status then we need to re-read the

	/* The TPM fails the I2C read until it is ready, so we do the entire

	 * transfer here and buffer it locally. This way the common code can

	/* Once the TPM has completed the command the command remains readable

 Default timeouts */

	/* There is no known way to probe for this device, and all version

	 * information seems to be read via TPM commands. Thus we rely on the

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) Microsoft Corporation

 *

 * Implements a firmware TPM as described here:

 * https://www.microsoft.com/en-us/research/publication/ftpm-software-implementation-tpm-chip/

 *

 * A reference implementation is available here:

 * https://github.com/microsoft/ms-tpm-20-ref/tree/master/Samples/ARM32-FirmwareTPM/optee_ta/fTPM

/*

 * TA_FTPM_UUID: BC50D971-D4C9-42C4-82CB-343FB7F37896

 *

 * Randomly generated, and must correspond to the GUID on the TA side.

 * Defined here in the reference implementation:

 * https://github.com/microsoft/ms-tpm-20-ref/blob/master/Samples/ARM32-FirmwareTPM/optee_ta/fTPM/include/fTPM.h#L42

/**

 * ftpm_tee_tpm_op_recv() - retrieve fTPM response.

 * @chip:	the tpm_chip description as specified in driver/char/tpm/tpm.h.

 * @buf:	the buffer to store data.

 * @count:	the number of bytes to read.

 *

 * Return:

 *	In case of success the number of bytes received.

 *	On failure, -errno.

/**

 * ftpm_tee_tpm_op_send() - send TPM commands through the TEE shared memory.

 * @chip:	the tpm_chip description as specified in driver/char/tpm/tpm.h

 * @buf:	the buffer to send.

 * @len:	the number of bytes to send.

 *

 * Return:

 *	In case of success, returns 0.

 *	On failure, -errno

 Invoke FTPM_OPTEE_TA_SUBMIT_COMMAND function of fTPM TA */

 Fill FTPM_OPTEE_TA_SUBMIT_COMMAND parameters */

 sanity check resp_len */

 sanity checks look good, cache the response */

 not supported */

/*

 * Check whether this driver supports the fTPM TA in the TEE instance

 * represented by the params (ver/data) to this function.

	/*

	 * Currently this driver only support GP Complaint OPTEE based fTPM TA

/**

 * ftpm_tee_probe() - initialize the fTPM

 * @pdev: the platform_device description.

 *

 * Return:

 *	On success, 0. On failure, -errno.

 Open context with TEE driver */

 Open a session with fTPM TA */

 Allocate dynamic shared memory with fTPM TA */

 Allocate new struct tpm_chip instance */

 Create a character device for the fTPM */

/**

 * ftpm_tee_remove() - remove the TPM device

 * @pdev: the platform_device description.

 *

 * Return:

 *	0 always.

 Release the chip */

 frees chip */

 Free the shared memory pool */

 close the existing session with fTPM TA*/

 close the context with TEE driver */

 memory allocated with devm_kzalloc() is freed automatically */

/**

 * ftpm_tee_shutdown() - shutdown the TPM device

 * @pdev: the platform_device description.

 UUID of the fTPM TA */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Infineon Technologies AG

 * Copyright (C) 2016 STMicroelectronics SAS

 *

 * Authors:

 * Peter Huewe <peter.huewe@infineon.com>

 * Christophe Ricard <christophe-h.ricard@st.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

 *

 * This device driver implements the TPM interface as defined in

 * the TCG TPM Interface Spec version 1.3, revision 27 via _raw/native

 * SPI access_.

 *

 * It is based on the original tpm_tis device driver from Leendert van

 * Dorn and Kyleen Hall and Jarko Sakkinnen.

/*

 * TCG SPI flow control is documented in section 6.4 of the spec[1]. In short,

 * keep trying to read from the device until MISO goes high indicating the

 * wait state has ended.

 *

 * [1] https://trustedcomputinggroup.org/resource/pc-client-platform-tpm-profile-ptp-specification/

 handle SPI wait states

 Flow control transfers are receive only */

 If the SPI device has an IRQ then use that */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2014 Intel Corporation

 *

 * Authors:

 * Xiaoyan Zhang <xiaoyan.zhang@intel.com>

 * Jiang Liu <jiang.liu@linux.intel.com>

 * Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * This file contains implementation of the sysfs interface for PPI.

 PPI 1.3 for TPM 2 */

	/*

	 * output.pointer should be of package type, including two integers.

	 * The first is function return code, 0 means success and 1 means

	 * error. The second is pending TPM operation requested by the OS, 0

	 * means none and >0 means operation value.

	/*

	 * the function to submit TPM operation request to pre-os environment

	 * is updated with function index from SUBREQ to SUBREQ2 since PPI

	 * version 1.1

	/*

	 * PPI spec defines params[3].type as ACPI_TYPE_PACKAGE. Some BIOS

	 * accept buffer/string/integer type, but some BIOS accept buffer/

	 * string/package type. For PPI version 1.0 and 1.1, use buffer type

	 * for compatibility, and use package type since 1.2 according to spec.

	/*

	 * PPI spec defines params[3].type as empty package, but some platforms

	 * (e.g. Capella with PPI 1.0) need integer/string/buffer type, so for

	 * compatibility, define params[3].type as buffer, if PPI version < 1.2

	/*

	 * parameter output.pointer should be of package type, including

	 * 3 integers. The first means function return code, the second means

	 * most recent TPM operation request, and the last means response to

	 * the most recent TPM operation request. Only if the first is 0, and

	 * the second integer is not 0, the response makes sense.

 Cache PPI version string. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org	 

 write status bits */

 read status bits */

 start reading header */

 size of the data received */

 clear the waiting data anyway */

 read all the data available */

 make sure data available is gone */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2020 Google Inc.

 *

 * Based on Infineon TPM driver by Peter Huewe.

 *

 * cr50 is a firmware for H1 secure modules that requires special

 * handling for the I2C interface.

 *

 * - Use an interrupt for transaction status instead of hardcoded delays.

 * - Must use write+wait+read read protocol.

 * - All 4 bytes of status register must be read/written at once.

 * - Burst count max is 63 bytes, and burst count behaves slightly differently

 *   than other I2C TPMs.

 * - When reading from FIFO the full burstcnt must be read instead of just

 *   reading header and determining the remainder.

 Short timeout during transactions */

 Timeout for TPM ready without IRQ */

 Device and vendor ID reg value */

 Max retries due to I2C errors */

 Min usecs between retries on I2C */

 Max usecs between retries on I2C */

/**

 * struct tpm_i2c_cr50_priv_data - Driver private data.

 * @irq:	Irq number used for this chip.

 *		If irq <= 0, then a fixed timeout is used instead of waiting for irq.

 * @tpm_ready:	Struct used by irq handler to signal R/W readiness.

 * @buf:	Buffer used for i2c writes, with i2c address prepended to content.

 *

 * Private driver struct used by kernel threads and interrupt context.

/**

 * tpm_cr50_i2c_int_handler() - cr50 interrupt handler.

 * @dummy:	Unused parameter.

 * @tpm_info:	TPM chip information.

 *

 * The cr50 interrupt handler signals waiting threads that the

 * interrupt has been asserted. It does not do any interrupt triggered

 * processing but is instead used to avoid fixed delays.

 *

 * Return:

 *	IRQ_HANDLED signifies irq was handled by this device.

/**

 * tpm_cr50_i2c_wait_tpm_ready() - Wait for tpm to signal ready.

 * @chip: A TPM chip.

 *

 * Wait for completion interrupt if available, otherwise use a fixed

 * delay for the TPM to be ready.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 Use a safe fixed delay if interrupt is not supported */

 Wait for interrupt to indicate TPM is ready to respond */

/**

 * tpm_cr50_i2c_enable_tpm_irq() - Enable TPM irq.

 * @chip: A TPM chip.

/**

 * tpm_cr50_i2c_disable_tpm_irq() - Disable TPM irq.

 * @chip: A TPM chip.

/**

 * tpm_cr50_i2c_transfer_message() - Transfer a message over i2c.

 * @dev:	Device information.

 * @adapter:	I2C adapter.

 * @msg:	Message to transfer.

 *

 * Call unlocked i2c transfer routine with the provided parameters and

 * retry in case of bus errors.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 Successfully transferred the message */

 No i2c message transferred */

/**

 * tpm_cr50_i2c_read() - Read from TPM register.

 * @chip:	A TPM chip.

 * @addr:	Register address to read from.

 * @buffer:	Read destination, provided by caller.

 * @len:	Number of bytes to read.

 *

 * Sends the register address byte to the TPM, then waits until TPM

 * is ready via interrupt signal or timeout expiration, then 'len'

 * bytes are read from TPM response into the provided 'buffer'.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 Prepare for completion interrupt */

 Send the register address byte to the TPM */

 Wait for TPM to be ready with response data */

 Read response data from the TPM */

/**

 * tpm_cr50_i2c_write()- Write to TPM register.

 * @chip:	A TPM chip.

 * @addr:	Register address to write to.

 * @buffer:	Data to write.

 * @len:	Number of bytes to write.

 *

 * The provided address is prepended to the data in 'buffer', the

 * cobined address+data is sent to the TPM, then wait for TPM to

 * indicate it is done writing.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 Prepend the 'register address' to the buffer */

 Prepare for completion interrupt */

 Send write request buffer with address */

 Wait for TPM to be ready, ignore timeout */

/**

 * tpm_cr50_check_locality() - Verify TPM locality 0 is active.

 * @chip: A TPM chip.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

/**

 * tpm_cr50_release_locality() - Release TPM locality.

 * @chip:	A TPM chip.

 * @force:	Flag to force release if set.

/**

 * tpm_cr50_request_locality() - Request TPM locality 0.

 * @chip: A TPM chip.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

/**

 * tpm_cr50_i2c_tis_status() - Read cr50 tis status.

 * @chip: A TPM chip.

 *

 * cr50 requires all 4 bytes of status register to be read.

 *

 * Return:

 *	TPM status byte.

/**

 * tpm_cr50_i2c_tis_set_ready() - Set status register to ready.

 * @chip: A TPM chip.

 *

 * cr50 requires all 4 bytes of status register to be written.

/**

 * tpm_cr50_i2c_get_burst_and_status() - Get burst count and status.

 * @chip:	A TPM chip.

 * @mask:	Status mask.

 * @burst:	Return value for burst.

 * @status:	Return value for status.

 *

 * cr50 uses bytes 3:2 of status register for burst count and

 * all 4 bytes must be read.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 wait for burstcount */

/**

 * tpm_cr50_i2c_tis_recv() - TPM reception callback.

 * @chip:	A TPM chip.

 * @buf:	Reception buffer.

 * @buf_len:	Buffer length to read.

 *

 * Return:

 * - >= 0:	Number of read bytes.

 * - -errno:	A POSIX error code.

 Read first chunk of burstcnt bytes */

 Determine expected data in the return buffer */

 Now read the rest of the data */

 Read updated burst count and check status */

 Ensure TPM is done reading data */

 Abort current transaction if still pending */

/**

 * tpm_cr50_i2c_tis_send() - TPM transmission callback.

 * @chip:	A TPM chip.

 * @buf:	Buffer to send.

 * @len:	Buffer length.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 Wait until TPM is ready for a command */

 Wait for data if this is not the first chunk */

 Read burst count and check status */

		/*

		 * Use burstcnt - 1 to account for the address byte

		 * that is inserted by tpm_cr50_i2c_write()

 Ensure TPM is not expecting more data */

 Start the TPM command */

 Abort current transaction if still pending */

/**

 * tpm_cr50_i2c_req_canceled() - Callback to notify a request cancel.

 * @chip:	A TPM chip.

 * @status:	Status given by the cancel callback.

 *

 * Return:

 *	True if command is ready, False otherwise.

/**

 * tpm_cr50_i2c_probe() - Driver probe function.

 * @client:	I2C client information.

 * @id:		I2C device id.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 cr50 is a TPM 2.0 chip */

 Default timeouts */

 Read four bytes from DID_VID register */

/**

 * tpm_cr50_i2c_remove() - Driver remove function.

 * @client: I2C client information.

 *

 * Return:

 * - 0:		Success.

 * - -errno:	A POSIX error code.

 SPDX-License-Identifier: GPL-2.0-or-later

 /******************************************************************************

 * Nuvoton TPM I2C Device Driver Interface for WPCT301/NPCT501/NPCT6XX,

 * based on the TCG TPM Interface Spec version 1.2.

 * Specifications at www.trustedcomputinggroup.org

 *

 * Copyright (C) 2011, Nuvoton Technology Corporation.

 *  Dan Morav <dan.morav@nuvoton.com>

 * Copyright (C) 2013, Obsidian Research Corp.

 *  Jason Gunthorpe <jgunthorpe@obsidianresearch.com>

 *

 * Nuvoton contact information: APC.Support@nuvoton.com

 I2C interface offsets */

/*

 * I2C bus device maximum buffer size w/o counting I2C address or command

 * i.e. max size required for I2C write is 34 = addr, command, 32 bytes data

 usec */

 usec */

 usec */

 usec */

 bit2...bit0 reads always 0 */

 ms */

 2 sec */

 read TPM_STS register */

 write byte to TPM_STS register */

 this causes the current command to be aborted */

 write commandReady to TPM_STS register */

 this causes the current command to be aborted */

/* read burstCount field from TPM_STS register

 wait for burstcount to be non-zero */

 in I2C burstCount is 1 byte */

/*

 * WPCT301/NPCT501/NPCT6XX SINT# supports only dataAvail

 * any call to this function which is not waiting for dataAvail will

 * set queue to NULL to avoid waiting for interrupt

		/* At this point we know that the SINT pin is asserted, so we

 check current status */

 use polling to wait for the event */

 wait for dataAvail field to be set in the TPM_STS register */

 Read @count bytes into @buf from TPM_RD_FIFO register */

 Read TPM command results */

 return to idle */

 if this is not the first trial, set responseRetry */

		/*

		 * read first available (> 10 bytes), including:

		 * tag, paramsize, and result

		/*

		 * convert number of expected bytes field from big endian 32 bit

		 * to machine native

/*

 * Send TPM command.

 *

 * If interrupts are used (signaled by an irq set in the vendor structure)

 * tpm.c can skip polling for the data to be available as the interrupt is

 * waited for here

 write last byte */

 retries == TPM_RETRY */

 execute the TPM command */

/* The only purpose for the handler is to signal to any waiting threads that

 * the interrupt is currently being asserted. The driver does not do any

 * processing triggered by interrupts, and the chip provides no way to mask at

 * the source (plus that would be slow over I2C). Run the IRQ as a one-shot,

 check WPCT301 values - ignore RID */

		/*

		 * f/w rev 2.81 has an issue where the VID_DID_RID is not

		 * reporting the right value. so give it another chance at

		 * offset 0x20 (FIFO_W).

 check WPCT301 values - ignore RID */

 Default timeouts */

	/*

	 * I2C intfcaps (interrupt capabilitieis) in the chip are hard coded to:

	 *   TPM_INTF_INT_LEVEL_LOW | TPM_INTF_DATA_AVAIL_INT

	 * The IRQ should be set in the i2c_board_info (which is done

 Clear any pending interrupt */

 - wait for TPM_STS==0xA0 (stsValid, commandReady) */

				/*

				 * TIS is in ready state

				 * write dummy byte to enter reception state

				 * TPM_DATA_FIFO_W <- rc (0)

 TPM_STS <- 0x40 (commandReady) */

				/*

				 * timeout_b reached - command was

				 * aborted. TIS should now be in idle state -

				 * only TPM_STS_VALID should be set

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 IBM Corporation

 * Copyright (C) 2014 Intel Corporation

 *

 * Authors:

 * Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * TPM chip management routines.

/**

 * tpm_chip_start() - power on the TPM

 * @chip:	a TPM chip to use

 *

 * Return:

 * * The response length	- OK

 * * -errno			- A system error

/**

 * tpm_chip_stop() - power off the TPM

 * @chip:	a TPM chip to use

 *

 * Return:

 * * The response length	- OK

 * * -errno			- A system error

/**

 * tpm_try_get_ops() - Get a ref to the tpm_chip

 * @chip: Chip to ref

 *

 * The caller must already have some kind of locking to ensure that chip is

 * valid. This function will lock the chip so that the ops member can be

 * accessed safely. The locking prevents tpm_chip_unregister from

 * completing, so it should not be held for long periods.

 *

 * Returns -ERRNO if the chip could not be got.

/**

 * tpm_put_ops() - Release a ref to the tpm_chip

 * @chip: Chip to put

 *

 * This is the opposite pair to tpm_try_get_ops(). After this returns chip may

 * be kfree'd.

/**

 * tpm_default_chip() - find a TPM chip and get a reference to it

/**

 * tpm_find_get_ops() - find and reserve a TPM chip

 * @chip:	a &struct tpm_chip instance, %NULL for the default chip

 *

 * Finds a TPM chip and reserves its class device and operations. The chip must

 * be released with tpm_put_ops() after use.

 * This function is for internal use only. It supports existing TPM callers

 * by accepting NULL, but those callers should be converted to pass in a chip

 * directly.

 *

 * Return:

 * A reserved &struct tpm_chip instance.

 * %NULL if a chip is not found.

 * %NULL if the chip is not available.

 release additional reference we got from tpm_default_chip() */

/**

 * tpm_dev_release() - free chip memory and the device number

 * @dev: the character device for the TPM chip

 *

 * This is used as the release function for the character device.

 release the master device reference */

/**

 * tpm_class_shutdown() - prepare the TPM device for loss of power.

 * @dev: device to which the chip is associated.

 *

 * Issues a TPM2_Shutdown command prior to loss of power, as required by the

 * TPM 2.0 spec. Then, calls bus- and device- specific shutdown code.

 *

 * Return: always 0 (i.e. success)

/**

 * tpm_chip_alloc() - allocate a new struct tpm_chip instance

 * @pdev: device to which the chip is associated

 *        At this point pdev mst be initialized, but does not have to

 *        be registered

 * @ops: struct tpm_class_ops instance

 *

 * Allocates a new struct tpm_chip instance and assigns a free

 * device number for it. Must be paired with put_device(&chip->dev).

	/* get extra reference on main device to hold on

	 * behalf of devs.  This holds the chip structure

	 * while cdevs is in use.  The corresponding put

	 * is in the tpm_devs_release (TPM2 only)

/**

 * tpmm_chip_alloc() - allocate a new struct tpm_chip instance

 * @pdev: parent device to which the chip is associated

 * @ops: struct tpm_class_ops instance

 *

 * Same as tpm_chip_alloc except devm is used to do the put_device

 Make the chip available. */

 Make the chip unavailable. */

 Make the driver uncallable. */

/* For compatibility with legacy sysfs paths we provide symlinks from the

 * parent dev directory to selected names within the tpm chip directory. Old

 * kernel versions created these files directly under the parent.

 All the names from tpm-sysfs */

/*

 * tpm_chip_register() - create a character device for the TPM chip

 * @chip: TPM chip to use.

 *

 * Creates a character device for the TPM chip and adds sysfs attributes for

 * the device. As the last step this function adds the chip to the list of TPM

 * chips available for in-kernel use.

 *

 * This function should be only called after the chip initialization is

 * complete.

/*

 * tpm_chip_unregister() - release the TPM driver

 * @chip: TPM chip to use.

 *

 * Takes the chip first away from the list of available TPM chips and then

 * cleans up all the resources reserved by tpm_chip_register().

 *

 * Once this function returns the driver call backs in 'op's will not be

 * running and will no longer start.

 *

 * NOTE: This function should be only called before deinitializing chip

 * resources.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2005, 2006 IBM Corporation

 * Copyright (C) 2014, 2015 Intel Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

 *

 * This device driver implements the TPM interface as defined in

 * the TCG TPM Interface Spec version 1.2, revision 1.0.

 check current status */

/* Before we attempt to access the TPM we must see that the valid bit is set.

 * The specification says that this bit is 0 at reset and remains 0 until the

 * 'TPM has gone through its self test and initialization and has established

 * correct values in the other bits.'

 wait for burstcount */

			/*

			 * If this trips, the chances are the read is

			 * returning 0xff because the locality hasn't been

			 * acquired.  Usually because tpm_try_get_ops() hasn't

			 * been called before doing a TPM operation.

			/*

			 * Dump stack for forensics, as invalid TPM_STS.x could be

			 * potentially triggered by impaired tpm_try_get_ops() or

			 * tpm_find_get_ops().

 this causes the current command to be aborted */

 wait for burstcount */

 read first 10 bytes, including tag, paramsize, and result */

 retry? */

/*

 * If interrupts are used (signaled by an irq set in the vendor structure)

 * tpm.c can skip polling for the data to be available as the interrupt is

 * waited for here

 write last byte */

/*

 * If interrupts are used (signaled by an irq set in the vendor structure)

 * tpm.c can skip polling for the data to be available as the interrupt is

 * waited for here

 go and do it */

 Verify receipt of the expected IRQ */

 STMicroelectronics 0x104a */

 Try to get a TPM version 1.2 or 1.1 TPM_CAP_VERSION_INFO */

 Atmel 3204 */

/*

 * Early probing for iTPM with STS_DATA_EXPECT flaw.

 * Try sending command without itpm flag set and if that

 * fails, repeat with itpm flag set.

 probe only iTPMS */

 Clear interrupts handled with TPM_EOI */

/* Register the IRQ and issue a command that will cause an interrupt. If an

 * irq is seen then leave the chip setup for IRQ operation, otherwise reverse

 * everything and leave in polling mode. Returns 0 on success.

 Clear all existing */

 Turn on */

	/* Generate an interrupt by having the core call through to

	 * tpm_tis_send

	/* tpm_tis_send will either confirm the interrupt is working or it

	 * will call disable_irq which undoes all of the above.

/* Try to find the IRQ the TPM is using. This is for legacy x86 systems that

 * do not have ACPI/etc. We typically expect the interrupt to be declared if

 * present.

/**

 * tpm_tis_clkrun_enable() - Keep clkrun protocol disabled for entire duration

 *                           of a single TPM command

 * @chip:	TPM chip to use

 * @value:	1 - Disable CLKRUN protocol, so that clocks are free running

 *		0 - Enable CLKRUN protocol

 * Call this function directly in tpm_tis_remove() in error or driver removal

 * path, since the chip->ops is set to NULL in tpm_chip_unregister().

 Disable LPC CLKRUN# */

		/*

		 * Write any random value on port 0x80 which is on LPC, to make

		 * sure LPC clock is running before sending any TPM command.

 Enable LPC CLKRUN# */

		/*

		 * Write any random value on port 0x80 which is on LPC, to make

		 * sure LPC clock is running before sending any TPM command.

 Maximum timeouts */

 Check if CLKRUN# is already not enabled in the LPC bus */

 Take control of the TPM's interrupt hardware and shut it off */

 Figure out the capabilities */

 INTERRUPT Setup */

		/*

		 * Before doing irq testing issue a command to the TPM in polling mode

		 * to make sure it works. May as well use that command to set the

		 * proper timeouts for the driver.

	/* reenable interrupts that device may have lost or

	 * BIOS/firmware may have disabled

	/*

	 * TPM 1.2 requires self-test on resume. This function actually returns

	 * an error code but for unknown reason it isn't handled.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2004 IBM Corporation

 * Copyright (C) 2014 Intel Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Dave Safford <safford@watson.ibm.com>

 * Reiner Sailer <sailer@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

/*

 * Array with one entry per ordinal defining the maximum amount

 * of time the chip could take to return the result.  The ordinal

 * designation of short, medium or long is defined in a table in

 * TCG Specification TPM Main Part 2 TPM Structures Section 17. The

 * values of the SHORT, MEDIUM, and LONG durations are retrieved

 * from the chip during initialization with a call to tpm_get_timeouts.

 0 */

 5 */

 10 */

 15 */

 20 */

 25 */

 30 */

 35 */

 40 */

 45 */

 50 */

 55 */

 60 */

 65 */

 70 */

 75 */

 80 */

 85 */

 90 */

 95 */

 100 */

 105 */

 110 */

 115 */

 120 */

 125 */

 130 */

 135 */

 140 */

 145 */

 150 */

 155 */

 160 */

 165 */

 170 */

 175 */

 180 */

 185 */

 190 */

 195 */

 200 */

 205 */

 210 */

 215 */

 220 */

 225 */

 230 */

 235 */

 240 */

/**

 * tpm1_calc_ordinal_duration() - calculate the maximum command duration

 * @chip:    TPM chip to use.

 * @ordinal: TPM command ordinal.

 *

 * The function returns the maximum amount of time the chip could take

 * to return the result for a particular ordinal in jiffies.

 *

 * Return: A maximal duration time for an ordinal in jiffies.

	/*

	 * We only have a duration table for protected commands, where the upper

	 * 16 bits are 0. For the few other ordinals the fallback will be used.

/**

 * tpm1_startup() - turn on the TPM

 * @chip: TPM chip to use

 *

 * Normally the firmware should start the TPM. This function is provided as a

 * workaround if this does not happen. A legal case for this could be for

 * example when a TPM emulator is used.

 *

 * Return: same as tpm_transmit_cmd()

	/*

	 * Provide ability for vendor overrides of timeout values in case

	 * of misreporting.

 Restore default if chip reported 0 */

 timeouts in msec rather usec */

 Report adjusted timeouts */

 not used under 1.2 */

	/*

	 * Provide the ability for vendor overrides of duration values in case

	 * of misreporting.

	/* The Broadcom BCM0102 chipset in a Dell Latitude D820 gets the above

	 * value wrong and apparently reports msecs rather than usecs. So we

	 * fix up the resulting too-small TPM_SHORT value to make things work.

	 * We also scale the TPM_MEDIUM and -_LONG values by 1000.

/**

 * tpm1_get_random() - get random bytes from the TPM's RNG

 * @chip:	a &struct tpm_chip instance

 * @dest:	destination buffer for the random bytes

 * @max:	the maximum number of bytes to write to @dest

 *

 * Return:

 * *  number of bytes read

 * * -errno (positive TPM return codes are masked to -EIO)

/**

 * tpm1_continue_selftest() - run TPM's selftest

 * @chip: TPM chip to use

 *

 * Returns 0 on success, < 0 in case of fatal error or a value > 0 representing

 * a TPM error code.

/**

 * tpm1_do_selftest - have the TPM continue its selftest and wait until it

 *                   can receive further commands

 * @chip: TPM chip to use

 *

 * Returns 0 on success, < 0 in case of fatal error or a value > 0 representing

 * a TPM error code.

	/* This may fail if there was no TPM driver during a suspend/resume

	 * cycle; some may return 10 (BAD_ORDINAL), others 28 (FAILEDSELFTEST)

 Attempt to read a PCR value */

		/* Some buggy TPMs will not respond to tpm_tis_ready() for

		 * around 300ms while the self test is ongoing, keep trying

		 * until the self test duration expires.

			/* TPM is disabled and/or deactivated; driver can

			 * proceed and TPM does handle commands for

			 * suspend/resume correctly

/**

 * tpm1_auto_startup - Perform the standard automatic TPM initialization

 *                     sequence

 * @chip: TPM chip to use

 *

 * Returns 0 on success, < 0 in case of fatal error.

/**

 * tpm1_pm_suspend() - pm suspend handler

 * @chip: TPM chip to use.

 * @tpm_suspend_pcr: flush pcr for buggy TPM chips.

 *

 * The functions saves the TPM state to be restored on resume.

 *

 * Return:

 * * 0 on success,

 * * < 0 on error.

 for buggy tpm, flush pcrs with extend to selected dummy */

 now do the actual savestate */

		/*

		 * If the TPM indicates that it is too busy to respond to

		 * this command then retry before giving up.  It can take

		 * several seconds for this TPM to be ready.

		 *

		 * This can happen if the TPM has already been sent the

		 * SaveState command before the driver has loaded.  TCG 1.2

		 * specification states that any communication after SaveState

		 * may cause the TPM to invalidate previously saved state.

/**

 * tpm1_get_pcr_allocation() - initialize the allocated bank

 * @chip: TPM chip to use.

 *

 * The function initializes the SHA1 allocated bank to extend PCR

 *

 * Return:

 * * 0 on success,

 * * < 0 on error.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012,2013 Infineon Technologies

 *

 * Authors:

 * Peter Huewe <peter.huewe@infineon.com>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

 *

 * This device driver implements the TPM interface as defined in

 * the TCG TPM Interface Spec version 1.2, revision 1.0 and the

 * Infineon I2C Protocol Stack Specification v0.20.

 *

 * It is based on the original tpm_tis device driver from Leendert van

 * Dorn and Kyleen Hall.

 max. number of iterations after I2C NAK */

/* max. number of iterations after I2C NAK for 'long' commands

 * we need this especially for sending TPM_READY, since the cleanup after the

 * transtion to the ready state may take some time, but it is unpredictable

 * how long it will take.

 After sending TPM_READY to 'reset' the TPM we have to sleep even longer */

 we want to use usleep_range instead of msleep for the 5ms TPM_TIMEOUT */

 expected value for DIDVID register */

	/* In addition to the data itself, the buffer must fit the 7-bit I2C

	 * address and the direction bit.

/*

 * iic_tpm_read() - read from TPM register

 * @addr: register address to read from

 * @buffer: provided by caller

 * @len: number of bytes to read

 *

 * Read len bytes from TPM register and put them into

 * buffer (little-endian format, i.e. first byte is put into buffer[0]).

 *

 * NOTE: TPM is big-endian for multi-byte values. Multi-byte

 * values have to be swapped.

 *

 * NOTE: We can't unfortunately use the combined read/write functions

 * provided by the i2c core as the TPM currently does not support the

 * repeated start condition and due to it's special requirements.

 * The i2c_smbus* functions do not work for this chip.

 *

 * Return -EIO on error, 0 on success.

 Lock the adapter for the duration of the whole sequence. */

		/* use a combined read for newer chips

		 * unfortunately the smbus functions are not suitable due to

		 * the 32 byte limit of the smbus.

		 * retries should usually not be needed, but are kept just to

		 * be on the safe side.

 break here to skip sleep */

		/* Expect to send one command message and one data message, but

		 * support looping over each or both if necessary.

 slb9635 protocol should work in all cases */

 break here to skip sleep */

			/* After the TPM has successfully received the register

			 * address it needs some time, thus we're sleeping here

			 * again, before retrieving the data

					/* Since len is unsigned, make doubly

					 * sure we do not underflow it.

				/* If the I2C adapter rejected the request (e.g

				 * when the quirk read_max_len < len) fall back

				 * to a sane minimum value and try again.

 take care of 'guard time' */

	/* __i2c_transfer returns the number of successfully transferred

	 * messages.

	 * So rc should be greater than 0 here otherwise we have an error.

 prepend the 'register address' to the buffer */

	/*

	 * NOTE: We have to use these special mechanisms here and unfortunately

	 * cannot rely on the standard behavior of i2c_transfer.

	 * Even for newer chips the smbus functions are not

	 * suitable due to the 32 byte limit of the smbus.

 take care of 'guard time' */

	/* __i2c_transfer returns the number of successfully transferred

	 * messages.

	 * So rc should be greater than 0 here otherwise we have an error.

/*

 * iic_tpm_write() - write to TPM register

 * @addr: register address to write to

 * @buffer: containing data to be written

 * @len: number of bytes to write

 *

 * Write len bytes from provided buffer to TPM register (little

 * endian format, i.e. buffer[0] is written as first byte).

 *

 * NOTE: TPM is big-endian for multi-byte values. Multi-byte

 * values have to be swapped.

 *

 * NOTE: use this function instead of the iic_tpm_write_generic function.

 *

 * Return -EIO on error, 0 on success

/*

 * This function is needed especially for the cleanup situation after

 * sending TPM_READY

 ms */

 2 sec */

 implementation similar to tpm_tis */

 wait for burstcount */

 NOTE: since I2C read may fail, return 0 in this case --> time-out */

 if locallity is set STS should not be 0xFF */

 this causes the current command to be aborted */

 wait for burstcount */

 which timeout value, spec has 2 answers (c & d) */

 Note: STS is little endian */

 check current status */

 since we just checked the status, give the TPM some time */

 burstcnt < 0 = TPM is busy */

 limit received data to max. left */

 avoid endless loop in case of broken HW */

 read first 10 bytes, including tag, paramsize, and result */

 retry? */

	/* The TPM needs some time to clean up here,

	 * so we sleep rather than keeping the bus busy

 burstcnt < 0 = TPM is busy */

 avoid endless loop in case of broken HW */

 write last byte */

 go and do it */

	/* The TPM needs some time to clean up here,

	 * so we sleep rather than keeping the bus busy

 Default timeouts */

 read four bytes from DID_VID register */

 We only support one client */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2020 IBM Corporation

 *

 * Author: Ashley Lai <ashleydlai@gmail.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

/**

 * ibmvtpm_send_crq_word() - Send a CRQ request

 * @vdev:	vio device struct

 * @w1:		pre-constructed first word of tpm crq (second word is reserved)

 *

 * Return:

 *	0 - Success

 *	Non-zero - Failure

/**

 * ibmvtpm_send_crq() - Send a CRQ request

 *

 * @vdev:	vio device struct

 * @valid:	Valid field

 * @msg:	Type field

 * @len:	Length field

 * @data:	Data field

 *

 * The ibmvtpm crq is defined as follows:

 *

 * Byte  |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7

 * -----------------------------------------------------------------------

 * Word0 | Valid | Type  |     Length    |              Data

 * -----------------------------------------------------------------------

 * Word1 |                Reserved

 * -----------------------------------------------------------------------

 *

 * Which matches the following structure (on bigendian host):

 *

 * struct ibmvtpm_crq {

 *         u8 valid;

 *         u8 msg;

 *         __be16 len;

 *         __be32 data;

 *         __be64 reserved;

 * } __attribute__((packed, aligned(8)));

 *

 * However, the value is passed in a register so just compute the numeric value

 * to load into the register avoiding byteswap altogether. Endian only affects

 * memory loads and stores - registers are internally represented the same.

 *

 * Return:

 *	0 (H_SUCCESS) - Success

 *	Non-zero - Failure

/**

 * tpm_ibmvtpm_recv - Receive data after send

 *

 * @chip:	tpm chip struct

 * @buf:	buffer to read

 * @count:	size of buffer

 *

 * Return:

 *	Number of bytes read

/**

 * ibmvtpm_crq_send_init - Send a CRQ initialize message

 * @ibmvtpm:	vtpm device struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * tpm_ibmvtpm_resume - Resume from suspend

 *

 * @dev:	device struct

 *

 * Return: Always 0.

/**

 * tpm_ibmvtpm_send() - Send a TPM command

 * @chip:	tpm chip struct

 * @buf:	buffer contains data to send

 * @count:	size of buffer

 *

 * Return:

 *   0 on success,

 *   -errno on error

 wait for previous command to finish */

	/*

	 * set the processing flag before the Hcall, since we may get the

	 * result (interrupt) before even being able to check rc.

		/*

		 * H_CLOSED can be returned after LPM resume.  Call

		 * tpm_ibmvtpm_resume() to re-enable the CRQ then retry

		 * ibmvtpm_send_crq() once before failing.

/**

 * ibmvtpm_crq_get_rtce_size - Send a CRQ request to get rtce size

 *

 * @ibmvtpm:	vtpm device struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * ibmvtpm_crq_get_version - Send a CRQ request to get vtpm version

 *			   - Note that this is vtpm version and not tpm version

 *

 * @ibmvtpm:	vtpm device struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * ibmvtpm_crq_send_init_complete - Send a CRQ initialize complete message

 * @ibmvtpm:	vtpm device struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * tpm_ibmvtpm_remove - ibm vtpm remove entry point

 * @vdev:	vio device struct

 *

 * Return: Always 0.

 For tpm_ibmvtpm_get_desired_dma */

/**

 * tpm_ibmvtpm_get_desired_dma - Get DMA size needed by this driver

 * @vdev:	vio device struct

 *

 * Return:

 *	Number of bytes the driver needs to DMA map.

	/*

	 * ibmvtpm initializes at probe time, so the data we are

	 * asking for may not be set yet. Estimate that 4K required

	 * for TCE-mapped buffer in addition to CRQ.

/**

 * tpm_ibmvtpm_suspend - Suspend

 * @dev:	device struct

 *

 * Return: Always 0.

/**

 * ibmvtpm_reset_crq - Reset CRQ

 *

 * @ibmvtpm:	ibm vtpm struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * ibmvtpm_crq_get_next - Get next responded crq

 *

 * @ibmvtpm:	vtpm device struct

 *

 * Return: vtpm crq pointer or NULL.

/**

 * ibmvtpm_crq_process - Process responded crq

 *

 * @crq:	crq to be processed

 * @ibmvtpm:	vtpm device struct

 *

 len of the data in rtce buffer */

/**

 * ibmvtpm_interrupt -	Interrupt handler

 *

 * @irq:		irq number to handle

 * @vtpm_instance:	vtpm that received interrupt

 *

 * Returns:

 *	IRQ_HANDLED

	/* while loop is needed for initial setup (get version and

	 * get rtce_size). There should be only one tpm request at any

	 * given time.

/**

 * tpm_ibmvtpm_probe - ibm vtpm initialize entry point

 *

 * @vio_dev:	vio device struct

 * @id:		vio device id struct

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * ibmvtpm_module_init - Initialize ibm vtpm module.

 *

 *

 * Return:

 *	0 on success.

 *	Non-zero on failure.

/**

 * ibmvtpm_module_exit - Tear down ibm vtpm module.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015, 2016 IBM Corporation

 * Copyright (C) 2016 Intel Corporation

 *

 * Author: Stefan Berger <stefanb@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for vTPM (vTPM proxy driver)

 public API flags */

 protect buffer and flags */

 internal state */

 waiting for emulator response */

 sending a driver specific command */

 length of queued TPM request */

 length of queued TPM response */

 request/response buffer */

 task that retrieves TPM timeouts */

 all supported flags */

/*

 * Functions related to 'server side'

/**

 * vtpm_proxy_fops_read - Read TPM commands on 'server side'

 *

 * @filp: file pointer

 * @buf: read buffer

 * @count: number of bytes to read

 * @off: offset

 *

 * Return:

 *	Number of bytes read or negative error code

/**

 * vtpm_proxy_fops_write - Write TPM responses on 'server side'

 *

 * @filp: file pointer

 * @buf: write buffer

 * @count: number of bytes to write

 * @off: offset

 *

 * Return:

 *	Number of bytes read or negative error value

/*

 * vtpm_proxy_fops_poll - Poll status on 'server side'

 *

 * @filp: file pointer

 * @wait: poll table

 *

 * Return: Poll flags

/*

 * vtpm_proxy_fops_open - Open vTPM device on 'server side'

 *

 * @filp: file pointer

 *

 * Called when setting up the anonymous file descriptor

/**

 * vtpm_proxy_fops_undo_open - counter-part to vtpm_fops_open

 *       Call to undo vtpm_proxy_fops_open

 *

 *@proxy_dev: tpm proxy device

 no more TPM responses -- wake up anyone waiting for them */

/*

 * vtpm_proxy_fops_release - Close 'server side'

 *

 * @inode: inode

 * @filp: file pointer

 * Return:

 *      Always returns 0.

/*

 * Functions invoked by the core TPM driver to send TPM commands to

 * 'server side' and receive responses from there.

/*

 * Called when core TPM driver reads TPM responses from 'server side'

 *

 * @chip: tpm chip to use

 * @buf: receive buffer

 * @count: bytes to read

 * Return:

 *      Number of TPM response bytes read, negative error value otherwise

 process gone ? */

/*

 * Called when core TPM driver forwards TPM requests to 'server side'.

 *

 * @chip: tpm chip to use

 * @buf: send buffer

 * @count: bytes to send

 *

 * Return:

 *      0 in case of success, negative error value otherwise.

 not supported */

/*

 * Code related to the startup of the TPM 2 and startup of TPM 1.2 +

 * retrieval of timeouts and durations.

/*

 * vtpm_proxy_work_stop: make sure the work has finished

 *

 * This function is useful when user space closed the fd

 * while the driver still determines timeouts.

/*

 * vtpm_proxy_work_start: Schedule the work for TPM 1.2 & 2 initialization

/*

 * Code related to creation and deletion of device pairs

/*

 * Undo what has been done in vtpm_create_proxy_dev

 frees chip */

/*

 * Create a /dev/tpm%d and 'server side' file descriptor pair

 *

 * Return:

 *      Returns file pointer on success, an error value otherwise

 setup an anonymous file for the server-side */

 from now on we can unwind with put_unused_fd() + fput() */

 simulate an open() on the server side */

/*

 * Counter part to vtpm_create_device.

	/*

	 * A client may hold the 'ops' lock, so let it know that the server

	 * side shuts down before we try to grab the 'ops' lock when

	 * unregistering the chip.

/*

 * Code related to the control device /dev/vtpmx

/**

 * vtpmx_ioc_new_dev - handler for the %VTPM_PROXY_IOC_NEW_DEV ioctl

 * @file:	/dev/vtpmx

 * @ioctl:	the ioctl number

 * @arg:	pointer to the struct vtpmx_proxy_new_dev

 *

 * Creates an anonymous file that is used by the process acting as a TPM to

 * communicate with the client processes. The function will also add a new TPM

 * device through which data is proxied to this TPM acting process. The caller

 * will be provided with a file descriptor to communicate with the clients and

 * major and minor numbers for the TPM device.

/*

 * vtpmx_fops_ioctl: ioctl on /dev/vtpmx

 *

 * Return:

 *      Returns 0 on success, a negative error code otherwise.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2016 Google, Inc

 *

 * This device driver implements a TCG PTP FIFO interface over SPI for chips

 * with Cr50 firmware.

 * It is based on tpm_tis_spi driver by Peter Huewe and Christophe Ricard.

/*

 * Cr50 timing constants:

 * - can go to sleep not earlier than after CR50_SLEEP_DELAY_MSEC.

 * - needs up to CR50_WAKE_START_DELAY_USEC to wake after sleep.

 * - requires waiting for "ready" IRQ, if supported; or waiting for at least

 *   CR50_NOIRQ_ACCESS_DELAY_MSEC between transactions, if IRQ is not supported.

 * - waits for up to CR50_FLOW_CONTROL for flow control 'ready' indication.

/*

 * The cr50 interrupt handler just signals waiting threads that the

 * interrupt was asserted.  It does not do any processing triggered

 * by interrupts but is instead used to avoid fixed delays.

/*

 * Cr50 needs to have at least some delay between consecutive

 * transactions. Make sure we wait.

	/*

	 * Note: There is a small chance, if Cr50 is not accessed in a few days,

	 * that time_in_range will not provide the correct result after the wrap

	 * around for jiffies. In this case, we'll have an unneeded short delay,

	 * which is fine.

/*

 * Cr50 might go to sleep if there is no SPI activity for some time and

 * miss the first few bits/bytes on the bus. In such case, wake it up

 * by asserting CS and give it time to start up.

	/*

	 * Note: There is a small chance, if Cr50 is not accessed in a few days,

	 * that time_in_range will not provide the correct result after the wrap

	 * around for jiffies. In this case, we'll probably timeout or read

	 * incorrect value from TPM_STS and just retry the operation.

 Assert CS, wait 1 msec, deassert CS */

 Wait for it to fully wake */

 Reset the time when we need to wake Cr50 again */

/*

 * Flow control: clock the bus and wait for cr50 to set LSB before

 * sending/receiving data. TCG PTP spec allows it to happen during

 * the last byte of header, but cr50 never does that in practice,

 * and earlier versions had a bug when it was set too early, so don't

 * check for it during header transfer.

	/*

	 * Do this outside of spi_bus_lock in case cr50 is not the

	 * only device on that spi bus.

	/*

	 * Write anything to TPM_CR50_FW_VER to start from the beginning

	 * of the version string

 Read the string, 4 bytes at a time, until we get '\0' */

			/*

			 * This is not fatal, the driver will fall back to

			 * delays automatically, since ready will never

			 * be completed without a registered irq handler.

			 * So, just fall through.

			/*

			 * IRQ requested, let's verify that it is actually

			 * triggered, before relying on it.

	/*

	 * Jiffies not increased during suspend, so we need to reset

	 * the time to wake Cr50 after resume.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2005, 2006 IBM Corporation

 * Copyright (C) 2014, 2015 Intel Corporation

 *

 * Authors:

 * Leendert van Doorn <leendert@watson.ibm.com>

 * Kylene Hall <kjhall@us.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Device driver for TCG/TCPA TPM (trusted platform module).

 * Specifications at www.trustedcomputinggroup.org

 *

 * This device driver implements the TPM interface as defined in

 * the TCG TPM Interface Spec version 1.2, revision 1.0.

	/* irq > 0 means: use irq $irq;

	 * irq = 0 means: autoprobe for an irq;

	 * irq = -1 means: no irq support

	/* If the ACPI TPM2 signature is matched then a global ACPI_SIG_TPM2

	 * table is mandatory

 The tpm2_crb driver handles this device */

/*

 * There is a known bug caused by 93e1b7d42e1e ("[PATCH] tpm: add HID module

 * parameter"). This commit added IFX0102 device ID, which is also used by

 * tpm_infineon but ignored to add quirks to probe which driver ought to be

 * used.

 TPM */

 Atmel */

 Infineon */

 Broadcom */

 Broadcom */

 National */

 Intel */

 Add new here */

 User Specified */

 Terminator */

 When forcing auto probe the IRQ */

	/* The driver core will match the name tpm_tis of the device to

	 * the tpm_tis platform driver and complete the setup via

	 * tpm_tis_plat_probe

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Linaro Ltd.

 *

 * This device driver implements MMIO TPM on SynQuacer Platform.

/*

 * irq > 0 means: use irq $irq;

 * irq = 0 means: autoprobe for an irq;

 * irq = -1 means: no irq support

	/*

	 * Due to the limitation of SPI controller on SynQuacer,

	 * 16/32 bits access must be done in byte-wise and descending order.

	/*

	 * Due to the limitation of SPI controller on SynQuacer,

	 * 16/32 bits access must be done in byte-wise and descending order.

	/*

	 * Due to the limitation of SPI controller on SynQuacer,

	 * 16/32 bits access must be done in byte-wise and descending order.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Implementation of the Xen vTPM device frontend

 *

 * Author:  Daniel De Graaf <dgdegra@tycho.nsa.gov>

 check current status */

 cancel requested, not yet canceled */

 Wait for completion of any existing command or cancellation */

 got a signal or timeout, try to cancel */

 In theory the wait at the end of _send makes this one unnecessary */

 caller must clean up in case of errors */

 A suspend/resume/migrate will interrupt a vTPM anyway */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Description:

 * Device Driver for the Infineon Technologies

 * SLD 9630 TT 1.1 and SLB 9635 TT 1.2 Trusted Platform Module

 * Specifications at www.trustedcomputinggroup.org

 *

 * Copyright (C) 2005, Marcel Selhorst <tpmdd@selhorst.net>

 * Sirrix AG - security technologies <tpmdd@sirrix.com> and

 * Applied Data Security Group, Ruhr-University Bochum, Germany

 * Project-Homepage: http://www.trust.rub.de/projects/linux-device-driver-infineon-tpm/ 

 Infineon specific definitions */

 maximum number of WTX-packages */

 msleep-Time for WTX-packages */

 msleep-Time --> Interval to check status register */

 gives number of max. msleep()-calls before throwing timeout */

 MMIO ioremap'd addr */

 phys MMIO base */

 MMIO region size */

 index register offset */

 Data registers */

 IO Port config index reg */

 TPM header definitions */

 some outgoing values */

	/* Note: The values which are currently in the FIFO of the TPM

	   are thrown away since there is no usage for them. Usually,

	   this has nothing to say, since the TPM will give its answer

	   immediately or will be aborted anyway, so the data here is

	   usually garbage and useless.

	   We have to clean this, because the next communication with

	   the TPM would be rubbish, if there is still some old data

	   in the Read FIFO.

 check the status-register if wait_for_bit is set */

 timeout occurs */

    /* Note: WTX means Waiting-Time-Extension. Whenever the TPM needs more

       calculation time, it sends a WTX-package, which has to be acknowledged

       or aborted. This usually occurs if you are hammering the TPM with key

       creation. Set the maximum number of WTX-packages in the definitions

       above, if the number is reached, the waiting-time will be denied

       and the TPM command has to be resend.

 start receiving header */

 size of the data received */

 Disabling Reset, LP and IRQC */

 Sending Header */

 Sending Data Header */

 Sending Data */

	/*

	   Since we are using the legacy mode to communicate

	   with the TPM, we have no cancel functions, but have

	   a workaround for interrupting the TPM through WTX.

 Infineon TPMs */

 read IO-ports through PnP */

 publish my base address and request region */

 publish my base address and request region */

		/*

		 * The only known MMIO based Infineon TPM system provides

		 * a single large mem region with the device config

		 * registers at the default TPM_ADDR.  The data registers

		 * seem like they could be placed anywhere within the MMIO

		 * region, but lets just put them at zero offset.

 query chip for its vendor, its version number a.s.o. */

 configure TPM with IO-ports */

 control if IO-ports are set correctly */

 activate register */

 disable RESET, LP and IRQC */

 Finally, we're done, print some infos */

 Re-configure TPM after suspending */

 activate register */

 disable RESET, LP and IRQC */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Google

 *

 * Authors:

 *      Thiebaud Weksteen <tweek@google.com>

 read binary bios log from EFI configuration table */

 malloc EventLog space */

	/*

	 * The 'final events log' size excludes the 'final events preboot log'

	 * at its beginning.

	/*

	 * Allocate memory for the 'combined log' where we will append the

	 * 'final events log' to.

	/*

	 * Append any of the 'final events log' that didn't also end up in the

	 * 'main log'. Events can be logged in both if events are generated

	 * between GetEventLog() and ExitBootServices().

	/*

	 * The size of the 'combined log' is the size of the 'main log' plus

	 * the size of the 'final events log'.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 IBM Corporation

 *

 * Authors:

 *      Nayna Jain <nayna@linux.vnet.ibm.com>

 *

 * Access to TPM 2.0 event log as written by Firmware.

 * It assumes that writer of event log has followed TCG Specification

 * for Family "2.0" and written the event data in little endian.

 * With that, it doesn't need any endian conversion for structure

 * content.

/*

 * calc_tpm2_event_size() - calculate the event size, where event

 * is an entry in the TPM 2.0 event log. The event is of type Crypto

 * Agile Log Entry Format as defined in TCG EFI Protocol Specification

 * Family "2.0".



 * @event: event whose size is to be calculated.

 * @event_header: the first event in the event log.

 *

 * Returns size of the event. If it is an invalid event, returns 0.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2012 IBM Corporation

 *

 * Author: Ashley Lai <ashleydlai@gmail.com>

 *         Nayna Jain <nayna@linux.vnet.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Read the event log created by the firmware on PPC64

	/*

	 * For both vtpm/tpm, firmware has log addr and log size in big

	 * endian format. But in case of vtpm, there is a method called

	 * sml-handover which is run during kernel init even before

	 * device tree is setup. This sml-handover function takes care

	 * of endianness and writes to sml-base and sml-size in little

	 * endian format. For this reason, vtpm doesn't need conversion

	 * but physical tpm needs the conversion.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2005, 2012 IBM Corporation

 *

 * Authors:

 *	Kent Yoder <key@linux.vnet.ibm.com>

 *	Seiji Munetoh <munetoh@jp.ibm.com>

 *	Stefan Berger <stefanb@us.ibm.com>

 *	Reiner Sailer <sailer@watson.ibm.com>

 *	Kylene Hall <kjhall@us.ibm.com>

 *	Nayna Jain <nayna@linux.vnet.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Access to the event log created by a system's firmware / BIOS

 returns pointer to start of pos. entry of tcg log */

 read over *pos measurements */

 check if current entry is valid */

 now check if current entry is valid */

 41 so there is room for 40 data and 1 nul */

 ToDo Row data -> Base64 */

 hash data */

 convert raw integers for endianness */

 1st: PCR */

 2nd: SHA1 */

 3rd: event type identifier */

 4th: eventname <= max + \'0' delimiter */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2005 IBM Corporation

 *

 * Authors:

 *	Seiji Munetoh <munetoh@jp.ibm.com>

 *	Stefan Berger <stefanb@us.ibm.com>

 *	Reiner Sailer <sailer@watson.ibm.com>

 *	Kylene Hall <kjhall@us.ibm.com>

 *	Nayna Jain <nayna@linux.vnet.ibm.com>

 *

 * Maintained by: <tpmdd-devel@lists.sourceforge.net>

 *

 * Access to the event log extended by the TCG BIOS of PC platform

 Check that the given log is indeed a TPM2 log. */

 read binary bios log */

	/* Unfortuntely ACPI does not associate the event log with a specific

	 * TPM, like PPI. Thus all ACPI TPMs will read the same log.

 Find TCPA entry in RSDT (ACPI_LOGICAL_ADDRESSING) */

 malloc EventLog space */

 try EFI log next */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2005, 2012 IBM Corporation

 *

 * Authors:

 *	Kent Yoder <key@linux.vnet.ibm.com>

 *	Seiji Munetoh <munetoh@jp.ibm.com>

 *	Stefan Berger <stefanb@us.ibm.com>

 *	Reiner Sailer <sailer@watson.ibm.com>

 *	Kylene Hall <kjhall@us.ibm.com>

 *	Nayna Jain <nayna@linux.vnet.ibm.com>

 *

 * Access to the event log created by a system's firmware / BIOS

 now register seq file */

/*

 * tpm_bios_log_setup() - Read the event log from the firmware

 * @chip: TPM chip to use.

 *

 * If an event log is found then the securityfs files are setup to

 * export it to userspace, otherwise nothing is done.

	/* NOTE: securityfs_create_dir can return ENODEV if securityfs is

	 * compiled out. The caller should ignore the ENODEV return code.

	/* securityfs_remove currently doesn't take care of handling sync

	 * between removal and opening of pseudo files. To handle this, a

	 * workaround is added by making i_private = NULL here during removal

	 * and to check it during open(), both within inode_lock()/unlock().

	 * This design ensures that open() either safely gets kref or fails.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * STMicroelectronics TPM Linux driver for TPM ST33ZP24

 * Copyright (C) 2009 - 2016 STMicroelectronics

/*

 * clear_interruption clear the pending interrupt.

 * @param: tpm_dev, the tpm device device.

 * @return: the interrupt status value.

 clear_interruption() */

/*

 * st33zp24_cancel, cancel the current command execution or

 * set STS to COMMAND READY.

 * @param: chip, the tpm_chip description as specified in driver/char/tpm/tpm.h

 st33zp24_cancel() */

/*

 * st33zp24_status return the TPM_STS register

 * @param: chip, the tpm chip description

 * @return: the TPM_STS register value.

 st33zp24_status() */

/*

 * check_locality if the locality is active

 * @param: chip, the tpm chip description

 * @return: true if LOCALITY0 is active, otherwise false

 check_locality() */

/*

 * request_locality request the TPM locality

 * @param: chip, the chip description

 * @return: the active locality or negative value.

 Request locality is usually effective after the request */

 could not get locality */

 request_locality() */

/*

 * release_locality release the active locality

 * @param: chip, the tpm chip description.

/*

 * get_burstcount return the burstcount value

 * @param: chip, the chip description

 * return: the burstcount or negative value.

 get_burstcount() */

/*

 * wait_for_tpm_stat_cond

 * @param: chip, chip description

 * @param: mask, expected mask value

 * @param: check_cancel, does the command expected to be canceled ?

 * @param: canceled, did we received a cancel request ?

 * @return: true if status == mask or if the command is canceled.

 * false in other cases.

/*

 * wait_for_stat wait for a TPM_STS value

 * @param: chip, the tpm chip description

 * @param: mask, the value mask to wait

 * @param: timeout, the timeout

 * @param: queue, the wait queue.

 * @param: check_cancel, does the command can be cancelled ?

 * @return: the tpm status, 0 if success, -ETIME if timeout is reached.

 check current status */

 wait_for_stat() */

/*

 * recv_data receive data

 * @param: chip, the tpm chip description

 * @param: buf, the buffer where the data are received

 * @param: count, the number of data to receive

 * @return: the number of bytes read from TPM FIFO.

/*

 * tpm_ioserirq_handler the serirq irq handler

 * @param: irq, the tpm chip description

 * @param: dev_id, the description of the chip

 * @return: the status of the handler.

 tpm_ioserirq_handler() */

/*

 * st33zp24_send send TPM commands through the I2C bus.

 *

 * @param: chip, the tpm_chip description as specified in driver/char/tpm/tpm.h

 * @param: buf,	the buffer to send.

 * @param: count, the number of bytes to send.

 * @return: In case of success the number of bytes sent.

 *			In other case, a < 0 value describing the issue.

/*

 * st33zp24_recv received TPM response through TPM phy.

 * @param: chip, the tpm_chip description as specified in driver/char/tpm/tpm.h.

 * @param: buf,	the buffer to store datas.

 * @param: count, the number of bytes to send.

 * @return: In case of success the number of bytes received.

 *	    In other case, a < 0 value describing the issue.

/*

 * st33zp24_req_canceled

 * @param: chip, the tpm_chip description as specified in driver/char/tpm/tpm.h.

 * @param: status, the TPM status.

 * @return: Does TPM ready to compute a new command ? true.

/*

 * st33zp24_probe initialize the TPM device

 * @param: client, the i2c_client description (TPM I2C description).

 * @param: id, the i2c_device_id struct.

 * @return: 0 in case of success.

 *	 -1 in other case.

 INTERRUPT Setup */

/*

 * st33zp24_remove remove the TPM device

 * @param: tpm_data, the tpm phy.

 * @return: 0 in case of success.

/*

 * st33zp24_pm_suspend suspend the TPM device

 * @param: tpm_data, the tpm phy.

 * @param: mesg, the power management message.

 * @return: 0 in case of success.

 st33zp24_pm_suspend() */

/*

 * st33zp24_pm_resume resume the TPM device

 * @param: tpm_data, the tpm phy.

 * @return: 0 in case of success.

 st33zp24_pm_resume() */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * STMicroelectronics TPM SPI Linux driver for TPM ST33ZP24

 * Copyright (C) 2009 - 2016 STMicroelectronics

/*

 * TPM command can be up to 2048 byte, A TPM response can be up to

 * 1024 byte.

 * Between command and response, there are latency byte (up to 15

 * usually on st33zp24 2 are enough).

 *

 * Overall when sending a command and expecting an answer we need if

 * worst case:

 * 2048 (for the TPM command) + 1024 (for the TPM answer).  We need

 * some latency byte before the answer is available (max 15).

 * We have 2048 + 1024 + 15.

/*

 * st33zp24_spi_send

 * Send byte to the TIS register according to the ST33ZP24 SPI protocol.

 * @param: phy_id, the phy description

 * @param: tpm_register, the tpm tis register where the data should be written

 * @param: tpm_data, the tpm_data to write inside the tpm_register

 * @param: tpm_size, The length of the data

 * @return: should be zero if success else a negative error code.

 Pre-Header */

 st33zp24_spi_send() */

/*

 * st33zp24_spi_read8_recv

 * Recv byte from the TIS register according to the ST33ZP24 SPI protocol.

 * @param: phy_id, the phy description

 * @param: tpm_register, the tpm tis register where the data should be read

 * @param: tpm_data, the TPM response

 * @param: tpm_size, tpm TPM response size to read.

 * @return: should be zero if success else a negative error code.

 Pre-Header */

 header + status byte + size of the data + status byte */

 st33zp24_spi_read8_reg() */

/*

 * st33zp24_spi_recv

 * Recv byte from the TIS register according to the ST33ZP24 SPI protocol.

 * @param: phy_id, the phy description

 * @param: tpm_register, the tpm tis register where the data should be read

 * @param: tpm_data, the TPM response

 * @param: tpm_size, tpm TPM response size to read.

 * @return: number of byte read successfully: should be one if success.

 st33zp24_spi_recv() */

 evaluate_latency() */

 Get LPCPD GPIO from ACPI */

		/*

		 * lpcpd pin is not specified. This is not an issue as

		 * power management can be also managed by TPM specific

		 * commands. So leave with a success status code.

 Get GPIO from device tree */

		/*

		 * lpcpd pin is not specified. This is not an issue as

		 * power management can be also managed by TPM specific

		 * commands. So leave with a success status code.

 GPIO request and configuration */

 store for late use */

/*

 * st33zp24_spi_probe initialize the TPM device

 * @param: dev, the spi_device description (TPM SPI description).

 * @return: 0 in case of success.

 *	 or a negative value describing the error.

 Check SPI platform functionnalities */

/*

 * st33zp24_spi_remove remove the TPM device

 * @param: client, the spi_device description (TPM SPI description).

 * @return: 0 in case of success.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * STMicroelectronics TPM I2C Linux driver for TPM ST33ZP24

 * Copyright (C) 2009 - 2016 STMicroelectronics

/*

 * write8_reg

 * Send byte to the TIS register according to the ST33ZP24 I2C protocol.

 * @param: tpm_register, the tpm tis register where the data should be written

 * @param: tpm_data, the tpm_data to write inside the tpm_register

 * @param: tpm_size, The length of the data

 * @return: Returns negative errno, or else the number of bytes written.

 write8_reg() */

/*

 * read8_reg

 * Recv byte from the TIS register according to the ST33ZP24 I2C protocol.

 * @param: tpm_register, the tpm tis register where the data should be read

 * @param: tpm_data, the TPM response

 * @param: tpm_size, tpm TPM response size to read.

 * @return: number of byte read successfully: should be one if success.

 read8_reg() */

/*

 * st33zp24_i2c_send

 * Send byte to the TIS register according to the ST33ZP24 I2C protocol.

 * @param: phy_id, the phy description

 * @param: tpm_register, the tpm tis register where the data should be written

 * @param: tpm_data, the tpm_data to write inside the tpm_register

 * @param: tpm_size, the length of the data

 * @return: number of byte written successfully: should be one if success.

/*

 * st33zp24_i2c_recv

 * Recv byte from the TIS register according to the ST33ZP24 I2C protocol.

 * @param: phy_id, the phy description

 * @param: tpm_register, the tpm tis register where the data should be read

 * @param: tpm_data, the TPM response

 * @param: tpm_size, tpm TPM response size to read.

 * @return: number of byte read successfully: should be one if success.

 Get LPCPD GPIO from ACPI */

		/*

		 * lpcpd pin is not specified. This is not an issue as

		 * power management can be also managed by TPM specific

		 * commands. So leave with a success status code.

 Get GPIO from device tree */

		/*

		 * lpcpd pin is not specified. This is not an issue as

		 * power management can be also managed by TPM specific

		 * commands. So leave with a success status code.

 GPIO request and configuration */

 store for late use */

/*

 * st33zp24_i2c_probe initialize the TPM device

 * @param: client, the i2c_client description (TPM I2C description).

 * @param: id, the i2c_device_id struct.

 * @return: 0 in case of success.

 *	 -1 in other case.

/*

 * st33zp24_i2c_remove remove the TPM device

 * @param: client, the i2c_client description (TPM I2C description).

 * @return: 0 in case of success.

 /*

  * A driver for the PCMCIA Smartcard Reader "Omnikey CardMan Mobile 4000"

  *

  * cm4000_cs.c support.linux@omnikey.com

  *

  * Tue Oct 23 11:32:43 GMT 2001 herp - cleaned up header files

  * Sun Jan 20 10:11:15 MET 2002 herp - added modversion header files

  * Thu Nov 14 16:34:11 GMT 2002 mh   - added PPS functionality

  * Tue Nov 19 16:36:27 GMT 2002 mh   - added SUSPEND/RESUME functionailty

  * Wed Jul 28 12:55:01 CEST 2004 mh  - kernel 2.6 adjustments

  *

  * current version: 2.4.0gm4

  *

  * (C) 2000,2001,2002,2003,2004 Omnikey AG

  *

  * (C) 2005-2006 Harald Welte <laforge@gnumonks.org>

  * 	- Adhere to Kernel process/coding-style.rst

  * 	- Port to 2.6.13 "new" style PCMCIA

  * 	- Check for copy_{from,to}_user return values

  * 	- Use nonseekable_open()

  * 	- add class interface for udev device creation

  *

  * All rights reserved. Licensed under dual BSD/GPL license.

 #define ATR_CSUM */

 n (debug level) is ignored */

/* additional debug output may be enabled by re-compiling with

 #define CM4000_DEBUG */

 major number we get from the kernel */

 note: the first state has to have number 0 always */

	wait_queue_head_t devq;		/* when removing cardman must not be

 if IO is locked, wait on this Q */

 wait for ATR valid */

 used by write to wake blk.read */

	/* warning: do not move this struct group.

 bytes avail. after write */

 latest read pos. write zeroes */

 T=0 procedure byte */

 state of card monitor */

 slow down warning */

 cardman IO-flags 0 */

 cardman IO-flags 1 */

 variable monitor speeds, in jiffies */

 baud value for speed */

 T=0, T=1, ... */

	unsigned long flags;	/* lock+flags (MONITOR,IO,ATR) * for concurrent

 used to keep monitor running */

/* This table doesn't use spaces after the comma between fields and thus

 * violates process/coding-style.rst.  However, I don't really think wrapping it around will

FI     00   01   02   03   04   05   06   07   08   09   10   11   12   13 */

DI */

 0 */ {0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11},

 1 */ {0x01,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x91,0x11,0x11,0x11,0x11},

 2 */ {0x02,0x12,0x22,0x32,0x11,0x11,0x11,0x11,0x11,0x92,0xA2,0xB2,0x11,0x11},

 3 */ {0x03,0x13,0x23,0x33,0x43,0x53,0x63,0x11,0x11,0x93,0xA3,0xB3,0xC3,0xD3},

 4 */ {0x04,0x14,0x24,0x34,0x44,0x54,0x64,0x11,0x11,0x94,0xA4,0xB4,0xC4,0xD4},

 5 */ {0x00,0x15,0x25,0x35,0x45,0x55,0x65,0x11,0x11,0x95,0xA5,0xB5,0xC5,0xD5},

 6 */ {0x06,0x16,0x26,0x36,0x46,0x56,0x66,0x11,0x11,0x96,0xA6,0xB6,0xC6,0xD6},

 7 */ {0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11,0x11},

 8 */ {0x08,0x11,0x28,0x38,0x48,0x58,0x68,0x11,0x11,0x98,0xA8,0xB8,0xC8,0xD8},

 9 */ {0x09,0x19,0x29,0x39,0x49,0x59,0x69,0x11,0x11,0x99,0xA9,0xB9,0xC9,0xD9}

 FI */

 DI */

 XXX PROTO */

 defaults to 9600 baud */

 read first interface byte and TA1 is present */

 TA(2) */

 no of int.face chars */

 ATRLENCK(dev,ix); */

 TDi */

 compute csum */

 XXX PROTO */

 ACOS */

 Motorola */

 ISO default */

 set baudrate */

 set stopbits */

 Fill PTS structure */

 Set new protocol */

 Correct Fi/Di according to CM4000 Fi/Di table */

 set Fi/Di according to ATR TA(1) */

 Calculate PCK character */

 check card convention */

 reset SM */

 Enable access to the message buffer */

 T_Active */

 inv parity */

 MSB-baud */

 write challenge to the buffer */

 buf data */

 set number of bytes to write */

 Trigger CARDMAN CONTROLLER */

 Monitor progress */

 wait for xmit done */

 check whether it is a short PTS reply? */

 Read PPS reply */

 CM4000_DEBUG */

 Compare ptsreq and ptsreply */

 setcardparameter according to PPS */

 short PTS reply, set card parameter to default values */

 note: statemachine is assumed to be reset */

 detect CMM = 1 -> failure */

 xoutb(0x40, REG_FLAGS1(iobase)); detectCMM */

 detect CMM=0 -> failure */

 clear detectCMM again by restoring original flags1 */

	/* tell the monitor to stop and wait until

	 * it terminates.

	/* now, LOCK_MONITOR has been set.

	 * allow a last cycle in the monitor.

	 * the monitor will indicate that it has

	 * finished by clearing this bit.

/*

 * monitor the card every 50msec. as a side-effect, retrieve the

 * atr once a card is inserted. another side-effect of retrieving the

 * atr is that the card will be powered on, so there is no need to

 * power on the card explicitly from the application: the driver

 * is already doing that for you.

 if someone has set the lock for us: we're done! */

 no */

 close et al. are sleeping on devq, so wake it */

 try to lock io: if it is already locked, just add another timer */

 is a card/a reader inserted at all ? */

 no smartcard inserted */

 no cardman inserted */

 no */

 only keep IO and MONITOR locks */

		/* cardman and card present but cardman was absent before

 wait until Flags0 indicate power is off */

			/* Flags0 indicate power off and no card inserted now;

			/* prepare for fetching ATR again: after card off ATR

 minimal gap between CARDOFF and read ATR is 50msec */

 9600 */

 stopbits=2 */

 baud value */

		xoutb(0x21, REG_FLAGS1(iobase));	/* T_Active=1, baud

 warm start vs. power on: */

 numRecBytes */

 infinite loop possible, since there is no timeout */

 + XX msec */

 set new timeout */

 reset SM */

 Deactivate T_Active flags */

 atr is present (which doesn't mean it's valid) */

 atr invalid */

 if ta1 == 0x11, no PPS necessary (default values) */

 do not do PPS with multi protocol cards */

 prepare for repowering  */

				/* for cards which use slightly different

 slow down warning, but prompt immediately after insertion */

 wake open */

 nothing */

 whoever needs IO */

 Interface to userland (file_operations) */

 according to manpage */

 device removed */

 also see the note about this in cmm_write */

 this one implements blocking IO */

 lock io */

 no smartcard inserted */

 no cardman inserted */

 MSB buf addr set */

 Clear T1Active */

 clear detectCMM */

 last check before exit */

 according to manpage */

 T0 must have at least 4 bytes */

 max bytes to write */

 device removed */

	/*

	 * wait for atr to become valid.

	 * note: it is important to lock this code. if we dont, the monitor

	 * could be run between test_bit and the call to sleep on the

	 * atr-queue.  if *then* the monitor detects atr valid, it will wake up

	 * any process on the atr-queue, *but* since we have been interrupted,

	 * we do not yet sleep on this queue. this would result in a missed

	 * wake_up and the calling process would sleep forever (until

	 * interrupted).  also, do *not* restore_flags before sleep_on, because

	 * this could result in the same situation!

 invalid atr */

 lock io */

 no smartcard inserted */

 no cardman inserted */

 reset SM  */

 reflect T=0 send/read mode in flags1 */

 dummy read, reset flag procedure received */

 T_Active */

 inverse parity  */

 MSB-Baud */

 xmit data */

 T_Active */

 SendT0 */

 inverse parity: */

 MSB-Baud */

 set address high */

 T=0 proto: 0 byte reply  */

 numSendBytes */

 T0: output procedure byte */

 SM_Active */

 power on if needed */

 T=1/T=0 */

 MSB numSendBytes */ );

 SM_Active */

 power on if needed */

 T=1/T=0 */

 MSB numSendBytes */

 wait for xmit done */

 T=1: wait for infoLen */

 wait until infoLen is valid */

 max waiting time of 1 min */

 numRecBytes | bit9 of numRecytes */

 max waiting time of 2 sec */

 numRecBytes | bit9 of numRecytes */

 reset timeout */

		/* T=0: we are done when numRecBytes doesn't

		 *      increment any more and NoProcedureByte

		 *      is set and numRecBytes == bytes sent + 6

		 *      (header bytes + data + 1 for sw2)

		 *      except when the card replies an error

		 *      which means, no data will be sent back.

 no procedure byte received since last read */

 i=0; */

 procedure byte received since last read */

 resettimeout */

 T=1: read offset=zero, T=0: read offset=after challenge */

 reset SM */

 tell read we have data */

 ITSEC E2: clear write buffer */

 return error or actually written bytes */

 reset monitor SM */

			/* clear other bits, but leave inserted & powered as

 allow nonblocking io and being interrupted */

 is a card inserted and powered? */

 get IO lock */

 Set Flags0 = 0x42 */

 release lock */

 wait for ATR to get valid */

 get IO lock */

 auto power_on again */

 release lock */

	/* init device variables, they may be "polluted" after close

	 * or, the device may never have been closed (i.e. open failed)

	/* opening will always block since the

	 * monitor will be started by open, which

	 * means we have to wait for ATR becoming

	 * valid = block until valid (or card

	 * inserted)

 start monitoring the cardstatus */

 only one open per device */

 only one open per device */

 socket removed? */

	/* dont terminate the monitor, rather rely on

	 * close doing that for us.

		/* note: don't interrupt us:

		 * close the applications which own

		 * the devices _first_ !

 dev->devq=NULL;	this cannot be zeroed earlier */

==== Interface to PCMCIA Layer =======================================*/

 read the config-tuples */

 delay release until device closed */

 create a new cm4000_cs device */

 find device */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * SCR24x PCMCIA Smart Card Reader Driver

 *

 * Copyright (C) 2005-2006 TL Sudheendran

 * Copyright (C) 2016 Lubomir Rintel

 *

 * Derived from "scr24x_v4.2.6_Release.tar.gz" driver by TL Sudheendran.

	/* We must not take the dev->lock here as scr24x_delete()

	 * might be called to remove the dev structure altogether.

	 * We don't need the lock anyway, since after the reference

	 * acquired in probe() is released in remove() the chrdev

	 * is already unregistered and noone can possibly acquire

/*

 * linux/drivers/char/pcmcia/synclink_cs.c

 *

 * $Id: synclink_cs.c,v 4.34 2005/09/08 13:20:54 paulkf Exp $

 *

 * Device driver for Microgate SyncLink PC Card

 * multiprotocol serial adapter.

 *

 * written by Paul Fulghum for Microgate Corporation

 * paulkf@microgate.com

 *

 * Microgate and SyncLink are trademarks of Microgate Corporation

 *

 * This code is released under the GNU General Public License (GPL)

 *

 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES

 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE

 * DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,

 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES

 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR

 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)

 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,

 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)

 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED

 * OF THE POSSIBILITY OF SUCH DAMAGE.

 unsigned long mode */

 unsigned char loopback; */

 unsigned short flags; */

 unsigned char encoding; */

 unsigned long clock_speed; */

 unsigned char addr_filter; */

 unsigned short crc_type; */

 unsigned char preamble_length; */

 unsigned char preamble; */

 unsigned long data_rate; */

 unsigned char data_bits; */

 unsigned char stop_bits; */

 unsigned char parity; */

 The queue of BH actions to be performed */

/*

 * Device instance data structure

 General purpose pointer (used by SPPP) */

 xon/xoff character */

 circular list of fixed length rx buffers */

 memory allocated for all rx buffers */

 size of memory allocated for rx buffers */

 index of next empty rx buffer */

 index of next full rx buffer */

 size in bytes of single rx buffer */

 total number of rx buffers */

 number of full rx buffers */

 HDLC transmit timeout timer */

 device list link */

 task structure for scheduling bh */

 check counts to prevent */

 too many IRQs if a signal */

 is floating */

 serial interface selection (RS-232, v.35 etc) */

 device instance name */

 base I/O address of adapter */

 communications parameters */

 current serial signal states */

 for diagnostics use */

 startup error (DIAGS)	*/

 PCMCIA support */

 SPPP/Cisco HDLC device parts */

/*

 * The size of the serial xmit buffer is 1 page, or 4096 bytes

 channel A offset */

 channel B offset */

/*

 *  FIXME: PPC has PVR defined in asm/reg.h.  For now we just undef it.

 IMR/ISR

 rx break detected

 receive data overflow

 all sent

 transmit data underrun

 timer interrupt

 CTS status change

 tx message repeat

 transmit pool ready

 receive message end

 receive frame start

 rx char timeout

 carrier detect status change

 receive frame overflow

 receive pool full

 STAR

 transmit FIFO write enable

 command executing

 CTS state

 0010b */

 1110b */

 0100b */

 Register access functions */

/*

 * interrupt enable/disable routines

/*

 * Bottom half interrupt handlers

/*

 * ioctl handlers

/*

 * Set this param to non-zero to load eax with the

 * .text section address and breakpoint on module load.

 * This is useful for use with gdb and add-symbol-file command.

/*

 * Driver major number, defaults to zero to get auto

 * assigned major number. May be forced as module parameter.

 number of characters left in xmit buffer before we ask for more */

 PCMCIA prototypes */

/*

 * 1st function defined in .text section. Calling this function in

 * init_module() followed by a breakpoint allows a remote debugger

 * (gdb) to get the .text address for the add-symbol-file command.

 * This allows remote debugging of dynamically loadable modules.

/**

 * line discipline callback wrappers

 *

 * The wrappers maintain line discipline references

 * while calling into the line discipline.

 *

 * ldisc_receive_buf  - pass receive data to line discipline

 Initialize the struct pcmcia_device structure */

/* Card has been inserted.

/* Card has been removed.

 * Unregister device and release PCMCIA configuration.

 * If device is open, postpone until it is closed.

 release current rx FIFO

 receiver reset

 release current tx FIFO

 transmit end message

 transmit reset

 wait for command completion */

/* Return next bottom half action to perform.

 * or 0 if nothing to do.

 Mark BH routine as complete */

 Process work item */

 unknown work item ID */

 eom: non-zero = end of frame */

 no more free buffers */

 end of frame, get FIFO count from RBCL register */

 frame too large, reset receiver and reset current buffer */

 early termination, get FIFO count from RBCL register */

		/* Zero fifo count could mean 0 or 32 bytes available.

		 * If BIT5 of STAR is set then at least 1 byte is available.

 Flush received async data to receive data buffer. */

 if no frameing/crc error then save data

 BIT7:parity error

 BIT6:framing error

 discard char if tty control flags say so */

/* Interrupt service routine entry point.

 *

 * Arguments:

 *

 * irq     interrupt number that caused interrupt

 * dev_id  device ID supplied during interrupt registration

 receive IRQs */

 transmit IRQs */

	/* Request bottom half processing if there's something

	 * for it to do and the bh is not already running

/* Initialize and start device.

 allocate a page of memory for a transmit buffer */

 Allocate and claim adapter resources */

 perform existence check and diagnostics */

 program hardware for current parameters */

/* Called by mgslpc_close() and mgslpc_hangup() to shutdown hardware

 clear status wait queue because status changes */

 can't happen after shutting down the hardware */

 TODO:disable interrupts instead of reset to preserve signal states */

/* Reconfigure adapter based on new parameters

 if B0 rate (hangup) specified then negate RTS and DTR */

 otherwise assert RTS and DTR */

 byte size and parity */

	/* calculate number of jiffies to transmit a full

	 * FIFO (32 bytes) at specified data rate

	/* if port data rate is set to 460800 or less then

	 * allow tty settings to override, otherwise keep the

	 * current data rate.

 Add .02 seconds of slop */

 process tty input control flags */

/* Add a character to the transmit buffer

/* Enable transmitter so remaining characters in the

 * transmit buffer are sent.

/* Send a block of data

 *

 * Arguments:

 *

 * tty        pointer to tty information structure

 * buf	      pointer to buffer containing send data

 * count      size of send data in bytes

 *

 * Returns: number of characters written

/* Return the count of free bytes in transmit buffer

 HDLC (frame oriented) mode */

/* Return the count of bytes in transmit buffer

/* Discard all data in the send buffer

/* Send a high-priority XON/XOFF character

/* Signal remote device to throttle send data (our receive data)

/* Signal remote device to stop throttling send data (our receive data)

/* get the current serial statistics

/* get the current serial parameters

/* set the serial parameters

 *

 * Arguments:

 *

 *	info		pointer to device instance data

 *	new_params	user buffer containing new serial params

 *

 * Returns:	0 if success, otherwise error code

		/* clear data count so FIFO is not filled on next IRQ.

		 * This results in underrun and abort transmission.

/* wait for specified event to occur

 *

 * Arguments:		info	pointer to device instance data

 *			mask	pointer to bitmask of events to wait for

 * Return Value:	0	if successful and bit mask updated with

 *				of events triggerred,

 *			otherwise error code

 return immediately if state matches requested events */

 save current irq counts */

 get current irq counts */

 if no change, wait aborted for some reason */

 save current irq counts */

 get new irq counts */

 if no change, wait aborted for some reason */

 check for change in caller specified modem input */

/* return the state of the serial control and status signals

/* set modem control signals (DTR/RTS)

/* Set or clear transmit break condition

 *

 * Arguments:		tty		pointer to tty instance data

 *			break_state	-1=set break condition, 0=clear

 kernel counter temps */

/* Service an IOCTL request

 *

 * Arguments:

 *

 *	tty	pointer to tty instance data

 *	cmd	IOCTL command code

 *	arg	command argument/context

 *

 * Return Value:	0 if success, otherwise error code

/* Set new termios settings

 *

 * Arguments:

 *

 *	tty		pointer to tty structure

 *	termios		pointer to buffer to hold returned old termios

 just return if nothing has changed */

 Handle transition to B0 status */

 Handle transition away from B0 status */

 Handle turning off CRTSCTS */

/* Wait until the transmitter is empty.

	/* Set check interval to 1/5 of estimated time to

	 * send a character, and make it at least 1. The check

	 * interval should also be less than the timeout.

	 * Note: use tight timings here to satisfy the NIST-PCTS.

/* Called by tty_hangup() when a hangup is signaled.

 * This is the same as closing all open files for the port.

 verify range of specified line number */

 find the info structure for the specified line */

 1st open on this device, init hardware */

/*

 * /proc fs routines....

 output current serial signal states */

 Append serial signal status to end */

/* Called to print information about devices

 each buffer has header and data */

 calculate total allocation size for 8 buffers */

 limit total allocated memory */

 calculate number of buffers */

 unused flag buffer to satisfy receive_buf calling interface */

/* Add the specified device instance data structure to the

 * global linked list of devices and increment the device count.

 *

 * Arguments:		info	pointer to device instance data

 Initialize the tty_driver structure */

	/* note:standard BRG mode is broken in V3.2 chip

	 * so enhanced mode is always used

		/* BGR[5..0] = N

		 * BGR[9..6] = M

		 * BGR[7..0] contained in BGR register

		 * BGR[9..8] contained in CCR2[7..6]

		 * divisor = (N+1)*2^M

		 *

		 * Note: M *must* not be zero (causes asymetric duty cycle)

/* Enabled the AUX clock output at the specified frequency.

	/* MODE

	 *

	 * 07..06  MDS[1..0] 10 = transparent HDLC mode

	 * 05      ADM Address Mode, 0 = no addr recognition

	 * 04      TMD Timer Mode, 0 = external

	 * 03      RAC Receiver Active, 0 = inactive

	 * 02      RTS 0=RTS active during xmit, 1=RTS always active

	 * 01      TRS Timer Resolution, 1=512

	 * 00      TLP Test Loop, 0 = no loop

	 *

	 * 1000 0010

 channel B RTS is used to enable AUXCLK driver on SP505 */

	/* CCR0

	 *

	 * 07      PU Power Up, 1=active, 0=power down

	 * 06      MCE Master Clock Enable, 1=enabled

	 * 05      Reserved, 0

	 * 04..02  SC[2..0] Encoding

	 * 01..00  SM[1..0] Serial Mode, 00=HDLC

	 *

	 * 11000000

	/* CCR1

	 *

	 * 07      SFLG Shared Flag, 0 = disable shared flags

	 * 06      GALP Go Active On Loop, 0 = not used

	 * 05      GLP Go On Loop, 0 = not used

	 * 04      ODS Output Driver Select, 1=TxD is push-pull output

	 * 03      ITF Interframe Time Fill, 0=mark, 1=flag

	 * 02..00  CM[2..0] Clock Mode

	 *

	 * 0001 0111

	/* CCR2 (Channel B)

	 *

	 * 07..06  BGR[9..8] Baud rate bits 9..8

	 * 05      BDF Baud rate divisor factor, 0=1, 1=BGR value

	 * 04      SSEL Clock source select, 1=submode b

	 * 03      TOE 0=TxCLK is input, 1=TxCLK is output

	 * 02      RWX Read/Write Exchange 0=disabled

	 * 01      C32, CRC select, 0=CRC-16, 1=CRC-32

	 * 00      DIV, data inversion 0=disabled, 1=enabled

	 *

	 * 0011 1000

	/* CCR4

	 *

	 * 07      MCK4 Master Clock Divide by 4, 1=enabled

	 * 06      EBRG Enhanced Baud Rate Generator Mode, 1=enabled

	 * 05      TST1 Test Pin, 0=normal operation

	 * 04      ICD Ivert Carrier Detect, 1=enabled (active low)

	 * 03..02  Reserved, must be 0

	 * 01..00  RFT[1..0] RxFIFO Threshold 00=32 bytes

	 *

	 * 0101 0000

	/* if auxclk not enabled, set internal BRG so

	 * CTS transitions can be detected (requires TxC)

 CCR1:02..00  CM[2..0] Clock Mode = 111 (clock mode 7) */

 CCR2:04 SSEL Clock source select, 1=submode b */

 set LinkSpeed if available, otherwise default to 2Mbps */

 MODE:00 TLP Test Loop, 1=loopback enabled */

 disable all interrupts */

 assume clock mode 0a, rcv=RxC xmt=TxC */

 clock mode 7a, rcv = DPLL, xmt = DPLL */

 clock mode 7b, rcv = BRG, xmt = BRG */

 clock mode 6b, rcv = DPLL, xmt = BRG/16 */

 clock mode 6a, rcv = DPLL, xmt = TxC */

 clock mode 0b, rcv = RxC, xmt = BRG */

	/* MODE

	 *

	 * 07..06  MDS[1..0] 10 = transparent HDLC mode

	 * 05      ADM Address Mode, 0 = no addr recognition

	 * 04      TMD Timer Mode, 0 = external

	 * 03      RAC Receiver Active, 0 = inactive

	 * 02      RTS 0=RTS active during xmit, 1=RTS always active

	 * 01      TRS Timer Resolution, 1=512

	 * 00      TLP Test Loop, 0 = no loop

	 *

	 * 1000 0010

 preserve RTS state */

	/* CCR0

	 *

	 * 07      PU Power Up, 1=active, 0=power down

	 * 06      MCE Master Clock Enable, 1=enabled

	 * 05      Reserved, 0

	 * 04..02  SC[2..0] Encoding

	 * 01..00  SM[1..0] Serial Mode, 00=HDLC

	 *

	 * 11000000

 FM0

 FM1

 Manchester

	/* CCR1

	 *

	 * 07      SFLG Shared Flag, 0 = disable shared flags

	 * 06      GALP Go Active On Loop, 0 = not used

	 * 05      GLP Go On Loop, 0 = not used

	 * 04      ODS Output Driver Select, 1=TxD is push-pull output

	 * 03      ITF Interframe Time Fill, 0=mark, 1=flag

	 * 02..00  CM[2..0] Clock Mode

	 *

	 * 0001 0000

	/* CCR2

	 *

	 * 07..06  BGR[9..8] Baud rate bits 9..8

	 * 05      BDF Baud rate divisor factor, 0=1, 1=BGR value

	 * 04      SSEL Clock source select, 1=submode b

	 * 03      TOE 0=TxCLK is input, 0=TxCLK is input

	 * 02      RWX Read/Write Exchange 0=disabled

	 * 01      C32, CRC select, 0=CRC-16, 1=CRC-32

	 * 00      DIV, data inversion 0=disabled, 1=enabled

	 *

	 * 0000 0000

	/* CCR3

	 *

	 * 07..06  PRE[1..0] Preamble count 00=1, 01=2, 10=4, 11=8

	 * 05      EPT Enable preamble transmission, 1=enabled

	 * 04      RADD Receive address pushed to FIFO, 0=disabled

	 * 03      CRL CRC Reset Level, 0=FFFF

	 * 02      RCRC Rx CRC 0=On 1=Off

	 * 01      TCRC Tx CRC 0=On 1=Off

	 * 00      PSD DPLL Phase Shift Disable

	 *

	 * 0000 0000

 PRE - Preamble pattern */

	/* CCR4

	 *

	 * 07      MCK4 Master Clock Divide by 4, 1=enabled

	 * 06      EBRG Enhanced Baud Rate Generator Mode, 1=enabled

	 * 05      TST1 Test Pin, 0=normal operation

	 * 04      ICD Ivert Carrier Detect, 1=enabled (active low)

	 * 03..02  Reserved, must be 0

	 * 01..00  RFT[1..0] RxFIFO Threshold 00=32 bytes

	 *

	 * 0101 0000

	/* RLCR Receive length check register

	 *

	 * 7     1=enable receive length check

	 * 6..0  Max frame length = (RL + 1) * 32

	/* XBCH Transmit Byte Count High

	 *

	 * 07      DMA mode, 0 = interrupt driven

	 * 06      NRM, 0=ABM (ignored)

	 * 05      CAS Carrier Auto Start

	 * 04      XC Transmit Continuously (ignored)

	 * 03..00  XBC[10..8] Transmit byte count bits 10..8

	 *

	 * 0000 0000

 PVR[3] 1=AUTO CTS active */

 clear pending IRQs */

	/* Master clock mode enabled above to allow reset commands

	 * to complete even if no data clocks are present.

	 *

	 * Disable master clock mode for normal communications because

	 * V3.2 of the ESCC2 has a bug that prevents the transmit all sent

	 * IRQ when in master clock mode.

	 *

	 * Leave master clock mode enabled for IRQ test because the

	 * timer IRQ used by the test can only happen in master clock mode.

 MODE:03 RAC Receiver Active, 0=inactive */

 MODE:03 RAC Receiver Active, 1=active */

 If auto RTS enabled and RTS is inactive, then assert */

 RTS and set a flag indicating that the driver should */

 negate RTS when the transmission completes. */

/* Reset the adapter to a known state and prepare it for further use.

 power up both channels (set BIT7) */

 disable all interrupts */

	/* PCR Port Configuration Register

	 *

	 * 07..04  DEC[3..0] Serial I/F select outputs

	 * 03      output, 1=AUTO CTS control enabled

	 * 02      RI Ring Indicator input 0=active

	 * 01      DSR input 0=active

	 * 00      DTR output 0=active

	 *

	 * 0000 0110

	/* PVR Port Value Register

	 *

	 * 07..04  DEC[3..0] Serial I/F select (0000=disabled)

	 * 03      AUTO CTS output 1=enabled

	 * 02      RI Ring Indicator input

	 * 01      DSR input

	 * 00      DTR output (1=inactive)

	 *

	 * 0000 0001

	write_reg(info, PVR, PVR_DTR);

	/* IPC Interrupt Port Configuration

	 *

	 * 07      VIS 1=Masked interrupts visible

	 * 06..05  Reserved, 0

	 * 04..03  SLA Slave address, 00 ignored

	 * 02      CASM Cascading Mode, 1=daisy chain

	 * 01..00  IC[1..0] Interrupt Config, 01=push-pull output, active low

	 *

	 * 0000 0101

 disable all interrupts */

	/* MODE

	 *

	 * 07      Reserved, 0

	 * 06      FRTS RTS State, 0=active

	 * 05      FCTS Flow Control on CTS

	 * 04      FLON Flow Control Enable

	 * 03      RAC Receiver Active, 0 = inactive

	 * 02      RTS 0=Auto RTS, 1=manual RTS

	 * 01      TRS Timer Resolution, 1=512

	 * 00      TLP Test Loop, 0 = no loop

	 *

	 * 0000 0110

 preserve RTS state */

	/* CCR0

	 *

	 * 07      PU Power Up, 1=active, 0=power down

	 * 06      MCE Master Clock Enable, 1=enabled

	 * 05      Reserved, 0

	 * 04..02  SC[2..0] Encoding, 000=NRZ

	 * 01..00  SM[1..0] Serial Mode, 11=Async

	 *

	 * 1000 0011

	/* CCR1

	 *

	 * 07..05  Reserved, 0

	 * 04      ODS Output Driver Select, 1=TxD is push-pull output

	 * 03      BCR Bit Clock Rate, 1=16x

	 * 02..00  CM[2..0] Clock Mode, 111=BRG

	 *

	 * 0001 1111

	/* CCR2 (channel A)

	 *

	 * 07..06  BGR[9..8] Baud rate bits 9..8

	 * 05      BDF Baud rate divisor factor, 0=1, 1=BGR value

	 * 04      SSEL Clock source select, 1=submode b

	 * 03      TOE 0=TxCLK is input, 0=TxCLK is input

	 * 02      RWX Read/Write Exchange 0=disabled

	 * 01      Reserved, 0

	 * 00      DIV, data inversion 0=disabled, 1=enabled

	 *

	 * 0001 0000

	/* CCR3

	 *

	 * 07..01  Reserved, 0

	 * 00      PSD DPLL Phase Shift Disable

	 *

	 * 0000 0000

	/* CCR4

	 *

	 * 07      MCK4 Master Clock Divide by 4, 1=enabled

	 * 06      EBRG Enhanced Baud Rate Generator Mode, 1=enabled

	 * 05      TST1 Test Pin, 0=normal operation

	 * 04      ICD Ivert Carrier Detect, 1=enabled (active low)

	 * 03..00  Reserved, must be 0

	 *

	 * 0101 0000

	/* DAFO Data Format

	 *

	 * 07      Reserved, 0

	 * 06      XBRK transmit break, 0=normal operation

	 * 05      Stop bits (0=1, 1=2)

	 * 04..03  PAR[1..0] Parity (01=odd, 10=even)

	 * 02      PAREN Parity Enable

	 * 01..00  CHL[1..0] Character Length (00=8, 01=7)

	 *

 7 bits */

 Parity enable */

	/* RFC Rx FIFO Control

	 *

	 * 07      Reserved, 0

	 * 06      DPS, 1=parity bit not stored in data byte

	 * 05      DXS, 0=all data stored in FIFO (including XON/XOFF)

	 * 04      RFDF Rx FIFO Data Format, 1=status byte stored in FIFO

	 * 03..02  RFTH[1..0], rx threshold, 11=16 status + 16 data byte

	 * 01      Reserved, 0

	 * 00      TCDE Terminate Char Detect Enable, 0=disabled

	 *

	 * 0101 1100

	/* RLCR Receive length check register

	 *

	 * Max frame length = (RL + 1) * 32

	/* XBCH Transmit Byte Count High

	 *

	 * 07      DMA mode, 0 = interrupt driven

	 * 06      NRM, 0=ABM (ignored)

	 * 05      CAS Carrier Auto Start

	 * 04      XC Transmit Continuously (ignored)

	 * 03..00  XBC[10..8] Transmit byte count bits 10..8

	 *

	 * 0000 0000

 MODE:03 RAC Receiver Active, 1=active */

 PVR[3] 1=AUTO CTS active */

 clear pending IRQs */

/* Set the HDLC idle mode for the transmitter.

 Note: ESCC2 only supports flags and one idle modes */

/* get state of the V24 status (input) signals.

 preserve RTS and DTR */

/* Set the state of RTS and DTR based on contents of

 * serial_signals member of device extension.

/* Attempt to return a received HDLC frame

 * Only frames received without errors are returned.

 *

 * Returns true if frame returned, otherwise false

	/* 07  VFR  1=valid frame

	 * 06  RDO  1=data overrun

	 * 05  CRC  1=OK, 0=error

	 * 04  RAB  1=frame aborted

 init hdlc mode */

 512 cycles */

/* HDLC frame time out

 * update stats and do tx completion processing

/**

 * called by generic HDLC layer when protocol selected (PPP, frame relay, etc.)

 * set encoding and frame check sequence (FCS) options

 *

 * dev       pointer to network device structure

 * encoding  serial encoding setting

 * parity    FCS setting

 *

 * returns 0 if success, otherwise error code

 return error if TTY interface open */

 if network interface up, reprogram hardware */

/**

 * called by generic HDLC layer to send frame

 *

 * skb  socket buffer containing HDLC frame

 * dev  pointer to network device structure

 stop sending until this frame completes */

 copy data to device buffers */

 update network statistics */

 done with socket buffer, so free it */

 save start time for transmit timeout detection */

 start hardware transmitter if necessary */

/**

 * called by network layer when interface enabled

 * claim resources and initialize hardware

 *

 * dev  pointer to network device structure

 *

 * returns 0 if success, otherwise error code

 generic HDLC layer open processing */

 arbitrate between network and tty opens */

 claim resources and init adapter */

 assert RTS and DTR, apply hardware settings */

 enable network layer transmit */

 inform generic HDLC layer of current DCD status */

/**

 * called by network layer when interface is disabled

 * shutdown hardware and release resources

 *

 * dev  pointer to network device structure

 *

 * returns 0 if success, otherwise error code

 shutdown adapter and release resources */

/**

 * called by network layer to process IOCTL call to network device

 *

 * dev  pointer to network device structure

 * ifs  pointer to network interface settings structure

 *

 * returns 0 if success, otherwise error code

 return error if TTY interface open */

 return current sync_serial_settings */

 data size wanted */

 set sync_serial_settings */

 if network interface up, reprogram hardware */

/**

 * called by network layer when transmit timeout is detected

 *

 * dev  pointer to network device structure

/**

 * called by device driver when transmit completes

 * reenable network layer transmit if stopped

 *

 * info  pointer to device instance information

/**

 * called by device driver when frame received

 * pass frame to network layer

 *

 * info  pointer to device instance information

 * buf   pointer to buffer contianing frame data

 * size  count of data bytes in buf

/**

 * called by device driver when adding device instance

 * do generic HDLC initialization

 *

 * info  pointer to device instance information

 *

 * returns 0 if success, otherwise error code

 allocate and initialize network and HDLC layer objects */

 for network layer reporting purposes only */

 network layer callbacks and settings */

 generic HDLC layer callbacks and settings */

 register objects with HDLC layer */

/**

 * called by device driver when removing device instance

 * do generic HDLC cleanup

 *

 * info  pointer to device instance information

 CONFIG_HDLC */

/*

 * A driver for the Omnikey PCMCIA smartcard reader CardMan 4040

 *

 * (c) 2000-2004 Omnikey AG (http://www.omnikey.com/)

 *

 * (C) 2005-2006 Harald Welte <laforge@gnumonks.org>

 * 	- add support for poll()

 * 	- driver cleanup

 * 	- add waitqueues

 * 	- adhere to linux kernel coding style and policies

 * 	- support 2.6.13 "new style" pcmcia interface

 * 	- add class interface for udev device creation

 *

 * The device basically is a USB CCID compliant device that has been

 * attached to an I/O-Mapped FIFO.

 *

 * All rights reserved, Dual BSD/GPL Licensed.

 n (debug level) is ignored */

/* additional debug output may be enabled by re-compiling with

 #define CM4040_DEBUG */

 how often to poll for fifo status change */

/* poll the device fifo status register.  not to be confused with

 Write to Sync Control Register */

 find device */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019-2020 ARM Limited or its affiliates. */

/* data circular buffer in words must be:

 *  - of a power-of-2 size (limitation of circ_buf.h macros)

 *  - at least 6, the size generated in the EHR according to HW implementation

/* The timeout for the TRNG operation should be calculated with the formula:

 * Timeout = EHR_NUM * VN_COEFF * EHR_LENGTH * SAMPLE_CNT * SCALE_VALUE

 * while:

 *  - SAMPLE_CNT is input value from the characterisation process

 *  - all the rest are constants

	/* Sampling interval for each ring oscillator:

	 * count of ring oscillator cycles between consecutive bits sampling.

	 * Value of 0 indicates non-valid rosc

 pending_hw - 1 when HW is pending, 0 when it is idle */

 protects against multiple concurrent consumers of data_buf */

 functions for write/read CC registers */

 pm_runtime_get_sync() can return 1 as a valid return code */

 must be before the enabling to avoid redundant suspending */

 set us as active - note we won't do PM ops until cc_trng_pm_go()! */

 enable the PM module*/

 ret will be set to 0 if at least one rosc has (sampling ratio > 0) */

 arm,rosc-ratio was not found in device tree */

 verify that at least one rosc has (sampling ratio > 0) */

 Set watchdog threshold to maximal allowed time (in CPU cycles) */

 enable the RND source */

 unmask RNG interrupts */

 increase circular data buffer index (head/tail) */

 current implementation ignores "wait" */

 concurrent consumers from data_buf cannot be served */

 copy till end of data buffer (without wrap back) */

 copy rest of data in data buffer */

 re-check space in buffer to avoid potential race */

 increment device's usage counter */

				/* schedule execution of deferred work handler

				 * for filling of data buffer

 enable the HW RND clock */

 do software reset */

	/* in order to verify that the reset has completed,

	 * the sample count need to be verified

 enable the HW RND clock   */

 set sampling ratio (rng_clocks) between consecutive bits */

 read the sampling ratio  */

 disable the RND source for setting new parameters in HW */

 Debug Control register: set to 0 - no bypasses */

 stop DMA and the RNG source */

 read RNG_ISR and check for errors */

 FIPS error is fatal */

 Clear all pending RNG interrupts */

 in case of AUTOCORR/TIMEOUT error, try the next ROSC */

 in case of VN error, ignore it */

 read EHR data from registers */

 calc word ptr in data_buf */

		/* EHR_DATA registers are cleared on read. In case 0 value was

		 * returned, restart the entropy collection.

 continue to fill data buffer if needed */

 Re-enable rnd source */

 trigger trng hw with next rosc */

 if driver suspended return, probably shared interrupt */

 read the interrupt status */

 Probably shared interrupt line */

 clear interrupt - must be before processing events */

 RNG interrupt - most probable */

 Mask RNG interrupts - will be unmasked in deferred work */

		/* We clear RNG interrupt here,

		 * to avoid it from firing as we'll unmask RNG interrupts.

 schedule execution of deferred work handler */

 Just warning */

 Then IRQ */

 parse sampling rate from device tree */

 register the driver isr function */

 Clear all pending interrupts */

 unmask HOST RNG interrupt */

 init PM */

 increment device's usage counter */

 set pending_hw to verify that HW won't be triggered from read */

 registration of the hwrng device */

 trigger HW to start generate data */

 All set, we can allow auto-suspend */

		/* in cc7x3 NVM_IS_IDLE indicates that CC reset is

		 *  completed and device is fully functional

 hw indicate reset completed */

 allow scheduling other process on the processor */

 reset not completed */

 Enables the device source clk */

 wait for Cryptocell reset completion */

 unmask HOST RNG interrupt */

 Compile time assertion checks */

 Module description */

 SPDX-License-Identifier: GPL-2.0

/*

 * Ingenic Random Number Generator driver

 * Copyright (c) 2017 PrasannaKumar Muralidharan <prasannatsmkumar@gmail.com>

 * Copyright (c) 2020 周琰杰 (Zhou Yanjie) <zhouyanjie@wanyeetech.com>

 RNG register offsets */

 bits within the ERND register */

 Device associated memory */

		/*

		 * A delay is required so that the current RNG data is not bit shifted

		 * version of previous RNG data which could happen if random data is

		 * read continuously from this device.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * APM X-Gene SoC RNG Driver

 *

 * Copyright (c) 2014, Applied Micro Circuits Corporation

 * Author: Rameshwar Prasad Sahu <rsahu@apm.com>

 *	   Shamal Winchurkar <swinchurkar@apm.com>

 *	   Feng Kan <fkan@apm.com>

 RNG  Registers */

 Failure count last minute */

 First failure timestamp */

 Clear failure counter as timer expired */

/*

 * Initialize or reinit free running oscillators (FROs)

		/*

		 * LFSR detected an out-of-bounds number of 1s after

		 * checking 20,000 bits (test T1 as specified in the

		 * AIS-31 standard)

		/*

		 * LFSR detected an out-of-bounds value in at least one

		 * of the 16 poker_count_X counters or an out of bounds sum

		 * of squares value after checking 20,000 bits (test T2 as

		 * specified in the AIS-31 standard)

		/*

		 * LFSR detected a sequence of 34 identical bits

		 * (test T4 as specified in the AIS-31 standard)

		/*

		 * LFSR detected an outof-bounds value for at least one

		 * of the running counters after checking 20,000 bits

		 * (test T3 as specified in the AIS-31 standard)

 LFSR detected a sequence of 48 identical bits */

		/*

		 * Detected output data registers generated same value twice

		 * in a row

 FROs shut down after a second error event. Try recover. */

 1st time, just recover */

			/*

			 * We must start a timer to clear out this error

			 * in case the system timer wrap around

 2nd time failure in lesser than 1 minute? */

 2nd time failure after 1 minutes, recover */

				/*

				 * We must start a timer to clear out this

				 * error in case the system timer wrap

				 * around

 Clear them all */

 RNG Alarm Counter overflow */

 Clear ready bit to start next transaction */

 Enable IP clock */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Nomadik RNG support

 *  Copyright 2009 Alessandro Rubini

	/*

	 * The register is 32 bits and gives 16 random bits (low half).

	 * A subsequent read will delay the core for 400ns, so we just read

	 * once and accept the very unlikely very small delay, even if wait==0.

 we have at most one RNG per machine, granted */

 top bits are rev and cfg: accept all */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Driver for Mediatek Hardware Random Number Generator

 *

 * Copyright (C) 2017 Sean Wang <sean.wang@mediatek.com>

 Runtime PM autosuspend timeout: */

 CONFIG_PM */

 CONFIG_PM */

/*

 * Hardware Random Number Generator support for Cavium Networks

 * Octeon processor family.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2009 Cavium Networks

 Enable the entropy source.  */

 Enable the RNG hardware.  */

 Disable everything.  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010 Michael Neuling IBM Corporation

 *

 * Driver for the pseries hardware RNG for POWER7+ and above

 The hypervisor interface returns 64 bits */

/*

 * pseries_rng_get_desired_dma - Return desired DMA allocate for CMO operations

 *

 * This is a required function for a driver to operate in a CMO environment

 * but this device does not make use of DMA allocations, return 0.

 *

 * Return value:

 *	Number of bytes of IO data the driver will need to perform well -> 0

 SPDX-License-Identifier: GPL-2.0-only

/* n2-drv.c: Niagara-2 RNG driver.

 *

 * Copyright (C) 2008, 2011 David S. Miller <davem@davemloft.net>

/* The Niagara2 RNG provides a 64-bit read-only random number

 * register, plus a control register.  Access to the RNG is

 * virtualized through the hypervisor so that both guests and control

 * nodes can access the device.

 *

 * The entropy source consists of raw entropy sources, each

 * constructed from a voltage controlled oscillator whose phase is

 * jittered by thermal noise sources.

 *

 * The oscillator in each of the three raw entropy sources run at

 * different frequencies.  Normally, all three generator outputs are

 * gathered, xored together, and fed into a CRC circuit, the output of

 * which is the 64-bit read-only register.

 *

 * Some time is necessary for all the necessary entropy to build up

 * such that a full 64-bits of entropy are available in the register.

 * In normal operating mode (RNG_CTL_LFSR is set), the chip implements

 * an interlock which blocks register reads until sufficient entropy

 * is available.

 *

 * A control register is provided for adjusting various aspects of RNG

 * operation, and to enable diagnostic modes.  Each of the three raw

 * entropy sources has an enable bit (RNG_CTL_ES{1,2,3}).  Also

 * provided are fields for controlling the minimum time in cycles

 * between read accesses to the register (RNG_CTL_WAIT, this controls

 * the interlock described in the previous paragraph).

 *

 * The standard setting is to have the mode bit (RNG_CTL_LFSR) set,

 * all three entropy sources enabled, and the interlock time set

 * appropriately.

 *

 * The CRC polynomial used by the chip is:

 *

 * P(X) = x64 + x61 + x57 + x56 + x52 + x51 + x50 + x48 + x47 + x46 +

 *        x43 + x42 + x41 + x39 + x38 + x37 + x35 + x32 + x28 + x25 +

 *        x22 + x21 + x17 + x15 + x13 + x12 + x11 + x7 + x5 + x + 1

 *

 * The RNG_CTL_VCO value of each noise cell must be programmed

 * separately.  This is why 4 control register values must be provided

 * to the hypervisor.  During a write, the hypervisor writes them all,

 * one at a time, to the actual RNG_CTL register.  The first three

 * values are used to setup the desired RNG_CTL_VCO for each entropy

 * source, for example:

 *

 *	control 0: (1 << RNG_CTL_VCO_SHIFT) | RNG_CTL_ES1

 *	control 1: (2 << RNG_CTL_VCO_SHIFT) | RNG_CTL_ES2

 *	control 2: (3 << RNG_CTL_VCO_SHIFT) | RNG_CTL_ES3

 *

 * And then the fourth value sets the final chip state and enables

 * desired.

/* In multi-socket situations, the hypervisor might need to

 * queue up the RNG control register write if it's for a unit

 * that is on a cpu socket other than the one we are executing on.

 *

 * We poll here waiting for a successful read of that control

 * register to make sure the write has been actually performed.

/* Just try to see if we can successfully access the control register

 * of the RNG on the domain on which we are currently executing.

		/* We purposefully give invalid arguments, HV_NOACCESS

		 * is higher priority than the errors we'd get from

		 * these other cases, and that's the error we are

		 * truly interested in.

	/* Not in the control domain, that's OK we are only a consumer

	 * of the RNG data, we don't setup and program it.

/* On a guest node, just make sure we can read random data properly.

 * If a control node reboots or reloads it's n2rng driver, this won't

 * work during that time.  So we have to keep probing until the device

 * becomes usable.

 Purposefully skip over the first word.  */

 yes, m4 uses the old value */

/* The sanity checks passed, install the final configuration into the

 * chip, it's ready to use.

		/* XXX This isn't the best.  We should fetch a bunch

		 * XXX of words using each entropy source combined XXX

		 * with each VCO setting, and see which combinations

		 * XXX give the best random data.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PIC32 RNG driver

 *

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2016 Microchip Technology Inc.  All rights reserved.

/*

 * The TRNG can generate up to 24Mbps. This is a timeout that should be safe

 * enough given the instructions in the loop and that the TRNG may not always

 * be at maximum rate.

 TRNG value comes through the seed registers */

 enable TRNG in enhanced mode */

 sentinel */ }

/*

 * Hardware Random Number Generator support for Cavium Inc.

 * Thunder processor family.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2016 Cavium, Inc.

 Enable the RNG hardware and activate the VF */

Map the RNG control */

 Enable the RNG hardware and entropy source */

 Enable the Cavium RNG as a VF */

 Disable the RNG hardware and entropy source */

 Disable VF and RNG Hardware */

 Remove the VF */

 Disable the RNG hardware and entropy source */

 Thunder RNM */

/*

 * RNG driver for Intel RNGs

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * with the majority of the code coming from:

 *

 * Hardware driver for the Intel/AMD/VIA Random Number Generators (RNG)

 * (c) Copyright 2003 Red Hat Inc <jgarzik@redhat.com>

 *

 * derived from

 *

 * Hardware driver for the AMD 768 Random Number Generator (RNG)

 * (c) Copyright 2001 Red Hat Inc

 *

 * derived from

 *

 * Hardware driver for Intel i810 Random Number Generator (RNG)

 * Copyright 2000,2001 Jeff Garzik <jgarzik@pobox.com>

 * Copyright 2000,2001 Philipp Rumpf <prumpf@mandrakesoft.com>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * RNG registers

/*

 * Magic address at which Intel PCI bridges locate the RNG

/*

 * LPC bridge PCI config space registers

 high byte of 16-bit register */

/*

 * Magic address at which Intel Firmware Hubs get accessed

/*

 * Intel Firmware Hub command codes (write to any address inside the device)

 aka READ_ARRAY */

/*

 * Intel Firmware Hub Read ID command result addresses

/*

 * Intel Firmware Hub Read ID command result values

/*

 * Data for PCI driver interface

 *

 * This data only exists for exporting the supported

 * PCI ids via MODULE_DEVICE_TABLE.  We do not actually

 * register a pci_driver, because someone else might one day

 * want to register another driver on the same PCI id.

/* AA

 AA */

/* AB

 AB */

/* ??

/* BAM, CAM, DBM, FBM, GxM

 BAM */

 CAM */

 DBM */

 FBM */

 GxM */

 GxM DH */

/* BA, CA, DB, Ex, 6300, Fx, 631x/632x, Gx

 BA */

 CA */

 DB */

 Ex */

 6300 */

 Fx */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 631x/632x */

 Gx */

/* E

 E  */

 terminate list */

 turn RNG h/w on, if it's off */

 interrupts disabled in stop_machine call */

 Check for Intel 82802 */

const*/ char warning[] =

 Device not found. */

	/*

	 * Since the BIOS code/data is going to disappear from its normal

	 * location with the Read ID command, all activity on the system

	 * must be stopped until the state is back to normal.

	 *

	 * Use stop_machine because IPIs can be blocked by disabling

	 * interrupts.

 Check for Random Number Generator */

/*

 * Hardware Random Number Generator support for Cavium, Inc.

 * Thunder processor family.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2016 Cavium, Inc.

 Read data from the RNG unit */

 Map Cavium RNG to an HWRNG object */

 Map the RNG result */

 SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause

/*

 * Copyright (c) 2016 BayLibre, SAS.

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 * Copyright (C) 2014 Amlogic, Inc.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2006-2007 PA Semi, Inc

 *

 * Maintained by: Olof Johansson <olof@lixom.net>

 *

 * Driver for the PWRficient onchip rng

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 HiSilicon Co., Ltd.

 get a random number as initial seed */

	/**

	 * The seed is reload periodically, there are two choice

	 * of seeds, default seed using the value from LFSR, or

	 * will use seed generated by ring oscillator.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2015, Daniel Thompson

 Manage timeout which is based on timer and take */

 care of initial delay time when enabling rng	*/

 If error detected or data not ready... */

 clear error indicators */

/*

 * omap3-rom-rng.c - RNG driver for TI OMAP3 CPU family

 *

 * Copyright (C) 2009 Nokia Corporation

 * Author: Juha Yrjola <juha.yrjola@solidboot.com>

 *

 * Copyright (C) 2013 Pali Rohár <pali@kernel.org>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * RNG driver for Freescale RNGA

 *

 * Copyright 2008-2009 Freescale Semiconductor, Inc. All Rights Reserved.

 * Author: Alan Carvalho de Assis <acassis@gmail.com>

/*

 *

 * This driver is based on other RNG drivers.

 RNGA Registers */

 RNGA Registers Range */

 RNGA Control Register */

 RNGA Status Register */

 how many random numbers are in FIFO? [0-16] */

 retrieve a random number from FIFO */

 some error while reading this random number? */

 if error: clear error interrupt, but doesn't return random number */

 wake up */

 verify if oscillator is working */

 go running */

 stop rnga */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Random Number Generator driver for the Keystone SOC

 *

 * Copyright (C) 2016 Texas Instruments Incorporated - https://www.ti.com

 *

 * Authors:	Sandeep Nair

 *		Vitaly Andrianov

 TRNG enable control in SA System module*/

 TRNG start control in TRNG module */

 Data ready indicator in STATUS register */

 Data ready clear control in INTACK register */

/*

 * Number of samples taken to gather entropy during startup.

 * If value is 0, the number of samples is 2^24 else

 * equals value times 2^8.

/*

 * Minimum number of samples taken to regenerate entropy

 * If value is 0, the number of samples is 2^24 else

 * equals value times 2^6.

/*

 * Maximum number of samples taken to regenerate entropy

 * If value is 0, the number of samples is 2^24 else

 * equals value times 2^8.

 Number of CLK input cycles between samples */

 Maximum retries to get rng data */

 Delay between retries (in usecs) */

 Enable RNG module */

 Configure RNG module */

 Disable all interrupts from TRNG */

 Enable RNG */

 Disable RNG */

 Read random data */

 Max delay expected here is 81920000 ns */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018-2019 Linaro Ltd.

/*

 * TA_CMD_GET_ENTROPY - Get Entropy from RNG

 *

 * param[0] (inout memref) - Entropy buffer memory reference

 * param[1] unused

 * param[2] unused

 * param[3] unused

 *

 * Result:

 * TEE_SUCCESS - Invoke command success

 * TEE_ERROR_BAD_PARAMETERS - Incorrect input param

 * TEE_ERROR_NOT_SUPPORTED - Requested entropy size greater than size of pool

 * TEE_ERROR_HEALTH_TEST_FAIL - Continuous health testing failed

/*

 * TA_CMD_GET_RNG_INFO - Get RNG information

 *

 * param[0] (out value) - value.a: RNG data-rate in bytes per second

 *                        value.b: Quality/Entropy per 1024 bit of data

 * param[1] unused

 * param[2] unused

 * param[3] unused

 *

 * Result:

 * TEE_SUCCESS - Invoke command success

 * TEE_ERROR_BAD_PARAMETERS - Incorrect input param

/**

 * struct optee_rng_private - OP-TEE Random Number Generator private data

 * @dev:		OP-TEE based RNG device.

 * @ctx:		OP-TEE context handler.

 * @session_id:		RNG TA session identifier.

 * @data_rate:		RNG data rate.

 * @entropy_shm_pool:	Memory pool shared with RNG device.

 * @optee_rng:		OP-TEE RNG driver structure.

 Invoke TA_CMD_GET_ENTROPY function of Trusted App */

 Fill invoke cmd params */

 Invoke TA_CMD_GET_RNG_INFO function of Trusted App */

 Fill invoke cmd params */

 Open context with TEE driver */

 Open session with hwrng Trusted App */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST Random Number Generator Driver ST's Platforms

 *

 * Author: Pankaj Dev: <pankaj.dev@st.com>

 *         Lee Jones <lee.jones@linaro.org>

 *

 * Copyright (C) 2015 STMicroelectronics (R&D) Limited

 Registers */

 Registers fields */

 2 Byte (16bit) samples */

/*

 * Samples are documented to be available every 0.667us, so in theory

 * the 4 sample deep FIFO should take 2.668us to fill.  However, during

 * thorough testing, it became apparent that filling the FIFO actually

 * takes closer to 12us.  We then multiply by 2 in order to account for

 * the lack of udelay()'s reliability, suggested by Russell King.

 Wait until FIFO is full - max 4uS*/

 No of bytes read */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/char/hw_random/timeriomem-rng.c

 *

 * Copyright (C) 2009 Alexander Clouter <alex@digriz.org.uk>

 *

 * Derived from drivers/char/hw_random/omap-rng.c

 *   Copyright 2005 (c) MontaVista Software, Inc.

 *   Author: Deepak Saxena <dsaxena@plexity.net>

 *

 * Overview:

 *   This driver is useful for platforms that have an IO range that provides

 *   periodic random data from a single IO memory address.  All the platform

 *   has to do is provide the address and 'wait time' that new data becomes

 *   available.

 *

 * TODO: add support for reading sizes other than 32bits and masking

	/*

	 * There may not have been enough time for new data to be generated

	 * since the last request.  If the caller doesn't want to wait, let them

	 * bail out.  Otherwise, wait for the completion.  If the new data has

	 * already been generated, the completion should already be available.

		/*

		 * After the first read, all additional reads will need to wait

		 * for the RNG to generate new data.  Since the period can have

		 * a wide range of values (1us to 1s have been observed), allow

		 * for 1% tolerance in the sleep time rather than a fixed value.

	/*

	 * Block any new callers until the RNG has had time to generate new

	 * data.

 Allocate memory for the device structure (and zero it) */

 Assume random data is already available. */

/*

 * Copyright (c) 2011 Peter Korsgaard <jacmet@sunsite.dk>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

 RNG */

 generate RN every 168 cycles */

 data ready? */

		/*

		  ensure data ready is only set again AFTER the next data

		  word is ready in case it got set between checking ISR

		  and reading ODATA, so we don't risk re-reading the

		  same word

 if peripheral clk is above 100MHz, set HALFR */

 CONFIG_PM */

 sentinel */

 CONFIG_PM */

/*

 * RNG driver for AMD RNGs

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * with the majority of the code coming from:

 *

 * Hardware driver for the Intel/AMD/VIA Random Number Generators (RNG)

 * (c) Copyright 2003 Red Hat Inc <jgarzik@redhat.com>

 *

 * derived from

 *

 * Hardware driver for the AMD 768 Random Number Generator (RNG)

 * (c) Copyright 2001 Red Hat Inc

 *

 * derived from

 *

 * Hardware driver for Intel i810 Random Number Generator (RNG)

 * Copyright 2000,2001 Jeff Garzik <jgarzik@pobox.com>

 * Copyright 2000,2001 Philipp Rumpf <prumpf@mandrakesoft.com>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * Data for PCI driver interface

 *

 * This data only exists for exporting the supported

 * PCI ids via MODULE_DEVICE_TABLE.  We do not actually

 * register a pci_driver, because someone else might one day

 * want to register another driver on the same PCI id.

 terminate list */

 We will wait at maximum one time per read */

	/*

	 * RNG data is available when RNGDONE is set to 1

	 * New random numbers are generated approximately 128 microseconds

	 * after RNGDATA is read

 Delay given by datasheet */

 RNG on */

 PMIO enable */

 RNG off */

 Device not found. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2019 Nuvoton Technology corporation.

 Control and status register */

 Data register */

 Mode register */

 20-25 MHz */

/*

 * omap-rng.c - RNG driver for TI OMAP CPU family

 *

 * Author: Deepak Saxena <dsaxena@plexity.net>

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * Mostly based on original driver:

 *

 * Copyright (C) 2005 Nokia Corporation

 * Author: Juha Yrjölä <juha.yrjola@nokia.com>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * EIP76 RNG takes approx. 700us to produce 16 bytes of output data

 * as per testing results. And to account for the lack of udelay()'s

 * reliability, we keep the timeout as 1000us.

/**

 * struct omap_rng_pdata - RNG IP block-specific data

 * @regs: Pointer to the register offsets structure.

 * @data_size: No. of bytes in RNG output.

 * @data_present: Callback to determine if data is available.

 * @init: Callback for IP specific initialization sequence.

 * @cleanup: Callback for IP specific cleanup sequence.

 Return if RNG is already running. */

	/*  Number of 512 bit blocks of raw Noise Source output data that must

	 *  be processed by either the Conditioning Function or the

	 *  SP 800-90 DRBG ‘BC_DF’ functionality to yield a ‘full entropy’

	 *  output value.

	/* Number of FRO samples that are XOR-ed together into one bit to be

	 * shifted into the main shift register

 Enable all available FROs */

 Enable TRNG */

 Return if RNG is already running. */

	/*

	 * Interrupt raised by a fro shutdown threshold, do the following:

	 * 1. Clear the alarm events.

	 * 2. De tune the FROs which are shutdown.

	 * 3. Re enable the shutdown FROs.

		/*

		 * On OMAP4, enabling the shutdown_oflo interrupt is

		 * done in the interrupt mask register. There is no

		 * such register on EIP76, and it's enabled by the

		 * same bit in the control register

 Only OMAP2/3 can be non-DT */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2020 Xiphera Ltd. */

 trng statuses */

 check for data */

			/*

			 * Inform the trng of the read

			 * and re-enable it to produce a new random number

	/*

	 * the trng needs to be reset first which might not happen in time,

	 * hence we incorporate a small delay to ensure proper behaviour

		/*

		 * there is a small chance the trng is just not ready yet,

		 * so we try one more time. If the second time fails, we give up

	/*

	 * once again, to ensure proper behaviour we sleep

	 * for a while after zeroizing the trng

 diagnose the reason for the failure */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * RNG driver for Freescale RNGC

 *

 * Copyright (C) 2008-2012 Freescale Semiconductor, Inc.

 * Copyright (C) 2017 Martin Kaiser <martin@kaiser.cx>

 the fields in the ver id register */

 the rng_type field */

 3 sec */

	/*

	 * err_reg is written only by the irq handler and read only

	 * when interrupts are masked, we need no spinlock

 mask interrupts */

	/*

	 * CLR_INT clears the interrupt only if there's no error

	 * CLR_ERR clear the interrupt and the error register if there

	 * is an error

 run self test */

 is there some error while reading this random number? */

 how many random numbers are in FIFO? [0-16] */

 retrieve a random number from FIFO */

	/*

	 * clearing the interrupt will also clear the error register

	 * read error and status before clearing

 clear error */

 create seed, repeat while there is some statistical error */

 seed creation */

	/*

	 * enable automatic seeding, the rngc creates a new seed automatically

	 * after serving 2^20 random 160-bit words

	/*

	 * if initialisation was successful, we keep the interrupt

	 * unmasked until imx_rngc_cleanup is called

	 * we mask the interrupt ourselves if we return an error

	/*

	 * This driver supports only RNGC and RNGB. (There's a different

	 * driver for RNGA.)

 sentinel */ }

/*

 * RNG driver for AMD Geode RNGs

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * with the majority of the code coming from:

 *

 * Hardware driver for the Intel/AMD/VIA Random Number Generators (RNG)

 * (c) Copyright 2003 Red Hat Inc <jgarzik@redhat.com>

 *

 * derived from

 *

 * Hardware driver for the AMD 768 Random Number Generator (RNG)

 * (c) Copyright 2001 Red Hat Inc

 *

 * derived from

 *

 * Hardware driver for Intel i810 Random Number Generator (RNG)

 * Copyright 2000,2001 Jeff Garzik <jgarzik@pobox.com>

 * Copyright 2000,2001 Philipp Rumpf <prumpf@mandrakesoft.com>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * Data for PCI driver interface

 *

 * This data only exists for exporting the supported

 * PCI ids via MODULE_DEVICE_TABLE.  We do not actually

 * register a pci_driver, because someone else might one day

 * want to register another driver on the same PCI id.

 terminate list */

 Device not found. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Randomness driver for virtio

 *  Copyright (C) 2007, 2008 Rusty Russell IBM Corporation

 data transfer */

 minimal size returned by rng_buffer_size() */

 We can get spurious callbacks, e.g. shared IRQs + virtio_pci. */

 There should always be room for one buffer. */

 copy available data */

	/* We have already copied available entropy,

	 * so either size is 0 or data_avail is 0

 data_avail is 0 but a request is pending */

		/* if vi->data_avail is 0, we have been interrupted

		 * by a cleanup, but buffer stays in the queue

 We expect a single virtqueue. */

 we always have a pending entropy request */

		/*

		 * Set hwrng_removed to ensure that virtio_read()

		 * does not block waiting for data before the

		 * registration is complete.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2010-2012 Broadcom. All rights reserved.

 * Copyright (c) 2013 Lubomir Rintel

 enable rng */

 the initial numbers generated are "less random" so will be discarded */

	/* MIPS chips strapped for BE will automagically configure the

	 * peripheral registers for CPU-native byte order.

 mask the interrupt */

 set warm-up count & enable */

 disable rng hardware */

 map peripheral */

 Clock is optional on most platforms */

 Check for rng init function, execute it */

 register driver */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Ingenic True Random Number Generator driver

 * Copyright (c) 2019 漆鹏振 (Qi Pengzhen) <aric.pzqi@ingenic.com>

 * Copyright (c) 2020 周琰杰 (Zhou Yanjie) <zhouyanjie@wanyeetech.com>

 DTRNG register offsets */

 bits within the CFG register */

 bits within the STATUS register */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2020 Silex Insight

 usec */

 usec */

 usec */

 usec */

 Disable interrupts, random generation and enable the softreset */

 Wait until the state changed */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Randomness driver for the ARM SMCCC TRNG Firmware Interface

 * https://developer.arm.com/documentation/den0098/latest/

 *

 *  Copyright (C) 2020 Arm Ltd.

 *

 * The ARM TRNG firmware interface specifies a protocol to read entropy

 * from a higher exception level, to abstract from any machine specific

 * implemenations and allow easier use in hypervisors.

 *

 * The firmware interface is realised using the SMCCC specification.

 We don't want to allow the firmware to stall us forever. */

/*

 * hw_random/core.c: HWRNG core API

 *

 * Copyright 2006 Michael Buesch <m@bues.ch>

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * Please read Documentation/admin-guide/hw_random.rst for details on use.

 *

 * This software may be used and distributed according to the terms

 * of the GNU General Public License, incorporated herein by reference.

 the current rng has been explicitly chosen by user via sysfs */

 list of registered rngs, sorted decending by quality */

 Protects rng_list and current_rng */

 Protects rng read functions, data_avail, rng_buffer and rng_fillbuf */

 = 0; default to "off" */

 decrease last reference for triggering the cleanup */

 Returns ERR_PTR(), NULL or refcounted hwrng */

	/*

	 * Hold rng_mutex here so we serialize in case they set_current_rng

	 * on rng again immediately.

 enforce read-only access to this chrdev */

 rng_list is sorted by quality, use the best (=first) one */

 Outside lock, sure, but y'know: randomness. */

 Must not register two RNGs with the same name. */

 rng_list is sorted by decreasing quality */

		/*

		 * Set new rng as current as the new rng source

		 * provides better entropy quality and was not

		 * chosen by userspace.

		/* to use current_rng in add_early_randomness() we need

		 * to take a ref

		/*

		 * Use a new device's input to add some randomness to

		 * the system.  If this rng device isn't going to be

		 * used right away, its init function hasn't been

		 * called yet by set_current_rng(); so only use the

		 * randomness from devices that don't need an init callback

 kmalloc makes this safe for virt_to_page() in virtio_rng.c */

/*

 * RNG driver for TX4939 Random Number Generators (RNG)

 *

 * Copyright (C) 2009 Atsushi Nemoto <anemo@mba.ocn.ne.jp>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

	/*

	 * readq is reading a 64-bit register using a 64-bit load.  On

	 * a 32-bit kernel however interrupts or any other processor

	 * exception would clobber the upper 32-bit of the processor

	 * register so interrupts need to be disabled.

 Start RNG */

 90 bus clock cycles by default for generation */

 Reset RNG */

 Start RNG */

	/*

	 * Drop first two results.  From the datasheet:

	 * The quality of the random numbers generated immediately

	 * after reset can be insufficient.  Therefore, do not use

	 * random numbers obtained from the first and second

	 * generations; use the ones from the third or subsequent

	 * generation.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2013 Michael Ellerman, Guo Chao, IBM Corp.

 We rely on rng_buffer_size() being >= sizeof(unsigned long) */

 We only register one device, ignore any others */

 SPDX-License-Identifier: GPL-2.0

/*

 * drivers/char/hw_random/ixp4xx-rng.c

 *

 * RNG driver for Intel IXP4xx family of NPUs

 *

 * Author: Deepak Saxena <dsaxena@plexity.net>

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * Fixes by Michael Buesch

 includes IXP455 */

/*

 * RNG driver for VIA RNGs

 *

 * Copyright 2005 (c) MontaVista Software, Inc.

 *

 * with the majority of the code coming from:

 *

 * Hardware driver for the Intel/AMD/VIA Random Number Generators (RNG)

 * (c) Copyright 2003 Red Hat Inc <jgarzik@redhat.com>

 *

 * derived from

 *

 * Hardware driver for the AMD 768 Random Number Generator (RNG)

 * (c) Copyright 2001 Red Hat Inc

 *

 * derived from

 *

 * Hardware driver for Intel i810 Random Number Generator (RNG)

 * Copyright 2000,2001 Jeff Garzik <jgarzik@pobox.com>

 * Copyright 2000,2001 Philipp Rumpf <prumpf@mandrakesoft.com>

 *

 * This file is licensed under  the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

 64 rand bits, 64 stored bits */

 32 rand bits, 32 stored bits */

 16 rand bits, 32 stored bits */

 8 rand bits, 32 stored bits */

/*

 * Investigate using the 'rep' prefix to obtain 32 bits of random data

 * in one insn.  The upside is potentially better performance.  The

 * downside is that the instruction becomes no longer atomic.  Due to

 * this, just like familiar issues with /dev/random itself, the worst

 * case of a 'rep xstore' could potentially pause a cpu for an

 * unreasonably long time.  In practice, this condition would likely

 * only occur when the hardware is failing.  (or so we hope :))

 *

 * Another possible performance boost may come from simply buffering

 * until we have 4 bytes, thus returning a u32 at a time,

 * instead of the current u8-at-a-time.

 *

 * Padlock instructions can generate a spurious DNA fault, but the

 * kernel doesn't use CR0.TS, so this doesn't matter.

 xstore %%edi (addr=%0) */"

	/* We choose the recommended 1-byte-per-instruction RNG rate,

	 * for greater randomness at the expense of speed.  Larger

	 * values 2, 4, or 8 bytes-per-instruction yield greater

	 * speed at lesser randomness.

	 *

	 * If you change this to another VIA_CHUNK_n, you must also

	 * change the ->n_bytes values in rng_vendor_ops[] tables.

	 * VIA_CHUNK_8 requires further code changes.

	 *

	 * A copy of MSR_VIA_RNG is placed in eax_out when xstore

	 * completes.

 paranoia, not really necessary */

	/* VIA Nano CPUs don't have the MSR_VIA_RNG anymore.  The RNG

	 * is always enabled if CPUID rng_en is set.  There is no

	 * RNG configuration like it used to be the case in this

	/* Control the RNG via MSR.  Tread lightly and pay very close

	 * close attention to values written, as the reserved fields

	 * are documented to be "undefined and unpredictable"; but it

	 * does not say to write them as zero, so I make a guess that

	 * we restore the values we find in the register.

 Enable secondary noise source on CPUs where it is present. */

 Nehemiah stepping 8 and higher */

 Esther */

	/* perhaps-unnecessary sanity check; remove after testing if

/*

* Copyright (C) 2015 Broadcom Corporation

*

* This program is free software; you can redistribute it and/or

* modify it under the terms of the GNU General Public License as

* published by the Free Software Foundation version 2.

*

* This program is distributed "as is" WITHOUT ANY WARRANTY of any

* kind, whether express or implied; without even the implied warranty

* of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

* GNU General Public License for more details.

/*

 * DESCRIPTION: The Broadcom iProc RNG200 Driver

 Registers */

 Clear all interrupt status */

 Reset RNG and RBG */

 Is RNG sane? If not, reset it. */

 Are there any random numbers available? */

 Buffer has room to store entire word */

 Buffer can only store partial word */

 Reset the IDLE timeout */

 Cannot wait, return immediately */

 Can wait, give others chance to run */

 Map peripheral */

 Register driver */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * s390 TRNG device driver

 *

 * Driver for the TRNG (true random number generation) command

 * available via CPACF extension MSA 7 on the s390 arch.



 * Copyright IBM Corp. 2017

 * Author(s): Harald Freudenberger <freude@de.ibm.com>

 trng related debug feature things */

 trng helpers */

 file io functions */

	/*

	 * use buf for requests <= sizeof(buf),

	 * otherwise allocate one page and fetch

	 * pagewise.

 sysfs */

 hwrng_register */

/*

 * hwrng register struct

 * The trng is supposed to have 100% entropy, and thus we register with a very

 * high quality value. If we ever have a better driver in the future, we should

 * change this value again when we merge this driver.

 init and exit */

 check if subfunction CPACF_PRNO_TRNG is available */

 SPDX-License-Identifier: GPL-2.0

/*

 * RNG driver for Exynos TRNGs

 *

 * Author: Łukasz Stelmach <l.stelmach@samsung.com>

 *

 * Copyright 2017 (c) Samsung Electronics Software, Inc.

 *

 * Based on the Exynos PRNG driver drivers/crypto/exynos-rng by

 * Krzysztof Kozłowski <krzk@kernel.org>

	/*

	 * For most TRNG circuits the clock frequency of under 500 kHz

	 * is safe.

 Enable the generator. */

	/*

	 * Disable post-processing. /dev/hwrng is supposed to deliver

	 * unprocessed data.

/*

*

* smapi.c -- SMAPI interface routines

*

*

* Written By: Mike Sullivan IBM Corporation

*

* Copyright (C) 1999 IBM Corporation

*

* This program is free software; you can redistribute it and/or modify

* it under the terms of the GNU General Public License as published by

* the Free Software Foundation; either version 2 of the License, or

* (at your option) any later version.

*

* This program is distributed in the hope that it will be useful,

* but WITHOUT ANY WARRANTY; without even the implied warranty of

* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

* GNU General Public License for more details.

*

* NO WARRANTY

* THE PROGRAM IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OR

* CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT

* LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT,

* MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is

* solely responsible for determining the appropriateness of using and

* distributing the Program and assumes all risks associated with its

* exercise of rights under this Agreement, including but not limited to

* the risks and costs of program errors, damage to or loss of data,

* programs or equipment, and unavailability or interruption of operations.

*

* DISCLAIMER OF LIABILITY

* NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY

* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL

* DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND

* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

* TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

* USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED

* HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES

*

* You should have received a copy of the GNU General Public License

* along with this program; if not, write to the Free Software

* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

*

*

* 10/23/2000 - Alpha Release

*	First release to the public

 CMOS defines */

 check for illegal values */

 check for illegal values */

 Check serial port A */

 bRC == 0 */

 serial port A is present */

 serial port is enabled */

 Check serial port B */

 bRC == 0 */

 serial port B is present */

 serial port is enabled */

 Check IR port */

 bRC == 0 */

 IR port not disabled */

 normal exit: */

 Message has already been printed */

  0  */

SmapiQuerySystemID();

/*

*

* mwavedd.c -- mwave device driver

*

*

* Written By: Mike Sullivan IBM Corporation

*

* Copyright (C) 1999 IBM Corporation

*

* This program is free software; you can redistribute it and/or modify

* it under the terms of the GNU General Public License as published by

* the Free Software Foundation; either version 2 of the License, or

* (at your option) any later version.

*

* This program is distributed in the hope that it will be useful,

* but WITHOUT ANY WARRANTY; without even the implied warranty of

* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

* GNU General Public License for more details.

*

* NO WARRANTY

* THE PROGRAM IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OR

* CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT

* LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT,

* MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is

* solely responsible for determining the appropriateness of using and

* distributing the Program and assumes all risks associated with its

* exercise of rights under this Agreement, including but not limited to

* the risks and costs of program errors, damage to or loss of data,

* programs or equipment, and unavailability or interruption of operations.

*

* DISCLAIMER OF LIABILITY

* NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY

* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL

* DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND

* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

* TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

* USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED

* HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES

*

* You should have received a copy of the GNU General Public License

* along with this program; if not, write to the Free Software

* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

*

*

* 10/23/2000 - Alpha Release

*	First release to the public

/*

* These parameters support the setting of MWave resources. Note that no

* checks are made against other devices (ie. superio) for conflicts.

* We'll depend on users using the tpctl utility to do that for now

 check whether an event was signalled by */

 the interrupt handler while we were gone */

 first int has occurred (race condition) */

 first int has been handled */

 either 1st int has not yet occurred, or we have already handled the first int */

 switch */

 OK */

 switch */

 port is okay */

 OK */

 switch */

 irq is okay */

 totally b0rked */

/*

 * sysfs support <paulsch@us.ibm.com>

 Prevent code redundancy, create a macro for mwave_show_* functions. */

 All of our attributes are read attributes. */

/*

* mwave_init is called on module load

*

* mwave_exit is called on module unload

* mwave_exit is also used to clean up after an aborted mwave_init

 no ints received yet */

 uart is registered */

 sysfs */

 SUCCESS! */

 clean up */

/*

*

* 3780i.c -- helper routines for the 3780i DSP

*

*

* Written By: Mike Sullivan IBM Corporation

*

* Copyright (C) 1999 IBM Corporation

*

* This program is free software; you can redistribute it and/or modify

* it under the terms of the GNU General Public License as published by

* the Free Software Foundation; either version 2 of the License, or

* (at your option) any later version.

*

* This program is distributed in the hope that it will be useful,

* but WITHOUT ANY WARRANTY; without even the implied warranty of

* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

* GNU General Public License for more details.

*

* NO WARRANTY

* THE PROGRAM IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OR

* CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT

* LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT,

* MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is

* solely responsible for determining the appropriateness of using and

* distributing the Program and assumes all risks associated with its

* exercise of rights under this Agreement, including but not limited to

* the risks and costs of program errors, damage to or loss of data,

* programs or equipment, and unavailability or interruption of operations.

*

* DISCLAIMER OF LIABILITY

* NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY

* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL

* DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND

* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

* TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

* USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED

* HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES

*

* You should have received a copy of the GNU General Public License

* along with this program; if not, write to the Free Software

* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

*

*

* 10/23/2000 - Alpha Release

*	First release to the public

 cond_resched() */

  0  */

 Issue a soft reset to the chip */

	/* Note: Since we may be coming in with 3780i clocks suspended, we must keep

	* soft-reset active for 10ms.

 Program our general configuration registers */

 Mask DSP to PC interrupt */

 Reset the core via the boot domain register */

 Reset all the chiplets and then reactivate them */

 Transition the core to a running state */

 Enable DSP to PC interrupt */

 Set the initial MSA address. No adjustments need to be made to data store addresses */

 Transfer the memory block */

 Set the initial MSA address. No adjustments need to be made to data store addresses */

 Transfer the memory block */

 Set the initial MSA address. No adjustments need to be made to data store addresses */

 Transfer the memory block */

	/*

	* Set the initial MSA address. To convert from an instruction store

	* address to an MSA address

	* shift the address two bits to the left and set bit 22

 Transfer the memory block */

	/*

	* Set the initial MSA address. To convert from an instruction store

	* address to an MSA address

	* shift the address two bits to the left and set bit 22

 Transfer the memory block */

	/*

	* Disable DSP to PC interrupts, read the interrupt register,

	* clear the pending IPC bits, and reenable DSP to PC interrupts

/*

*

* tp3780i.c -- board driver for 3780i on ThinkPads

*

*

* Written By: Mike Sullivan IBM Corporation

*

* Copyright (C) 1999 IBM Corporation

*

* This program is free software; you can redistribute it and/or modify

* it under the terms of the GNU General Public License as published by

* the Free Software Foundation; either version 2 of the License, or

* (at your option) any later version.

*

* This program is distributed in the hope that it will be useful,

* but WITHOUT ANY WARRANTY; without even the implied warranty of

* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

* GNU General Public License for more details.

*

* NO WARRANTY

* THE PROGRAM IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OR

* CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT

* LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT,

* MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is

* solely responsible for determining the appropriateness of using and

* distributing the Program and assumes all risks associated with its

* exercise of rights under this Agreement, including but not limited to

* the risks and costs of program errors, damage to or loss of data,

* programs or equipment, and unavailability or interruption of operations.

*

* DISCLAIMER OF LIABILITY

* NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY

* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL

* DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND

* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

* TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

* USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED

* HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES

*

* You should have received a copy of the GNU General Public License

* along with this program; if not, write to the Free Software

* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

*

*

* 10/23/2000 - Alpha Release

*	First release to the public

 try next IPC */

 Sanity check */

 no conflict just release */

 @BUG @TBD EnableSRAM(pBDData);

 fill out standard constant fields */

 fill out dynamically determined fields */

 Fill out Mwave OS and BIOS task names */

/*

 * Transmeta's Efficeon AGPGART driver.

 *

 * Based upon a diff by Linus around November '02.

 *

 * Ported to the 2.6 kernel by Carlos Puchol <cpglinux@puchol.com>

 * and H. Peter Anvin <hpa@transmeta.com>.

/*

 * NOTE-cpg-040217:

 *

 *   - when compiled as a module, after loading the module,

 *     it will refuse to unload, indicating it is in use,

 *     when it is not.

 *   - no s3 (suspend to ram) testing.

 *   - tested on the efficeon integrated nothbridge for tens

 *     of iterations of starting x and glxgears.

 *   - tested with radeon 9000 and radeon mobility m9 cards

 *   - tested with c3/c4 enabled (with the mobility m9 card)

/*

 * The real differences to the generic AGP code is

 * in the GART mappings - a two-level setup with the

 * first level being an on-chip 64-entry table.

 *

 * The page array is filled through the ATTPAGE register

 * (Aperture Translation Table Page Register) at 0xB8. Bits:

 *  31:20: physical page address

 *   11:9: Page Attribute Table Index (PATI)

 *	   must match the PAT index for the

 *	   mapped pages (the 2nd level page table pages

 *	   themselves should be just regular WB-cacheable,

 *	   so this is normally zero.)

 *      8: Present

 *    7:6: reserved, write as zero

 *    5:0: GATT directory index: which 1st-level entry

 *

 * The Efficeon AGP spec requires pages to be WB-cacheable

 * but to be explicitly CLFLUSH'd after any changes.

 Number of PDE pages */

 This function does the same thing as mask_memory() for this chipset... */

/*

 * Control interfaces are largely identical to

 * the legacy Intel 440BX..

 aperture size */

 address to map to */

 agpctrl */

 paccfg/nbxcfg */

 clear any possible error conditions */

/*

 * Since we don't need contiguous memory we just try

 * to get the gatt table once

 There are 2^10 PTE pages per PDE page */

 clflush is slow, so don't clflush until we have to */

 Efficeon-specific GATT table setup / populate / teardown

 true might be faster?

 Generic

 Probe for Efficeon controller */

	/*

	* If the device has not been properly setup, the following will catch

	* the problem and should stop the system from crashing.

	* 20030610 - hamish@zot.org

	/*

	* The following fixes the case where the BIOS has "forgotten" to

	* provide an address range for the GART.

	* 20030610 - hamish@zot.org

 Fill in the mode register */

/*

 * Nvidia AGPGART routines.

 * Based upon a 2.4 agpgart diff by the folks from NVIDIA, and hacked up

 * to work in 2.5 by Dave Jones.

 NVIDIA registers */

 Find the iorr that is already used for the base */

 If not found, determine the uppermost available iorr */

 aperture size */

 address to map to */

 directory size is 64k */

 attbase */

 gtlb control */

 gart control */

 map aperture */

 gart control */

 gtlb control */

 unmap aperture */

 restore previous aperture size */

 restore iorr for previous aperture size */

/*

 * Note we can't use the generic routines, even though they are 99% the same.

 * Aperture sizes <64M still requires a full 64k GART directory, but

 * only use the portion of the TLB entries that correspond to the apertures

 * alignment inside the surrounding 64M block.

 PCI Posting. */

 flush chipset */

 flush TLB entries */

 The 32M mode still requires a 64k gatt */

 Fill in the mode register */

 set power state 0 and restore PCI space */

 reconfigure AGP hardware again */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VIA AGPGART routines.

 aperture size */

 address to map to */

 GART control register */

 attbase - aperture GATT base */

	/* Do not disable by writing 0 to VIA_ATTBASE, it screws things up

	 * during reinitialization.

 address to map to */

 attbase - aperture GATT base */

	/* 1. Enable GTLB in RX90<7>, all AGP aperture access needs to fetch

	 *    translation table first.

	 * 2. Enable AGP aperture in RX91<0>. This bit controls the enabling of the

	 *    graphics AGP aperture for the AGP3.0 port.

 VT8601 */

 VT82C693A / VT28C694T */

 VT8633 */

 VT8361 */

 VT8365 / VT8362 */

 VT8753A */

 VT8366 */

 VT8633 (for CuMine/ Celeron) */

 KM266 / PM266 */

 CLE266 */

	/* VT8604 / VT8605 / VT8603

 P4M266x/P4N266 */

 VT8754 */

 P4X600 */

 KM400 */

 PT880 */

 PT880 Ultra */

 PT890 */

 PM800/PN800/PM880/PN880 */

 KT880 */

 KTxxx/Px8xx */

 P4M800 */

 P4M800CE */

 VT3324 / CX700 */

	/* VT3336 - this is a chipset for AMD Athlon/K8 CPU. Due to K8's unique

	 * architecture, the AGP resource and behavior are different from

	 * the traditional AGP which resides only in chipset. AGP is used

	 * by 3D driver which wasn't available for the VT3336 and VT3364

	 * generation until now.  Unfortunately, by testing, VT3364 works

	 * but VT3336 doesn't. - explanation from via, just leave this as

	 * as a placeholder to avoid future patches adding it back in.

 P4M890 */

 P4M900 */

 dummy final entry, always present */

/*

 * VIA's AGP3 chipsets do magick to put the AGP bridge compliant

 * with the same standards version as the graphics card.

 Check AGP 2.0 compatibility mode. */

	/*

	 * Garg, there are KT400s with KT266 IDs.

 Is there a KT400 subsystem ? */

 If this is an AGP3 bridge, check which mode its in and adjust. */

 Fill in the mode register */

 CONFIG_PM */

 must be the same order as name table above */

/*

 * ALi AGPGART routines.

 PAGE_SIZE */

 clear tag

 aperture size and gatt addr */

 tlb control */

 address to map to */

enable TLB

 is this really needed?  --hch */

 Setup function */

 dummy final entry, always present */

 probe for known chipsets */

 Fill in the mode register */

 SPDX-License-Identifier: GPL-2.0

/*

 * Setup routines for AGP 3.5 compliant bridges.

 Generic AGP 3.5 enabling routines */

/*

 * Initialize all isochronous transfer parameters for an AGP 3.0

 * node (i.e. a host bridge in combination with the adapters

 * lying behind it...)

	/*

	 * Convenience structure to make the calculations clearer

	 * here.  The field names come straight from the AGP 3.0 spec.

	/*

	 * We'll work with an array of isoch_data's (one for each

	 * device in dev_list) throughout this function.

	/*

	 * Sort the device list by maxbw.  We need to do this because the

	 * spec suggests that the devices with the smallest requirements

	 * have their resources allocated first, with all remaining resources

	 * falling to the device with the largest requirement.

	 *

	 * We don't exactly do this, we divide target resources by ndevs

	 * and split them amongst the AGP 3.0 devices.  The remainder of such

	 * division operations are dropped on the last device, sort of like

	 * the spec mentions it should be done.

	 *

	 * We can't do this sort when we initially construct the dev_list

	 * because we don't know until this function whether isochronous

	 * transfers are enabled and consequently whether maxbw will mean

	 * anything.

 Extract power-on defaults from the target */

	/*

	 * Extract power-on defaults for each device in dev_list.  Along

	 * the way, calculate the total isochronous bandwidth required

	 * by these devices and the largest requested payload size.

 Check if this configuration has any chance of working */

	/*

	 * Write the calculated payload size into the target's NICMD

	 * register.  Doing this directly effects the ISOCH_N value

	 * in the target's NISTAT register, so we need to do this now

	 * to get an accurate value for ISOCH_N later.

 Reread the target's ISOCH_N */

 Calculate the minimum ISOCH_N needed by each master */

	/* Exit if the minimal ISOCH_N allocation among the masters is more

	/* Calculate left over ISOCH_N capability in the target.  We'll give

	/*

	 * Calculate the minimum isochronous RQ depth needed by each master.

	 * Along the way, distribute the extra ISOCH_N capability calculated

	 * above.

		/*

		 * This is a little subtle.  If ISOCH_Y > 64B, then ISOCH_Y

		 * byte isochronous writes will be broken into 64B pieces.

		 * This means we need to budget more RQ depth to account for

		 * these kind of writes (each isochronous write is actually

		 * many writes on the AGP bus).

	/* Figure the number of isochronous and asynchronous RQ slots the

	/* Exit if the minimal RQ needs of the masters exceeds what the target

	/* Calculate asynchronous RQ capability in the target (per master) as

	/* Distribute the extra RQ slots calculated above and write our

/*

 * This function basically allocates request queue slots among the

 * AGP 3.0 systems in nonisochronous nodes.  The algorithm is

 * pretty stupid, divide the total number of RQ slots provided by the

 * target by ndevs.  Distribute this many slots to each AGP 3.0 device,

 * giving any left over slots to the last device in dev_list.

/*

 * Fully configure and enable an AGP 3.0 host bridge and all the devices

 * lying behind it.

 Extract some power-on defaults from the target */

 isoch xfers not available, bail out. */

	/*

	 * Allocate a head for our AGP 3.5 device list

	 * (multiple AGP v3 devices are allowed behind a single bridge).

 Find all AGP devices, and add them to dev_list. */

 Bridge */

 Skip bridges. We should call this function for each one. */

 Unclassified device */

 Don't know what this is, but log it for investigation. */

 Display controller */

 Multimedia controller */

	/*

	 * Take an initial pass through the devices lying behind our host

	 * bridge.  Make sure each one is actually an AGP 3.0 device, otherwise

	 * exit with an error message.  Along the way store the AGP 3.0

	 * cap_ptr for each device

	/*

	 * Call functions to divide target resources amongst the AGP 3.0

	 * masters.  This process is dramatically different depending on

	 * whether isochronous transfers are supported.

 Be sure to free the dev_list */

/*

 * AGPGART driver frontend compatibility ioctls

 * Copyright (C) 2004 Silicon Graphics, Inc.

 * Copyright (C) 2002-2003 Dave Jones

 * Copyright (C) 1999 Jeff Hartmann

 * Copyright (C) 1999 Precision Insight, Inc.

 * Copyright (C) 1999 Xi Graphics, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included

 * in all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * JEFF HARTMANN, OR ANY OTHER CONTRIBUTORS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE

 * OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 remove a client */

 client is already removed */

 Create the client and add the segment */

 Will never really happen */

		/* Use the original pid of the controller,

/*

 * AGPGART driver.

 * Copyright (C) 2004 Silicon Graphics, Inc.

 * Copyright (C) 2002-2005 Dave Jones.

 * Copyright (C) 1999 Jeff Hartmann.

 * Copyright (C) 1999 Precision Insight, Inc.

 * Copyright (C) 1999 Xi Graphics, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included

 * in all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * JEFF HARTMANN, OR ANY OTHER CONTRIBUTORS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE

 * OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 * TODO:

 * - Allocate more than order 0 pages to avoid too much linear map splitting.

/*

 * Needed by the Nforce GART driver for the time being. Would be

 * nice to do this some other way instead of needing this export.

/*

 * Generic routines for handling agp_memory structures -

 * They use the basic page allocation routines to do the brunt of the work.

/*

 * Use kmalloc if possible for the page list. Otherwise fall back to

 * vmalloc. This speeds things up and also saves memory for small AGP

 * regions.

/**

 *	agp_free_memory - free memory associated with an agp_memory pointer.

 *

 *	@curr:		agp_memory pointer to be freed.

 *

 *	It is the only function that can be called when the backend is not owned

 *	by the caller.  (So it can free memory on client death.)

/**

 *	agp_allocate_memory  -  allocate a group of pages of a certain type.

 *

 *	@bridge: an agp_bridge_data struct allocated for the AGP host bridge.

 *	@page_count:	size_t argument of the number of pages

 *	@type:	u32 argument of the type of memory to be allocated.

 *

 *	Every agp bridge device will allow you to allocate AGP_NORMAL_MEMORY which

 *	maps to physical ram.  Any other type is device dependent.

 *

 *	It returns NULL whenever memory is unavailable.

 End - Generic routines for handling agp_memory structures */

/**

 *	agp_copy_info  -  copy bridge state information

 *

 *	@bridge: an agp_bridge_data struct allocated for the AGP host bridge.

 *	@info:		agp_kern_info pointer.  The caller should insure that this pointer is valid.

 *

 *	This function copies information about the agp bridge device and the state of

 *	the agp backend into an agp_kern_info pointer.

 End - Routine to copy over information structure */

/*

 * Routines for handling swapping of agp_memory into the GATT -

 * These routines take agp_memory and insert them into the GATT.

 * They call device specific routines to actually write to the GATT.

/**

 *	agp_bind_memory  -  Bind an agp_memory structure into the GATT.

 *

 *	@curr:		agp_memory pointer

 *	@pg_start:	an offset into the graphics aperture translation table

 *

 *	It returns -EINVAL if the pointer == NULL.

 *	It returns -EBUSY if the area of the table requested is already in use.

/**

 *	agp_unbind_memory  -  Removes an agp_memory structure from the GATT

 *

 * @curr:	agp_memory pointer to be removed from the GATT.

 *

 * It returns -EINVAL if this piece of agp_memory is not currently bound to

 * the graphics aperture translation table or if the agp_memory pointer == NULL

 End - Routines for handling swapping of agp_memory into the GATT */

 Generic Agp routines - Start */

	/*

	 * Some dumb bridges are programmed to disobey the AGP2 spec.

	 * This is likely a BIOS misprogramming rather than poweron default, or

	 * it would be a lot more common.

	 * https://bugs.freedesktop.org/show_bug.cgi?id=8816

	 * AGPv2 spec 6.1.9 states:

	 *   The RATE field indicates the data transfer rates supported by this

	 *   device. A.G.P. devices must report all that apply.

	 * Fix them up as best we can.

 Check the speed bits make sense. Only one should be set. */

 rate=2 */

 rate=4*/

 disable SBA if it's not supported */

 Set rate */

 Now we know what mode it should be, clear out the unwanted bits. */

 4X */

 2X */

 1X */

 Apply any errata. */

 If we've dropped down to 1X, disable fast writes. */

/*

 * requested_mode = Mode requested by (typically) X.

 * bridge_agpstat = PCI_AGP_STATUS from agp bridge.

 * vga_agpstat = PCI_AGP_STATUS from graphic card.

 Check the speed bits make sense. */

	/* ARQSZ - Set the value to the maximum one.

	/* Calibration cycle.

 SBA *must* be supported for AGP v3 */

	/*

	 * Set speed.

	 * Check for invalid speeds. This can happen when applications

	 * written before the AGP 3.0 standard pass AGP2.x modes to AGP3 hardware

		/*

		 * Caller hasn't a clue what it is doing. Bridge is in 3.0 mode,

		 * have been passed a 3.0 mode, but with 2.x speed bits set.

		 * AGP2.x 4x -> AGP3.0 4x.

		/*

		 * The caller doesn't know what they are doing. We are in 3.0 mode,

		 * but have been passed an AGP 2.x mode.

		 * Convert AGP 1x,2x,4x -> AGP 3.0 4x.

 All set, bridge & device can do AGP x8*/

		/*

		 * If we didn't specify an AGP mode, we see if both

		 * the graphics card, and the bridge can do x8, and use if so.

		 * If not, we fall back to x4 mode.

 Apply any errata. */

/**

 * agp_collect_device_status - determine correct agp_cmd from various agp_stat's

 * @bridge: an agp_bridge_data struct allocated for the AGP host bridge.

 * @requested_mode: requested agp_stat from userspace (Typically from X)

 * @bridge_agpstat: current agp_stat from AGP bridge.

 *

 * This function will hunt for an AGP graphics card, and try to match

 * the requested mode to the capabilities of both the bridge and the card.

	/*

	 * Ok, here we have a AGP device. Disable impossible

	 * settings, and adjust the readqueue to the minimum.

 adjust RQ depth */

 disable FW if it's not supported */

 Check to see if we are operating in 3.0 mode */

 Exit early if already set by errata workarounds. */

 Something bad happened. FIXME: Return error code? */

 Do AGP version specific frobbing. */

 If we have 3.5, we can do the isoch stuff. */

 Disable calibration cycle in RX91<1> when not in AGP3.0 mode of operation.*/

 AGP v<3 */

 The generic routines can't handle 2 level gatt's */

 This case will never really happen. */

 These cases will never really happen. */

 AK: bogus, should encode addresses > 4GB */

 PCI Posting. */

 The generic routines can't deal with 2 level gatt's */

	/* Do not worry about freeing memory, because if this is

	 * called, then all agp memory is deallocated and removed

 The generic routines can't deal with 2 level gatt's */

 The generic routines know nothing of memory types */

 PCI Posting. */

 The generic routines know nothing of memory types */

 AK: bogus, should encode addresses > 4GB */

 PCI Posting. */

/*

 * Basic Page Allocation Routines -

 * These routines handle page allocation and by default they reserve the allocated

 * memory.  They also handle incrementing the current_memory_agp value, Which is checked

 * against a maximum value.

 agp_free_memory() needs gart address */

 End Basic Page Allocation Routines */

/**

 * agp_enable  -  initialise the agp point-to-point connection.

 *

 * @bridge: an agp_bridge_data struct allocated for the AGP host bridge.

 * @mode:	agp mode register value to configure with.

/* When we remove the global variable agp_bridge from all drivers

 * then agp_alloc_bridge and agp_generic_find_bridge need to be updated

 memory type is ignored in the generic routine */

/*

 * These functions are implemented according to the AGPv3 spec,

 * which covers implementation details that had previously been

 * left open.

 set aperture size */

 set gart pointer */

 enable aperture and GTLB */

/*

 * For documentation on the i460 AGP interface, see Chapter 7 (AGP Subsystem) of

 * the "Intel 460GTX Chipset Software Developer's Manual":

 * http://www.intel.com/design/archives/itanium/downloads/248704.htm 

/*

 * 460GX support by Chris Ahna <christopher.j.ahna@intel.com>

 * Clean up & simplification by David Mosberger-Tang <davidm@hpl.hp.com>

/*

 * The i460 can operate with large (4MB) pages, but there is no sane way to support this

 * within the current kernel/DRM environment, so we disable the relevant code for now.

 * See also comments in ia64_alloc_page()...

 Control bits for Out-Of-GART coherency and Burst Write Combining */

/*

 * gatt_table entries are 32-bits wide on the i460; the generic code ought to declare the

 * gatt_table and gatt_table_real pointers a "void *"...

/*

 * The 460 spec says we have to read the last location written to make sure that all

 * writes have taken effect

 ioremap'd GATT area */

 i460 supports multiple GART page sizes, so GART pageshift is dynamic: */

 BIOS configures chipset to one of 2 possible apbase values: */

 structure for tracking partial use of 4MB GART pages: */

 bitmap of kernel-pages in use */

 number of kernel pages using the large page */

 physical address of large page */

 page pointer */

	/*

	 * The 32GB aperture is only available with a 4M GART page size.  Due to the

	 * dynamic GART page size, we can't figure out page_order or num_entries until

	 * runtime.

 Determine the GART page size */

 Exit now if the IO drivers for the GART SRAMS are turned off */

 Make sure we don't try to create an 2 ^ 23 entry GATT */

 Determine the proper APBASE register */

		/*

		 * Dynamically calculate the proper num_entries and page_order values for

		 * the define aperture sizes. Take care not to shift off the end of

		 * values[i].size.

 Neglect control bits when matching up size_value */

 There isn't anything to do here since 460 has no GART TLB. */

/*

 * This utility function is needed to prevent corruption of the control bits

 * which are stored along with the aperture size in 460's AGPSIZ register

	/*

	 * Do the necessary rigmarole to read all eight bytes of APBASE.

	 * This has to be done since the AGP aperture can be above 4GB on

	 * 460 based systems.

 Clear BAR control bits */

	/*

	 * Initialize partial allocation trackers if a GART page is bigger than a kernel

	 * page.

	/*

	 * Load up the fixed address of the GART SRAMS which hold our GATT table.

 These are no good, the should be removed from the agp_bridge strucure... */

/*

 * The following functions are called when the I/O (GART) page size is smaller than

 * PAGE_SIZE.

/*

 * These functions are called when the I/O (GART) page size exceeds PAGE_SIZE.

 *

 * This situation is interesting since AGP memory allocations that are smaller than a

 * single GART page are possible.  The i460.lp_desc array tracks partial allocation of the

 * large GART pages to work around this issue.

 *

 * i460.lp_desc[pg_num].refcount tracks the number of kernel pages in use within GART page

 * pg_num.  i460.lp_desc[pg_num].paddr is the physical address of the large page and

 * i460.lp_desc[pg_num].alloced_map is a bitmap of kernel pages that are in use (allocated).

 Figure out what pg_start means in terms of our large GART pages */

 Check if the requested region of the aperture is free */

 OK, the entire large page is available... */

 Allocate new GART pages... */

 Figure out what pg_start means in terms of our large GART pages */

 Free GART pages if they are unused */

 Wrapper routines to call the approriate {small_io_page,large_io_page} function */

/*

 * If the I/O (GART) page size is bigger than the kernel page size, we don't want to

 * allocate memory until we know where it is to be bound in the aperture (a

 * multi-kernel-page alloc might fit inside of an already allocated GART page).

 *

 * Let's just hope nobody counts on the allocated AGP memory being there before bind time

 * (I don't think current drivers do)...

 Returning NULL would cause problems */

 AK: really dubious code. */

 I460_LARGE_IO_PAGES */

 Make sure the returned address is a valid GATT entry */

/*

 * ATi AGPGART routines.

 PCI Posting. */

 PCI Posting. */

 Write back the previous size and disable gart translation */

 Get the memory mapped registers */

 address to map to */

	/*

	agp_bridge.gart_bus_addr = pci_bus_address(agp_bridge.dev,

						   AGP_APERTURE_BAR);

	printk(KERN_INFO PFX "IGP320 gart_bus_addr: %x\n", agp_bridge.gart_bus_addr);

 PCI Posting.*/

 SIGNALED_SYSTEM_ERROR @ NB_STATUS */

 Write out the address of the gatt table */

 PCI Posting. */

/*

 *Since we don't need contiguous memory we just try

 * to get the gatt table once

CACHE_FLUSH(); */

 PCI posting */

 PCI posting */

 Write out the size register */

	/*

	 * Get the address for the gart region.

	 * This is a bus address even on the alpha, b/c its

	 * used to program the agp master not the cpu

 Calculate the agp offset */

 PCI Posting. */

 dummy final entry, always present */

 probe for known chipsets */

 Fill in the mode register */

/*

 * AGPGART driver backend routines.

 * Copyright (C) 2004 Silicon Graphics, Inc.

 * Copyright (C) 2002-2003 Dave Jones.

 * Copyright (C) 1999 Jeff Hartmann.

 * Copyright (C) 1999 Precision Insight, Inc.

 * Copyright (C) 1999 Xi Graphics, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included

 * in all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * JEFF HARTMANN, DAVE JONES, OR ANY OTHER CONTRIBUTORS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE

 * OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 * TODO:

 * - Allocate more than order 0 pages to avoid too much linear map splitting.

/* Due to XFree86 brain-damage, we can't go to 1.0 until they

 * fix some real stupidity. It's only by chance we can bump

/**

 *	agp_backend_acquire  -  attempt to acquire an agp backend.

 *

/**

 *	agp_backend_release  -  release the lock on the agp backend.

 *

 *	The caller must insure that the graphics aperture translation table

 *	is read for use by another entity.

 *

 *	(Ensure that all memory it bound is unbound.)

 FIXME vmalloc'd memory not guaranteed contiguous */

 cannot be __exit b/c as it could be called from __init code */

/* When we remove the global variable agp_bridge from all drivers

 * then agp_alloc_bridge and agp_generic_find_bridge need to be updated

 Grab reference on the chipset driver. */

/*

 * AGPGART driver frontend

 * Copyright (C) 2004 Silicon Graphics, Inc.

 * Copyright (C) 2002-2003 Dave Jones

 * Copyright (C) 1999 Jeff Hartmann

 * Copyright (C) 1999 Precision Insight, Inc.

 * Copyright (C) 1999 Xi Graphics, Inc.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the "Software"),

 * to deal in the Software without restriction, including without limitation

 * the rights to use, copy, modify, merge, publish, distribute, sublicense,

 * and/or sell copies of the Software, and to permit persons to whom the

 * Software is furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included

 * in all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS

 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL

 * JEFF HARTMANN, OR ANY OTHER CONTRIBUTORS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE

 * OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

 Check to see if this is even in the memory pool */

 This is the first item on the list */

/*

 * Routines for managing each client's segment list -

 * These routines handle adding and removing segments

 * to each auth'ed client.

 End - Routines for managing each client's segment list */

 This function must only be called when current_controller != NULL */

 File private list routines */

 End - File flag list routines */

/*

 * Wrappers for agp_free_memory & agp_allocate_memory

 * These make sure that internal lists are kept updated.

/* Routines for managing the list of controllers -

 * These routines manage the current controller, and the list of

 * controllers

/*

 * Routines for managing client lists -

 * These routines are for managing the list of auth'ed clients.

 End - Routines for managing client lists */

 File Operations */

 Root priv, can be controller */

 remove a client */

 client is already removed */

 Create the client and add the segment */

 Will never really happen */

 This function is not currently implemented */

		/* Use the original pid of the controller,

/*

 * Serverworks AGPGART routines.

 Memory mapped registers */

 device one */

 Red Pen: Everyone else does pci posting flush here */

 Create a fake scratch directory */

	/* Get the address for the gart region.

	 * This is a bus address even on the alpha, b/c its

	 * used to program the agp master not the cpu

 Calculate the agp offset */

/*

 * This routine could be implemented by taking the addresses

 * written to the GATT, and flushing them individually.  However

 * currently it just flushes the whole table.  Which is probably

 * more efficient, since agp_memory blocks can be a large number of

 * entries.

 Get the memory mapped registers */

 PCI Posting. */

 PCI Posting. */

 Agp Enable bit */

 Fill in the mode register */

 disable FW */

 Everything is on func 1 here so we are hardcoding function one */

/*

 * SiS AGPGART routines.

		/*

		 * Weird: on some sis chipsets any rate change in the target

		 * command register triggers a 5ms screwup during which the master

		 * cannot be configured

 chipsets that require the 'delay hack'

 terminator

 sis chipsets that indicate less than agp3.5

 are not actually fully agp3 compliant

 Fill in the mode register */

 CONFIG_PM */

/*

 * Intel AGPGART routines.

	/* Intel 815 chipsets have a _weird_ APSIZE register with only

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 paccfg/nbxcfg */

 clear any possible error conditions */

 attbase - aperture base */

	/* the Intel 815 chipset spec. says that bits 29-31 in the

 aperture size */

 address to map to */

 agpctrl */

 apcont */

 clear any possible error conditions */

 Oddness : this chipset seems to have no ERRSTS register ! */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 global enable aperture access */

 This flag is not accessed through MCHCFG register as in */

 i850 chipset. */

 clear any possible AGP-related error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 mcgcfg */

 clear any possible error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 agpm */

 clear any possible error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 mcgcfg */

 clear any possible AGP-related error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 mcgcfg */

 clear any possible AGP-related error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 gmch */

 clear any possible AGP-related error conditions */

 aperture size */

 address to map to */

 attbase - aperture base */

 agpctrl */

 mchcfg */

 Setup function */

/* Table to describe Intel GMCH and AGP/PCIE GART drivers.  At least one of

 * driver and gmch_driver must be non-null, and find_gmch will determine

 * which one should be used if a gmch_chip_id is present.

		/* In case that multiple models of gfx chip may

		   stand on same host bridge type, this can be

	/*

	* The following fixes the case where the BIOS has "forgotten" to

	* provide an address range for the GART.

	* 20030610 - hamish@zot.org

	* This happens before pci_enable_device() intentionally;

	* calling pci_enable_device() before assigning the resource

	* will result in the GART being disabled on machines with such

	* BIOSs (the GART ends up with a BAR starting at 0, which

	* conflicts a lot of other devices).

	/*

	* If the device has not been properly setup, the following will catch

	* the problem and should stop the system from crashing.

	* 20030610 - hamish@zot.org

 Fill in the mode register */

 for HAS2 support */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HP zx1 AGPGART routines.

 *

 * (c) Copyright 2002, 2003 Hewlett-Packard Development Company, L.P.

 *	Bjorn Helgaas <bjorn.helgaas@hp.com>

 ACPI reports SBA, we want IOC */

 HP ZX1 IOC registers */

 AGP bridge need not be PCI device, but DRM thinks it is. */

 filled in by hp_zx1_fetch_size() */

 PDIR for entire IOVA

 PDIR just for GART (subset of above)

 do we own it, or share it with sba_iommu?

 IOC ps config

	/*

	 * IOC already configured by sba_iommu module; just use

	 * its setup.  We assume:

	 *	- IOVA space is 1Gb in size

	 *	- first 512Mb is IOMMU, second 512Mb is GART

 Normal case when no AGP device in system */

	/*

	 * Select an IOV page size no larger than system page size.

	/*

	 * If the IOTLB is currently disabled, we can take it over.

	 * Otherwise, we have to share with sba_iommu.

 keep looking for another bridge */

 Look for an enclosing IOC scope and find its CSR space */

 TBD check _CID also */

 found no enclosing IOC */

 we only support one bridge; quit looking */

/*

 * AMD K7 AGPGART routines.

 In mmio region (16-bit register) */

 In mmio region (32-bit register) */

 In mmio region (32-bit register) */

 In mmio region (32-bit register) */

 PCI Posting. */

/* Since we don't need contiguous memory we just try

 * to get the gatt table once

	/* Get the address for the gart region.

	 * This is a bus address even on the alpha, b/c its

	 * used to program the agp master not the cpu

 Calculate the agp offset */

 PCI Posting. */

 PCI Posting. */

 Get the memory mapped registers */

 Write out the address of the gatt table */

 PCI Posting. */

 Write the Sync register */

 Set indexing mode */

 Write the enable register */

 PCI Posting. */

 Write out the size register */

 Flush the tlb */

 PCI Posting.*/

 PCI Posting. */

 Write back the previous size and disable gart translation */

/*

 * This routine could be implemented by taking the addresses

 * written to the GATT, and flushing them individually.  However

 * currently it just flushes the whole table.  Which is probably

 * more efficient, since agp_memory blocks can be a large number of

 * entries.

 PCI Posting. */

 PCI Posting. */

 PCI Posting. */

 dummy final entry, always present */

	/* 751 Errata (22564_B-1.PDF)

	   erratum 20: strobe glitch with Nvidia NV10 GeForce cards.

	   system controller may experience noise due to strong drive strengths

		/* With so many variants of NVidia cards, it's simpler just

		   to blacklist them all, and then whitelist them as needed

	/* 761 Errata (23613_F.pdf)

	 * Revisions B0/B1 were a disaster.

	 * erratum 44: SYSCLK/AGPCLK skew causes 2X failures -- Force mode to 1X

	 * erratum 45: Timing problem prevents fast writes -- Disable fast write.

	 * erratum 46: Setup violation on AGP SBA pins - Disable side band addressing.

 Fill in the mode register */

 CONFIG_PM */

 must be the same order as name table above */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HP Quicksilver AGP GART routines

 *

 * Copyright (c) 2006, Kyle McMartin <kyle@parisc-linux.org>

 *

 * Based on drivers/char/agpgart/hp-agp.c which is

 * (c) Copyright 2002, 2003 Hewlett-Packard Development Company, L.P.

 *	Bjorn Helgaas <bjorn.helgaas@hp.com>

 filled in by parisc_agp_fetch_size() */

 flush */

 Find our parent Pluto */

 Now search our Pluto for our precious AGP device... */

 w00t, let's go find our cookies... */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2001-2003 SuSE Labs.

 * Distributed under the GNU public license, v2.

 *

 * This is a GART driver for the AMD Opteron/Athlon64 on-CPU northbridge.

 * It also includes support for the AMD 8151 AGP bridge,

 * although it doesn't actually do much, as all the real

 * work is done in the northbridge(s).

 PAGE_SIZE */

 NVIDIA K8 registers */

 ULi K8 registers */

 Make sure we can fit the range in the gatt table. */

 FIXME: could wrap */

 gatt table should be empty. */

 PCI Posting. */

/*

 * This hack alters the order element according

 * to the size of a long. It sucks. I totally disown this, even

 * though it does appear to work for the most part.

/*

 * Get the current Aperture size from the x86-64.

 * Note, that there may be multiple x86-64's, but we just return

 * the value from the first one we find. The set_size functions

 * keep the rest coherent anyway. Or at least should do.

/*

 * In a multiprocessor x86-64 system, this function gets

 * called once for each CPU.

 Address to map to */

 0 0 0 0 0 0 */

 1 0 0 0 0 0 */

 1 1 0 0 0 0 */

 1 1 1 0 0 0 */

 1 1 1 1 0 0 */

 1 1 1 1 1 0 */

 1 1 1 1 1 1 */

 Configure AGP regs in each x86-64 host bridge. */

 disable gart translation */

 Some basic sanity checks for the aperture. */

	/* Request the Aperture. This catches cases when someone else

	   already put a mapping in there - happens with some very broken BIOS



	   Maybe better to use pci_assign_resource/pci_enable_device instead

/*

 * W*s centric BIOS sometimes only set up the aperture in the AGP

 * bridge, not the northbridge. On AMD64 this is handled early

 * in aperture.c, but when IOMMU is not enabled or we run

 * on a 32bit kernel this needs to be redone.

 * Unfortunately it is impossible to fix the aperture here because it's too late

 * to allocate that much memory. But at least error out cleanly instead of

 * crashing.

 Northbridge seems to contain crap. Try the AGP bridge. */

 Some BIOS use weird encodings not in the AGPv3 table. */

	/*

	 * On some sick chips APSIZE is 0. This means it wants 4G

	 * so let double check that order, and lets trust the AMD NB settings

 should port this to i386 */

 Handle AMD 8151 quirks */

	/*

	 * Work around errata.

	 * Chips before B2 stepping incorrectly reporting v3.5

 shadow x86-64 registers into ULi registers */

 if x86-64 aperture base is beyond 4G, exit here */

 Handle shadow device of the Nvidia NForce3 */

 CHECK-ME original 2.4 version set up some IORRs. Check if that is needed. */

 shadow x86-64 registers into NVIDIA registers */

 if x86-64 aperture base is beyond 4G, exit here */

 The Highlander principle */

 Could check for AGPv3 here */

 Fill in the mode register */

 CONFIG_PM */

 ULi M1689 */

 VIA K8T800Pro */

 VIA K8T800 */

 VIA K8M800 / K8N800 */

 VIA K8M890 / K8N890 */

 VIA K8T890 */

 VIA K8T800/K8M800/K8N800 */

 NForce3 */

 SIS 755 */

 SIS 760 */

 ALI/ULI M1695 */

 Not static due to IOMMU code calling it early. */

 First check that we have at least one AMD64 NB */

 Look for any AGP bridge */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * UniNorth AGPGART routines.

/*

 * NOTES for uninorth3 (G5 AGP) supports :

 *

 * There maybe also possibility to have bigger cache line size for

 * agp (see pmac_pci.c and look for cache line). Need to be investigated

 * by someone.

 *

 * PAGE size are hardcoded but this may change, see asm/page.h.

 *

 * Jerome Glisse <j.glisse@gmail.com>

 aperture size and gatt addr */

	/* HACK ALERT

	 * UniNorth seem to be buggy enough not to handle properly when

	 * the AGP aperture isn't mapped at bus physical address 0

 Assume U3 or later on PPC64 systems */

 high 4 bits of GART physical address go in UNI_N_CFG_AGP_BASE */

 We know nothing of memory types */

 We know nothing of memory types */

		/*

		 * Darwin disable AGP 4x on this revision, thus we

		 * may assume it's broken. This is an AGP2 controller.

		/*

		 * We need to set REQ_DEPTH to 7 for U3 versions 1.0, 2.1,

		 * 2.2 and 2.3, Darwin do so.

 This is an AGP V3 */

 AGP V2 */

/*

 * These Power Management routines are _not_ called by the normal PCI PM layer,

 * but directly by the video driver through function pointers in the device

 * tree.

 Only one suspend supported */

 turn off AGP on the video chip, if it was enabled */

 Don't touch the bridge yet, device first */

		/* Only deal with devices on the same bus here, no Mac has a P2P

		 * bridge on the AGP port, and mucking around the entire PCI

		 * tree is source of problems on some machines because of a bug

		 * in some versions of pci_find_capability() when hitting a dead

		 * device

 turn off AGP on the bridge */

 turn off the GART */

 CONFIG_PM */

 We can't handle 2 level gatt's */

 Need to clear out any dirty data still sitting in caches */

	/* Do not worry about freeing memory, because if this is

	 * called, then all agp memory is deallocated and removed

	 * from the table.

 Setup function */

/*

 * Not sure that u3 supports that high aperture sizes but it

 * would strange if it did not :)

 probe for known chipsets */

 Set revision to 0 if we could not read it. */

 Locate core99 Uni-N */

 Locate G5 u3 */

 Inform platform of our suspend/resume caps */

 Allocate & setup our driver */

 Fill in the mode register */

 Inform platform of our suspend/resume caps */

 no translation */

	/*

	 * Get the page, inc the use count, and return it

 filled in by alpha_core_agp_setup */

 faked */

	/*

	 * Build the aperture size descriptor

	/*

	 * Build a fake pci_dev struct

 only 1 size */

/*

 * Intel GTT (Graphics Translation Table) routines

 *

 * Caveat: This driver implements the linux agp interface, but this is far from

 * a agp driver! GTT support ended up here for purely historical reasons: The

 * old userspace intel graphics drivers needed an interface to map memory into

 * the GTT. And the drm provides a default interface for graphic devices sitting

 * on an agp port. So it made sense to fake the GTT support as an agp port to

 * avoid having to create a new api.

 *

 * With gem this does not make much sense anymore, just needlessly complicates

 * the code. But as long as the old graphics stack is still support, it's stuck

 * here.

 *

 * /fairy-tale-mode off

/*

 * If we have Intel graphics, we're not going to have anything other than

 * an Intel IOMMU. So make the correct use of the PCI DMA API contingent

 * on the Intel IOMMU support (CONFIG_INTEL_IOMMU).

 * Only newer chipsets need to bother with this, of course.

 Chipset specific GTT setup */

	/* This should undo anything done in ->setup() save the unmapping

	/* Flags is a more or less chipset specific opaque value.

	 * For chipsets that need to support old ums (non-gem) code, this

 device one */

 I915G */

 on first access via agp, fill with scratch */

 Whether i915 needs to use the dmar apis or not. */

  Size of memory reserved for graphics by the BIOS */

 Total number of gtt entries. */

	/* Part of the gtt that is mappable by the cpu, for those chips where

 Exists to support ARGB cursors */

 i81x does not preallocate the gtt. It's always 64kb in size. */

/*

 * The i810/i830 requires a physical address to program its mouse

 * pointer into hardware.

 * However the Xserver still writes to it through the agp aperture.

 kludge to get 4 physical pages for ARGB cursor */

 kludge to get 4 physical pages for ARGB cursor */

 no stolen mem on i81x */

 ensure that ppgtt is disabled */

 write the new ggtt size */

 GTT pagetable sizes bigger than 512KB are not possible on G33! */

		/* On previous hardware, the GTT size was just what was

		 * required to map the aperture.

 9xx supports large sizes, just look at the length */

/* Certain Gen5 chipsets require require idling the GPU before

 * unmapping anything from the GTT when VT-d is enabled.

	/* Query intel_iommu to see if we need the workaround. Presumably that

	 * was loaded first.

 Reports of major corruption with ILK vt'd enabled */

 save the PGETBL reg for resume */

 we only ever restore the register when enabling the PGTBL... */

 FIXME: ? */

/* The chipset_flush interface needs to get data that has already been

 * flushed out of the CPU all the way out to main memory, because the GPU

 * doesn't snoop those buffers.

 *

 * The 8xx series doesn't have the same lovely interface for flushing the

 * chipset write buffers that the later chips do. According to the 865

 * specs, it's 64 octwords, or 1KB.  So, to get those previous things in

 * that buffer out, we just fill 1KB and clflush it out, on the assumption

 * that it'll push whatever was in there out.  It appears to work.

	/* Forcibly evict everything from the CPU write buffers.

	 * clflush appears to be insufficient.

	/* Now we've only seen documents for this magic bit on 855GM,

	 * we hope it exists for the other gen2 chipsets...

	 *

	 * Also works as advertised on my 845G.

	/* On the resume path we may be adjusting the PGTBL value, so

	 * be paranoid and flush all chipset write buffers...

	/* sg may merge pages, but we have to separate

 always return NULL for other allocation types for now */

 some BIOSes reserve this area in a pnp some don't */

 some BIOSes reserve this area in a pnp some don't */

 return if already configured */

 setup a resource for this object */

 Setup chipset flush for 915 */

 Shift high bits down */

 i945 is the last gpu to need phys mem (for overlay and cursors). */

/* Table to describe Intel GMCH and AGP/PCIE GART drivers.  At least one of

 * driver and gmch_driver must be non-null, and find_gmch will determine

 * which one should be used if a gmch_chip_id is present.

	/*

	 * Can be called from the fake agp driver but also directly from

	 * drm/i915.ko. Hence we need to check whether everything is set up

	 * already.

/*****************************************************************************

 *

 *     Author: Xilinx, Inc.

 *

 *     This program is free software; you can redistribute it and/or modify it

 *     under the terms of the GNU General Public License as published by the

 *     Free Software Foundation; either version 2 of the License, or (at your

 *     option) any later version.

 *

 *     XILINX IS PROVIDING THIS DESIGN, CODE, OR INFORMATION "AS IS"

 *     AS A COURTESY TO YOU, SOLELY FOR USE IN DEVELOPING PROGRAMS AND

 *     SOLUTIONS FOR XILINX DEVICES.  BY PROVIDING THIS DESIGN, CODE,

 *     OR INFORMATION AS ONE POSSIBLE IMPLEMENTATION OF THIS FEATURE,

 *     APPLICATION OR STANDARD, XILINX IS MAKING NO REPRESENTATION

 *     THAT THIS IMPLEMENTATION IS FREE FROM ANY CLAIMS OF INFRINGEMENT,

 *     AND YOU ARE RESPONSIBLE FOR OBTAINING ANY RIGHTS YOU MAY REQUIRE

 *     FOR YOUR IMPLEMENTATION.  XILINX EXPRESSLY DISCLAIMS ANY

 *     WARRANTY WHATSOEVER WITH RESPECT TO THE ADEQUACY OF THE

 *     IMPLEMENTATION, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OR

 *     REPRESENTATIONS THAT THIS IMPLEMENTATION IS FREE FROM CLAIMS OF

 *     INFRINGEMENT, IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS

 *     FOR A PARTICULAR PURPOSE.

 *

 *     (c) Copyright 2002 Xilinx Inc., Systems Engineering Group

 *     (c) Copyright 2004 Xilinx Inc., Systems Engineering Group

 *     (c) Copyright 2007-2008 Xilinx Inc.

 *     All rights reserved.

 *

 *     You should have received a copy of the GNU General Public License along

 *     with this program; if not, write to the Free Software Foundation, Inc.,

 *     675 Mass Ave, Cambridge, MA 02139, USA.

 *

/*

 * This is the code behind /dev/icap* -- it allows a user-space

 * application to use the Xilinx ICAP subsystem.

 *

 * The following operations are possible:

 *

 * open         open the port and initialize for access.

 * release      release port

 * write        Write a bitstream to the configuration processor.

 * read         Read a data stream from the configuration processor.

 *

 * After being opened, the port is initialized and accessed to avoid a

 * corrupted first read which may occur with some hardware.  The port

 * is left in a desynched state, requiring that a synch sequence be

 * transmitted before any valid configuration data.  A user will have

 * exclusive access to the device while it remains open, and the state

 * of the ICAP cannot be guaranteed after the device is closed.  Note

 * that a complete reset of the core and the state of the ICAP cannot

 * be performed on many versions of the cores, hence users of this

 * device should avoid making inconsistent accesses to the device.  In

 * particular, accessing the read interface, without first generating

 * a write containing a readback packet can leave the ICAP in an

 * inaccessible state.

 *

 * Note that in order to use the read interface, it is first necessary

 * to write a request packet to the write interface.  i.e., it is not

 * possible to simply readback the bitstream (or any configuration

 * bits) from a device without specifically requesting them first.

 * The code to craft such packets is intended to be part of the

 * user-space application code that uses this device.  The simplest

 * way to use this interface is simply:

 *

 * cp foo.bit /dev/icap0

 *

 * Note that unless foo.bit is an appropriately constructed partial

 * bitstream, this has a high likelihood of overwriting the design

 * currently programmed in the FPGA.

 For open firmware. */

 An array, which is set to true when the device is registered. */

/**

 * hwicap_command_desync - Send a DESYNC command to the ICAP port.

 * @drvdata: a pointer to the drvdata.

 *

 * Returns: '0' on success and failure value on error

 *

 * This command desynchronizes the ICAP After this command, a

 * bitstream containing a NULL packet, followed by a SYNCH packet is

 * required before the ICAP will recognize commands.

	/*

	 * Create the data to be written to the ICAP.

	/*

	 * Write the data to the FIFO and intiate the transfer of data present

	 * in the FIFO to the ICAP device.

/**

 * hwicap_get_configuration_register - Query a configuration register.

 * @drvdata: a pointer to the drvdata.

 * @reg: a constant which represents the configuration

 * register value to be returned.

 * Examples: XHI_IDCODE, XHI_FLR.

 * @reg_data: returns the value of the register.

 *

 * Returns: '0' on success and failure value on error

 *

 * Sends a query packet to the ICAP and then receives the response.

 * The icap is left in Synched state.

	/*

	 * Create the data to be written to the ICAP.

	/*

	 * Write the data to the FIFO and initiate the transfer of data present

	 * in the FIFO to the ICAP device.

 If the syncword was not found, then we need to start over. */

	/*

	 * Write the data to the FIFO and intiate the transfer of data present

	 * in the FIFO to the ICAP device.

	/*

	 * Read the configuration register

	/* Abort any current transaction, to make sure we have the

	 * ICAP in a good state.

	/* Attempt to read the IDCODE from ICAP.  This

	 * may not be returned correctly, due to the design of the

	 * hardware.

 If there are leftover bytes in the buffer, just */

 return them and don't try to read more from the */

 ICAP device. */

 Return the data currently in the read buffer. */

 Get new data from the ICAP, and return was was requested. */

 The ICAP device is only able to read complete */

 words.  If a number of bytes that do not correspond */

 to complete words is requested, then we read enough */

 words to get the required number of bytes, and then */

 save the remaining bytes for the next read. */

 Determine the number of words to read, rounding up */

 if necessary. */

 Ensure we only read a complete number of words. */

 If we didn't read correctly, then bail out. */

 If we fail to return the data to the user, then bail out. */

 Only write multiples of 4 bytes. */

 only write multiples of 4 bytes, so there might */

 be as many as 3 bytes left (at the end). */

 Flush write buffer. */

 success */

 success */

	/* It's most likely that we're using V4, if the family is not

	 * specified

 CONFIG_OF */

	/* It's most likely that we're using V4, if the family is not

	 * specified

 Match table for device tree binding */

/*****************************************************************************

 *

 *     Author: Xilinx, Inc.

 *

 *     This program is free software; you can redistribute it and/or modify it

 *     under the terms of the GNU General Public License as published by the

 *     Free Software Foundation; either version 2 of the License, or (at your

 *     option) any later version.

 *

 *     XILINX IS PROVIDING THIS DESIGN, CODE, OR INFORMATION "AS IS"

 *     AS A COURTESY TO YOU, SOLELY FOR USE IN DEVELOPING PROGRAMS AND

 *     SOLUTIONS FOR XILINX DEVICES.  BY PROVIDING THIS DESIGN, CODE,

 *     OR INFORMATION AS ONE POSSIBLE IMPLEMENTATION OF THIS FEATURE,

 *     APPLICATION OR STANDARD, XILINX IS MAKING NO REPRESENTATION

 *     THAT THIS IMPLEMENTATION IS FREE FROM ANY CLAIMS OF INFRINGEMENT,

 *     AND YOU ARE RESPONSIBLE FOR OBTAINING ANY RIGHTS YOU MAY REQUIRE

 *     FOR YOUR IMPLEMENTATION.  XILINX EXPRESSLY DISCLAIMS ANY

 *     WARRANTY WHATSOEVER WITH RESPECT TO THE ADEQUACY OF THE

 *     IMPLEMENTATION, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OR

 *     REPRESENTATIONS THAT THIS IMPLEMENTATION IS FREE FROM CLAIMS OF

 *     INFRINGEMENT, IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS

 *     FOR A PARTICULAR PURPOSE.

 *

 *     (c) Copyright 2007-2008 Xilinx Inc.

 *     All rights reserved.

 *

 *     You should have received a copy of the GNU General Public License along

 *     with this program; if not, write to the Free Software Foundation, Inc.,

 *     675 Mass Ave, Cambridge, MA 02139, USA.

 *

 Register offsets for the XHwIcap device. */

 Device Global Interrupt Enable Reg */

 Interrupt Status Register */

 Interrupt Enable Register */

 Write FIFO */

 Read FIFO */

 Size Register */

 Control Register */

 Status Register */

 Write FIFO Vacancy Register */

 Read FIFO Occupancy Register */

 Device Global Interrupt Enable Register (GIER) bit definitions */

 Global Interrupt enable Mask */

/**

 * HwIcap Device Interrupt Status/Enable Registers

 *

 * Interrupt Status Register (IPISR) : This register holds the

 * interrupt status flags for the device. These bits are toggle on

 * write.

 *

 * Interrupt Enable Register (IPIER) : This register is used to enable

 * interrupt sources for the device.

 * Writing a '1' to a bit enables the corresponding interrupt.

 * Writing a '0' to a bit disables the corresponding interrupt.

 *

 * IPISR/IPIER registers have the same bit definitions and are only defined

 * once.

 Read FIFO Full */

 Write FIFO Empty */

 Read FIFO half full */

 Write FIFO half full */

 Mask of all interrupts */

 Control Register (CR) */

 SW Reset Mask */

 FIFO Clear Mask */

 Read from ICAP to FIFO */

 Write from FIFO to ICAP */

 Max Write FIFO Vacancy, in words */

 Max Read FIFO Occupancy, in words */

/* The maximum amount we can request from fifo_icap_get_configuration

/**

 * fifo_icap_fifo_write - Write data to the write FIFO.

 * @drvdata: a pointer to the drvdata.

 * @data: the 32-bit value to be written to the FIFO.

 *

 * This function will silently fail if the fifo is full.

/**

 * fifo_icap_fifo_read - Read data from the Read FIFO.

 * @drvdata: a pointer to the drvdata.

 *

 * This function will silently fail if the fifo is empty.

/**

 * fifo_icap_set_read_size - Set the the size register.

 * @drvdata: a pointer to the drvdata.

 * @data: the size of the following read transaction, in words.

/**

 * fifo_icap_start_config - Initiate a configuration (write) to the device.

 * @drvdata: a pointer to the drvdata.

/**

 * fifo_icap_start_readback - Initiate a readback from the device.

 * @drvdata: a pointer to the drvdata.

/**

 * fifo_icap_get_status - Get the contents of the status register.

 * @drvdata: a pointer to the drvdata.

 *

 * The status register contains the ICAP status and the done bit.

 *

 * D8 - cfgerr

 * D7 - dalign

 * D6 - rip

 * D5 - in_abort_l

 * D4 - Always 1

 * D3 - Always 1

 * D2 - Always 1

 * D1 - Always 1

 * D0 - Done bit

/**

 * fifo_icap_busy - Return true if the ICAP is still processing a transaction.

 * @drvdata: a pointer to the drvdata.

/**

 * fifo_icap_write_fifo_vacancy - Query the write fifo available space.

 * @drvdata: a pointer to the drvdata.

 *

 * Return the number of words that can be safely pushed into the write fifo.

/**

 * fifo_icap_read_fifo_occupancy - Query the read fifo available data.

 * @drvdata: a pointer to the drvdata.

 *

 * Return the number of words that can be safely read from the read fifo.

/**

 * fifo_icap_set_configuration - Send configuration data to the ICAP.

 * @drvdata: a pointer to the drvdata.

 * @frame_buffer: a pointer to the data to be written to the

 *		ICAP device.

 * @num_words: the number of words (32 bit) to write to the ICAP

 *		device.



 * This function writes the given user data to the Write FIFO in

 * polled mode and starts the transfer of the data to

 * the ICAP device.

	/*

	 * Check if the ICAP device is Busy with the last Read/Write

	/*

	 * Set up the buffer pointer and the words to be transferred.

		/*

		 * Wait until we have some data in the fifo.

		/*

		 * Write data into the Write FIFO.

 Start pushing whatever is in the FIFO into the ICAP. */

 Wait until the write has finished. */

	/*

	 * If the requested number of words have not been read from

	 * the device then indicate failure.

/**

 * fifo_icap_get_configuration - Read configuration data from the device.

 * @drvdata: a pointer to the drvdata.

 * @data: Address of the data representing the partial bitstream

 * @size: the size of the partial bitstream in 32 bit words.

 *

 * This function reads the specified number of words from the ICAP device in

 * the polled mode.

	/*

	 * Check if the ICAP device is Busy with the last Write/Read

		/* The hardware has a limit on the number of words

 Wait until we have some data in the fifo. */

 Read the data from the Read FIFO. */

/**

 * buffer_icap_reset - Reset the logic of the icap device.

 * @drvdata: a pointer to the drvdata.

 *

 * This function forces the software reset of the complete HWICAP device.

 * All the registers will return to the default value and the FIFO is also

 * flushed as a part of this software reset.

	/*

	 * Reset the device by setting/clearing the RESET bit in the

	 * Control Register.

/**

 * fifo_icap_flush_fifo - This function flushes the FIFOs in the device.

 * @drvdata: a pointer to the drvdata.

	/*

	 * Flush the FIFO by setting/clearing the FIFO Clear bit in the

	 * Control Register.

/*****************************************************************************

 *

 *     Author: Xilinx, Inc.

 *

 *     This program is free software; you can redistribute it and/or modify it

 *     under the terms of the GNU General Public License as published by the

 *     Free Software Foundation; either version 2 of the License, or (at your

 *     option) any later version.

 *

 *     XILINX IS PROVIDING THIS DESIGN, CODE, OR INFORMATION "AS IS"

 *     AS A COURTESY TO YOU, SOLELY FOR USE IN DEVELOPING PROGRAMS AND

 *     SOLUTIONS FOR XILINX DEVICES.  BY PROVIDING THIS DESIGN, CODE,

 *     OR INFORMATION AS ONE POSSIBLE IMPLEMENTATION OF THIS FEATURE,

 *     APPLICATION OR STANDARD, XILINX IS MAKING NO REPRESENTATION

 *     THAT THIS IMPLEMENTATION IS FREE FROM ANY CLAIMS OF INFRINGEMENT,

 *     AND YOU ARE RESPONSIBLE FOR OBTAINING ANY RIGHTS YOU MAY REQUIRE

 *     FOR YOUR IMPLEMENTATION.  XILINX EXPRESSLY DISCLAIMS ANY

 *     WARRANTY WHATSOEVER WITH RESPECT TO THE ADEQUACY OF THE

 *     IMPLEMENTATION, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OR

 *     REPRESENTATIONS THAT THIS IMPLEMENTATION IS FREE FROM CLAIMS OF

 *     INFRINGEMENT, IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS

 *     FOR A PARTICULAR PURPOSE.

 *

 *     (c) Copyright 2003-2008 Xilinx Inc.

 *     All rights reserved.

 *

 *     You should have received a copy of the GNU General Public License along

 *     with this program; if not, write to the Free Software Foundation, Inc.,

 *     675 Mass Ave, Cambridge, MA 02139, USA.

 *

 Indicates how many bytes will fit in a buffer. (1 BRAM) */

 File access and error constants */

 Constants for checking transfer status */

 buffer_icap register offsets */

 Size of transfer, read & write */

 offset into bram, read & write */

 Read not Configure, direction of transfer.  Write only */

 Indicates transfer complete. Read only */

 Constants for setting the RNC register */

 Constants for the Done register */

/**

 * buffer_icap_get_status - Get the contents of the status register.

 * @drvdata: a pointer to the drvdata.

 *

 * The status register contains the ICAP status and the done bit.

 *

 * D8 - cfgerr

 * D7 - dalign

 * D6 - rip

 * D5 - in_abort_l

 * D4 - Always 1

 * D3 - Always 1

 * D2 - Always 1

 * D1 - Always 1

 * D0 - Done bit

/**

 * buffer_icap_get_bram - Reads data from the storage buffer bram.

 * @base_address: contains the base address of the component.

 * @offset: The word offset from which the data should be read.

 *

 * A bram is used as a configuration memory cache.  One frame of data can

 * be stored in this "storage buffer".

/**

 * buffer_icap_busy - Return true if the icap device is busy

 * @base_address: is the base address of the device

 *

 * The queries the low order bit of the status register, which

 * indicates whether the current configuration or readback operation

 * has completed.

/**

 * buffer_icap_set_size - Set the size register.

 * @base_address: is the base address of the device

 * @data: The size in bytes.

 *

 * The size register holds the number of 8 bit bytes to transfer between

 * bram and the icap (or icap to bram).

/**

 * buffer_icap_set_offset - Set the bram offset register.

 * @base_address: contains the base address of the device.

 * @data: is the value to be written to the data register.

 *

 * The bram offset register holds the starting bram address to transfer

 * data from during configuration or write data to during readback.

/**

 * buffer_icap_set_rnc - Set the RNC (Readback not Configure) register.

 * @base_address: contains the base address of the device.

 * @data: is the value to be written to the data register.

 *

 * The RNC register determines the direction of the data transfer.  It

 * controls whether a configuration or readback take place.  Writing to

 * this register initiates the transfer.  A value of 1 initiates a

 * readback while writing a value of 0 initiates a configuration.

/**

 * buffer_icap_set_bram - Write data to the storage buffer bram.

 * @base_address: contains the base address of the component.

 * @offset: The word offset at which the data should be written.

 * @data: The value to be written to the bram offset.

 *

 * A bram is used as a configuration memory cache.  One frame of data can

 * be stored in this "storage buffer".

/**

 * buffer_icap_device_read - Transfer bytes from ICAP to the storage buffer.

 * @drvdata: a pointer to the drvdata.

 * @offset: The storage buffer start address.

 * @count: The number of words (32 bit) to read from the

 *           device (ICAP).

 setSize count*4 to get bytes. */

/**

 * buffer_icap_device_write - Transfer bytes from ICAP to the storage buffer.

 * @drvdata: a pointer to the drvdata.

 * @offset: The storage buffer start address.

 * @count: The number of words (32 bit) to read from the

 *           device (ICAP).

 setSize count*4 to get bytes. */

/**

 * buffer_icap_reset - Reset the logic of the icap device.

 * @drvdata: a pointer to the drvdata.

 *

 * Writing to the status register resets the ICAP logic in an internal

 * version of the core.  For the version of the core published in EDK,

 * this is a noop.

/**

 * buffer_icap_set_configuration - Load a partial bitstream from system memory.

 * @drvdata: a pointer to the drvdata.

 * @data: Kernel address of the partial bitstream.

 * @size: the size of the partial bitstream in 32 bit words.

 Loop through all the data */

 Copy data to bram */

 Write data to ICAP */

 abort. */

 Write unwritten data to ICAP */

 Write data to ICAP */

 abort. */

/**

 * buffer_icap_get_configuration - Read configuration data from the device.

 * @drvdata: a pointer to the drvdata.

 * @data: Address of the data representing the partial bitstream

 * @size: the size of the partial bitstream in 32 bit words.

 Loop through all the data */

 Read data from ICAP */

 abort. */

 Copy data from bram */

 SPDX-License-Identifier: GPL-2.0+

/*

 * There can be 4 IO ports passed in (with or without IRQs), 4 addresses,

 * a default IO port, and 1 ACPI/SPMI address.  That sets SI_MAX_DRIVERS.

 Parse out the si_type string into its components. */

/*

 * Returns true of the given address exists as a hardcoded address,

 * false if not.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Add an IPMI platform device.

 Last entry must be left NULL to terminate it. */

 An invalid or SSIF interface, no resources. */

	/*

	 * Register spacing is derived from the resources in

	 * the IPMI platform code.

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_si.c

 *

 * The interface to the IPMI driver for the system interfaces (KCS, SMIC,

 * BT).

 *

 * Author: MontaVista Software, Inc.

 *         Corey Minyard <minyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002 MontaVista Software Inc.

 * Copyright 2006 IBM Corp., Christian Krafft <krafft@de.ibm.com>

/*

 * This file holds the "policy" for the interface to the SMI state

 * machine.  It does the configuration, handles timers and interrupts,

 * and drives the real SMI state machine.

 Measure times between events in the driver. */

 Call every 10 ms. */

#define SI_SHORT_TIMEOUT_USEC  250 /* .25ms when the SM request a

 FIXME - add watchdog stuff. */

 Some BT-specific defines we need here. */

 'invalid' to allow a firmware-specified interface to be disabled */

/*

 * Indexes into stats[] in smi_info below.

	/*

	 * Number of times the driver requested a timer while an operation

	 * was in progress.

	/*

	 * Number of times the driver requested a timer while nothing was in

	 * progress.

 Number of times the interface was idle while being polled. */

 Number of interrupts the driver handled. */

 Number of time the driver got an ATTN from the hardware. */

 Number of times the driver requested flags from the hardware. */

 Number of times the hardware didn't follow the state machine. */

 Number of completed messages. */

 Number of IPMI events received from the hardware. */

 Number of watchdog pretimeouts. */

 Number of asynchronous messages received. */

 This *must* remain last, add new values above this. */

	/*

	 * Used to handle the various types of I/O that can occur with

	 * IPMI

	/*

	 * Per-OEM handler, called from handle_flags().  Returns 1

	 * when handle_flags() needs to be re-run or 0 indicating it

	 * set si_state itself.

	/*

	 * Flags from the last GET_MSG_FLAGS command, used when an ATTN

	 * is set to hold the flags until we are done handling everything

	 * from the flags.

 Does the BMC have an event buffer? */

	/*

	 * If set to true, this will request events the next time the

	 * state machine is idle.

	/*

	 * If true, run the state machine to completion on every send

	 * call.  Generally used after a panic to make sure stuff goes

	 * out.

 The timer for this si. */

 This flag is set, if the timer can be set */

 This flag is set, if the timer is running (timer_pending() isn't enough) */

 The time (in jiffies) the last timeout occurred at. */

 Are we waiting for the events, pretimeouts, received msgs? */

	/*

	 * The driver will disable interrupts when it gets into a

	 * situation where it cannot handle messages due to lack of

	 * memory.  Once that situation clears up, it will re-enable

	 * interrupts.

	/*

	 * Does the BMC support events?

	/*

	 * Can we disable interrupts the global enables receive irq

	 * bit?  There are currently two forms of brokenness, some

	 * systems cannot disable the bit (which is technically within

	 * the spec but a bad idea) and some systems have the bit

	 * forced to zero even though interrupts work (which is

	 * clearly outside the spec).  The next bool tells which form

	 * of brokenness is present.

	/*

	 * Some systems are broken and cannot set the irq enable

	 * bit, even if they support interrupts.

 Is the driver in maintenance mode? */

	/*

	 * Did we get an attention that we did not handle?

 From the get device id response... */

 Have we added the device group to the device? */

 Counters and things for the proc filesystem. */

 Deliver the message to the upper layer. */

 else use it as is */

 Make it a response */

/*

 * Start a new message and (re)start the timer and thread.

 Make sure the watchdog pre-timeout flag is not set at startup. */

/*

 * When we have a situtaion where we run out of memory and cannot

 * allocate messages, we just leave them in the BMC and run the system

 * polled until we can allocate some memory.  Once we have some

 * memory, we will re-enable the interrupt.

 *

 * Note that we cannot just use disable_irq(), since the interrupt may

 * be shared.

/*

 * Allocate a message.  If unable to allocate, start the interrupt

 * disable process and return NULL.  If able to allocate but

 * interrupts are disabled, free the message and return NULL after

 * starting the interrupt enable process.

 Watchdog pre-timeout */

 Messages available. */

 Events available. */

/*

 * Global enables we care about.

		/*

		 * Do this here becase deliver_recv_msg() releases the

		 * lock, and a new message can be put in during the

		 * time the lock is released.

 We got the flags from the SMI, now handle them. */

 Error fetching flags, just give up for now. */

			/*

			 * Hmm, no flags.  That's technically illegal, but

			 * don't use uninitialized data.

 We cleared the flags. */

 Error clearing flags */

		/*

		 * Do this here becase deliver_recv_msg() releases the

		 * lock, and a new message can be put in during the

		 * time the lock is released.

 Error getting event, probably done. */

 Take off the event flag. */

			/*

			 * Do this before we deliver the message

			 * because delivering the message releases the

			 * lock and something else can mess with the

			 * state.

		/*

		 * Do this here becase deliver_recv_msg() releases the

		 * lock, and a new message can be put in during the

		 * time the lock is released.

 Error getting event, probably done. */

 Take off the msg flag. */

			/*

			 * Do this before we deliver the message

			 * because delivering the message releases the

			 * lock and something else can mess with the

			 * state.

 We got the flags from the SMI, now handle them. */

 BT has its own interrupt enable bit. */

 Enables are not correct, fix them. */

/*

 * Called on timeouts and events.  Timeouts should pass the elapsed

 * time, interrupts should pass in zero.  Must be called with

 * si_lock held and interrupts disabled.

	/*

	 * There used to be a loop here that waited a little while

	 * (around 25us) before giving up.  That turned out to be

	 * pointless, the minimum delays I was seeing were in the 300us

	 * range, which is far too long to wait in an interrupt.  So

	 * we just run until the state machine tells us something

	 * happened or it needs a delay.

		/*

		 * Do the before return_hosed_msg, because that

		 * releases the lock.

			/*

			 * If we were handling a user message, format

			 * a response to send to the upper layer to

			 * tell it about the error.

	/*

	 * We prefer handling attn over new messages.  But don't do

	 * this if there is not yet an upper layer to handle anything.

			/*

			 * We got an ATTN, but we are doing something else.

			 * Handle the ATTN later.

			/*

			 * Got a attn, send down a get message flags to see

			 * what's causing it.  It would be better to handle

			 * this in the upper layer, but due to the way

			 * interrupts work with the SMI, that's not really

			 * possible.

 If we are currently idle, try to start the next message. */

		/*

		 * We are idle and the upper layer requested that I fetch

		 * events, so do so.

		/*

		 * Take this opportunity to check the interrupt and

		 * message enable state for the BMC.  The BMC can be

		 * asynchronously reset, and may thus get interrupts

		 * disable and messages disabled.

 Ok it if fails, the timer will just go off. */

	/*

	 * Currently, this function is called only in run-to-completion

	 * mode.  This means we are single-threaded, no need for locks.

		/*

		 * If we are running to completion, start it.  Upper

		 * layer will call flush_messages to clear it out.

	/*

	 * The following two lines don't need to be under the lock for

	 * the lock's sake, but they do need SMP memory barriers to

	 * avoid getting things out of order.  We are already claiming

	 * the lock, anyway, so just do it under the lock to avoid the

	 * ordering problem.

/*

 * Use -1 as a special constant to tell that we are spinning in kipmid

 * looking for something and not delaying between checks

/*

 * A busy-waiting loop for speeding up IPMI operation.

 *

 * Lousy hardware makes this hard.  This is only enabled for systems

 * that are not BT and do not have interrupts.  It starts spinning

 * when an operation is complete or until max_busy tells it to stop

 * (if that is enabled).  See the paragraph on kimid_max_busy_us in

 * Documentation/driver-api/ipmi.rst for details.

		/*

		 * If the driver is doing something, there is a possible

		 * race with the timer.  If the timer handler see idle,

		 * and the thread here sees something else, the timer

		 * handler won't restart the timer even though it is

		 * required.  So start it here if necessary.

 do nothing */

			/*

			 * In maintenance mode we run as fast as

			 * possible to allow firmware updates to

			 * complete as fast as possible, but normally

			 * don't bang on the scheduler.

 Wait to be woken up when we are needed. */

	/*

	 * Make sure there is some delay in the poll loop so we can

	 * drive time forward and timeout things.

 Running with interrupts, only do long timeouts. */

	/*

	 * If the state machine asks for a short delay, then shorten

	 * the timer timeout.

 We need to clear the IRQ flag for the BT interface. */

 Set up the timer that drives the interface. */

 Try to claim any interrupts. */

	/*

	 * Check if the user forcefully enabled the daemon.

	/*

	 * The BT interface is efficient enough to not need a thread,

	 * and there is no need for a thread if we have interrupts.

 Used to sequence the SMIs */

 Enable the interrupt in the BT interface. */

 Disable the interrupt in the BT interface. */

		/*

		 * We couldn't get the state machine to run, so whatever's at

		 * the port is probably not an IPMI SMI interface.

	/*

	 * Do a Get Device ID command, since it comes back with some

	 * useful info.

 Check and record info from the get device id, in case we need it. */

 record completion code */

/*

 * Returns 1 if it gets an error from the command.

/*

 * Some BMCs do not support clearing the receive irq bit in the global

 * enables (even if they don't support interrupts on the BMC).  Check

 * for this and handle it properly.

 Already clear, should work ok. */

		/*

		 * An error when setting the event buffer bit means

		 * clearing the bit is not supported.

/*

 * Some BMCs do not support setting the interrupt bits in the global

 * enables even if they support interrupts.  Clearly bad, but we can

 * compensate.

		/*

		 * An error when setting the event buffer bit means

		 * setting the bit is not supported.

 buffer is already enabled, nothing to do. */

		/*

		 * An error when setting the event buffer bit means

		 * that the event buffer is not supported.

/*

 * oem_data_avail_to_receive_msg_avail

 * @info - smi_info structure with msg_flags set

 *

 * Converts flags from OEM_DATA_AVAIL to RECEIVE_MSG_AVAIL

 * Returns 1 indicating need to re-run handle_flags().

/*

 * setup_dell_poweredge_oem_data_handler

 * @info - smi_info.device_id must be populated

 *

 * Systems that match, but have firmware version < 1.40 may assert

 * OEM0_DATA_AVAIL on their own, without being told via Set Flags that

 * it's safe to do so.  Such systems will de-assert OEM1_DATA_AVAIL

 * upon receipt of IPMI_GET_MSG_CMD, so we should treat these flags

 * as RECEIVE_MSG_AVAIL instead.

 *

 * As Dell has no plans to release IPMI 1.5 firmware that *ever*

 * assert the OEM[012] bits, and if it did, the driver would have to

 * change to handle that properly, we don't actually check for the

 * firmware version.

 * Device ID = 0x20                BMC on PowerEdge 8G servers

 * Device Revision = 0x80

 * Firmware Revision1 = 0x01       BMC version 1.40

 * Firmware Revision2 = 0x40       BCD encoded

 * IPMI Version = 0x51             IPMI 1.5

 * Manufacturer ID = A2 02 00      Dell IANA

 *

 * Additionally, PowerEdge systems with IPMI < 1.5 may also assert

 * OEM0_DATA_AVAIL and needs to be treated as RECEIVE_MSG_AVAIL.

 *

 Make it a response */

/*

 * dell_poweredge_bt_xaction_handler

 * @info - smi_info.device_id must be populated

 *

 * Dell PowerEdge servers with the BT interface (x6xx and 1750) will

 * not respond to a Get SDR command if the length of the data

 * requested is exactly 0x3A, which leads to command timeouts and no

 * data returned.  This intercepts such commands, and causes userspace

 * callers to try again with a different-sized buffer, which succeeds.

/*

 * setup_dell_poweredge_bt_xaction_handler

 * @info - smi_info.device_id must be filled in already

 *

 * Fills in smi_info.device_id.start_transaction_pre_hook

 * when we know what function to use there.

/*

 * setup_oem_data_handler

 * @info - smi_info.device_id must be filled in already

 *

 * Fills in smi_info.device_id.oem_data_available_handler

 * when we know what function to use there.

			/*

			 * This is a cheap hack, ACPI doesn't have a defined

			 * slave address but SMBIOS does.  Pick it up from

			 * any source that has it available.

	/*

	 * If the user gave us a hard-coded device at the same

	 * address, they presumably want us to use it and not what is

	 * in the firmware.

 We prefer ACPI over SMBIOS. */

/*

 * Try to start up an interface.  Must be called with smi_infos_lock

 * held, primarily to keep smi_num consistent, we only one to do these

 * one at a time.

 No support for anything else yet. */

 Do this early so it's available for logs. */

 Allocate the state machine's data and initialize it. */

 Now that we know the I/O size, we can set up the I/O. */

 Do low-level detection first. */

	/*

	 * Attempt a get device id command.  If it fails, we probably

	 * don't have a BMC here.

	/*

	 * Start clearing the flags before we enable interrupts or the

	 * timer to avoid racing with the timer.

	/*

	 * IRQ is defined to be set when non-zero.  req_events will

	 * cause a global flags check that will enable interrupts.

 Don't increment till we know we have succeeded. */

	/* We prefer devices with interrupts, but in the case of a machine

	   with multiple BMCs we assume that there will be several instances

	   of a given type so if we succeed in registering a type then also

		/* Try to register a device if it has an IRQ and we either

		   haven't successfully registered a device yet or this

 type will only have been set if we successfully registered an si */

 Fall back to the preferred device */

	/*

	 * Make sure that interrupts, the timer and the thread are

	 * stopped and will not run again.

	/*

	 * Wait until we know that we are out of any interrupt

	 * handlers might have been running before we freed the

	 * interrupt.

	/*

	 * Timeouts are stopped, now make sure the interrupts are off

	 * in the BMC.  Note that timers and CPU interrupts are off,

	 * so no need for locks.

/*

 * Must be called with smi_infos_lock held, to serialize the

 * smi_info->intf check.

 remove */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_poweroff.c

 *

 * MontaVista IPMI Poweroff extension to sys_reboot

 *

 * Author: MontaVista Software, Inc.

 *         Steven Dake <sdake@mvista.com>

 *         Corey Minyard <cminyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002,2004 MontaVista Software Inc.

/* Definitions for controlling power off (if the system supports it).  It

 power down, the default. */

 power cycle */

 the IPMI data command */

 Which interface to use, -1 means the first we see. */

 Our local state. */

 Holds the old poweroff function so we can restore it on removal. */

 parameter definition to allow user to flag power cycle */

 Stuff from the get device id command. */

/*

 * We use our own messages for this operation, we don't let the system

 * allocate them, since we may be in a panic situation.  The whole

 * thing is single-threaded, anyway, so multiple messages are not

 * required.

/*

 * Code to send a message and wait for the response.

 Wait for message to complete, spinning. */

	/*

	 * Spin until our message is done.

/*

 * ATCA Support

	/*

	 * Configure IPMI address for local access

	/*

	 * Configure IPMI address for local access

	/*

	 * Use get address info to check and see if we are ATCA

	/*

	 * Configure IPMI address for local access

	/*

	 * Power down

 FRU id */

 Power Level */

 Don't change saved presets */

	/*

	 * At this point, the system may be shutting down, and most

	 * serial drivers (if used) will have interrupts turned off

	 * it may be better to ignore IPMI_UNKNOWN_ERR_COMPLETION_CODE

	 * return code

/*

 * CPI1 Support

	/*

	 * Configure IPMI address for local access

	/*

	 * Get IPMI ipmb address

	/*

	 * Get active event receiver

	/*

	 * Setup IPMB address target instead of local target

	/*

	 * Send request hotswap control to remove blade from dpv

	/*

	 * Set reset asserted

 Reset asserted state */

	/*

	 * Power down

 Power down state */

/*

 * ipmi_dell_chassis_detect()

 * Dell systems with IPMI < 1.5 don't set the chassis capability bit

 * but they can handle a chassis poweroff or powercycle command.

/*

 * ipmi_hp_chassis_detect()

 * HP PA-RISC servers rp3410/rp3440, the C8000 workstation and the rx2600 and

 * zx6000 machines support IPMI vers 1 and don't set the chassis capability bit

 * but they can handle a chassis poweroff or powercycle command.

/*

 * Standard chassis support

 Chassis support, use it. */

	/*

	 * Configure IPMI address for local access

	/*

	 * Power down

 power cycle failed, default to power down */

 Table of possible power off functions. */

	/* Chassis should generally be last, other things should override

 Called on a powerdown request. */

 Use run-to-completion mode, since interrupts may be off. */

/* Wait for an IPMI interface to be installed, the first one installed

	/*

	 * Do a get device ide and store some results, since this is

	 * used by several functions.

 Scan for a poweroff method */

 CONFIG_PROC_FS */

/*

 * Startup and shutdown functions.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2015-2016, IBM Corporation.

/*

 * This is a BMC device used to communicate to the host

/*

 * The BT (Block Transfer) interface means that entire messages are

 * buffered by the host before a notification is sent to the BMC that

 * there is data to be read. The first byte is the length and the

 * message data follows. The read operation just tries to capture the

 * whole before returning it to userspace.

 *

 * BT Message format :

 *

 *    Byte 1  Byte 2     Byte 3  Byte 4  Byte 5:N

 *    Length  NetFn/LUN  Seq     Cmd     Data

 *

	/*

	 * The BT frames start with the message length, which does not

	 * include the length byte.

 We pass the length back to userspace as well */

/*

 * BT Message response format :

 *

 *    Byte 1  Byte 2     Byte 3  Byte 4  Byte 5  Byte 6:N

 *    Length  NetFn/LUN  Seq     Cmd     Code    Data

	/*

	 * send a minimum response size

	/*

	 * There's no interrupt for clearing bmc busy so we have to

	 * poll

 ack pending IRQs */

	/*

	 * Configure IRQs on the bmc clearing the H2B and HBUSY bits;

	 * H2B will be asserted when the bmc has data for us; HBUSY

	 * will be cleared (along with B2H) when we can write the next

	 * message to the BT buffer

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2018, Intel Corporation.

 * Copyright (c) 2021, IBM Corp.

 Implement both the device and client interfaces here */

 Record registered devices and drivers */

 Consumer data access */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_si_hotmod.c

 *

 * Handling for dynamically adding/removing IPMI devices through

 * a module parameter (and thus sysfs).

/*

 * Parms come in as <op1>[:op2[:op3...]].  ops are:

 *   add|remove,kcs|bt|smic,mem|i/o,<address>[,<opt1>[,<opt2>[,...]]]

 * Options are:

 *   rsp=<regspacing>

 *   rsi=<regsize>

 *   rsh=<regshift>

 *   irq=<irq>

 *   ipmb=<ipmb addr>

 Kill any trailing spaces, as we can get a "\n" from echo. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_watchdog.c

 *

 * A watchdog timer based upon the IPMI interface.

 *

 * Author: MontaVista Software, Inc.

 *         Corey Minyard <minyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002 MontaVista Software Inc.

/*

 * This is ugly, but I've determined that x86 is the only architecture

 * that can reasonably support the IPMI NMI watchdog timeout at this

 * time.  If another architecture adds this capability somehow, it

 * will have to be a somewhat different mechanism and I have no idea

 * how it will work.  So in the unlikely event that another

 * architecture supports this, we can figure out a good generic

 * mechanism for it at that time.

/*

 * The IPMI command/response information for the watchdog timer.

 values for byte 1 of the set command, byte 2 of the get response. */

 values for byte 2 of the set command, byte 3 of the get response. */

 Operations that can be performed on a pretimout. */

 Cause data to be available to read.  Doesn't work in NMI mode. */

 Actions to perform on a full timeout. */

/*

 * Byte 3 of the get command, byte 4 of the get response is the

 * pre-timeout in seconds.

 Bits for setting byte 4 of the set command, byte 5 of the get response. */

/*

 * Setting/getting the watchdog timer value.  This is for bytes 5 and

 * 6 (the timeout time) of the set command, and bytes 6 and 7 (the

 * timeout time) and 8 and 9 (the current countdown value) of the

 * response.  The timeout value is given in seconds (in the command it

 * is 100ms intervals).

 Default the timeout to 10 seconds. */

 The pre-timeout is disabled by default. */

 Default timeout to set on panic */

 Default action is to reset the board on a timeout. */

 Parameters to ipmi_set_timeout */

/*

 * If true, the driver will start running as soon as it is configured

 * and ready.

 Default state of the timer. */

 Is someone using the watchdog?  Only one user is allowed. */

/*

 * If set to 1, the heartbeat command will set the state to reset and

 * start the timer.  The timer doesn't normally run when the driver is

 * first opened until the heartbeat is set the first time, this

 * variable is used to accomplish this.

 IPMI version of the BMC. */

 If a pretimeout occurs, this is used to allow only one panic to happen. */

/*

 * We use a mutex to make sure that only one thing can send a set a

 * message at one time.  The mutex is claimed when a message is sent

 * and freed when both the send and receive messages are free.

 This is an IPMI 1.5-only feature. */

			/*

			 * In ipmi 1.0, setting the timer stops the watchdog, we

			 * need to start it back up again.

 No pretimeout. */

	/*

	 * Don't reset the timer if we have the timer turned off, that

	 * re-enables the watchdog.

/*

 * Special call, doesn't claim any locks.  This is only to be called

 * at panic or halt time, in run-to-completion mode, when the caller

 * is the only CPU and the only thing that will be going is these IPMI

 * calls.

 Wait for the messages to be free. */

	/*

	 * Don't reset the timer if we have the timer turned off, that

	 * re-enables the watchdog.

 Wait for the heartbeat to be sent. */

		/*

		 * The timer was not initialized, that means the BMC was

		 * probably reset and lost the watchdog information.  Attempt

		 * to restore the timer's info.  Note that we still hold

		 * the heartbeat lock, to keep a heartbeat from happening

		 * in this process, so must say no heartbeat to avoid a

		 * deadlock on this mutex

 Might need a heartbeat send, go ahead and do it. */

		/*

		 * Got an error in the heartbeat response.  It was already

		 * reported in ipmi_wdog_msg_handler, but we should return

		 * an error here.

		/*

		 * A pretimeout occurred, make sure we set the timeout.

		 * We don't want to set the action, though, we want to

		 * leave that alone (thus it can't be combined with the

		 * above operation.

 WDIOF_SETTIMEOUT, */

 In case it was set long ago */

	/*

	 * Reading returns if the pretimeout has gone off, and it only does

	 * it once per pretimeout.

		/*

		 * Don't start the timer now, let it start on the

		 * first heartbeat.

	/*

	 * On some machines, the heartbeat will give an error and not

	 * work unless we re-enable the timer.  So do so.

	/*

	 * On a panic, if we have a panic timeout, make sure to extend

	 * the watchdog timer to a reasonable value to complete the

	 * panic, if the watchdog timer is running.  Plus the

	 * pretimeout is meaningless at panic time.

 Make sure we do this only once. */

		/*

		 * Set the pretimeout to go off in a second and give

		 * ourselves plenty of time to stop the timer.

 Make sure nothing happens */

 Run from startup, so start the timer now. */

 Disable this function after first startup. */

 Stop the timer now. */

 Make sure no one can call us any more. */

	/*

	 * Wait to make sure the message makes it out.  The lower layer has

	 * pointers to our buffers, we want to make sure they are done before

	 * we release our memory.

 Disconnect from IPMI. */

 If it comes back, restart it properly. */

	/*

	 * If we get here, it's an NMI that's not a memory or I/O

	 * error.  We can't truly tell if it's from IPMI or not

	 * without sending a message, and sending a message is almost

	 * impossible because of locking.

 If we are not expecting a timeout, ignore it. */

	/*

	 * If no one else handled the NMI, we assume it was the IPMI

	 * watchdog.

		/* On some machines, the heartbeat will give

		   an error and not work unless we re-enable

 Make sure we only do this once. */

 Disable the WDT if we are shutting down. */

			/* Set a long timer to let the reboot happen or

			   reset if it hangs, but only if the watchdog

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018, Nuvoton Corporation.

 * Copyright (c) 2018, Intel Corporation.

/*

 * 7.2.4 Core KCS Registers

 * Registers in this module are 8 bits. An 8-bit register must be accessed

 * by an 8-bit read or write.

 *

 * sts: KCS Channel n Status Register (KCSnST).

 * dob: KCS Channel n Data Out Buffer Register (KCSnDO).

 * dib: KCS Channel n Data In Buffer Register (KCSnDI).

 * ctl: KCS Channel n Control Register (KCSnCTL).

 * ie : KCS Channel n  Interrupt Enable Register (KCSnIE).

 SPDX-License-Identifier: GPL-2.0+

	/*

	 * Figure out the actual readb/readw/readl/etc routine to use based

	 * upon the register size.

	/*

	 * Some BIOSes reserve disjoint memory regions in their ACPI

	 * tables.  This causes problems when trying to request the

	 * entire region.  Therefore we must request each register

	 * separately.

 Undo allocations */

	/*

	 * Calculate the total amount of memory to claim.  This is an

	 * unusual looking calculation, but it avoids claiming any

	 * more memory than it has to.  It will claim everything

	 * between the first address to the end of the last full

	 * register.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2018, Intel Corporation.

/* Different phases of the KCS BMC module.

 *  KCS_PHASE_IDLE:

 *            BMC should not be expecting nor sending any data.

 *  KCS_PHASE_WRITE_START:

 *            BMC is receiving a WRITE_START command from system software.

 *  KCS_PHASE_WRITE_DATA:

 *            BMC is receiving a data byte from system software.

 *  KCS_PHASE_WRITE_END_CMD:

 *            BMC is waiting a last data byte from system software.

 *  KCS_PHASE_WRITE_DONE:

 *            BMC has received the whole request from system software.

 *  KCS_PHASE_WAIT_READ:

 *            BMC is waiting the response from the upper IPMI service.

 *  KCS_PHASE_READ:

 *            BMC is transferring the response to system software.

 *  KCS_PHASE_ABORT_ERROR1:

 *            BMC is waiting error status request from system software.

 *  KCS_PHASE_ABORT_ERROR2:

 *            BMC is waiting for idle status afer error from system software.

 *  KCS_PHASE_ERROR:

 *            BMC has detected a protocol violation at the interface level.

 IPMI 2.0 - Table 9-4, KCS Interface Status Codes */

 IPMI 2.0 - Table 9-1, KCS Interface Status Register Bits */

 IPMI 2.0 - Table 9-2, KCS Interface State Bits */

 IPMI 2.0 - Table 9-3, KCS Interface Control Codes */

 a minimum response size '3' : netfn + cmd + ccode */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_ssif.c

 *

 * The interface to the IPMI driver for SMBus access to a SMBus

 * compliant device.  Called SSIF by the IPMI spec.

 *

 * Author: Intel Corporation

 *         Todd Davis <todd.c.davis@intel.com>

 *

 * Rewritten by Corey Minyard <minyard@acm.org> to support the

 * non-blocking I2C interface, add support for multi-part

 * transactions, add PEC support, and general clenaup.

 *

 * Copyright 2003 Intel Corporation

 * Copyright 2005 MontaVista Software

/*

 * This file holds the "policy" for the interface to the SSIF state

 * machine.  It does the configuration, handles timers and interrupts,

 * and drives the real SSIF state machine.

/* ssif_debug is a bit-field

 *	SSIF_DEBUG_MSG -	commands and their responses

 *	SSIF_DEBUG_STATES -	message states

 *	SSIF_DEBUG_TIMING -	 Measure times between events in the driver

/*

 * Timer values

 20ms between message tries. */

 5ms for a message part */

 How many times to we retry sending/receiving the message. */

/*

 * Timeout for the watch, only used for get flag timer.

 FIXME - add watchdog stuff. */

/*

 * Indexes into stats[] in ssif_info below.

 Number of total messages sent. */

	/*

	 * Number of message parts sent.  Messages may be broken into

	 * parts if they are long.

	/*

	 * Number of time a message was retried.

	/*

	 * Number of times the send of a message failed.

	/*

	 * Number of message responses received.

	/*

	 * Number of message fragments received.

	/*

	 * Number of times the receive of a message was retried.

	/*

	 * Number of errors receiving messages.

	/*

	 * Number of times a flag fetch was requested.

	/*

	 * Number of times the hardware didn't follow the state machine.

	/*

	 * Number of received events.

 Number of asyncronous messages received. */

 Number of watchdog pretimeouts. */

 Number of alers received. */

 Always add statistics before this value, it must be last. */

 ACPI, PCI, SMBIOS, hardcode, etc. */

	/*

	 * Flags from the last GET_MSG_FLAGS command, used when an ATTN

	 * is set to hold the flags until we are done handling everything

	 * from the flags.

	/*

	 * Used to tell what we should do with alerts.  If we are

	 * waiting on a response, read the data immediately.

	/*

	 * If set to true, this will request events the next time the

	 * state machine is idle.

	/*

	 * If set to true, this will request flags the next time the

	 * state machine is idle.

	/*

	 * Used to perform timer operations when run-to-completion

	 * mode is on.  This is a countdown timer.

 Used for sending/receiving data.  +1 for the length. */

 Temp receive buffer, gets copied into data. */

 Thread interface handling */

 Timeout for flags check, 0 if off. */

 Flag fetch timer. */

 Info from SSIF cmd */

 See test_multipart_messages() for details. */

 Make it a response */

 Unknown error. */

/*

 * Must be called with the message lock held.  This will release the

 * message lock.  Note that the caller will check SSIF_IDLE and start a

 * new operation, so there is no need to check for new messages to

 * start in here.

 Make sure the watchdog pre-timeout flag is not set at startup. */

 Error, just go to normal state. */

/*

 * Must be called with the message lock held.  This will release the

 * message lock.  Note that the caller will check SSIF_IDLE and start a

 * new operation, so there is no need to check for new messages to

 * start in here.

 Watchdog pre-timeout */

 Messages available. */

 Events available. */

 Wait for something to do */

 Releases lock */

	/*

	 * We are single-threaded here, so no need for a lock until we

	 * start messing with driver states or the queues.

 Start of multi-part read.  Start the next transaction. */

 Remove the multi-part read marker. */

 Middle of multi-part read.  Start the next transaction. */

 All blocks but the last must have 31 data bytes. */

 Received message too big, abort the operation. */

 End of read */

			/*

			 * Out of sequence block, just abort.  Block

			 * numbers start at zero for the second block,

			 * but multi_pos starts at one, so the +1.

 We got the flags from the SSIF, now handle them. */

			/*

			 * Error fetching flags, or invalid length,

			 * just give up for now.

			/*

			 * Don't abort here, maybe it was a queued

			 * response to a previous command.

 We cleared the flags. */

 Error clearing flags */

 Error getting event, probably done. */

 Take off the event flag. */

 Take off the event flag. */

 Error getting event, probably done. */

 Take off the msg flag. */

 Take off the msg flag. */

 We are single-threaded here, so no need for a lock. */

 request failed, just return the error. */

		/*

		 * Got an error on transmit, let the done routine

		 * handle it.

		/*

		 * In the middle of a multi-data write.  See the comment

		 * in the SSIF_MULTI_n_PART case in the probe function

		 * for details on the intricacies of this.

 Length byte. */

 Ready to request the result. */

 The result is already ready, just start it. */

 Wait a jiffie then request the next message */

		/*

		 * Subtle thing, this is 32, not 33, because we will

		 * overwrite the thing at position 32 (which was just

		 * transmitted) with the new length.

 Must be called with the message lock held. */

/*

 * Upper layer wants us to request events.

/*

 * Upper layer is changing the flag saying whether we need to request

 * flags periodically or not.

/*

 * Bit 0 enables message debugging, bit 1 enables state debugging, and

 * bit 2 enables timing debugging.  This is an array indexed by

 * interface number"

 make sure the driver is not looking for flags any more. */

	/*

	 * After this point, we won't deliver anything asychronously

	 * to the message handler.  We can unregister ourself.

 Validate that the response is correct. */

 Do a Get Device ID command, since it is required. */

 One is NULL and one is not */

 Names do not match */

 Try to get an exact match first, then try with a NULL name */

	/*

	 * The specification is all messed up dealing with sending

	 * multi-part messages.  Per what the specification says, it

	 * is impossible to send a message that is a multiple of 32

	 * bytes, except for 32 itself.  It talks about a "start"

	 * transaction (cmd=6) that must be 32 bytes, "middle"

	 * transaction (cmd=7) that must be 32 bytes, and an "end"

	 * transaction.  The "end" transaction is shown as cmd=7 in

	 * the text, but if that's the case there is no way to

	 * differentiate between a middle and end part except the

	 * length being less than 32.  But there is a table at the far

	 * end of the section (that I had never noticed until someone

	 * pointed it out to me) that mentions it as cmd=8.

	 *

	 * After some thought, I think the example is wrong and the

	 * end transaction should be cmd=8.  But some systems don't

	 * implement cmd=8, they use a zero-length end transaction,

	 * even though that violates the SMBus specification.

	 *

	 * So, to work around this, this code tests if cmd=8 works.

	 * If it does, then we use that.  If not, it tests zero-

	 * byte end transactions.  If that works, good.  If not,

	 * we only allow 63-byte transactions max.

 End transactions work, we are good. */

 Zero-size end parts work, use those. */

 Limit to 63 bytes and use a short middle command to mark the end. */

/*

 * Global enables we care about.

/*

 * Prefer ACPI over SMBIOS, if both are available.

 * So if we get an ACPI interface and have already registered a SMBIOS

 * interface at the same address, remove the SMBIOS and add the ACPI one.

 Must have come in through sysfs. */

 If rv is 0 and addr source is not SI_ACPI, continue probing */

 Now check for system interface capabilities */

 SSIF */

 Got a good SSIF response, handle it. */

 Sanitize the data */

 We take whatever size given, but do some testing. */

 Data is not sane, just give up. */

 Assume no multi-part or PEC support */

 Make sure the NMI timeout is cleared. */

 Attempt to enable the event buffer. */

 Not fatal */

 buffer is already enabled, nothing to do. */

 Not fatal */

 A successful return means the event buffer is supported. */

 Some systems don't behave well if you enable alerts. */

 Not fatal */

 A successful return means the alert is supported. */

 Address list will get it */

 Found a dup. */

 Didn't find it in the list. */

 build list for i2c from addr list */

 SPDX-License-Identifier: GPL-2.0+

 for register_parisc_driver() stuff */

 no interrupt */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_si_pci.c

 *

 * Handling for IPMI devices on the PCI bus.

 detect 1, 4, 16byte spacing */

 write invalid cmd */

 read status back */

	/*

	 * This is a "Virtual IPMI device", whatever that is.  It appears

	 * as a KCS device by the class, but it is not one.

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver to talk to a remote management controller on IPMB.

 Add room for the two slave addresses, two checksums, and rqSeq. */

 Transmit thread. */

 Minimum message size. */

 Is it a response? */

 Response messages have an added completion code. */

 Ignore commands until we are up. */

 It's a command, allocate a message for it. */

			/*

			 * Responses should carry the sequence we sent

			 * them with.  If it's a transmitted response,

			 * ignore it.  And if the message hasn't been

			 * transmitted, ignore it.

 NetFn/LUN */

		/*

		 * Keep the source address, rqSeq.  Drop the trailing

		 * checksum.

 NetFn/LUN */

		/*

		 * Skip the source address, rqSeq.  Drop the trailing

		 * checksum.

/*

 * The IPMB protocol only supports i2c writes so there is no need to

 * support I2C_SLAVE_READ* events, except to know if the other end has

 * issued a read without going to stop mode.

		/*

		 * First byte is the slave address, to ease the checksum

		 * calculation.

		/*

		 * It's a response being sent, we needto return a

		 * response response.  Fake a send msg command

		 * response with channel 0.  This will always be ipmb

		 * direct.

 If it's a command, put in our own sequence number. */

 Now add on the final checksums. */

 Wait for a message to send */

 Rely on i2c_transfer for a barrier. */

			/*

			 * It's a response, nothing will be returned

			 * by the other end.

 A command was sent, wait for its response. */

		/*

		 * Grab the message if we can.  If the handler hasn't

		 * already handled it, the message will still be there.

			/*

			 * If working_msg is not set and we timed out,

			 * that means the message grabbed by

			 * check_msg_done before we could grab it

			 * here.  Wait again for check_msg_done to up

			 * the semaphore.

 Return an unspecified error. */

 We don't fetch events here. */

 Can't have the write bit set. */

 SPDX-License-Identifier: GPL-2.0

/*

 * IPMB driver to receive a request and send a response

 *

 * Copyright (C) 2019 Mellanox Techologies, Ltd.

 *

 * This was inspired by Brendan Higgins' ipmi-bmc-bt-i2c driver.

 checksum2 is included in payload */

	/*

	 * subtract 1 byte (rq_sa) from the length of the msg passed to

	 * raw i2c_transfer

 Assign message to buffer except first 2 bytes (length and address) */

 Check i2c block transfer vs smbus */

	/*

	 * subtract rq_sa and netf_rq_lun from the length of the msg. Fill the

	 * temporary client. Note that its use is an exception for IPMI.

 Called with ipmb_dev->lock held. */

 The 8 lsb of the sum is 0 when the checksum is valid */

/*

 * Verify if message has proper ipmb header with minimum length

 * and correct checksum byte.

/*

 * The IPMB protocol only supports I2C Writes so there is no need

 * to support I2C_SLAVE_READ* events.

 * This i2c callback function only monitors IPMB request messages

 * and adds them in a queue, so that they can be handled by

 * receive_ipmb_request.

		/*

		 * At index 0, ipmb_msg stores the length of msg,

		 * skip it for now.

		 * The len will be populated once the whole

		 * buf is populated.

		 *

		 * The I2C bus driver's responsibility is to pass the

		 * data bytes to the backend driver; it does not

		 * forward the i2c slave address.

		 * Since the first byte in the IPMB message is the

		 * address of the responder, it is the responsibility

		 * of the IPMB driver to format the message properly.

		 * So this driver prepends the address of the responder

		 * to the received i2c data before the request message

		 * is handled in userland.

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_si_platform.c

 *

 * Handling for platform devices in IPMI (ACPI, OF, and things

 * coming from the platform.

 For GPE-type interrupts. */

 User disabled this in hardcode. */

 _IFT tells us the interface type: KCS, BT, etc */

 SSIF, just ignore */

 If _GPE exists, use it; otherwise use standard interrupts */

 SPDX-License-Identifier: GPL-2.0+

	/*

	 * Figure out the actual inb/inw/inl/etc routine to use based

	 * upon the register size.

	/*

	 * Some BIOSes reserve disjoint I/O regions in their ACPI

	 * tables.  This causes problems when trying to register the

	 * entire I/O region.  Therefore we must register each I/O

	 * port separately.

 Undo allocations */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2018, Intel Corporation.

/*

 * Field class descriptions

 *

 * LPCyE	Enable LPC channel y

 * IBFIEy	Input Buffer Full IRQ Enable for LPC channel y

 * IRQxEy	Assert SerIRQ x for LPC channel y (Deprecated, use IDyIRQX, IRQXEy)

 * IDyIRQX	Use the specified 4-bit SerIRQ for LPC channel y

 * SELyIRQX	SerIRQ polarity for LPC channel y (low: 0, high: 1)

 * IRQXEy	Assert the SerIRQ specified in IDyIRQX for LPC channel y

 IRQ{12,1}E1 are deprecated as of AST2600 A3 but necessary for prior chips */

 Trigger the upstream IRQ on ODR writes, if enabled */

/*

 * AST_usrGuide_KCS.pdf

 * 2. Background:

 *   we note D for Data, and C for Cmd/Status, default rules are

 *     A. KCS1 / KCS2 ( D / C:X / X+4 )

 *        D / C : CA0h / CA4h

 *        D / C : CA8h / CACh

 *     B. KCS3 ( D / C:XX2h / XX3h )

 *        D / C : CA2h / CA3h

 *        D / C : CB2h / CB3h

 *     C. KCS4

 *        D / C : CA4h / CA5h

 Needs IRQxE1 rather than (ID1IRQX, SEL1IRQX, IRQXE1) before AST2600 A3 */

 We don't have an OBE IRQ, emulate it */

 Don't translate addresses, we want offsets for the regmaps */

 Host to BMC IRQ */

 BMC to Host IRQ */

 Make sure it's proper dead */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_devintf.c

 *

 * Linux device interface for the IPMI message handler.

 *

 * Author: MontaVista Software, Inc.

 *         Corey Minyard <minyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002 MontaVista Software Inc.

 Use the low-level defaults. */

	/* From here out we cannot return, we must jump to "out" for

	/* We claim a mutex because we don't want two

	   users getting something from the queue at a time.

	   Since we have to release the spinlock before we can

	   copy the data to the user, it's possible another

	   user will grab something from the queue, too.  Then

	   the messages might get out of order if something

	   fails and the message gets put back onto the

 Grab the message off the list. */

	/* If we got an error, put the message back onto

 The next four are legacy, not per-channel. */

/*

 * The following code contains code for supporting 32-bit compatible

 * ioctls on 64-bit kernels.  This allows running 32-bit apps on the

 * 64-bit kernel

/*

 * Define some helper functions for copying IPMI data

/*

 * Handle compatibility ioctls

 Keep track of the devices that are registered. */

 SPDX-License-Identifier: GPL-2.0+

/*

 *  ipmi_bt_sm.c

 *

 *  The state machine for an Open IPMI BT sub-driver under ipmi_si.c, part

 *  of the driver architecture at http://sourceforge.net/projects/openipmi 

 *

 *  Author:	Rocky Craig <first.last@hp.com>

 So dev_dbg() is always available. */

 For printk. */

 for completion codes */

 Used in production */

 Generic messages */

 Prints all request/response buffers */

 Verbose look at state changes */

/*

 * BT_DEBUG_OFF must be zero to correspond to the default uninitialized

 * value

 0 == BT_DEBUG_OFF */

/*

 * Typical "Get BT Capabilities" values are 2-3 retries, 5-10 seconds,

 * and 64 byte buffers.  However, one HP implementation wants 255 bytes of

 * buffer (with a documented message of 160 bytes) so go for the max.

 * Since the Open IPMI architecture is single-message oriented at this

 * stage, the queue depth of BT is of no concern.

 seconds */

 seconds after warm reset */

/*

 * States are written in chronological order and usually cover

 * multiple rows of the state table discussion in the IPMI spec.

 Order is critical in this list */

 These must come last */

 BT doesn't get hosed :-) */

/*

 * Macros seen at the end of state "case" blocks.  They help with legibility

 * and debugging.

 BT sequence number */

 +2 for memcpy */

 +2 for memcpy */

 microseconds countdown */

 end of "common" fields */

 hung BMCs stay all 0 */

 to divert the state machine */

 Recommended retries */

 See IPMI 1.5 table 11.6.4 */

/*

 * Some bits are toggled on each write: write once to set it, once

 * more to clear it; writing a zero does nothing.  To absolutely

 * clear it, check its state and write if set.  This avoids the "get

 * current then use as mask" scheme to modify one bit.  Note that the

 * variable "bt" is hardcoded into these macros.

/*

 * Convenience routines for debugging.  These are not multi-open safe!

 * Note the macros have hardcoded variables in them.

	/*

	 * This cannot be called by two threads at the same time and

	 * the buffer is always consumed immediately, so the static is

	 * safe to use.

 called externally at insmod time, and internally on cleanup */

 external: one-time only things */

 start here */

 end here */

 We claim 3 bytes of space; ought to check SPMI table */

 Jam a completion code (probably an error) into a response */

 # following bytes */

 Odd NetFn/LUN */

 seq (ignored) */

 Command */

 The upper state machine starts here */

 all data plus seq byte */

 NetFn/LUN */

/*

 * After the upper state machine has been told SI_SM_TRANSACTION_COMPLETE

 * it calls this.  Strip out the length and seq bytes.

 account for length & seq */

 This bit's functionality is optional */

 force clear */

 always reset */

 always clear */

/*

 * Get rid of an unwanted/stale response.  This should only be needed for

 * BMCs that support multiple outstanding requests.

 Not signalling a response */

 now set */

 always clear */

 pause */

 some BMCs are stubborn */

 always reset */

 now clear */

	/*

	 * length is "framing info", minimum = 4: NetFn, Seq, Cmd, cCode.

	 * Keep layout of first four bytes aligned with write_data[]

 let next XACTION START clean it up */

 Account internally for length byte */

 per the spec, the (NetFn[1], Seq[2], Cmd[3]) tuples must match */

 Restart if retries are left, or return an error completion code */

 open-ended line */

	/*

	 * Per the IPMI spec, retries are based on the sequence number

	 * known only to this module, so manage a restart here.

 this is most likely during insmod */

	/*

	 * Concoct a useful error message, set up the next state, and

	 * be done with this sequence.

 Check status and (usually) take action and change this state machine. */

	/*

	 * Commands that time out may still (eventually) provide a response.

	 * This stale response will get in the way of a new response so remove

	 * it if possible (hopefully during IDLE).  Even if it comes up later

	 * it will be rejected by its (now-forgotten) seq number.

 check timeout */

	/*

	 * Idle state first checks for asynchronous messages from another

	 * channel, then does some opportunistic housekeeping.

 clear it */

 clear a leftover H_BUSY */

 force clear */

 clear */

 can clear too fast to catch */

 Spinning hard can suppress B2H_ATN and force a timeout */

 set */

		/*

		 * Uncached, ordered writes should just proceed serially but

		 * some BMCs don't clear B2H_ATN with one hit.  Fast-path a

		 * workaround without too much penalty to the general case.

 clear it to ACK the BMC */

 keep hitting it */

 check in case of retry */

 start of BMC2HOST buffer */

 true == packet seq match */

 NOW clear */

 Not my message */

 where to next? */

 normal */

 Startup magic */

 For example: after FW update */

 next state is now IDLE */

 No repeat printing */

 Send a soft reset */

 number of bytes following */

 NetFn/LUN == Application, LUN 0 */

 Sequence number */

 Cmd == Soft reset */

 Hold off everything for a bit */

 don't reset retries or seq! */

 should never occur */

	/*

	 * It's impossible for the BT status and interrupt registers to be

	 * all 1's, (assuming a properly functioning, self-initialized BMC)

	 * but that's what you get from reading a bogus address, so we

	 * test that first.  The calling routine uses negative logic.

	/*

	 * Try getting the BT capabilities here.

 SPDX-License-Identifier: GPL-2.0+

/*

 * A hack to create a platform device from a DMI entry.  This will

 * allow autoloading of the IPMI drive based on SMBIOS entries.

 addr space for si, intf# for ssif */

/*

 * Look up the slave address for a given interface.  This is here

 * because ACPI doesn't have a slave address while SMBIOS does, but we

 * prefer using ACPI so the ACPI code can use the IPMI namespace.

 * This function allows an ACPI-specified IPMI device to look up the

 * slave address from the DMI table.

 Match I2C interface 0. */

				/*

				 * Some broken systems put the I2C address in

				 * the slave address field.  We try to

				 * accommodate them here.

 I/O */

 Memory */

			/*

			 * If bit 4 of byte 0x10 is set, then the lsb

			 * for the address is odd.

			/*

			 * The top two bits of byte 0x10 hold the

			 * register spacing.

 Byte boundaries */

 32-bit boundaries */

 16-byte boundaries */

 Old DMI spec. */

		/*

		 * Note that technically, the lower bit of the base

		 * address should be 1 if the address is I/O and 0 if

		 * the address is in memory.  So many systems get that

		 * wrong (and all that I have seen are I/O) so we just

		 * ignore that bit and assume I/O.  Systems that use

		 * memory should use the newer spec, anyway.

 SPDX-License-Identifier: GPL-2.0+

/*

 * PowerNV OPAL IPMI driver

 *

 * Copyright 2014 IBM Corp.

	/**

	 * We assume that there can only be one outstanding request, so

	 * keep the pending message in cur_msg. We protect this from concurrent

	 * updates through send & recv calls, (and consequently opal_msg, which

	 * is in-use when cur_msg is set) with msg_lock

 ensure data_len will fit in the opal_ipmi_msg buffer... */

 ... and that we at least have netfn and cmd bytes */

 format our data for the OPAL API */

 data_size already includes the netfn and cmd bytes */

 If came via the poll, and response was not yet ready */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_smic_sm.c

 *

 * The state-machine driver for an IPMI SMIC driver

 *

 * It started as a copy of Corey Minyard's driver for the KSC interface

 * and the kernel patch "mmcdev-patch-245" by HP

 *

 * modified by:	Hannes Schulz <schulz@schwaar.com>

 *		ipmi@schwaar.com

 *

 *

 * Corey Minyard's driver for the KSC interface has the following

 * copyright notice:

 *   Copyright 2002 MontaVista Software Inc.

 *

 * the kernel patch "mmcdev-patch-245" by HP has the following

 * copyright notice:

 * (c) Copyright 2001 Grant Grundler (c) Copyright

 * 2001 Hewlett-Packard Company

 So dev_dbg() is always available. */

 For printk. */

 for completion codes */

/* smic_debug is a bit-field

 *	SMIC_DEBUG_ENABLE -	turned on for now

 *	SMIC_DEBUG_MSG -	commands and their responses

 *	SMIC_DEBUG_STATES -	state machine

 Timeouts in microseconds. */

 SMIC Flags Register Bits */

/*

 * SMIC_SMI and SMIC_EVM_DATA_AVAIL are only used by

 * a few systems, and then only by Systems Management

 * Interrupts, not by the OS.  Always ignore these bits.

 *

 SMIC Error Codes */

 We use 3 bytes of I/O. */

  SMIC Control/Status Code Components */

 Control form's name */

 Status  form's name */

 Unified Control/Status names... */

  SMIC Control Codes */

  SMIC Status Codes */

/* these are the control/status codes we actually use

	SMIC_CC_SMS_GET_STATUS	0x40

	SMIC_CC_SMS_WR_START	0x41

	SMIC_CC_SMS_WR_NEXT	0x42

	SMIC_CC_SMS_WR_END	0x43

	SMIC_CC_SMS_RD_START	0x44

	SMIC_CC_SMS_RD_NEXT	0x45

	SMIC_CC_SMS_RD_END	0x46



	SMIC_SC_SMS_READY	0xC0

	SMIC_SC_SMS_WR_START	0xC1

	SMIC_SC_SMS_WR_NEXT	0xC2

	SMIC_SC_SMS_WR_END	0xC3

	SMIC_SC_SMS_RD_START	0xC4

	SMIC_SC_SMS_RD_NEXT	0xC5

	SMIC_SC_SMS_RD_END	0xC6

		/*

		 * FIXME: smic_event is sometimes called with time >

		 * SMIC_RETRY_TIMEOUT

 in IDLE we check for available messages */

 sanity check whether smic is really idle */

 this should not happen */

 OK so far; smic is idle let us start ... */

		/*

		 * we must not issue WR_(NEXT|END) unless

		 * TX_DATA_READY is set

 last byte */

 this is the same code as in SMIC_WRITE_START */

 data register holds an error code */

		/*

		 * we must wait for RX_DATA_READY to be set before we

		 * can continue

		/*

		 * smic tells us that this is the last byte to be read

		 * --> clean up

 data register holds an error code */

	/*

	 * It's impossible for the SMIC fnags register to be all 1's,

	 * (assuming a properly functioning, self-initialized BMC)

	 * but that's what you get from reading a bogus address, so we

	 * test that first.

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_msghandler.c

 *

 * Incoming and outgoing message routing for an IPMI interface.

 *

 * Author: MontaVista Software, Inc.

 *         Corey Minyard <minyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002 MontaVista Software Inc.

 Numbers in this enumerator should be mapped to ipmi_panic_event_str */

 Indices in this array should be mapped to enum ipmi_panic_event_op */

 Remain in auto-maintenance mode for this amount of time (in ms). */

/*

 * Don't let a message sit in a queue forever, always time it with at lest

 * the max message timer.  This is in milliseconds.

/*

 * Timeout times below are in milliseconds, and are done off a 1

 * second timer.  So setting the value to 1000 would mean anything

 * between 0 and 1000ms.  So really the only reasonable minimum

 * setting it 2000ms, which is between 1 and 2 seconds.

 The default timeout for message retries. */

 The default timeout for maintenance mode message retries. */

 The default maximum number of retries */

 Call every ~1000 ms. */

 How many jiffies does it take to get to the timeout time. */

/*

 * Request events from the queue every second (this is the number of

 * IPMI_TIMEOUT_TIMES between event requests).  Hopefully, in the

 * future, IPMI will add a way to know immediately if an event is in

 * the queue and this silliness can go away.

 How long should we cache dynamic device IDs? */

/*

 * The main "user" data structure.

	/*

	 * Set to NULL when the user is destroyed, a pointer to myself

	 * so srcu_dereference can be used on it.

 The upper layer that handles receive messages. */

 The interface this user is bound to. */

 Does this interface receive IPMI events? */

 Free must run in process context for RCU cleanup. */

	/*

	 * This is used to form a linked lised during mass deletion.

	 * Since this is in an RCU list, we cannot use the link above

	 * or change any data until the RCU period completes.  So we

	 * use this next variable during mass deletion so we can have

	 * a list and don't have to wait and restart the search on

	 * every individual deletion of a command.

	/*

	 * To verify on an incoming send message response that this is

	 * the message that the response is for, we keep a sequence id

	 * and increment it every time we send a message.

	/*

	 * This is held so we can properly respond to the message on a

	 * timeout, and it is used to hold the temporary data for

	 * retransmission, too.

/*

 * Store the information in a msgid (long) to allow us to find a

 * sequence table entry from the msgid.

	/*

	 * My slave address.  This is initialized to IPMI_BMC_SLAVE_ADDR,

	 * but may be changed by the user.

	/*

	 * My LUN.  This should generally stay the SMS LUN, but just in

	 * case...

/*

 * Note that the product id, manufacturer id, guid, and device id are

 * immutable in this structure, so dyn_mutex is not required for

 * accessing those.  If those change on a BMC, a new BMC is allocated.

 Interfaces on this BMC. */

 Protects id, intfs, & dyn* */

 completion code */

/*

 * Various statistics for IPMI, these index stats[] in the ipmi_smi

 * structure.

 Commands we got from the user that were invalid. */

 Commands we sent to the MC. */

 Responses from the MC that were delivered to a user. */

 Responses from the MC that were not delivered to a user. */

 Commands we sent out to the IPMB bus. */

 Commands sent on the IPMB that had errors on the SEND CMD */

 Each retransmit increments this count. */

	/*

	 * When a message times out (runs out of retransmits) this is

	 * incremented.

	/*

	 * This is like above, but for broadcasts.  Broadcasts are

	 * *not* included in the above count (they are expected to

	 * time out).

 Responses I have sent to the IPMB bus. */

 The response was delivered to the user. */

 The response had invalid data in it. */

 The response didn't have anyone waiting for it. */

 Commands we sent out to the IPMB bus. */

 Commands sent on the IPMB that had errors on the SEND CMD */

 Each retransmit increments this count. */

	/*

	 * When a message times out (runs out of retransmits) this is

	 * incremented.

 Responses I have sent to the IPMB bus. */

 The response was delivered to the user. */

 The response had invalid data in it. */

 The response didn't have anyone waiting for it. */

 The command was delivered to the user. */

 The command had invalid data in it. */

 The command didn't have anyone waiting for it. */

 Invalid data in an event. */

 Events that were received with the proper format. */

 Retransmissions on IPMB that failed. */

 Retransmissions on LAN that failed. */

 This *must* remain last, add new values above this. */

 What interface number are we? */

 Set when the interface is being unregistered. */

 Used for a list of interfaces. */

	/*

	 * The list of upper layers that are using me.  seq_lock write

	 * protects this.  Read protection is with srcu.

 Used for wake ups at startup. */

	/*

	 * Prevents the interface from being unregistered when the

	 * interface is used by being looked up through the BMC

	 * structure.

 Handle recursive situations.  Yuck. */

 Driver-model device for the system interface. */

	/*

	 * A table of sequence numbers for this interface.  We use the

	 * sequence numbers for IPMB messages that go out of the

	 * interface to match them up with their responses.  A routine

	 * is called periodically to time the items in this list.

	/*

	 * Messages queued for delivery.  If delivery fails (out of memory

	 * for instance), They will stay in here to be processed later in a

	 * periodic timer interrupt.  The tasklet is for handling received

	 * messages directly from the handler.

	/*

	 * The list of command receivers that are registered for commands

	 * on this interface.

	/*

	 * Events that were queues because no one was there to receive

	 * them.

 For dealing with event stuff. */

 How many events in queue? */

 How many users are waiting for events? */

 For dealing with watch stuff below. */

 How many users are waiting for commands? */

 How many users are waiting for watchdogs? */

 How many users are waiting for message responses? */

	/*

	 * Tells what the lower layer has last been asked to watch for,

	 * messages and/or watchdogs.  Protected by watch_lock.

	/*

	 * The event receiver for my BMC, only really used at panic

	 * shutdown as a place to store this.

 For handling of maintenance mode. */

 Used in a timer... */

	/*

	 * If we are doing maintenance on something on IPMB, extend

	 * the timeout time to avoid timeouts writing firmware and

	 * such.

	/*

	 * A cheap hack, if this is non-null and a message to an

	 * interface comes in with a NULL user, call this routine with

	 * it.  Note that the message will still be freed by the

	 * caller.  This only works on the system interface.

	 *

	 * Protected by bmc_reg_mutex.

	/*

	 * When we are scanning the channels for an SMI, this will

	 * tell which channel we are scanning.

 Channel information */

 First index into the following. */

	/*

	 * run_to_completion duplicate of smb_info, smi_info

	 * and ipmi_serial_info structures. Used to decrease numbers of

	 * parameters passed by "low" level IPMI code.

/**

 * The driver model view of the IPMI messaging driver.

/*

 * This mutex keeps us from adding the same BMC twice.

/*

 * List of watchers that want to know when smi's are added and deleted.

 Invalid */

	/*

	 * Wholesale remove all the entries from the list in the

	 * interface and wait for RCU to know that none are in use.

	/*

	 * Make sure the driver is actually initialized, this handles

	 * problems with initialization order.

/*

 * Must be called with smi_watchers_mutex held.

 Special handling for NULL users. */

 No handler, so give up. */

		/*

		 * If we are running in the panic context, calling the

		 * receive handler doesn't much meaning and has a deadlock

		 * risk.  At this moment, simply skip it in that case.

 User went away, give up. */

 Convert to a response. */

/*

 * Find the next sequence number not being used and add the given

 * message with the given timeout to the sequence table.  This must be

 * called with the interface's seq_lock held.

		/*

		 * Start with the maximum timeout, when the send response

		 * comes in we will start the real timer.

/*

 * Return the receive message for the given sequence number and

 * release the sequence number so it can be reused.  Some other data

 * is passed in to be sure the message matches up correctly (to help

 * guard against message coming in after their timeout and the

 * sequence number being reused).

 Start the timer for a specific sequence table entry. */

	/*

	 * We do this verification because the user can be deleted

	 * while a message is outstanding.

 Got an error for the send message for a specific sequence number. */

	/*

	 * We do this verification because the user can be deleted

	 * while a message is outstanding.

	/*

	 * There is no module usecount here, because it's not

	 * required.  Since this can only be used by and called from

	 * other modules, they will implicitly use this module, and

	 * thus this can't be removed unless the other modules are

	 * removed.

	/*

	 * Make sure the driver is actually initialized, this handles

	 * problems with initialization order.

 Not found, return an error */

 Note that each existing user holds a refcount to the interface. */

 User wants pretimeouts, so make sure to watch for them. */

 Not found, return an error */

 SRCU cleanup must happen in task context. */

		/*

		 * The user has already been cleaned up, just make sure

		 * nothing is using it and return.

 Remove the user from the interface's sequence table. */

	/*

	 * Remove the user from the command receiver's table.  First

	 * we build a list of everything (not using the standard link,

	 * since other things may be using it till we do

	 * synchronize_srcu()) then free everything in that list.

		/*

		 * Another thread is delivering events for this, so

		 * let it handle any new events.

 Deliver any queued events. */

 Make sure the command/netfn is not already registered. */

 Format the IPMB header data. */

 Now tack on the data to the message. */

 Now calculate the checksum and tack it on. */

	/*

	 * Add on the checksum size and the offset from the

	 * broadcast.

 Format the IPMB header data. */

 Now tack on the data to the message. */

 Now calculate the checksum and tack it on. */

	/*

	 * Add on the checksum size and the offset from the

	 * broadcast.

 Responses are not allowed to the SMI. */

		/*

		 * We don't let the user do these, since we manage

		 * the sequence numbers.

		/*

		 * Broadcasts add a zero at the beginning of the

		 * message, but otherwise is the same as an IPMB

		 * address.

 Don't retry broadcasts. */

	/*

	 * 9 for the header and 1 for the checksum, plus

	 * possibly one for the broadcast.

		/*

		 * It's a response, so use the user's sequence

		 * from msgid.

		/*

		 * Save the receive message so we can use it

		 * to deliver the response.

 It's a command, so get a sequence for it. */

 Different default in maintenance mode */

		/*

		 * Create a sequence number with a 1 second

		 * timeout and 4 retries.

			/*

			 * We have used up all the sequence numbers,

			 * probably, so abort.

		/*

		 * Store the sequence number in the message,

		 * so that when the send message response

		 * comes back we can start the timer.

		/*

		 * Copy the message into the recv message data, so we

		 * can retransmit it later if necessary.

		/*

		 * We don't unlock until here, because we need

		 * to copy the completed message into the

		 * recv_msg before we release the lock.

		 * Otherwise, race conditions may bite us.  I

		 * know that's pretty paranoid, but I prefer

		 * to be correct.

 Responses must have a completion code. */

 11 for the header and 1 for the checksum. */

		/*

		 * It's a response, so use the user's sequence

		 * from msgid.

		/*

		 * Save the receive message so we can use it

		 * to deliver the response.

 It's a command, so get a sequence for it. */

		/*

		 * Create a sequence number with a 1 second

		 * timeout and 4 retries.

			/*

			 * We have used up all the sequence numbers,

			 * probably, so abort.

		/*

		 * Store the sequence number in the message,

		 * so that when the send message response

		 * comes back we can start the timer.

		/*

		 * Copy the message into the recv message data, so we

		 * can retransmit it later if necessary.

		/*

		 * We don't unlock until here, because we need

		 * to copy the completed message into the

		 * recv_msg before we release the lock.

		 * Otherwise, race conditions may bite us.  I

		 * know that's pretty paranoid, but I prefer

		 * to be correct.

/*

 * Separate from ipmi_request so that the user does not have to be

 * supplied in certain circumstances (mainly at panic time).  If

 * messages are supplied, they will be freed, even if an error

 * occurs.

 The put happens when the message is freed. */

	/*

	 * Store the message to send in the receive message so timeout

	 * responses can get the proper response data.

 Unknown address type. */

 record completion code when error */

		/*

		 * Make sure the id data is available before setting

		 * dyn_id_set.

 Something went wrong in the fetch. */

 dyn_id_set makes the id data available. */

/*

 * Fetch the device id for the bmc/interface.  You must pass in either

 * bmc or intf, this code will get the other one.  If the data has

 * been recently fetched, this will just use the cached data.  Otherwise

 * it will run a new fetch.

 *

 * Except for the first time this is called (in ipmi_add_smi()),

 * this will always return good data;

 If we have a valid and current ID, just return that. */

	/*

	 * The guid, device id, manufacturer id, and product id should

	 * not change on a BMC.  If it does we have to do some dancing.

 Fill in the temporary BMC for good measure. */

 Retry later on an error. */

			/*

			 * We weren't given the interface on the

			 * command line, so restart the operation on

			 * the next interface for the BMC.

 We have a new BMC, set it up. */

 Version info changes, scan the channels again. */

 Ignore failures if we have previous data. */

			/*

			 * The guid used to be valid and it failed to fetch,

			 * just use the cached value.

/*

 * Returns with the bmc's usecount incremented, if it is non-NULL.

/*

 * Returns with the bmc's usecount incremented, if it is non-NULL.

 Unregister overwrites id */

	/*

	 * Remove the platform device in a work queue to avoid issues

	 * with removing the device attributes while reading a device

	 * attribute.

/*

 * Must be called with intf->bmc_reg_mutex held.

/*

 * Must be called with intf->bmc_reg_mutex held.

	/*

	 * platform_device_register() can cause bmc_reg_mutex to

	 * be claimed because of the is_visible functions of

	 * the attributes.  Eliminate possible recursion and

	 * release the lock.

	/*

	 * Try to find if there is an bmc_device struct

	 * representing the interfaced BMC already

	/*

	 * If there is already an bmc_device, free the new one,

	 * otherwise register the new BMC device

		/*

		 * Note: old_bmc already has usecount incremented by

		 * the BMC find functions.

	/*

	 * create symlink from system interface device to bmc device

	 * and back.

 Not for me */

 Error from getting the GUID, the BMC doesn't have one. */

	/*

	 * Make sure the guid data is available before setting

	 * dyn_guid_set.

 Send failed, no GUID available. */

 dyn_guid_set makes the guid data available. */

 It's the one we want */

 Got an error from the channel, just go on. */

				/*

				 * If the MC does not support this

				 * command, that is legal.  We just

				 * assume it has one IPMB at channel

				 * zero.

 Message not big enough, just go on. */

 Got an error somehow, just give up. */

/*

 * Must be holding intf->bmc_reg_mutex to call this.

		/*

		 * Start scanning the channels to see what is

		 * available.

 Wait for the channel info to be read. */

 Assume a single IPMB channel at zero. */

 In case something came in */

	/*

	 * Make sure the driver is actually initialized, this handles

	 * problems with initialization order.

 Mark it invalid for now. */

 Look for a hole in the numbers. */

 Add the new interface in numeric order. */

	/*

	 * Keep memory order straight for RCU readers.  Make

	 * sure everything else is committed to memory before

	 * setting intf_num to mark the interface valid.

 After this point the interface is legal to use. */

 It's an error, so it will never requeue, no need to check return. */

 Clear out our transmit queues and hold the messages. */

 Current message first, to preserve order */

 Wait for the message to clear out. */

 No need for locks, the interface is down. */

	/*

	 * Return errors for all pending messages in queue and in the

	 * tables waiting for remote responses.

 At this point no users can be added to the interface. */

	/*

	 * Call all the watcher interfaces to tell them that

	 * an interface is going away.

	/*

	 * This is 11, not 10, because the response must contain a

	 * completion code.

 Message not big enough, just ignore it. */

 An error getting the response, just ignore it. */

	/*

	 * It's a response from a remote entity.  Look up the sequence

	 * number and handle the response.

		/*

		 * We were unable to find the sequence number,

		 * so just nuke the message.

	/*

	 * The other fields matched, so no need to set them, except

	 * for netfn, which needs to be the response that was

	 * returned, not the request value.

 Message not big enough, just ignore it. */

 An error getting the response, just ignore it. */

 We didn't find a user, deliver an error response. */

 rqseq/lun */

 cmd */

			/*

			 * We used the message, so return the value

			 * that causes it to not be freed or

			 * queued.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling

			 * later.

 Extract the source address from the data. */

			/*

			 * Extract the rest of the message information

			 * from the IPMB header.

			/*

			 * We chop off 10, not 9 bytes because the checksum

			 * at the end also needs to be removed.

 We always use channel 0 for direct messages. */

 We didn't find a user, deliver an error response. */

			/*

			 * We used the message, so return the value

			 * that causes it to not be freed or

			 * queued.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling

			 * later.

 Extract the source address from the data. */

			/*

			 * Extract the rest of the message information

			 * from the IPMB header.

	/*

	 * This is 13, not 12, because the response must contain a

	 * completion code.

 Message not big enough, just ignore it. */

 An error getting the response, just ignore it. */

	/*

	 * It's a response from a remote entity.  Look up the sequence

	 * number and handle the response.

		/*

		 * We were unable to find the sequence number,

		 * so just nuke the message.

	/*

	 * The other fields matched, so no need to set them, except

	 * for netfn, which needs to be the response that was

	 * returned, not the request value.

 Message not big enough, just ignore it. */

 An error getting the response, just ignore it. */

 We didn't find a user, just give up. */

		/*

		 * Don't do anything with these messages, just allow

		 * them to be freed.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling later.

 Extract the source address from the data. */

			/*

			 * Extract the rest of the message information

			 * from the IPMB header.

			/*

			 * We chop off 12, not 11 bytes because the checksum

			 * at the end also needs to be removed.

/*

 * This routine will handle "Get Message" command responses with

 * channels that use an OEM Medium. The message format belongs to

 * the OEM.  See IPMI 2.0 specification, Chapter 6 and

 * Chapter 22, sections 22.6 and 22.24 for more details.

	/*

	 * We expect the OEM SW to perform error checking

	 * so we just do some basic sanity checks

 Message not big enough, just ignore it. */

 An error getting the response, just ignore it. */

	/*

	 * This is an OEM Message so the OEM needs to know how

	 * handle the message. We do no interpretation.

 We didn't find a user, just give up. */

		/*

		 * Don't do anything with these messages, just allow

		 * them to be freed.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling

			 * later.

			/*

			 * OEM Messages are expected to be delivered via

			 * the system interface to SMS software.  We might

			 * need to visit this again depending on OEM

			 * requirements

			/*

			 * The message starts at byte 4 which follows the

			 * the Channel Byte in the "GET MESSAGE" command

 Message is too small to be an IPMB event. */

 An error getting the event, just ignore it. */

	/*

	 * Allocate and fill in one message for every user that is

	 * getting events.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling

			 * later.

 Now deliver all the messages. */

		/*

		 * No one to receive the message, put it in queue if there's

		 * not already too many things in the queue.

			/*

			 * We couldn't allocate memory for the

			 * message, so requeue it for handling

			 * later.

		/*

		 * There's too many things in the queue, discard this

		 * message.

/*

 * Handle a received message.  Return 1 if the message should be requeued,

 * 0 if the message should be freed, or -1 if the message should not

 * be freed or requeued.

 Message is too small to be correct. */

 Generate an error response for the message. */

 commands must have at least 3 bytes, responses 4. */

		/*

		 * This is the local response to a command send, start

		 * the timer for these.  The user_data will not be

		 * NULL if this is a response send, and we will let

		 * response sends just go through.

		/*

		 * Check for errors, if we get certain errors (ones

		 * that mean basically we can try again later), we

		 * ignore them and start the timer.  Otherwise we

		 * report the error immediately.

 Got an error sending the message, handle it. */

 The message was sent, start the timer. */

		/*

		 * The NetFN and Command in the response is not even

		 * marginally correct.

 It's a response to a sent response. */

		/*

		 * It's a response to a response we sent.  For this we

		 * deliver a send message response to the user.

 Invalid channel number */

 It's from the receive queue. */

 Invalid channel number */

		/*

		 * We need to make sure the channels have been initialized.

		 * The channel_handler routine will set the "curr_channel"

		 * equal to or greater than IPMI_MAX_CHANNELS when all the

		 * channels for this interface have been initialized.

 Throw the message away */

				/*

				 * It's a response, so find the

				 * requesting message and send it up.

				/*

				 * It's a command to the SMS from some other

				 * entity.  Handle that.

				/*

				 * It's a response, so find the

				 * requesting message and send it up.

				/*

				 * It's a command to the SMS from some other

				 * entity.  Handle that.

			/* Check for OEM Channels.  Clients had better

				/*

				 * We don't handle the channel type, so just

				 * free the message.

 It's an asynchronous event. */

 It's a response from the local BMC. */

/*

 * If there are messages in the queue or pretimeouts, handle them.

 See if any waiting messages need to be processed. */

			/*

			 * To preserve message order, quit if we

			 * can't handle a message.  Add the message

			 * back at the head, this is safe because this

			 * tasklet is the only thing that pulls the

			 * messages.

 Message handled */

 If rv < 0, fatal error, del but don't free. */

	/*

	 * If the pretimout count is non-zero, decrement one from it and

	 * deliver pretimeouts to all the users.

 keep us warning-free. */

	/*

	 * Start the next message if available.

	 *

	 * Do this here, not in the actual receiver, because we may deadlock

	 * because the lower layer is allowed to hold locks while calling

	 * message delivery.

 Pick the high priority queue first. */

 Handle a new message from the lower layer. */

 keep us warning-free. */

	/*

	 * To preserve message order, we keep a queue and deliver from

	 * a tasklet.

	/*

	 * We can get an asynchronous event or receive message in addition

	 * to commands we send.

		/*

		 * If we can't allocate the message, then just return, we

		 * get 4 retries, so this should be ok.

 The message has used all its retries. */

 More retries, send again. */

		/*

		 * Start with the max timer, set to normal timer after

		 * the message is sent.

		/*

		 * Send the new message.  We send with a zero

		 * priority.  It timed out, I doubt time is that

		 * critical now, and high priority messages are really

		 * only for messages to the local MC, which don't get

		 * resent.

	/*

	 * Go through the seq table and find any messages that

	 * have timed out, putting them in the timeouts

	 * list.

	/*

	 * Maintenance mode handling.  Check the timeout

	 * optimistically before we claim the lock.  It may

	 * mean a timeout gets missed occasionally, but that

	 * only means the timeout gets extended by one period

	 * in that case.  No big deal, and it avoids the lock

	 * most of the time.

 No event requests when in maintenance mode. */

 Racy, but worst case we start the timer twice. */

 Try to keep as much stuff out of the panic path as possible. */

 Try to keep as much stuff out of the panic path as possible. */

/*

 * Inside a panic, send a message and wait for a response.

 Don't retry, and don't wait. */

 A get event receiver command, save it. */

		/*

		 * A get device id command, save if we are an event

		 * receiver or generator.

 Fill in an event telling that we have failed. */

 Sensor or Event. */

 Platform event command. */

 Kernel generator ID, IPMI table 5-4 */

 This is for IPMI 1.0. */

 OS Critical Stop, IPMI table 36-3 */

 Sensor specific, IPMI table 36-1 */

 Runtime stop OEM bytes 2 & 3. */

	/*

	 * Put a few breadcrumbs in.  Hopefully later we can add more things

	 * to make the panic events more useful.

 Send the event announcing the panic. */

	/*

	 * On every interface, dump a bunch of OEM event holding the

	 * string.

	/*

	 * intf_num is used as an marker to tell if the

	 * interface is valid.  Thus we need a read barrier to

	 * make sure data fetched before checking intf_num

	 * won't be used.

	/*

	 * First job here is to figure out where to send the

	 * OEM events.  There's no way in IPMI to send OEM

	 * events using an event send command, so we have to

	 * find the SEL to put them in and stick them in

	 * there.

 Get capabilities from the get device id. */

 Request the device info from the local MC. */

 Request the event receiver from the local MC. */

	/*

	 * Validate the event receiver.  The low bit must not

	 * be 1 (it must be a valid IPMB address), it cannot

	 * be zero, and it must not be my address.

		/*

		 * The event receiver is valid, send an IPMB

		 * message.

 FIXME - is this right? */

		/*

		 * The event receiver was not valid (or was

		 * me), but I am an SEL device, just dump it

		 * in my SEL.

 No where to send the event. */

 Storage. */

 OEM event without timestamp. */

 sequence # */

		/*

		 * Always give 11 bytes, so strncpy will fill

		 * it with zeroes for me.

 For every registered interface, set it to run to completion. */

 Interface is not ready. */

		/*

		 * If we were interrupted while locking xmit_msgs_lock or

		 * waiting_rcv_msgs_lock, the corresponding list may be

		 * corrupted.  In this case, drop items on the list for

		 * the safety.

 Must be called with ipmi_interfaces_mutex held. */

 priority: INT_MAX >= x >= 0 */

		/*

		 * This can't be called if any interfaces exist, so no worry

		 * about shutting down the interfaces.

		/*

		 * Tell the timer to stop, then wait for it to stop.  This

		 * avoids problems with race conditions removing the timer

		 * here.

 Check for buffer leaks. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * ipmi_kcs_sm.c

 *

 * State machine for handling IPMI KCS interfaces.

 *

 * Author: MontaVista Software, Inc.

 *         Corey Minyard <minyard@mvista.com>

 *         source@mvista.com

 *

 * Copyright 2002 MontaVista Software Inc.

/*

 * This state machine is taken from the state machine in the IPMI spec,

 * pretty much verbatim.  If you have questions about the states, see

 * that document.

 So dev_dbg() is always available. */

 For printk. */

 for completion codes */

/* kcs_debug is a bit-field

 *	KCS_DEBUG_ENABLE -	turned on for now

 *	KCS_DEBUG_MSG    -	commands and their responses

 *	KCS_DEBUG_STATES -	state machine

 The states the KCS driver may be in. */

 The KCS interface is currently doing nothing. */

	/*

	 * We are starting an operation.  The data is in the output

	 * buffer, but nothing has been done to the interface yet.  This

	 * was added to the state machine in the spec to wait for the

	 * initial IBF.

 We have written a write cmd to the interface. */

 We are writing bytes to the interface. */

	/*

	 * We have written the write end cmd to the interface, and

	 * still need to write the last byte.

 We are waiting to read data from the interface. */

	/*

	 * State to transition to the error handler, this was added to

	 * the state machine in the spec to be sure IBF was there.

	/*

	 * First stage error handler, wait for the interface to

	 * respond.

	/*

	 * The abort cmd has been written, wait for the interface to

	 * respond.

	/*

	 * We wrote some data to the interface, wait for it to switch

	 * to read mode.

 The hardware failed to follow the state machine. */

 Timeouts in microseconds. */

 Reserve 2 I/O bytes. */

 Control codes. */

 Status bits. */

 Throw the data away and mark it truncated. */

		/* Guarantee that we return at least 3 bytes, with an

		/*

		 * Report a truncated error.  We might overwrite

		 * another error, but that's too bad, the user needs

		 * to know it was truncated.

/*

 * This implements the state machine defined in the IPMI manual, see

 * that for details on how this works.  Divide that flowchart into

 * sections delimited by "Wait for IBF" and this will become clear.

 All states wait for ibf, so just do it here. */

 Just about everything looks at the KCS state, so grab that, too. */

 If there's and interrupt source, turn it off. */

			/*

			 * We don't implement this exactly like the state

			 * machine in the spec.  Some broken hardware

			 * does not write the final dummy byte to the

			 * read register.  Thus obf will never go high

			 * here.  We just go straight to idle, and we

			 * handle clearing out obf in idle state if it

			 * happens to come in.

 controller isn't responding */

	/*

	 * It's impossible for the KCS status register to be all 1's,

	 * (assuming a properly functioning, self-initialized BMC)

	 * but that's what you get from reading a bogus address, so we

	 * test that first.

 SPDX-License-Identifier: GPL-2.0-or-later

 Copyright (c) 2021 IBM Corp. */

 Use kzalloc() as the allocation is cleaned up with kfree() via serio_unregister_port() */

 kfree()s priv->port via put_device() */

 Ensure the IBF IRQ is disabled if we were the active client */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright(c) 2007 - 2009 Intel Corporation. All rights reserved.

/*

 * This driver supports an interface for DCA clients and providers to meet.

 at this point only one domain in the list is expected */

/**

 * dca_add_requester - add a dca client to the list

 * @dev - the device that wants dca service

 check if the requester has not been added already */

/**

 * dca_remove_requester - remove a dca client from the list

 * @dev - the device that wants dca service

/**

 * dca_common_get_tag - return the dca tag (serves both new and old api)

 * @dev - the device that wants dca service

 * @cpu - the cpuid as returned by get_cpu()

/**

 * dca3_get_tag - return the dca tag to the requester device

 *                for the given cpu (new api)

 * @dev - the device that wants dca service

 * @cpu - the cpuid as returned by get_cpu()

/**

 * dca_get_tag - return the dca tag for the given cpu (old api)

 * @cpu - the cpuid as returned by get_cpu()

/**

 * alloc_dca_provider - get data struct for describing a dca provider

 * @ops - pointer to struct of dca operation function pointers

 * @priv_size - size of extra mem to be added for provider's needs

/**

 * free_dca_provider - release the dca provider data struct

 * @ops - pointer to struct of dca operation function pointers

 * @priv_size - size of extra mem to be added for provider's needs

/**

 * register_dca_provider - register a dca provider

 * @dca - struct created by alloc_dca_provider()

 * @dev - device providing dca services

 Recheck, we might have raced after dropping the lock */

/**

 * unregister_dca_provider - remove a dca provider

 * @dca - struct created by alloc_dca_provider()

/**

 * dca_register_notify - register a client's notifier callback

/**

 * dca_unregister_notify - remove a client's notifier callback

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright(c) 2007 - 2009 Intel Corporation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0+

 Copyright 2018 IBM Corp

/*

 * A FSI master controller, using a simple GPIO bit-banging interface

 Common SCU based coprocessor control registers */

 AST2500 specific ones */

 CVIC registers */

/*

 * System register base address (needed for configuring the

 * coldfire maps)

 Amount of SRAM required */

 mutex for command ordering */

 Voltage translator */

 FSI enable */

 Mux control */

 start bit, and any non-aligned top bits */

 aligned bits */

 Left align message */

 this will also handle LAST_ADDR_INVALID */

	/* We may be in 23-bit addressing mode, which uses the id as the

	 * top two address bits. So, if we're referencing a different ID,

	 * use absolute addresses.

 remove the top two bits from any 23-bit addressing */

	/* We know that the addresses are limited to 21 bits, so this won't

/*

 * Encode an Absolute/Relative/Same Address command

 we have 21 bits of address max */

 cmd opcodes are variable length - SAME_AR is only two bits */

 we still address the byte offset within the word */

 8 bits plus sign */

	/*

	 * The read/write size is encoded in the lower bits of the address

	 * (as it must be naturally-aligned), and the following ds bit.

	 *

	 *	size	addr:1	addr:0	ds

	 *	1	x	x	0

	 *	2	x	0	1

	 *	4	0	1	1

	 *

 Send command */

 Ring doorbell if any */

 Wait for status to indicate completion (or error) */

 Store message into SRAM */

 we have a whole message now; check CRC */

		/*

		 * Check if it's all 1's or all 0's, that probably means

		 * the host is off

 Handle retries on CRC errors */

 Too many retries ? */

			/*

			 * Pass it up as a -EIO otherwise upper level will retry

			 * the whole command which isn't what we want here.

		/*

		 * Its necessary to clock slave before issuing

		 * d-poll, not indicated in the hardware protocol

		 * spec. < 20 clocks causes slave to hang, 21 ok.

 Pace it a bit before retry */

 Wait for logic reset to take effect */

	/*

	 * Note about byteswap setting: the bus is wired backwards,

	 * so setting the byteswap bit actually makes the ColdFire

	 * work "normally" for a BE processor, ie, put the MSB in

	 * the lowest address byte.

	 *

	 * We thus need to set the bit for our main memory which

	 * contains our program code. We create two mappings for

	 * the register, one with each setting.

	 *

	 * Segments 2 and 3 has a "swapped" mapping (BE)

	 * and 6 and 7 have a non-swapped mapping (LE) which allows

	 * us to avoid byteswapping register accesses since the

	 * registers are all LE.

 Setup segment 0 to our memory region */

 Segments 2 and 3 to sysregs with byteswap (for SRAM) */

 And segment 6 and 7 to sysregs no byteswap */

 Memory cachable, regs and SRAM not cachable */

 Setup segment 0 to our memory region */

 Segments 2 to sysregs with byteswap (for SRAM) */

 And segment 6 to sysregs no byteswap */

 Memory cachable, regs and SRAM not cachable */

 This aren't under ColdFire control, just set them up appropriately */

 Those are under ColdFire control, let it configure them */

 Get the binary */

 Which image do we want ? (shared vs. split clock/data GPIOs) */

 Try to find it */

 Check version and signature */

	/*

	 * Enable coprocessor interrupt input. I've had problems getting the

	 * value to stick, so try in a loop

 Make sure the ColdFire is stopped  */

	/*

	 * Clear SRAM. This needs to happen before we setup the GPIOs

	 * as we might start trying to arbitrate as soon as that happens.

 Configure GPIOs */

 Load the firmware into the reserved memory */

 Read signature and check versions */

 Setup coldfire memory map */

 Start the ColdFire */

	/* Wait for status register to indicate command completion

	 * which signals the initialization is complete

 Configure echo & send delay */

 Enable SW interrupt to copro if any */

 An error occurred, don't leave the coprocessor running */

 Release the GPIOs */

	/*

	 * A GPIO arbitration requestion could come in while this is

	 * happening. To avoid problems, we disable interrupts so it

	 * cannot preempt us on this CPU

 Stop the coprocessor */

 We mark the copro not-started */

	/* We mark the ARB register as having given up arbitration to

	 * deal with a potential race with the arbitration request

 Return the GPIOs to the ARM */

 Setup GPIOs for external FSI master (FSP box) */

 Note: This doesn't require holding out mutex */

 Write reqest */

	/*

	 * There is a race (which does happen at boot time) when we get an

	 * arbitration request as we are either about to or just starting

	 * the coprocessor.

	 *

	 * To handle it, we first check if we are running. If not yet we

	 * check whether the copro is started in the SCU.

	 *

	 * If it's not started, we can basically just assume we have arbitration

	 * and return. Otherwise, we wait normally expecting for the arbitration

	 * to eventually complete.

 Ring doorbell if any */

 If it failed, override anyway */

 Write release */

 Ring doorbell if any */

 Cleanup, stop coprocessor */

 Free resources */

 AST2400 vs. AST2500 */

 Grab the SCU, we'll need to access it to configure the coprocessor */

 Grab all the GPIOs we need */

 Optional GPIOs */

 Grab the reserved memory region (use DMA API instead ?) */

 AST2500 has a SW interrupt to the coprocessor */

 Grab the CVIC (ColdFire interrupts controller) */

 Grab the SRAM */

 Current microcode only deals with fixed location in SRAM */

	/*

	 * Hookup with the GPIO driver for arbitration of GPIO banks

	 * ownership.

 Default FSI command delays */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) IBM Corporation 2017

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERGCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

/*

 * The SBEFIFO is a pipe-like FSI device for communicating with

 * the self boot engine on POWER processors.

/*

 * Register layout

 Register banks */

 FSI -> Host */

 Host -> FSI */

 Per-bank registers */

 The FIFO itself */

 Status register */

 (Up only) Set End Of Transfer */

 (Up only) Reset Request */

 (Down only) Perform Reset */

 (Down only) Acknowledge EOT */

 (Down only) Max transfer */

 CFAM GP Mailbox SelfBoot Message register */

 Converted 0x2809 */

 Unkown, initial state

 IPL'ing - autonomous mode (transient)

 ISTEP - Running IPL by steps (transient)

 MPIPL

 SBE Runtime

 Dead Man Timer State (transient)

 Dumping

 Internal SBE failure

 Final state - needs SBE reset to get out

 FIFO depth */

 Helpers */

 Reset request timeout in ms */

 Timeouts for commands in ms */

 Other constants */

 "RSET" */

 "SBEF" */

	/*

	 * Primary status don't have the top bit set, so can't be confused with

	 * Linux negative error codes, so return the status word whole.

 SBE booted at all ? */

 Check its state */

 Not sure about that one */

 Is there async FFDC available ? Remember it */

 Don't flip endianness of data to/from FIFO, just pass through. */

 Mark broken first, will be cleared if reset succeeds */

 Send reset request */

 Wait for it to complete */

 If broken, we don't need to look at status, go straight to reset */

 Will try reset again on next attempt at using it */

 Will try reset again on next attempt at using it */

 The FIFO already contains a reset request from the SBE ? */

 Parity error on either FIFO ? */

 Either FIFO not empty ? */

 Mark broken, will be cleared if/when reset succeeds */

 As long as there's something to send */

 Wait for room in the FIFO */

 Write as much as we can */

 If there's no room left, wait for some to write EOT */

 Send an EOT */

 Grab FIFO status (this will handle parity errors) */

 Decode status */

 Go through the chunk */

 Read the data */

 Was it an EOT ? */

				/*

				 * There should be nothing else in the FIFO,

				 * if there is, mark broken, this will force

				 * a reset on next use, but don't fail the

				 * command.

 We are done */

				/*

				 * If that write fail, still complete the request but mark

				 * the fifo as broken for subsequent reset (not much else

				 * we can do here).

 Tell whether we overflowed */

 Store it if there is room */

 Next EOT bit */

 Shouldn't happen */

 Try sending the command */

 Now, get the response */

 First ensure the HW is in a clean state */

 Look for async FFDC first if any */

	/*

	 * On failure, attempt a reset. Ignore the result, it will mark

	 * the fifo broken if the reset fails

 Return original error */

/**

 * sbefifo_submit() - Submit and SBE fifo command and receive response

 * @dev: The sbefifo device

 * @command: The raw command data

 * @cmd_len: The command size (in 32-bit words)

 * @response: The output response buffer

 * @resp_len: In: Response buffer size, Out: Response size

 *

 * This will perform the entire operation. If the reponse buffer

 * overflows, returns -EOVERFLOW

 Prepare iov iterator */

 Perform the command */

 Extract the response length */

/*

 * Char device interface

 Cronus relies on -EAGAIN after a short read */

 Prepare iov iterator */

 Perform the command */

 Extract the response length */

 Can we use the pre-allocate buffer ? If not, allocate */

 Copy the command into the staging buffer */

 Check for the magic reset command */

 Clear out any pending command */

 Trigger reset request */

 Update the staging buffer size */

 And that's it, we'll issue the command on a read */

/*

 * Probe/remove

 Grab a reference to the device (parent of our cdev), we'll drop it later */

	/*

	 * Try cleaning up the FIFO. If this fails, we still register the

	 * driver and will try cleaning things up again on the next access.

 Create chardev for userspace access */

 Allocate a minor in the FSI space */

 Create platform devs for dts child nodes (occ, etc) */

 SPDX-License-Identifier: GPL-2.0

 Normal mode, OCB channel 2 */

 two bytes checksum */

 We allocate a 1-page buffer, make sure it all fits */

 This should not be possible ... */

 Grab how much data we have to read */

 Construct the command */

	/*

	 * Copy the user command (assume user data follows the occ command

	 * format)

	 * byte 0: command type

	 * bytes 1-2: data length (msb first)

	 * bytes 3-n: data

 Extract data length */

 Submit command; 4 bytes before the data and 2 bytes after */

 Set read tracking data */

 Done */

 SBE words are four bytes */

 Fetch the two bytes after the data for the checksum. */

 must be multiples of 8 B */

	/*

	 * Magic sequence to do SBE getsram command. SBE will fetch data from

	 * specified SRAM address.

 Normal mode */

 must be multiples of 8 B */

	/*

	 * Magic sequence to do SBE putsram command. SBE will transfer

	 * data to specified SRAM address.

 Normal mode */

	/*

	 * Overwrite the first byte with our sequence number and the last two

	 * bytes with the checksum.

 Circular mode */

 Circular mode, OCB Channel 1 */

 Chip-op length in words */

 Data length in bytes */

 Trigger OCC attention */

 Checksum the request, ignoring first byte (sequence number). */

	/*

	 * Get a sequence number and update the counter. Avoid a sequence

	 * number of 0 which would pass the response check below even if the

	 * OCC response is uninitialized. Any sequence number the user is

	 * trying to send is overwritten since this function is the only common

	 * interface to the OCC and therefore the only place we can guarantee

	 * unique sequence numbers.

 Read occ response header */

 Extract size of response data */

 Message size is data length + 5 bytes header + 2 bytes checksum */

 Grab the rest */

 already got 3 bytes resp, also need 2 bytes checksum */

 SBE words are always four bytes */

 make sure we don't have a duplicate from dts */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A FSI master controller, using a simple GPIO bit-banging interface

 Standard pin delay in nS */

 mutex for command ordering */

 Voltage translator */

 FSI enable */

 Mux control */

 Dummy read to feed the synchronizers */

 Actual data read */

 Data is active low */

 Data is active low */

 Send the start bit */

 Send the message */

 start bit, and any non-aligned top bits */

 aligned bits */

 this will also handle LAST_ADDR_INVALID */

	/* We may be in 23-bit addressing mode, which uses the id as the

	 * top two address bits. So, if we're referencing a different ID,

	 * use absolute addresses.

 remove the top two bits from any 23-bit addressing */

	/* We know that the addresses are limited to 21 bits, so this won't

/*

 * Encode an Absolute/Relative/Same Address command

 we have 21 bits of address max */

 cmd opcodes are variable length - SAME_AR is only two bits */

 we still address the byte offset within the word */

 8 bits plus sign */

	/*

	 * The read/write size is encoded in the lower bits of the address

	 * (as it must be naturally-aligned), and the following ds bit.

	 *

	 *	size	addr:1	addr:0	ds

	 *	1	x	x	0

	 *	2	x	0	1

	 *	4	0	1	1

	 *

/*

 * Note: callers rely specifically on this returning -EAGAIN for

 * a CRC error detected in the response. Use other error code

 * for other situations. It will be converted to something else

 * higher up the stack before it reaches userspace.

 wait for the start bit */

 Read slave ID & response tag */

 If we have an ACK and we're expecting data, clock the data in too */

 read CRC */

 we have a whole message now; check CRC */

 Check if it's all 1's, that probably means the host is off */

 Handle retries on CRC errors */

 Too many retries ? */

			/*

			 * Pass it up as a -EIO otherwise upper level will retry

			 * the whole command which isn't what we want here.

 clear crc & mask */

		/*

		 * Its necessary to clock slave before issuing

		 * d-poll, not indicated in the hardware protocol

		 * spec. < 20 clocks causes slave to hang, 21 ok.

	/*

	 * tSendDelay clocks, avoids signal reflections when switching

	 * from receive of response back to send of data.

 Pace it a bit before retry */

 Wait for logic reset to take effect */

 todo: evaluate if clocks can be reduced */

 Optional GPIOs */

	/*

	 * Check if GPIO block is slow enought that no extra delays

	 * are necessary. This improves performance on ast2500 by

	 * an order of magnitude.

 Default FSI command delays */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * FSI core driver

 *

 * Copyright (C) IBM Corporation 2016

 *

 * TODO:

 *  - Rework topology

 *  - s/chip_id/chip_loc

 *  - s/cfam/chip (cfam_id -> chip_id etc...)

/*

 * FSI slave engine control register offsets

 R/W: Mode register */

 R/W: Interrupt condition */

 R  : Slave status */

 W  : LBUS Ownership */

 R/W: Link layer mode register */

/*

 * SMODE fields

 Warm start done */

 Hw CRC check */

 ID shift */

 ID Mask */

 Echo delay shift */

 Echo delay mask */

 Send delay shift */

 Send delay mask */

 Clk ratio shift */

 Clk ratio mask */

/*

 * SLBUS fields

 Force LBUS ownership */

/*

 * LLMODE fields

 FSI address */

 FSI link# */

 size of slave address space */

 Legacy /dev numbering: 4 devices per chip, 16 chips */

/*

 * fsi_device_read() / fsi_device_write() / fsi_device_peek()

 *

 * FSI endpoint-device support

 *

 * Read / write / peek accessors for a client

 *

 * Parameters:

 * dev:  Structure passed to FSI client device drivers on probe().

 * addr: FSI address of given device.  Client should pass in its base address

 *       plus desired offset to access its register space.

 * val:  For read/peek this is the value read at the specified address. For

 *       write this is value to write to the specified address.

 *       The data in val must be FSI bus endian (big endian).

 * size: Size in bytes of the operation.  Sizes supported are 1, 2 and 4 bytes.

 *       Addresses must be aligned on size boundaries or an error will result.

 FSI slave support */

	/* For 23 bit addressing, we encode the extra two bits in the slave

	 * id (and the slave's actual ID needs to be 0).

 clear interrupts */

 Encode slave local bus echo delay */

 Encode slave local bus send delay */

 Encode slave local bus clock rate ratio */

 Encode slave ID */

	/* set our smode register with the slave ID field to 0; this enables

	 * extended slave addressing

	/* try a simple clear of error conditions, which may fail if we've lost

	 * communication with the slave

 send a TERM and retry */

 getting serious, reset the slave via BREAK */

 todo: check for overlapping claims */

/* Find a matching node for the slave engine at @address, using @size bytes

 * of space. Returns NULL if not found, or a matching node with refcount

 * already incremented.

	/*

	 * scan engines

	 *

	 * We keep the peek mode and slave engines for the core; so start

	 * at the third slot in the configuration table. We also need to

	 * skip the chip ID entry at the start of the address space.

		/*

		 * Unused address areas are marked by a zero type value; this

		 * skips the defined address areas

 create device */

	/* Criteria:

	 *

	 * 1. Access size must be less than or equal to the maximum access

	 *    width or the highest power-of-two factor of offset

	 * 2. Access size must be less than or equal to the amount specified by

	 *    count

	 *

	 * The access width is optimal if we can calculate 1 to be strictly

	 * equal while still satisfying 2.

 Find 1 by the bottom bit of offset (with a 4 byte access cap) */

 Find 2 by the top bit of count */

 Constrain the maximum access width to the minimum of both criteria */

	/* Ensure we have the correct format for addresses and sizes in

	 * reg properties

/* Find a matching node for the slave at (link, id). Returns NULL if none

 * found, or a matching node with refcount already incremented.

 Current HW mandates that send and echo delay are identical */

 Backward compatible /dev/ numbering in "old style" mode */

 Check if we qualify for legacy numbering */

 Try reserving the legacy number */

 Other failure */

 Fallback to non-legacy allocation */

	/* Currently, we only support single slaves on a link, and use the

	 * full 23-bit address range

	/* If we're behind a master that doesn't provide a self-running bus

	 * clock, put the slave into async mode

	/* We can communicate with a slave; create the slave device and

	 * register.

 Get chip ID if any */

 Allocate a minor in the FSI space */

 Create chardev for userspace access */

	/* Now that we have the cdev registered with the core, any fatal

	 * failures beyond this point will need to clean up through

	 * cdev_device_del(). Fortunately though, nothing past here is fatal.

 Legacy raw file -> to be removed */

 FSI master support */

/*

 * Issue a break command on this link

 FSI core & Linux bus type definitions */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SCOM FSI Client device driver

 *

 * Copyright (C) IBM Corporation 2016

 SCOM engine register set */

 Read */

 Write */

 Command register */

 Status register bits */

 SCOM address encodings */

 SCOM indirect stuff */

 Retries */

 Retries indirect not ready */

	/*

	 * Read the data registers even on error, so we don't have

	 * to interpret the status register here.

 Reset the bridge */

 Still need to find out how to get "protected" */

 Grab a reference to the device (parent of our cdev), we'll drop it later */

 Create chardev for userspace access */

 Allocate a minor in the FSI space */

 SPDX-License-Identifier: GPL-2.0-or-later

 Copyright (C) IBM Corporation 2018

 FSI master driver for AST2600

 protect HW access */

 Control register (size 0x400) */

 OPBn_STATUS */

 OPB_IRQ_MASK */

 OPB_RW */

 OPBx_XFER_SIZE */

 in mS */

 Run the bus at maximum speed by default */

	/*

	 * The ordering of these writes up until the trigger

	 * write does not matter, so use writel_relaxed.

 Return error when poll timed out */

 Command failed, master will reset */

	/*

	 * The ordering of these writes up until the trigger

	 * write does not matter, so use writel_relaxed.

 Return error when poll timed out */

 Command failed, master will reset */

 Check MAEB (0x70) ? */

 Then clear errors in master */

 TODO: log? return different code? */

 TODO: confirm that 0x70 was okay */

 This will pass through timeout errors */

 mmode encoders */

 Initialize the MFSI (hub master) engine */

 Leave enabled long enough for master logic to set up */

 Reset the master bridge */

	/*

	 * The routing GPIO is a jumper indicating we should mux for the

	 * externally connected FSI cable.

 If the routing GPIO is high we should set the mux to low. */

		/*

		 * Cable signal integrity means we should run the bus

		 * slightly slower. Do not override if a kernel param

		 * has already overridden.

 TODO: determine an appropriate value */

 Set read data order */

 Set write data order */

	/*

	 * Select OPB0 for all operations.

	 * Will need to be reworked when enabling DMA or anything that uses

	 * OPB1.

	/* At this point, fsi_master_register performs the device_initialize(),

	 * and holds the sole reference on master.dev. This means the device

	 * will be freed (via ->release) during any subsequent call to

	 * fsi_master_unregister.  We add our own reference to it here, so we

	 * can perform cleanup (in _remove()) without it being freed before

	 * we're ready.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * FSI hub master driver

 *

 * Copyright (C) IBM Corporation 2016

 in mS */

/*

 * FSI hub master support

 *

 * A hub master increases the number of potential target devices that the

 * primary FSI master can access. For each link a primary master supports,

 * each of those links can in turn be chained to a hub master with multiple

 * links of its own.

 *

 * The hub is controlled by a set of control registers exposed as a regular fsi

 * device (the hub->upstream device), and provides access to the downstream FSI

 * bus as through an address range on the slave itself (->addr and ->size).

 *

 * [This differs from "cascaded" masters, which expose the entire downstream

 * bus entirely through the fsi device address range, and so have a smaller

 * accessible address space.]

 slave-relative addr of */

 master address space */

 mmode encoders */

 Initialize the MFSI (hub master) engine */

 Leave enabled long enough for master logic to set up */

 Reset the master bridge */

	/* At this point, fsi_master_register performs the device_initialize(),

	 * and holds the sole reference on master.dev. This means the device

	 * will be freed (via ->release) during any subsequent call to

	 * fsi_master_unregister.  We add our own reference to it here, so we

	 * can perform cleanup (in _remove()) without it being freed before

	 * we're ready.

	/*

	 * master.dev will likely be ->release()ed after this, which free()s

	 * the hub

/*

 * Copyright (C) 2017 Intel Corporation.

 *

 * This file is released under the GPL.

/*

 * Contruct an unstriped mapping.

 * <number of stripes> <chunk size> <stripe #> <dev_path> <offset>

 Shift us up to the right "row" on the stripe */

 Account for what stripe we're operating on */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 Arrikto, Inc. All Rights Reserved.

/*

 * Minimum and maximum allowed region sizes

 4KB */

 1GB */

 Size of hydration mempool */

 1 region */

 Hydrate in batches of 1 region */

 1 sec */

/*

 * Hydration hash table size: 1 << HASH_TABLE_BITS

 Slab cache for struct dm_clone_region_hydration */

 dm-clone metadata modes */

 metadata may be changed */

 metadata may not be changed */

 all metadata I/O fails */

	/*

	 * A metadata commit and the actions taken in case it fails should run

	 * as a single atomic step.

 Region hydration hash table */

	/*

	 * We defer incoming WRITE bios for regions that are not hydrated,

	 * until after these regions have been hydrated.

	 *

	 * Also, we defer REQ_FUA and REQ_PREFLUSH bios, until after the

	 * metadata have been committed.

 Maximum number of regions being copied during background hydration. */

 Number of regions to batch together during background hydration. */

 Which region to hydrate next */

	/*

	 * Save a copy of the table line rather than reconstructing it for the

	 * status.

/*

 * dm-clone flags

---------------------------------------------------------------------------*/

/*

 * Metadata failure handling.

 Never move out of fail mode */

 Reload the on-disk bitset */

	/*

	 * dm_clone_reload_in_core_bitset() may run concurrently with either

	 * dm_clone_set_region_hydrated() or dm_clone_cond_set_range(), but

	 * it's safe as we have already set the metadata to read-only mode.

---------------------------------------------------------------------------*/

 Wake up anyone waiting for region hydrations to stop */

---------------------------------------------------------------------------*/

/*

 * bio helper functions.

 Get the address of the region in sectors */

 Get the region number of the bio */

 Get the region range covered by the bio */

 Check whether a bio overwrites a region */

/*

 * Submit bio to the underlying device.

 *

 * If the bio triggers a commit, delay it, until after the metadata have been

 * committed.

 *

 * NOTE: The bio remapping must be performed by the caller.

	/*

	 * If the metadata mode is RO or FAIL we won't be able to commit the

	 * metadata, so we complete the bio with an error.

	/*

	 * Batch together any bios that trigger commits and then issue a single

	 * commit for them in process_deferred_flush_bios().

/*

 * Remap bio to the destination device and submit it.

 *

 * If the bio triggers a commit, delay it, until after the metadata have been

 * committed.

/*

 * Issue bios that have been deferred until after their region has finished

 * hydrating.

 *

 * We delegate the bio submission to the worker thread, so this is safe to call

 * from interrupt context.

	/*

	 * If the bio has the REQ_FUA flag set we must commit the metadata

	 * before signaling its completion.

	 *

	 * complete_overwrite_bio() is only called by hydration_complete(),

	 * after having successfully updated the metadata. This means we don't

	 * need to call dm_clone_changed_this_transaction() to check if the

	 * metadata has changed and thus we can avoid taking the metadata spin

	 * lock.

	/*

	 * If the metadata mode is RO or FAIL we won't be able to commit the

	 * metadata, so we complete the bio with an error.

	/*

	 * Batch together any bios that trigger commits and then issue a single

	 * commit for them in process_deferred_flush_bios().

	/*

	 * If the destination device supports discards, remap and trim the

	 * discard bio and pass it down. Otherwise complete the bio

	 * immediately.

	/*

	 * The covered regions are already hydrated so we just need to pass

	 * down the discard.

	/*

	 * If the metadata mode is RO or FAIL we won't be able to update the

	 * metadata for the regions covered by the discard so we just ignore

	 * it.

	/*

	 * Defer discard processing.

---------------------------------------------------------------------------*/

/*

 * dm-clone region hydrations.

 Used by hydration batching */

 Used by hydration hash table */

/*

 * Hydration hash table implementation.

 *

 * Ideally we would like to use list_bl, which uses bit spin locks and employs

 * the least significant bit of the list head to lock the corresponding bucket,

 * reducing the memory overhead for the locks. But, currently, list_bl and bit

 * spin locks don't support IRQ safe versions. Since we have to take the lock

 * in both process and interrupt context, we must fall back to using regular

 * spin locks; one per hash table bucket.

 Spinlock protecting the bucket */

/*

 * Search hash table for a hydration with hd->region_nr == region_nr

 *

 * NOTE: Must be called with the bucket lock held

/*

 * Insert a hydration into the hash table.

 *

 * NOTE: Must be called with the bucket lock held.

/*

 * This function inserts a hydration into the hash table, unless someone else

 * managed to insert a hydration for the same region first. In the latter case

 * it returns the existing hydration descriptor for this region.

 *

 * NOTE: Must be called with the hydration hash table lock held.

---------------------------------------------------------------------------*/

 Allocate a hydration */

	/*

	 * Allocate a hydration from the hydration mempool.

	 * This might block but it can't fail.

 Initialize a hydration */

---------------------------------------------------------------------------*/

/*

 * Update dm-clone's metadata after a region has finished hydrating and remove

 * hydration from the hash table.

 Update the metadata */

 Remove hydration from hash table */

/*

 * Complete a region's hydration:

 *

 *	1. Update dm-clone's metadata.

 *	2. Remove hydration from hash table.

 *	3. Complete overwrite bio.

 *	4. Issue deferred bios.

 *	5. If this was the last hydration, wake up anyone waiting for

 *	   hydrations to finish.

 Complete batched hydrations */

 Continue background hydration, if there is no I/O in-flight */

		/*

		 * The last region of the target might be smaller than

		 * region_size.

 Issue copy */

	/*

	 * We don't need to save and restore bio->bi_private because device

	 * mapper core generates a new bio for us to use, with clean

	 * bi_private.

/*

 * Hydrate bio's region.

 *

 * This function starts the hydration of the bio's region and puts the bio in

 * the list of deferred bios for this region. In case, by the time this

 * function is called, the region has finished hydrating it's submitted to the

 * destination device.

 *

 * NOTE: The bio remapping must be performed by the caller.

 Someone else is hydrating the region */

 The region has been hydrated */

	/*

	 * We must allocate a hydration descriptor and start the hydration of

	 * the corresponding region.

 Check if the region has been hydrated in the meantime. */

 Someone else started the region's hydration. */

	/*

	 * If the metadata mode is RO or FAIL then there is no point starting a

	 * hydration, since we will not be able to update the metadata when the

	 * hydration finishes.

	/*

	 * Start region hydration.

	 *

	 * If a bio overwrites a region, i.e., its size is equal to the

	 * region's size, then we don't need to copy the region from the source

	 * to the destination device.

---------------------------------------------------------------------------*/

/*

 * Background hydrations.

/*

 * Batch region hydrations.

 *

 * To better utilize device bandwidth we batch together the hydration of

 * adjacent regions. This allows us to use small region sizes, e.g., 4KB, which

 * is good for small, random write performance (because of the overwriting of

 * un-hydrated regions) and at the same time issue big copy requests to kcopyd

 * to achieve high hydration bandwidth.

 Try to extend the current batch */

 Check if we should issue the current batch */

 We treat max batch sizes of zero and one equivalently */

 Start a new batch */

 Try to find a region to hydrate. */

 Batch hydration */

/*

 * This function searches for regions that still reside in the source device

 * and starts their hydration.

	/*

	 * Avoid race with device suspension.

	/*

	 * Make sure atomic_inc() is ordered before test_bit(), otherwise we

	 * might race with clone_postsuspend() and start a region hydration

	 * after the target has been suspended.

	 *

	 * This is paired with the smp_mb__after_atomic() in

	 * clone_postsuspend().

---------------------------------------------------------------------------*/

/*

 * A non-zero return indicates read-only or fail mode.

 Update the metadata */

		/*

		 * A discard request might cover regions that have been already

		 * hydrated. There is no need to update the metadata for these

		 * regions.

	/*

	 * If there are any deferred flush bios, we must commit the metadata

	 * before issuing them or signaling their completion.

			/* We just flushed the destination device as part of

			 * the metadata commit, so there is no reason to send

			 * another flush.

	/*

	 * process_deferred_flush_bios():

	 *

	 *   - Commit metadata

	 *

	 *   - Process deferred REQ_FUA completions

	 *

	 *   - Process deferred REQ_PREFLUSH bios

 Background hydration */

/*

 * Commit periodically so that not too much unwritten data builds up.

 *

 * Also, restart background hydration, if it has been stopped by in-flight I/O.

---------------------------------------------------------------------------*/

/*

 * Target methods

	/*

	 * REQ_PREFLUSH bios carry no data:

	 *

	 * - Commit metadata, if changed

	 *

	 * - Pass down to destination device

	/*

	 * dm-clone interprets discards and performs a fast hydration of the

	 * discarded regions, i.e., we skip the copy from the source device and

	 * just mark the regions as hydrated.

	/*

	 * If the bio's region is hydrated, redirect it to the destination

	 * device.

	 *

	 * If the region is not hydrated and the bio is a READ, redirect it to

	 * the source device.

	 *

	 * Else, defer WRITE bio until after its region has been hydrated and

	 * start the region's hydration immediately.

/*

 * Status format:

 *

 * <metadata block size> <#used metadata blocks>/<#total metadata blocks>

 * <clone region size> <#hydrated regions>/<#total regions> <#hydrating regions>

 * <#features> <features>* <#core args> <core args>* <clone metadata mode>

 Commit to ensure statistics aren't out-of-date */

---------------------------------------------------------------------------*/

/*

 * Construct a clone device mapping:

 *

 * clone <metadata dev> <destination dev> <source dev> <region size>

 *	[<#feature args> [<feature arg>]* [<#core args> [key value]*]]

 *

 * metadata dev: Fast device holding the persistent metadata

 * destination dev: The destination device, which will become a clone of the

 *                  source device

 * source dev: The read-only source device that gets cloned

 * region size: dm-clone unit size in sectors

 *

 * #feature args: Number of feature arguments passed

 * feature args: E.g. no_hydration, no_discard_passdown

 *

 * #core arguments: An even number of core arguments

 * core arguments: Key/value pairs for tuning the core

 *		   E.g. 'hydration_threshold 256'

 No feature arguments supplied */

 Initialize core arguments */

 No core arguments supplied */

 Check region size is a power of 2 */

 Validate the region size against the device logical block size */

	/*

	 * dm_bitset restricts us to 2^32 regions. test_bit & co. restrict us

	 * further to 2^31 regions.

 Initialize dm-clone flags */

 Check for overflow */

 Load metadata */

 Allocate hydration hash table */

 Save a copy of the table line */

 Enable flushes */

 Enable discards */

---------------------------------------------------------------------------*/

	/*

	 * To successfully suspend the device:

	 *

	 *	- We cancel the delayed work for periodic commits and wait for

	 *	  it to finish.

	 *

	 *	- We stop the background hydration, i.e. we prevent new region

	 *	  hydrations from starting.

	 *

	 *	- We wait for any in-flight hydrations to finish.

	 *

	 *	- We flush the workqueue.

	 *

	 *	- We commit the metadata.

	/*

	 * Make sure set_bit() is ordered before atomic_read(), otherwise we

	 * might race with do_hydration() and miss some started region

	 * hydrations.

	 *

	 * This is paired with smp_mb__after_atomic() in do_hydration().

/*

 * If discard_passdown was enabled verify that the destination device supports

 * discards. Disable discard_passdown if not.

 No passdown is done so we set our own virtual limits */

	/*

	 * clone_iterate_devices() is stacking both the source and destination

	 * device limits but discards aren't passed to the source device, so

	 * inherit destination's limits.

	/*

	 * If the system-determined stacked limits are compatible with

	 * dm-clone's region size (io_opt is a factor) do not override them.

/*

 * dm-clone message functions.

	/*

	 * If user space sets hydration_threshold to zero then the hydration

	 * will stop. If at a later time the hydration_threshold is increased

	 * we must restart the hydration process by waking up the worker.

---------------------------------------------------------------------------*/

 Module functions */

 Module hooks */

/*

 * Copyright (C) 2017 Red Hat. All rights reserved.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

 already present */

		/*

		 * There was a race, we'll just ignore this second

		 * bit of work for the same oblock.

/*

 * Returns -ENODATA if there's no work.

----------------------------------------------------------------*/

/*

 * Copyright (C) 2001 Sistina Software (UK) Limited.

 * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

/*

 * Similar to ceiling(log_size(n))

/*

 * Calculate the index of the child node of the n'th node k'th key.

/*

 * Return the n'th node of level l from table t.

/*

 * Return the highest key that you could lookup from the n'th

 * node on level l of the btree.

/*

 * Fills in a level of the btree based on the highs of the level

 * below it.

/*

 * highs, and targets are managed as dynamic arrays during a

 * table load.

	/*

	 * Allocate both the target array and offset array at once.

 free the indexes */

 free the targets */

 free the device list */

/*

 * See if we've already got a device in the list.

/*

 * If possible, this checks an area of a destination device is invalid.

	/*

	 * If the target is mapped to zoned block device(s), check

	 * that the zones are not partially mapped.

		/*

		 * Note: The last zone of a zoned block device may be smaller

		 * than other zones. So for a target mapping the end of a

		 * zoned block device with such a zone, len would not be zone

		 * aligned. We do not allow such last smaller zone to be part

		 * of the mapping here to ensure that mappings with multiple

		 * devices do not end up with a smaller zone in the middle of

		 * the sector range.

/*

 * This upgrades the mode on an already open dm_dev, being

 * careful to leave things as they were if we fail to reopen the

 * device and not to touch the existing bdev field in case

 * it is accessed concurrently.

/*

 * Convert the path to a device

/*

 * Add a device to the list, or just increment the usage count if

 * it's already present.

 Extract the major/minor numbers */

/*

 * Decrement a device's use count and remove it if necessary.

/*

 * Checks to see if the target joins onto the end of the table.

/*

 * Used to dynamically allocate the arg array.

 *

 * We do first allocation with GFP_NOIO because dm-mpath and dm-thin must

 * process messages even if some device is suspended. These messages have a

 * small fixed number of arguments.

 *

 * On the other hand, dm-switch needs to process bulk data using messages and

 * excessive use of GFP_NOIO could cause trouble.

/*

 * Destructively splits up the argument list to pass to ctr.

 Skip whitespace */

 success, we hit the end */

 'out' is used to remove any back-quotes */

 Everything apart from '\0' can be quoted */

 end of token */

 have we already filled the array ? */

 we know this is whitespace */

 terminate the string and put it in the array */

/*

 * Impose necessary and sufficient conditions on a devices's table such

 * that any incoming bio which respects its logical_block_size can be

 * processed successfully.  If it falls across the boundary between

 * two or more targets, the size of each piece it gets split into must

 * be compatible with the logical_block_size of the target processing it.

	/*

	 * This function uses arithmetic modulo the logical_block_size

	 * (in units of 512-byte sectors).

	/*

	 * Offset of the start of the next table entry, mod logical_block_size.

	/*

	 * Given an aligned bio that extends beyond the end of a

	 * target, how many sectors must the next target handle?

	/*

	 * Check each entry in the table in turn.

 combine all target devices' limits */

		/*

		 * If the remaining sectors fall entirely within this

		 * table entry are they compatible with its logical_block_size?

 Error */

	/*

	 * Does this target adjoin the previous one ?

/*

 * Target argument parsing helpers.

 validate the dax capability of the target device span */

 Check devices support synchronous DAX */

 Ensure that all targets support DAX. */

 request-based cannot stack on partitions! */

 target already set the table's type */

 possibly upgrade to a variant of bio-based */

		/*

		 * The targets can work either way.

		 * Determine the type from the live device.

		 * Default to bio-based if device is new.

 We must use this table as bio-based */

 No targets in this table */

	/*

	 * Request-based dm supports only tables that have a single target now.

	 * To support multiple targets, request splitting support is needed,

	 * and that needs lots of changes in the block-layer.

	 * (e.g. request completion process for partial completion.)

 inherit live table's type */

 Non-request-stackable devices can't be used for request-based dm */

 Immutable target is implicitly a singleton */

 allocate the space for *all* the indexes */

 set up internal nodes, bottom-up */

/*

 * Builds the btree to index the map.

 how many indexes will the btree have ? */

 leaf layer has already been set up */

/*

 * Get a disk whose integrity profile reflects the table's profile.

 * Returns NULL if integrity support was inconsistent or unavailable.

/*

 * Register the mapped device for blk_integrity support if the

 * underlying devices have an integrity profile.  But all devices may

 * not have matching profiles (checking all devices isn't reliable

 * during table load because this table may use other DM device(s) which

 * must be resumed before they will have an initialized integity

 * profile).  Consequently, stacked DM devices force a 2 stage integrity

 * profile validation: First pass during table load, final pass during

 * resume.

 If target handles integrity itself do not register it here. */

		/*

		 * Register integrity profile during table load; we can do

		 * this because the final profile must match during resume.

	/*

	 * If DM device already has an initialized integrity

	 * profile the new profile should not conflict.

 Preserve existing integrity profile */

 Always try to evict the key from all devices. */

/*

 * When an inline encryption key is evicted from a device-mapper device, evict

 * it from all the underlying devices.

/*

 * Constructs and initializes t->crypto_profile with a crypto profile that

 * represents the common set of crypto capabilities of the devices described by

 * the dm_table.  However, if the constructed crypto profile doesn't support all

 * crypto capabilities that are supported by the current mapped_device, it

 * returns an error instead, since we don't support removing crypto capabilities

 * on table changes.  Finally, if the constructed crypto profile is "empty" (has

 * no crypto capabilities at all), it just sets t->crypto_profile to NULL.

	/*

	 * If the new profile doesn't actually support any crypto capabilities,

	 * we may as well represent it with a NULL profile.

	/*

	 * t->crypto_profile is only set temporarily while the table is being

	 * set up, and it gets set to NULL after the profile has been

	 * transferred to the request_queue.

 Make the crypto profile less restrictive. */

 CONFIG_BLK_INLINE_ENCRYPTION */

 !CONFIG_BLK_INLINE_ENCRYPTION */

/*

 * Prepares the table for use by building the indices,

 * setting the type, and allocating mempools.

/*

 * Search the btree for the correct target.

 *

 * Caller should check returned pointer for NULL

 * to trap I/O beyond end of device.

/*

 * type->iterate_devices() should be called when the sanity check needs to

 * iterate and check all underlying data devices. iterate_devices() will

 * iterate all underlying data devices until it encounters a non-zero return

 * code, returned by whether the input iterate_devices_callout_fn, or

 * iterate_devices() itself internally.

 *

 * For some target type (e.g. dm-stripe), one call of iterate_devices() may

 * iterate multiple underlying devices internally, in which case a non-zero

 * return code returned by iterate_devices_callout_fn will stop the iteration

 * in advance.

 *

 * Cases requiring _any_ underlying device supporting some kind of attribute,

 * should use the iteration structure like dm_table_any_dev_attr(), or call

 * it directly. @func should handle semantics of positive examples, e.g.

 * capable of something.

 *

 * Cases requiring _all_ underlying devices supporting some kind of attribute,

 * should use the iteration structure like dm_table_supports_nowait() or

 * dm_table_supports_discards(). Or introduce dm_table_all_devs_attr() that

 * uses an @anti_func that handle semantics of counter examples, e.g. not

 * capable of something. So: return !dm_table_any_dev_attr(t, anti_func, data);

/*

 * Check whether a table has no data devices attached using each

 * target's iterate_devices method.

 * Returns false if the result is unknown because a target doesn't

 * support iterate_devices.

/*

 * Check the device zoned model based on the target feature flag. If the target

 * has the DM_TARGET_ZONED_HM feature flag set, host-managed zoned devices are

 * also accepted but all devices must have the same zoned model. If the target

 * has the DM_TARGET_MIXED_ZONED_MODEL feature set, the devices can have any

 * zoned model with all zoned devices having the same zone size.

/*

 * Check consistency of zoned model and zone sectors across all targets. For

 * zone sectors, if the destination device is a zoned block device, it shall

 * have the specified zone_sectors.

 Check zone size validity and compatibility */

/*

 * Establish the new table's queue_limits and validate them.

		/*

		 * Combine queue limits of all the devices this target uses.

			/*

			 * After stacking all limits, validate all devices

			 * in table support this zoned model and zone sectors.

 Set I/O hints portion of queue limits */

		/*

		 * Check each device area is consistent with the target's

		 * overall queue limits.

		/*

		 * Merge this target's queue limits into the overall limits

		 * for the table.

	/*

	 * Verify that the zoned model and zone sectors, as determined before

	 * any .io_hints override, are the same across all devices in the table.

	 * - this is especially relevant if .io_hints is emulating a disk-managed

	 *   zoned model (aka BLK_ZONED_NONE) on host-managed zoned block devices.

	 * BUT...

		/*

		 * ...IF the above limits stacking determined a zoned model

		 * validate that all of the table's devices conform to it.

/*

 * Verify that all devices have an integrity profile that matches the

 * DM device's registered integrity profile.  If the profiles don't

 * match then unregister the DM device's integrity profile.

		/*

		 * Verify that the original integrity profile

		 * matches all the devices in this table.

	/*

	 * Require at least one underlying device to support flushes.

	 * t->devices includes internal dm devices such as mirror logs

	 * so we need to use iterate_devices here, which targets

	 * supporting flushes must provide.

		/*

		 * Either the target provides discard support (as implied by setting

		 * 'discards_supported') or it relies on _all_ data devices having

		 * discard support.

	/*

	 * Copy table's limits to the DM device's request_queue

 Must also clear discard limits... */

 Ensure that all underlying devices are non-rotational. */

	/*

	 * Some devices don't use blk_integrity but still want stable pages

	 * because they do their own checksumming.

	 * If any underlying device requires stable pages, a table must require

	 * them as well.  Only targets that support iterate_devices are considered:

	 * don't want error, zero, etc to require stable pages.

	/*

	 * Determine whether or not this queue's I/O timings contribute

	 * to the entropy pool, Only request-based targets use this.

	 * Clear QUEUE_FLAG_ADD_RANDOM if any underlying device does not

	 * have it set.

	/*

	 * For a zoned target, setup the zones related queue attributes

	 * and resources necessary for zone append emulation if necessary.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Microsoft Corporation.

 *

 * Author:  Jaskaran Singh Khurana <jaskarankhurana@linux.microsoft.com>

 *

/*

 * verify_verify_roothash - Verify the root hash of the verity hash device

 *			     using builtin trusted keys.

 *

 * @root_hash: For verity, the roothash/data to be verified.

 * @root_hash_len: Size of the roothash/data to be verified.

 * @sig_data: The trusted signature that verifies the roothash/data.

 * @sig_len: Size of the signature.

 *

/*

 * Copyright (C) 2016 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

/*

 * One of these is allocated per request.

/*

 * Request-based DM's mempools' reserved IOs set by the user.

/*

 * Partial completion handling for request-based dm

		/*

		 * An error has already been detected on the request.

		 * Once error occurred, just let clone->end_io() handle

		 * the remainder.

		/*

		 * Don't notice the error to the upper layer yet.

		 * The error handling decision is made by the target driver,

		 * when the request is completed.

	/*

	 * I/O for the bio successfully completed.

	 * Notice the data completion to the upper layer.

	/*

	 * Update the original request.

	 * Do not use blk_mq_end_request() here, because it may complete

	 * the original request before the clone, and break the ordering.

/*

 * Don't touch any member of the md after calling this function because

 * the md may be freed in dm_put() at the end of this function.

 * Or do dm_get() before calling this function and dm_put() later.

	/*

	 * dm_put() must be at the end of this function. See the comment above

/*

 * Complete the clone and the original request.

 * Must be called without clone's queue lock held,

 * see end_clone_request() for more details.

 The target wants to complete the I/O */

 The target will handle the I/O */

 The target wants to requeue the I/O */

 The target wants to requeue the I/O after a delay */

/*

 * Request completion handler for request-based dm

/*

 * Complete the clone and the original request with the error status

 * through softirq context.

/*

 * Complete the not-mapped clone and the original request with the error status

 * through softirq context.

 * Target's rq_end_io() function isn't called.

 * This may be used when the target's clone_and_map_rq() function fails.

 must complete clone in terms of original request */

	/*

	 * Avoid initializing info for blk-mq; it passes

	 * target-specific data through info.ptr

	 * (see: dm_mq_init_request)

/*

 * Returns:

 * DM_MAPIO_*       : the request has been processed as indicated

 * DM_MAPIO_REQUEUE : the original request needs to be immediately requeued

 * < 0              : the request was completed due to failure

 The target has taken the I/O to submit by itself later */

 -ENOMEM */

 The target has remapped the I/O so dispatch it */

 The target wants to requeue the I/O */

 The target wants to requeue the I/O after a delay */

 The target wants to complete the I/O */

 DEPRECATED: previously used for request-based merge heuristic in dm_request_fn() */

	/*

	 * Hold the md reference here for the in-flight I/O.

	 * We can't rely on the reference count by device opener,

	 * because the device may be closed during the request completion

	 * when all bios are completed.

	 * See the comment in rq_completed() too.

	/*

	 * Must initialize md member of tio, otherwise it won't

	 * be available in dm_mq_queue_rq.

 target-specific per-io data is immediately after the tio */

	/*

	 * blk-mq's unquiesce may come from outside events, such as

	 * elevator switch, updating nr_requests or others, and request may

	 * come during suspend, so simply ask for blk-mq to requeue it.

 Init tio using md established in .init_request */

	/*

	 * Establish tio->ti before calling map_request().

 Direct call is fine since .queue_rq allows allocations */

 Undo dm_start_request() before requeuing */

 any target-specific per-io data is immediately after the tio */

 Unused, but preserved for userspace compatibility */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 Red Hat, Inc.

 *

 * This is a test "dust" device, which fails reads on specified

 * sectors, emulating the behavior of a hard disk drive sending

 * a "Read Medium Error" sense.

 *

/*

 * Target parameters:

 *

 * <device_path> <offset> <blksz>

 *

 * device_path: path to the block device

 * offset: offset to data area from start of device_path

 * blksz: block size (minimum 512, maximum 1073741824, must be a power of 2)

	/*

	 * Whether to fail a read on a "bad" block.

	 * Defaults to false; enabled later by message.

	/*

	 * Initialize bad block list rbtree.

	/*

	 * Only pass ioctls through if the device sizes match exactly.

/*

 * Copyright (C) 2003 Sistina Software

 * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the LGPL.

/*

 * get_type

 * @type_name

 *

 * Attempt to retrieve the dm_dirty_log_type by name.  If not already

 * available, attempt to load the appropriate module.

 *

 * Log modules are named "dm-log-" followed by the 'type_name'.

 * Modules may contain multiple types.

 * This function will first try the module "dm-log-<type_name>",

 * then truncate 'type_name' on the last '-' and try again.

 *

 * For example, if type_name was "clustered-disk", it would search

 * 'dm-log-clustered-disk' then 'dm-log-clustered'.

 *

 * Returns: dirty_log_type* on success, NULL on failure

/*-----------------------------------------------------------------

 * Persistent and core logs share a lot of their implementation.

 * FIXME: need a reload method to be called from a resume

/*

 * Magic for persistent mirrors: "MiRr"

/*

 * The on-disk version of the metadata.

	/*

	 * Simple, incrementing version. no backward

	 * compatibility.

 FIXME: this seems excessive */

 Resync flag */

 Synchronize if necessary */

 Devices known to be already in sync */

 Force a sync to happen */

	/*

	 * Disk log fields

/*

 * The touched member needs to be updated every time we access

 * one of the bitsets.

/*----------------------------------------------------------------

 * Header IO

 New log required? */

/*----------------------------------------------------------------

 * core log constructor/destructor

 *

 * argv contains region_size followed optionally by [no]sync

	/*

	 * Work out how many "unsigned long"s we need to hold the bitset.

	/*

	 * Disk log?

		/*

		 * Buffer holds both header and bitset.

/*----------------------------------------------------------------

 * disk log constructor/destructor

 *

 * argv contains log_device region_size followed optionally by [no]sync

 read the disk header */

		/*

		 * If the log device cannot be read, we must assume

		 * all regions are out-of-sync.  If we simply return

		 * here, the state will be uninitialized and could

		 * lead us to return 'in-sync' status for regions

		 * that are actually 'out-of-sync'.

 set or clear any new bits -- device has grown */

 FIXME: amazingly inefficient */

 FIXME: amazingly inefficient */

 clear any old bits -- device has shrunk */

 copy clean across to sync */

 set the correct number of regions in the header */

 write the new header */

 no op */

 only write if the log has changed */

		/*

		 * At this point it is impossible to determine which

		 * regions are clean and which are dirty (without

		 * re-reading the log off disk). So mark all of them

		 * dirty.

/*

 * Copyright (C) 2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

 Purely for userspace compatibility */

/*

 * Initialize kobj

 * because nobody using md yet, no need to call explicit dm_get/put

/*

 * Remove kobj, called after all references removed

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Red Hat, Inc.

 *

 * Author: Mikulas Patocka <mpatocka@redhat.com>

 *

 * Based on Chromium dm-verity driver (C) 2011 The Chromium OS Authors

 *

 * In the file "/sys/module/dm_verity/parameters/prefetch_cluster" you can set

 * default prefetch value. Data are read in "prefetch_cluster" chunks from the

 * hash device. Setting this greatly improves performance when data and hash

 * are on the same disk on different partitions on devices with poor random

 * access behavior.

/*

 * Auxiliary structure appended to each dm-bufio buffer. If the value

 * hash_verified is nonzero, hash of the block has been verified.

 *

 * The variable hash_verified is set to 0 when allocating the buffer, then

 * it can be changed to 1 and it is never reset to 0 again.

 *

 * There is no lock around this value, a race condition can at worst cause

 * that multiple processes verify the hash of the same buffer simultaneously

 * and write 1 to hash_verified simultaneously.

 * This condition is harmless, so we don't need locking.

/*

 * Initialize struct buffer_aux for a freshly created buffer.

/*

 * Translate input sector number to the sector number on the target device.

/*

 * Return hash position of a specified block at a specified tree level

 * (0 is the lowest level).

 * The lowest "hash_per_block_bits"-bits of the result denote hash position

 * inside a hash block. The remaining bits denote location of the hash block.

/*

 * Wrapper for crypto_ahash_init, which handles verity salting.

/*

 * Handle verification errors.

 Corruption should be visible in device status in all modes */

/*

 * Verify hash of a metadata block pertaining to the specified data block

 * ("block" argument) at a specified level ("level" argument).

 *

 * On successful return, verity_io_want_digest(v, io) contains the hash value

 * for a lower tree level or for the data block (if we're at the lowest level).

 *

 * If "skip_unverified" is true, unverified buffer is skipped and 1 is returned.

 * If "skip_unverified" is false, unverified buffer is hashed and verified

 * against current value of verity_io_want_digest(v, io).

/*

 * Find a hash for a given block, write it to digest and verify the integrity

 * of the hash tree if necessary.

		/*

		 * First, we try to get the requested hash for

		 * the current block. If the hash block itself is

		 * verified, zero is returned. If it isn't, this

		 * function returns 1 and we fall back to whole

		 * chain verification.

/*

 * Calculates the digest for the given bio

		/*

		 * Operating on a single page at a time looks suboptimal

		 * until you consider the typical block size is 4,096B.

		 * Going through this loops twice should be very rare.

/*

 * Calls function process for 1 << v->data_dev_block_bits bytes in the bio_vec

 * starting from iter.

/*

 * Moves the bio iter one data block forward.

/*

 * Verify one "dm_verity_io" structure.

			/*

			 * If we expect a zero block, don't validate, just

			 * return zeros.

				/*

				 * Error correction failed; Just return error

/*

 * Skip verity work in response to I/O error when system is shutting down.

/*

 * End one "io" structure with a given error.

/*

 * Prefetch buffers for the specified io.

 * The root buffer is not prefetched, it is assumed that it will be cached

 * all the time.

/*

 * Bio map function. It allocates dm_verity_io structure and bio vector and

 * fills them. Then it issues prefetches and the I/O.

/*

 * Status: V (valid) or C (corruption found)

 the bitset can only handle INT_MAX blocks */

 verity_dtr will free zero_digest */

/*

 * Target parameters:

 *	<version>	The current format is version 1.

 *			Vsn 0 is compatible with original Chromium OS releases.

 *	<data device>

 *	<hash device>

 *	<data block size>

 *	<hash block size>

 *	<the number of data blocks>

 *	<hash start block>

 *	<algorithm>

 *	<digest>

 *	<salt>		Hex string or "-" if no salt.

	/*

	 * dm-verity performance can vary greatly depending on which hash

	 * algorithm implementation is used.  Help people debug performance

	 * problems by logging the ->cra_driver_name.

 Optional parameters */

 Root hash signature is  a optional parameter*/

 WQ_UNBOUND greatly improves performance when running on ramdisk */

/*

 * Copyright (C) 2006-2009 Red Hat, Inc.

 *

 * This file is released under the LGPL.

/*

 * Netlink/Connector is an unreliable protocol.  How long should

 * we wait for a response before assuming it was lost and retrying?

 * (If we do receive a response after this time, it will be discarded

 * and the response to the resent request will be waited for.

/*

 * Pre-allocated space for speed

/*

 * Parameters for this function can be either msg or tfr, but not

 * both.  This function fills in the reply for a waiting request.

 * If just msg is given, then the reply is simply an ACK from userspace

 * that the request was received.

 *

 * Returns: 0 on success, -ENOENT on failure

	/*

	 * The 'receiving_pkg' entries in this list are statically

	 * allocated on the stack in 'dm_consult_userspace'.

	 * Each process that is waiting for a reply from the user

	 * space server will have an entry in this list.

	 *

	 * We are safe to do it this way because the stack space

	 * is unique to each process, but still addressable by

	 * other processes.

			/*

			 * If we are trying again, we will need to know our

			 * storage capacity.  Otherwise, along with the

			 * error code, we make explicit that we have no data.

/*

 * This is the connector callback that delivers data

 * that was sent from userspace.

/**

 * dm_consult_userspace

 * @uuid: log's universal unique identifier (must be DM_UUID_LEN in size)

 * @luid: log's local unique identifier

 * @request_type:  found in include/linux/dm-log-userspace.h

 * @data: data to tx to the server

 * @data_size: size of data in bytes

 * @rdata: place to put return data from server

 * @rdata_size: value-result (amount of space given/amount of space used)

 *

 * rdata_size is undefined on failure.

 *

 * Memory used to communicate with userspace is zero'ed

 * before populating to ensure that no unwanted bits leak

 * from kernel space to user-space.  All userspace log communications

 * between kernel and user space go through this function.

 *

 * Returns: 0 on success, -EXXX on failure

	/*

	 * Given the space needed to hold the 'struct cn_msg' and

	 * 'struct dm_ulog_request' - do we have enough payload

	 * space remaining?

	/*

	 * We serialize the sending of requests so we can

	 * use the preallocated space.

	/*

	 * Must be valid request type (all other bits set to

	 * zero).  This reserves other bits for possible future

	 * use.

/*

 * Copyright (C) 2001-2003 Sistina Software (UK) Limited.

 *

 * This file is released under the GPL.

/*

 * Linear: maps a linear range of a device.

/*

 * Construct a linear mapping: <dev_path> <offset>

	/*

	 * Only pass ioctls through if the device sizes match exactly.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Western Digital Corporation or its affiliates.

/*

 * For internal zone reports bypassing the top BIO submission path.

/*

 * User facing dm device block device report zone operation. This calls the

 * report_zones operation for each target of a device table. This operation is

 * generally implemented by targets using dm_report_zones().

	/*

	 * Ignore zones beyond the target range.

	/*

	 * Remap the start sector and write pointer position of the zone

	 * to match its position in the target range.

/*

 * Helper for drivers of zoned targets to implement struct target_type

 * report_zones operation.

	/*

	 * Set the target mapping start sector first so that

	 * dm_report_zones_cb() can correctly remap zone information.

		/*

		 * Conventional, offline and read-only zones do not have a valid

		 * write pointer. Use 0 as for an empty zone.

/*

 * Revalidate the zones of a mapped device to initialize resource necessary

 * for zone append emulation. Note that we cannot simply use the block layer

 * blk_revalidate_disk_zones() function here as the mapped device is suspended

 * (this is called from __bind() context).

	/*

	 * Check if something changed. If yes, cleanup the current resources

	 * and reallocate everything.

	/*

	 * Scan all zones to initialize everything. Ensure that all vmalloc

	 * operations in this context are done as if GFP_NOIO was specified.

	/*

	 * For a zoned target, the number of zones should be updated for the

	 * correct value to be exposed in sysfs queue/nr_zones.

 Check if zone append is natively supported */

	/*

	 * Mark the mapped device as needing zone append emulation and

	 * initialize the emulation resources once the capacity is set.

	/*

	 * Ensure that all memory allocations in this context are done as if

	 * GFP_NOIO was specified.

/*

 * First phase of BIO mapping for targets with zone append emulation:

 * check all BIO that change a zone writer pointer and change zone

 * append operations into regular write operations.

	/*

	 * If the target zone is in an error state, recover by inspecting the

	 * zone to get its current write pointer position. Note that since the

	 * target zone is already locked, a BIO issuing context should never

	 * see the zone write in the DM_ZONE_UPDATING_WP_OFST state.

 Writes must be aligned to the zone write pointer */

		/*

		 * Change zone append operations into a non-mergeable regular

		 * writes directed at the current write pointer position of the

		 * target zone.

 Cannot write to a full zone */

/*

 * Second phase of BIO mapping for targets with zone append emulation:

 * update the zone write pointer offset array to account for the additional

 * data written to a zone. Note that at this point, the remapped clone BIO

 * may already have completed, so we do not touch it.

 The clone BIO may already have been completed and failed */

 Update the zone wp offset */

		/*

		 * Check that the target did not truncate the write operation

		 * emulating a zone append.

	/*

	 * Special processing is not needed for operations that do not need the

	 * zone write lock, that is, all operations that target conventional

	 * zones and all operations that do not modify directly a sequential

	 * zone write pointer.

/*

 * Special IO mapping for targets needing zone append emulation.

	/*

	 * IOs that do not change a zone write pointer do not need

	 * any additional special processing.

 Lock the target zone */

	/*

	 * Check that the bio and the target zone write pointer offset are

	 * both valid, and if the bio is a zone append, remap it to a write.

	/*

	 * The target map function may issue and complete the IO quickly.

	 * Take an extra reference on the IO to make sure it does disappear

	 * until we run dm_zone_map_bio_end().

 Let the target do its work */

		/*

		 * The target submitted the clone BIO. The target zone will

		 * be unlocked on completion of the clone.

		/*

		 * The target only remapped the clone BIO. In case of error,

		 * unlock the target zone here as the clone will not be

		 * submitted.

 Drop the extra reference on the IO */

/*

 * IO completion callback called from clone_endio().

	/*

	 * For targets that do not emulate zone append, we only need to

	 * handle native zone-append bios.

		/*

		 * Get the offset within the zone of the written sector

		 * and add that to the original bio sector position.

	/*

	 * For targets that do emulate zone append, if the clone BIO does not

	 * own the target zone write lock, we have nothing to do.

		/*

		 * BIOs that modify a zone write pointer may leave the zone

		 * in an unknown state in case of failure (e.g. the write

		 * pointer was only partially advanced). In this case, set

		 * the target zone write pointer as invalid unless it is

		 * already being updated.

		/*

		 * Get the written sector for zone append operation that were

		 * emulated using regular write operations.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Western Digital Corporation or its affiliates.

 *

 * This file is released under the GPL.

/*

 * Zone BIO context.

/*

 * Chunk work descriptor.

/*

 * Target descriptor.

 Zoned block device information */

 For metadata handling */

 For chunk work */

 For cloned BIOs to zones */

 For flush */

/*

 * Flush intervals (seconds).

/*

 * Target BIO completion.

/*

 * Completion callback for an internally cloned target BIO. This terminates the

 * target BIO when there are no more references to its context.

/*

 * Issue a clone of a target BIO. The clone may only partially process the

 * original target BIO.

/*

 * Zero out pages of discarded blocks accessed by a read BIO.

 Clear nr_blocks */

/*

 * Process a read BIO.

 Read into unmapped chunks need only zeroing the BIO buffer */

 Check block validity to determine the read location */

 Test block validity in the data zone */

 Read data zone blocks */

		/*

		 * No valid blocks found in the data zone.

		 * Check the buffer zone, if there is one.

 Read buffer zone blocks */

 Valid blocks found: read them */

 No valid block: zeroout the current BIO block */

/*

 * Write blocks directly in a data zone, at the write pointer.

 * If a buffer zone is assigned, invalidate the blocks written

 * in place.

 Submit write */

	/*

	 * Validate the blocks in the data zone and invalidate

	 * in the buffer zone, if there is one.

/*

 * Write blocks in the buffer zone of @zone.

 * If no buffer zone is assigned yet, get one.

 * Called with @zone write locked.

 Get the buffer zone. One will be allocated if needed */

 Submit write */

	/*

	 * Validate the blocks in the buffer zone

	 * and invalidate in the data zone.

/*

 * Process a write BIO.

		/*

		 * zone is a random zone or it is a sequential zone

		 * and the BIO is aligned to the zone write pointer:

		 * direct write the zone.

	/*

	 * This is an unaligned write in a sequential zone:

	 * use buffered write.

/*

 * Process a discard BIO.

 For unmapped chunks, there is nothing to do */

	/*

	 * Invalidate blocks in the data zone and its

	 * buffer zone if one is mapped.

/*

 * Process a BIO.

	/*

	 * Get the data zone mapping the chunk. There may be no

	 * mapping for read and discard. If a mapping is obtained,

	 + the zone returned will be set to active state.

 Process the BIO */

	/*

	 * Release the chunk mapping. This will check that the mapping

	 * is still valid, that is, that the zone used still has valid blocks.

/*

 * Increment a chunk reference counter.

/*

 * Decrement a chunk work reference count and

 * free it if it becomes 0.

/*

 * Chunk BIO work function.

 Process the chunk BIOs */

 Queueing the work incremented the work refcount */

/*

 * Flush work.

 Flush dirty metadata blocks */

 Process queued flush requests */

/*

 * Get a chunk work and start it to process a new BIO.

 * If the BIO chunk has no work yet, create one.

 Get the BIO chunk work. If one is not active yet, create one */

 Create a new chunk work */

/*

 * Check if the backing device is being removed. If it's on the way out,

 * start failing I/O. Reclaim and metadata components also call this

 * function to cleanly abort operation in the event of such failure.

/*

 * Check the backing device availability. This detects such events as

 * backing device going offline due to errors, media removals, etc.

 * This check is less efficient than dmz_bdev_is_dying() and should

 * only be performed as a part of error handling.

/*

 * Process a new BIO.

 The BIO should be block aligned */

 Initialize the BIO context */

 Set the BIO pending in the flush list */

 Split zone BIOs to fit entirely into a zone */

 Now ready to handle this BIO */

/*

 * Get zoned device information.

 Get the target device */

/*

 * Cleanup zoned device information.

	/*

	 * When we have more than on devices, the first one must be a

	 * regular block device and the others zoned block devices.

/*

 * Setup target.

 Check arguments */

 Allocate and initialize the target descriptor */

 Get the target zoned block device */

 Initialize metadata */

 Set target (no write same support) */

 The exposed capacity is the number of chunks that can be mapped */

 Zone BIO */

 Chunk BIO work */

 Flush work */

 Initialize reclaim */

/*

 * Cleanup target.

/*

 * Setup target request queue limits.

 FS hint to try to align to the device zone size */

 We are exposing a drive-managed zoned block device */

/*

 * Pass on ioctl to the backend device.

/*

 * Stop works on suspend.

/*

 * Restart works on resume or if suspend failed.

			/*

			 * For a multi-device setup the first device

			 * contains only cache zones.

/*

 * Copyright (C) 2010-2012 by Dell Inc.  All rights reserved.

 * Copyright (C) 2011-2013 Red Hat, Inc.

 *

 * This file is released under the GPL.

 *

 * dm-switch is a device-mapper target that maps IO to underlying block

 * devices efficiently when there are a large number of fixed-sized

 * address regions but there is no simple pattern to allow for a compact

 * mapping representation such as dm-stripe.

/*

 * One region_table_slot_t holds <region_entries_per_slot> region table

 * entries each of which is <region_table_entry_bits> in size.

/*

 * A device with the offset to its start sector.

/*

 * Context block for a dm switch device.

 Number of paths in path_list. */

 Region size in 512-byte sectors */

 Number of regions making up the device */

 log2 of region_size or -1 */

 Number of bits in one region table entry */

 Number of entries in one region table slot */

 log2 of region_entries_per_slot or -1 */

 Region table */

	/*

	 * Array of dm devices to switch between.

/*

 * Find which path to use at given offset.

 This can only happen if the processor uses non-atomic stores. */

/*

 * Fill the region table with an initial round robin pattern.

/*

 * Destructor: Don't free the dm_target, just the ti->private data (if any).

/*

 * Constructor arguments:

 *   <num_paths> <region_size> <num_optional_args> [<optional_args>...]

 *   [<dev_path> <offset>]+

 *

 * Optional args are to allow for future extension: currently this

 * parameter must be 0.

 parse optional arguments here, if we add any */

 For UNMAP, sending the request down any path is sufficient */

/*

 * We need to parse hex numbers in the message as quickly as possible.

 *

 * This table-based hex parser improves performance.

 * It improves a time to load 1000000 entries compared to the condition-based

 * parser.

 *		table-based parser	condition-based parser

 * PA-RISC	0.29s			0.31s

 * Opteron	0.0495s			0.0498s

/*

 * Messages are processed one-at-a-time.

 *

 * Only set_region_mappings is supported.

/*

 * Switch ioctl:

 *

 * Passthrough all ioctls to the path for sector 0

	/*

	 * Only pass ioctls through if the device sizes match exactly.

/*

 * Copyright (C) 2003 Sistina Software.

 * Copyright (C) 2004 Red Hat, Inc. All rights reserved.

 *

 * Module Author: Heinz Mauelshagen

 *

 * This file is released under the GPL.

 *

 * Path selector registration.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Western Digital Corporation or its affiliates.

 *

 * This file is released under the GPL.

 Last target access time */

/*

 * Reclaim state flags.

/*

 * Number of seconds of target BIO inactivity to consider the target idle.

/*

 * Percentage of unmapped (free) random zones below which reclaim starts

 * even if the target is busy.

/*

 * Percentage of unmapped (free) random zones above which reclaim will

 * stop if the target is busy.

/*

 * Align a sequential zone write pointer to chunk_block.

	/*

	 * Zeroout the space between the write

	 * pointer and the requested position.

/*

 * dm_kcopyd_copy end notification.

/*

 * Copy valid blocks of src_zone into dst_zone.

 Get a valid region from the source zone */

		/*

		 * If we are writing in a sequential zone, we must make sure

		 * that writes are sequential. So Zeroout any eventual hole

		 * between writes.

 Copy the valid region */

 Wait for copy to complete */

/*

 * Move valid blocks of dzone buffer zone into dzone (after its write pointer)

 * and free the buffer zone.

 Flush data zone into the buffer zone */

 Validate copied blocks */

 Free the buffer zone */

/*

 * Merge valid blocks of dzone into its buffer zone and free dzone.

 Flush data zone into the buffer zone */

 Validate copied blocks */

		/*

		 * Free the data zone and remap the chunk to

		 * the buffer zone.

/*

 * Move valid blocks of the random data zone dzone into a free sequential zone.

 * Once blocks are moved, remap the zone chunk to the sequential zone.

 Get a free random or sequential zone */

 Flush the random data zone into the sequential zone */

 Validate copied blocks */

 Free the sequential zone */

 Free the data zone and remap the chunk */

/*

 * Reclaim an empty zone.

/*

 * Test if the target device is idle.

/*

 * Find a candidate zone for reclaim and process it.

 Get a data zone */

 Empty zone */

			/*

			 * Reclaim the random data zone by moving its

			 * valid data blocks to a free sequential zone.

			/*

			 * The buffer zone is empty or its valid blocks are

			 * after the data zone write pointer.

			/*

			 * Reclaim the data zone by merging it into the

			 * buffer zone so that the buffer zone itself can

			 * be later reclaimed.

/*

 * Test if reclaim is necessary.

		/*

		 * The first device in a multi-device

		 * setup only contains cache zones, so

		 * never start reclaim there.

 Reclaim when idle */

 If there are still plenty of cache zones, do not reclaim */

	/*

	 * If the percentage of unmapped cache zones is low,

	 * reclaim even if the target is busy.

/*

 * Reclaim work function.

	/*

	 * We need to start reclaiming random zones: set up zone copy

	 * throttling to either go fast if we are very low on random zones

	 * and slower if there are still some free random zones to avoid

	 * as much as possible to negatively impact the user workload.

 Idle or very low percentage: go fast */

 Busy but we still have some random zone: throttle */

/*

 * Initialize reclaim.

 Reclaim kcopyd client */

 Reclaim work */

/*

 * Terminate reclaim.

/*

 * Suspend reclaim.

/*

 * Resume reclaim.

/*

 * BIO accounting.

/*

 * Start reclaim if necessary.

/*

 * Copyright (C) 2003 Sistina Software Limited.

 * Copyright (C) 2004-2005 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

 Path properties */

 Owning PG */

 Cumulative failure count */

 Path status */

/*

 * Paths are grouped into Priority Groups and numbered from 1 upwards.

 * Each has a path selector which controls which path gets used.

 Owning multipath instance */

 Reference number */

 Number of paths in PG */

 Temporarily bypass this PG? */

 Multipath context */

 Multipath state flags */

 Switch to this PG if set */

 Total number of usable paths */

 Wait for pg_init completion */

 Number of times to retry pg_init */

 Number of msecs before pg_init retry */

 Only one pg_init allowed at once */

 Number of times pg_init called */

 Timeout for queue_if_no_path */

/*

 * Context information attached to each io we process.

/*-----------------------------------------------

 * Multipath state flags.

 Must we queue all I/O? */

 Queue I/O if last path fails? */

 Saved state during suspension */

 If there's already a hw_handler present, don't change it. */

 pg_init is not currently allowed */

 pg_init needs calling? */

 Delay pg_init retry? */

/*-----------------------------------------------

 * Allocation routines

		/*

		 * bio-based doesn't support any direct scsi_dh management;

		 * it just discovers if a scsi_dh is attached.

	/*

	 * Init fields that are only used when a scsi_dh is attached

	 * - must do this unconditionally (really doesn't hurt non-SCSI uses)

 dm_bio_details is immediately after the dm_mpath_io in bio's per-bio-data */

/*-----------------------------------------------

 * Path selection

 Check here to reset pg_init_required */

 Skip failed paths */

 Must we initialise the PG first, and queue I/O till it's ready? */

 Only update current_pgpath if pg changed */

 Were we instructed to switch PG? */

 Don't change PG until it has no remaining paths */

	/*

	 * Loop through priority groups until we find a valid path.

	 * First time we skip PGs marked 'bypassed'.

	 * Second time we only try the ones we skipped, but set

	 * pg_init_delay_retry so we do not hammer controllers.

/*

 * dm_report_EIO() is a macro instead of a function to make pr_debug_ratelimited()

 * report the function name and line number of the function from which

 * it has been invoked.

/*

 * Check whether bios must be queued in the device-mapper core rather

 * than here in the target.

/*

 * Map cloned requests (request-based multipath)

 Do we need to select a new pgpath? */

 Failed */

 EBUSY, ENODEV or EWOULDBLOCK: requeue */

		/*

		 * blk-mq's SCHED_RESTART can cover this requeue, so we

		 * needn't deal with it by DELAY_REQUEUE. More importantly,

		 * we have to return DM_MAPIO_REQUEUE so that blk-mq can

		 * get the queue busy feedback (via BLK_STS_RESOURCE),

		 * otherwise I/O merging can suffer.

		/*

		 * non-NULL map_context means caller is still map

		 * method; must undo multipath_clone_and_map()

/*

 * Map cloned bios (bio-based multipath)

 Queue for the daemon to resubmit */

 Do we need to select a new pgpath? */

/*

 * If we run out of usable paths, should we queue I/O or error it?

 due to "fail_if_no_path" message, need to honor it. */

/*

 * If the queue_if_no_path timeout fires, turn off queue_if_no_path and

 * process any queued I/O.

/*

 * Enable the queue_if_no_path timeout if necessary.

 * Called with m->lock held.

/*

 * An event is triggered whenever a path is taken out of use.

 * Includes path failure and PG bypass.

/*-----------------------------------------------------------------

 * Constructor/argument parsing:

 * <#multipath feature args> [<arg>]*

 * <#hw_handler args> [hw_handler [<arg>]*]

 * <#priority groups>

 * <initial priority group>

 *     [<selector> <#selector args> [<arg>]*

 *      <#paths> <#per-path selector args>

 *         [<path> [<arg>]* ]+ ]+

			/*

			 * Clear any hw_handler_params associated with a

			 * handler that isn't already attached.

			/*

			 * Reset hw_handler_name to match the attached handler

			 *

			 * NB. This modifies the table line to show the actual

			 * handler instead of the original table passed in.

 we need at least a path arg */

	/*

	 * read the paths

 target arguments */

 parse the priority groups */

/*

 * Take a path out of use.

/*

 * Reinstate a previously-failed path

/*

 * Fail or reinstate all paths that match the provided struct dm_dev.

/*

 * Temporarily try to avoid having to use the specified PG

/*

 * Switch to using the specified PG from the next I/O that gets mapped

/*

 * Set/clear bypassed status of a PG.

 * PGs are numbered upwards from 1 in the order they were declared.

/*

 * Should we retry pg_init immediately?

 device or driver problems */

		/*

		 * Fail path for now, so we do not ping pong

		/*

		 * Probably doing something like FW upgrade on the

		 * controller so try the other pg.

 Wait before retrying. */

		/*

		 * We probably do not want to fail the path for a device

		 * error, but this is what the old dm did. In future

		 * patches we can do more advanced handling.

 Activations of other paths are still on going */

	/*

	 * Wake up any thread waiting to suspend.

	/*

	 * We don't queue any clone request inside the multipath target

	 * during end I/O handling, since those clone requests don't have

	 * bio clones.  If we queue them inside the multipath target,

	 * we need to make bio clones, that requires memory allocation.

	 * (See drivers/md/dm-rq.c:end_clone_bio() about why the clone requests

	 *  don't have bio clones.)

	 * Instead of queueing the clone request here, we queue the original

	 * request into dm core, which will remake a clone request and

	 * clone bios for it and resubmit it later.

 complete with the original error */

/*

 * Suspend with flush can't complete until all the I/O is processed

 * so if the last path fails we must error any remaining I/O.

 * - Note that if the freeze_bdev fails while suspending, the

 *   queue_if_no_path state is lost - userspace should reset it.

 * Otherwise, during noflush suspend, queue_if_no_path will not change.

 FIXME: bio-based shouldn't need to always disable queue_if_no_path */

/*

 * Restore the queue_if_no_path setting.

/*

 * Info output has the following format:

 * num_multipath_feature_args [multipath_feature_args]*

 * num_handler_status_args [handler_status_args]*

 * num_groups init_group_number

 *            [A|D|E num_ps_status_args [ps_status_args]*

 *             num_paths num_selector_args

 *             [path_dev A|F fail_count [selector_args]* ]+ ]+

 *

 * Table output has the following format (identical to the constructor string):

 * num_feature_args [features_args]*

 * num_handler_args hw_handler [hw_handler_args]*

 * num_groups init_group_number

 *     [priority selector-name num_ps_args [ps_args]*

 *      num_paths num_selector_args [path_dev [selector_args]* ]+ ]+

 Features */

 Disabled */

 Currently Active */

 Enabled */

reset the result pointer*/

 Disabled */

 Currently Active */

 Enabled */

 pg_init has not started or completed */

 No path is available */

 Path status changed, redo selection */

	/*

	 * Only pass ioctls through if the device sizes match exactly.

/*

 * We return "busy", only when we can map I/Os but underlying devices

 * are busy (so even if we map I/Os now, the I/Os will wait on

 * the underlying queue).

 * In other words, if we want to kill I/Os or queue them inside us

 * due to map unavailability, we don't return "busy".  Otherwise,

 * dm core won't give us the I/Os and we can't do what we want.

 pg_init in progress */

 no paths available, for blk-mq: rely on IO mapping to delay requeue */

 Guess which priority_group will be used at next mapping time */

		/*

		 * We don't know which pg will be used at next mapping time.

		 * We don't call choose_pgpath() here to avoid to trigger

		 * pg_init just by busy checking.

		 * So we don't know whether underlying devices we will be using

		 * at next mapping time are busy or not. Just try mapping.

	/*

	 * If there is one non-busy active path at least, the path selector

	 * will be able to select it. So we consider such a pg as not busy.

		/*

		 * No active path in this pg, so this pg won't be used and

		 * the current_pg will be changed at next mapping time.

		 * We need to try mapping to determine it.

/*-----------------------------------------------------------------

 * Module setup

	/*

	 * A separate workqueue is used to handle the device handlers

	 * to avoid overloading existing workqueue. Overloading the

	 * old workqueue would also create a bottleneck in the

	 * path of the storage hardware device activation.

/*

 * Copyright (C) 2012 Red Hat. All rights reserved.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * Glossary:

 *

 * oblock: index of an origin block

 * cblock: index of a cache block

 * promotion: movement of a block from origin to cache

 * demotion: movement of a block from cache to origin

 * migration: movement of a block between the origin and cache device,

 *	      either direction

----------------------------------------------------------------*/

/*

 * Represents a chunk of future work.  'input' allows continuations to pass

 * values between themselves, typically error values.

----------------------------------------------------------------*/

/*

 * The batcher collects together pieces of work that need a particular

 * operation to occur before they can proceed (typically a commit).

	/*

	 * The operation that everyone is waiting for.

	/*

	 * This is how bios should be issued once the commit op is complete

	 * (accounted_request).

	/*

	 * Queued work gets put on here after commit.

	/*

	 * We have to grab these before the commit_op to avoid a race

	 * condition.

 to avoid a WARN_ON */

/*

 * Bios are errored if commit failed.

/*

 * Call this if some urgent work is waiting for the commit to complete.

/*

 * There are a couple of places where we let a bio run, but want to do some

 * work before calling its endio function.  We do this by temporarily

 * changing the endio fn.

----------------------------------------------------------------*/

/*

 * The block size of the device holding cache data must be

 * between 32KB and 1GB.

 metadata may be changed */

 metadata may not be changed */

	/*

	 * Data is written to cached blocks only.  These blocks are marked

	 * dirty.  If you lose the cache device you will lose data.

	 * Potential performance increase for both reads and writes.

	/*

	 * Data is written to both cache and origin.  Blocks are never

	 * dirty.  Potential performance benfit for reads only.

	/*

	 * A degraded mode useful for various cache coherency situations

	 * (eg, rolling back snapshots).  Reads and writes always go to the

	 * origin.  If a write goes to a cached oblock, then the cache

	 * block is invalidated.

	/*

	 * Fields for converting from sectors to blocks.

	/*

	 * Metadata is written to this device.

	/*

	 * The slower of the two data devices.  Typically a spindle.

	/*

	 * The faster of the two data devices.  Typically an SSD.

	/*

	 * Size of the origin device in _complete_ blocks and native sectors.

	/*

	 * Size of the cache device in blocks.

	/*

	 * Invalidation fields.

	/*

	 * The number of in flight migrations that are performing

	 * background io. eg, promotion, writeback.

	/*

	 * origin_blocks entries, discarded if set.

 a power of 2 times sectors per block */

	/*

	 * Rather than reconstructing the table line for the status we just

	 * save it and regurgitate.

	/*

	 * cache_size entries, dirty if set

	/*

	 * Cache features such as write-through.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * We have two lock levels.  Level 0, which is used to prevent WRITEs, and

 * level 1 which prevents *both* READs and WRITEs.

/*----------------------------------------------------------------

 * Per bio data

----------------------------------------------------------------*/

----------------------------------------------------------------*/

 FIXME: allow wait if calling from worker */

		/*

		 * Failed to get the lock.

----------------------------------------------------------------*/

/*

 * These two are called when setting after migrations to force the policy

 * and dirty bitset to be in sync.

----------------------------------------------------------------*/

/*----------------------------------------------------------------

 * Remapping

 FIXME: check_if_tick_bio_needed() is called way too much through this interface

/*

 * When running in writethrough mode we need to send writes to clean blocks

 * to both the cache and origin devices.  Clone the bio and send them in parallel.

	/*

	 * Passing false to __remap_to_origin_clear_discard() skips

	 * all code that might use per_bio_data (since clone doesn't have it)

/*----------------------------------------------------------------

 * Failure modes

 Never move out of fail mode */

----------------------------------------------------------------*/

/*----------------------------------------------------------------

 * Migration processing

 *

 * Migration covers moving data from the origin device to the cache, or

 * vice versa.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * The overwrite bio is part of the copy operation, as such it does

	 * not set/clear discard or dirty flags.

/*

 * Migration steps:

 *

 * 1) exclusive lock preventing WRITEs

 * 2) quiesce

 * 3) copy or issue overwrite bio

 * 4) upgrade to exclusive lock preventing READs and WRITEs

 * 5) quiesce

 * 6) update metadata and commit

 * 7) unlock

		/*

		 * We clear dirty here to update the nr_dirty counter.

		/*

		 * It would be nice if we only had to commit when a REQ_FLUSH

		 * comes through.  But there's one scenario that we have to

		 * look out for:

		 *

		 * - vblock x in a cache block

		 * - domotion occurs

		 * - cache block gets reallocated and over written

		 * - crash

		 *

		 * When we recover, because there was no commit the cache will

		 * rollback to having the data for vblock x in the cache block.

		 * But the cache block has since been overwritten, so it'll end

		 * up pointing to data that was never in 'x' during the history

		 * of the device.

		 *

		 * To avoid this issue we require a commit as part of the

		 * demotion operation.

	/*

	 * Did the copy succeed?

	/*

	 * Did the copy succeed?

		/*

		 * Now we want the lock to prevent both reads and writes.

		/*

		 * No exclusive lock was held when we last checked if the bio

		 * was optimisable.  So we have to check again in case things

		 * have changed (eg, the block may no longer be discarded).

			/*

			 * Fallback to a real full copy after doing some tidying up.

 An exclussive lock must _not_ be held for this block */

		/*

		 * It's safe to do this here, even though it's new data

		 * because all IO has been locked out of the block.

		 *

		 * mg_lock_writes() already took READ_WRITE_LOCK_LEVEL

		 * so _not_ using mg_upgrade_lock() as continutation.

	/*

	 * Prevent writes to the block, but allow reads to continue.

	 * Unless we're using an overwrite bio, in which case we lock

	 * everything.

/*----------------------------------------------------------------

 * invalidation processing

		/*

		 * Harmless, already unmapped.

		/*

		 * We can't call invalidate_remove() directly here because we

		 * might still be in request context.

/*----------------------------------------------------------------

 * bio processing

----------------------------------------------------------------*/

		/*

		 * An exclusive lock is held for this block, so we have to

		 * wait.  We set the commit_needed flag so the current

		 * transaction will be committed asap, allowing this lock

		 * to be dropped.

		/*

		 * Miss.

			/*

			 * This is a duplicate writethrough io that is no

			 * longer needed because the block has been demoted.

		/*

		 * Hit.

		/*

		 * Passthrough always maps to the origin, invalidating any

		 * cache blocks that are written to.

	/*

	 * dm core turns FUA requests into a separate payload and FLUSH req.

		/*

		 * issue_after_commit will call accounted_begin a second time.  So

		 * we call accounted_complete() to avoid double accounting.

/*

 * A non-zero return indicates read_only or fail_io mode.

/*

 * Used by the batcher.

----------------------------------------------------------------*/

 FIXME: do we need to lock the region?  Or can we just assume the

 user wont be so foolish as to issue discard concurrently with

 other IO?

/*----------------------------------------------------------------

 * Main worker loop

/*

 * We want to commit periodically so that not too much

 * unwritten metadata builds up.

/*----------------------------------------------------------------

 * Target methods

/*

 * This function gets called on the error paths of the constructor, so we

 * have to cope with a partially initialised struct.

----------------------------------------------------------------*/

/*

 * Construct a cache device mapping.

 *

 * cache <metadata dev> <cache dev> <origin dev> <block size>

 *       <#feature args> [<feature arg>]*

 *       <policy> <#policy args> [<policy arg>]*

 *

 * metadata dev    : fast device holding the persistent metadata

 * cache dev	   : fast device holding cached data blocks

 * origin dev	   : slow device holding original data blocks

 * block size	   : cache unit size in sectors

 *

 * #feature args   : number of feature arguments passed

 * feature args    : writethrough.  (The default is writeback.)

 *

 * policy	   : the replacement policy to use

 * #policy args    : an even number of policy arguments corresponding

 *		     to key/value pairs passed to the policy

 * policy args	   : key/value pairs passed to the policy

 *		     E.g. 'sequential_threshold 1024'

 *		     See cache-policies.txt for details.

 *

 * Optional feature arguments are:

 *   writethrough  : write through caching that prohibits cache block

 *		     content from being different from origin block content.

 *		     Without this argument, the default behaviour is to write

 *		     back cache block contents later for performance reasons,

 *		     so they may differ from the corresponding origin blocks.

----------------------------------------------------------------*/

/*

 * We want the discard block size to be at least the size of the cache

 * block size and have no more than 2^14 discard blocks across the origin.

 Create bioset for writethrough bios issued to origin */

----------------------------------------------------------------*/

		/*

		 * This can only occur if the io goes to a partial block at

		 * the end of the origin device.  We don't cache these.

		 * Just remap to the origin and carry on.

/*

 * returns true on success

	/*

	 * If writing the above metadata failed, we still commit, but don't

	 * set the clean shutdown flag.  This will effectively force every

	 * dirty bit to be set on reload.

	/*

	 * If it's a flush suspend there won't be any deferred bios, so this

	 * call is harmless.

/*

 * The discard block size in the on disk metadata is not

 * neccessarily the same as we're currently using.  So we have to

 * be careful to only set the discarded attribute if we know it

 * covers a complete block of the new size.

	/*

	 * These blocks are sized using the on disk dblock size, rather

	 * than the current one.

	/*

	 * Convert to sectors.

	/*

	 * Then convert back to the current dblock size.

	/*

	 * The origin may have shrunk, so we need to check we're still in

	 * bounds.

			/*

			 * We're already in a discard range, just extend it.

			/*

			 * Emit the old range and start a new one.

	/*

	 * We can't drop a dirty block when shrinking the cache.

	/*

	 * Check to see if the cache has resized.

		/*

		 * The discard bitset could have been resized, or the

		 * discard block size changed.  To be safe we start by

		 * setting every dblock to not discarded.

/*

 * Status format:

 *

 * <metadata block size> <#used metadata blocks>/<#total metadata blocks>

 * <cache block size> <#used cache blocks>/<#total cache blocks>

 * <#read hits> <#read misses> <#write hits> <#write misses>

 * <#demotions> <#promotions> <#dirty>

 * <#features> <features>*

 * <#core args> <core args>

 * <policy name> <#policy args> <policy args>* <cache metadata mode> <needs_check>

 Commit to ensure statistics aren't out-of-date */

/*

 * Defines a range of cblocks, begin to (end - 1) are in the range.  end is

 * the one-past-the-end value.

/*

 * A cache block range can take two forms:

 *

 * i) A single cblock, eg. '3456'

 * ii) A begin and end cblock with a dash between, eg. 123-234

	/*

	 * Try and parse form (ii) first.

	/*

	 * That didn't work, try form (i).

	/*

	 * We don't need to do any locking here because we know we're in

	 * passthrough mode.  There's is potential for a race between an

	 * invalidation triggered by an io and an invalidation message.  This

	 * is harmless, we must not worry if the policy call fails.

		/*

		 * Pass begin and end origin blocks to the worker and wake it.

/*

 * Supports

 *	"<key> <value>"

 * and

 *     "invalidate_cblocks [(<begin>)|(<begin>-<end>)]*

 *

 * The key migration_threshold is supported by the cache target core.

/*

 * If discard_passdown was enabled verify that the origin device

 * supports discards.  Disable discard_passdown if not.

 No passdown is done so setting own virtual limits */

	/*

	 * cache_iterate_devices() is stacking both origin and fast device limits

	 * but discards aren't passed to fast device, so inherit origin's limits.

	/*

	 * If the system-determined stacked limits are compatible with the

	 * cache's blocksize (io_opt is a factor) do not override them.

----------------------------------------------------------------*/

/*

 * Copyright (C) 2007-2009 NEC Corporation.  All Rights Reserved.

 *

 * Module Author: Kiyoshi Ueda

 *

 * This file is released under the GPL.

 *

 * Throughput oriented path selector.

 Total size of in-flight I/Os */

	/*

	 * Arguments: [<repeat_count> [<relative_throughput>]]

	 * 	<repeat_count>: The number of I/Os before switching path.

	 * 			If not given, default (ST_MIN_IO) is used.

	 * 	<relative_throughput>: The relative throughput value of

	 *			the path among all paths in the path-group.

	 * 			The valid range: 0-<ST_MAX_RELATIVE_THROUGHPUT>

	 *			If not given, minimum value '1' is used.

	 *			If '0' is given, the path isn't selected while

	 * 			other paths having a positive value are

	 * 			available.

 allocate the path */

/*

 * Compare the estimated service time of 2 paths, pi1 and pi2,

 * for the incoming I/O.

 *

 * Returns:

 * < 0 : pi1 is better

 * 0   : no difference between pi1 and pi2

 * > 0 : pi2 is better

 *

 * Description:

 * Basically, the service time is estimated by:

 *     ('pi->in-flight-size' + 'incoming') / 'pi->relative_throughput'

 * To reduce the calculation, some optimizations are made.

 * (See comments inline)

	/*

	 * Case 1: Both have same throughput value. Choose less loaded path.

	/*

	 * Case 2a: Both have same load. Choose higher throughput path.

	 * Case 2b: One path has no throughput value. Choose the other one.

	/*

	 * Case 3: Calculate service time. Choose faster path.

	 *         Service time using pi1:

	 *             st1 = (sz1 + incoming) / pi1->relative_throughput

	 *         Service time using pi2:

	 *             st2 = (sz2 + incoming) / pi2->relative_throughput

	 *

	 *         To avoid the division, transform the expression to use

	 *         multiplication.

	 *         Because ->relative_throughput > 0 here, if st1 < st2,

	 *         the expressions below are the same meaning:

	 *             (sz1 + incoming) / pi1->relative_throughput <

	 *                 (sz2 + incoming) / pi2->relative_throughput

	 *             (sz1 + incoming) * pi2->relative_throughput <

	 *                 (sz2 + incoming) * pi1->relative_throughput

	 *         So use the later one.

		/*

		 * Size may be too big for multiplying pi->relative_throughput

		 * and overflow.

		 * To avoid the overflow and mis-selection, shift down both.

	/*

	 * Case 4: Service time is equal. Choose higher throughput path.

 Move most recently used to least preferred to evenly balance. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * raid5.c : Multiple Devices driver for Linux

 *	   Copyright (C) 1996, 1997 Ingo Molnar, Miguel de Icaza, Gadi Oxman

 *	   Copyright (C) 1999, 2000 Ingo Molnar

 *	   Copyright (C) 2002, 2003 H. Peter Anvin

 *

 * RAID-4/5/6 management functions.

 * Thanks to Penguin Computing for making the RAID-6 development possible

 * by donating a test server!

/*

 * BITMAP UNPLUGGING:

 *

 * The sequencing for updating the bitmap reliably is a little

 * subtle (and I got it wrong the first time) so it deserves some

 * explanation.

 *

 * We group bitmap updates into batches.  Each batch has a number.

 * We may write out several batches at once, but that isn't very important.

 * conf->seq_write is the number of the last batch successfully written.

 * conf->seq_flush is the number of the last batch that was closed to

 *    new additions.

 * When we discover that we will need to write to any block in a stripe

 * (in add_stripe_bio) we update the in-memory bitmap and record in sh->bm_seq

 * the number of the batch it will be in. This is seq_flush+1.

 * When we are ready to do a write, if that batch hasn't been written yet,

 *   we plug the array and queue the stripe for later.

 * When an unplug happens, we increment bm_flush, thus closing the current

 *   batch.

 * When we notice that bm_flush > bm_write, we write out all pending updates

 * to the bitmap, and advance bm_write to where bm_flush was.

 * This may occasionally write a bit out twice, but is sure never to

 * miss any bits.

 Find first data disk in a raid6 stripe */

 ddf always start from first device */

 md starts just after Q block */

/* When walking through the disks in a raid5, starting at raid6_d0,

 * We need to map each disk to a 'slot', where the data disks are slot

 * 0 .. raid_disks-3, the parity disk is raid_disks-2 and the Q disk

 * is raid_disks-1.  This help does that mapping.

 at least one worker should run to avoid race */

 wakeup more workers */

 number of date pages with R5_InJournal */

	/*

	 * In the following cases, the stripe cannot be released to cached

	 * lists. Therefore, we make the stripe write out and set

	 * STRIPE_HANDLE:

	 *   1. when quiesce in r5c write back;

	 *   2. when resync is requested fot the stripe.

 full stripe */

					/*

					 * STRIPE_R5C_PARTIAL_STRIPE is set in

					 * r5c_try_caching_write(). No need to

					 * set it again.

/*

 * @hash could be NR_STRIPE_HASH_LOCKS, then we have a list of inactive_list

 *

 * Be careful: Only one task can add/delete stripes from temp_inactive_list at

 * given time. Adding stripes only takes device lock, while deleting stripes

 * only takes hash lock.

		/*

		 * We don't hold any lock here yet, raid5_get_active_stripe() might

		 * remove stripes from the list

 should hold conf->device_lock already */

 sh could be readded after STRIPE_ON_RELEASE_LIST is cleard */

		/*

		 * Don't worry the bit is set here, because if the bit is set

		 * again, the count is always > 1. This is true for

		 * STRIPE_ON_UNPLUG_LIST bit too.

	/* Avoid release_list until the last reference.

 we are ok here if STRIPE_ON_RELEASE_LIST is set or not */

 find an idle stripe, make sure it is unhashed, and return it. */

 Have not allocate page pool */

 The page have allocated. */

 Each of the sh->dev[i] need one conf->stripe_size */

 Free pages */

/*

 * Need to check if array has failed when deciding whether to:

 *  - start an array

 *  - remove non-faulty devices

 *  - add a spare

 *  - allow a reshape

 * This determination is simple when no reshape is happening.

 * However if there is a reshape, we need to carefully check

 * both the before and after sections.

 * This is because some failed devices may only affect one

 * of the two sections, and some non-in_sync devices may

 * be insync in the section most affected by failed devices.

			/* not in-sync or faulty.

			 * If the reshape increases the number of devices,

			 * this is being recovered by the reshape, so

			 * this 'previous' section is not in_sync.

			 * If the number of devices is being reduced however,

			 * the device can only be part of the array if

			 * we are reverting a reshape, so this section will

			 * be in-sync.

			/* not in-sync or faulty.

			 * If reshape increases the number of devices, this

			 * section has already been recovered, else it

			 * almost certainly hasn't.

 Only freshly new full stripe normal write stripe can be added to a batch list */

 we only do back search */

 Don't cross chunks, so stripe pd_idx/qd_idx is the same */

 clear_batch_ready clear the flag */

 This batch list is already running */

		/*

		 * We must assign batch_head of this stripe within the

		 * batch_lock, otherwise clear_batch_ready of batch head

		 * stripe could clear BATCH_READY bit of this stripe and

		 * this stripe->batch_head doesn't get assigned, which

		 * could confuse clear_batch_ready for this stripe

		/*

		 * at this point, head's BATCH_READY could be cleared, but we

		 * can still add the stripe to batch list

/* Determine if 'data_offset' or 'new_data_offset' should be used

 * in this stripe_head.

	/* Need a memory barrier to make sure we see the value

	 * of conf->generation, or ->data_offset that was set before

	 * reshape_progress was updated.

	/* We are in a reshape, and this is a new-generation stripe,

	 * so use new_data_offset.

 temporarily move the head */

 list isn't empty */

 For writing to replacement */

 Ensure that if rrdev is NULL, rdev won't be */

 We raced and saw duplicates */

		/* We have already checked bad blocks for reads.  Now

		 * need to check for writes.  We never accept write errors

		 * on the replacement, so we don't to check rrdev.

					/* It is very unlikely, but we might

					 * still need to write out the

					 * bad block log - better give it

				/*

				 * Because md_wait_for_blocked_rdev

				 * will dec nr_pending, we must

				 * increment it first.

 Acknowledged bad block - skip the write */

				/*

				 * issuing read for a page in journal, this

				 * must be preparing for prexor in rmw; read

				 * the data into orig_page

			/*

			 * If this is discard request, set bi_vcnt 0. We don't

			 * want to confuse SCSI because SCSI will replace payload

			/*

			 * If this is discard request, set bi_vcnt 0. We don't

			 * want to confuse SCSI because SCSI will replace payload

 chain the operations */

 hit end of page */

 clear completed biofills */

 acknowledge completion of a biofill operation */

		/* and check if we need to reply to a read request,

		 * new R5_Wantfill requests are held off until

		 * !STRIPE_BIOFILL_RUN

 mark the computed target(s) as uptodate */

 return a pointer to the address conversion region of the scribble buffer */

 return a pointer to the address conversion region of the scribble buffer */

/*

 * Return a pointer to record offset address.

/* set_syndrome_sources - populate source buffers for gen_syndrome

 * @srcs - (struct page *) array of size sh->disks

 * @offs - (unsigned int) array of offset for each page

 * @sh - stripe_head to parse

 *

 * Populates srcs in proper layout order for the stripe and returns the

 * 'count' of sources to be used in a call to async_gen_syndrome.  The P

 * destination buffer is recorded in srcs[count] and the Q destination

 * is recorded in srcs[count+1]].

			/*

			 * For R5_InJournal, PAGE_SIZE must be 4KB and will

			 * not shared page. In that case, dev[i].offset

			 * is 0.

 we should only have one valid target */

 regenerating p is not necessary */

 q should already be set */

 Compute any data- or p-drive using XOR */

	/* we need to open-code set_syndrome_sources to handle the

	 * slot number conversion for 'faila' and 'failb'

 Q disk is one of the missing disks */

 Missing P+Q, just recompute */

 Missing D+Q: recompute D from P, then recompute Q */

 We're missing D+P. */

 We're missing D+D. */

		/*

		 * raid5-cache write back uses orig_page during prexor.

		 * After prexor, it is time to free orig_page

 existing parity data subtracted */

 Only process blocks that are known to be uptodate */

			/*

			 * For this case, PAGE_SIZE must be equal to 4KB and

			 * page offset is zero.

			/*

			 * clear R5_InJournal, so when rewriting a page in

			 * journal, it is not skipped by r5l_log_stripe()

	/* check if prexor is active which means only process blocks

	 * that are part of a read-modify-write (written)

	/* 1/ if we prexor'd then the dest is reused as a source

	 * 2/ if we did not prexor then we are redoing the parity

	 * set ASYNC_TX_XOR_DROP_DST and ASYNC_TX_XOR_ZERO_DST

	 * for the synchronous xor case

 terminate the chain if reconstruct is not set to be run */

 we just created an active stripe so... */

/**

 * scribble_alloc - allocate percpu scribble buffer for required size

 *		    of the scribble region

 * @percpu: from for_each_present_cpu() of the caller

 * @num: total number of disks in the array

 * @cnt: scribble objs count for required size of the scribble region

 *

 * The scribble buffer size must be enough to contain:

 * 1/ a struct page pointer for each device in the array +2

 * 2/ room to convert each entry in (1) to its corresponding dma

 *    (dma_map_page()) or page (page_address()) address.

 *

 * Note: the +2 is for the destination buffers of the ddf/raid6 case where we

 * calculate over all devices (not just the data blocks), using zeros in place

 * of the P and Q blocks.

	/*

	 * If here is in raid array suspend context, it is in memalloc noio

	 * context as well, there is no potential recursive memory reclaim

	 * I/Os with the GFP_KERNEL flag.

	/*

	 * Never shrink. And mddev_suspend() could deadlock if this is called

	 * from raid5d. In that case, scribble_disks and scribble_sectors

	 * should equal to new_disks and new_sectors

	/* Make all the stripes able to hold 'newsize' devices.

	 * New slots in each stripe get 'page' set to a new page.

	 *

	 * This happens in stages:

	 * 1/ create a new kmem_cache and allocate the required number of

	 *    stripe_heads.

	 * 2/ gather all the old stripe_heads and transfer the pages across

	 *    to the new stripe_heads.  This will have the side effect of

	 *    freezing the array as once all stripe_heads have been collected,

	 *    no IO will be possible.  Old stripe heads are freed once their

	 *    pages have been transferred over, and the old kmem_cache is

	 *    freed when all stripes are done.

	 * 3/ reallocate conf->disks to be suitable bigger.  If this fails,

	 *    we simple return a failure status - no need to clean anything up.

	 * 4/ allocate new pages for the new slots in the new stripe_heads.

	 *    If this fails, we don't bother trying the shrink the

	 *    stripe_heads down again, we just leave them as they are.

	 *    As each stripe_head is processed the new one is released into

	 *    active service.

	 *

	 * Once step2 is started, we cannot afford to wait for a write,

	 * so we use GFP_NOIO allocations.

 Step 1 */

 Need to ensure auto-resizing doesn't interfere */

 didn't get enough, give up */

	/* Step 2 - Must use GFP_NOIO now.

	 * OK, we have enough stripes, start collecting inactive

	 * stripes and copying them over

	/* Step 3.

	 * At this point, we are holding all the stripes so the array

	 * is completely stalled, so now is a good time to resize

	 * conf->disks and the scribble region

 Step 4, return new stripes to service */

 critical section pass, GFP_NOIO no longer needed */

		/* If replacement finished while this request was outstanding,

		 * 'replacement' might be NULL already.

		 * In that case it moved down to 'rdev'.

		 * rdev is not removed until all requests are finished.

			/* Note that this cannot happen on a

			 * replacement device.  We just fail those on

			 * any error

			/*

			 * end read for a page in journal, this

			 * must be preparing for prexor in rmw

 Oh, no!!! */

				/* rdev was removed and 'replacement'

				 * replaced it.  rdev is not removed

				 * until all requests are finished.

				/* That was a successful write so make

				 * sure it looks like we already did

				 * a re-write.

		/*

		 * Don't allow to achieve failed state

		 * Don't try to recover this device

/*

 * Input: a 'big' sector number,

 * Output: index of the data and parity disk, and the sector # in them.

 First compute the information on this sector */

	/*

	 * Compute the chunk number and the sector offset inside the chunk

	/*

	 * Compute the stripe number

	/*

	 * Select the parity disk based on the user selected algorithm.

 Q D D D P */

 D D P Q D */

 Q D D D P */

 D D P Q D */

			/* Exactly the same as RIGHT_ASYMMETRIC, but or

			 * of blocks for computing Q is different.

 Q D D D P */

 D D P Q D */

			/* Same a left_asymmetric, by first stripe is

			 * D D D P Q  rather than

			 * Q D D D P

 Q D D D P */

 D D P Q D */

 Same as left_symmetric but Q is before P */

 RAID5 left_asymmetric, with Q on last device */

	/*

	 * Finally, compute the new sector number

 It is the Q disk */

 Q D D D P */

 D D P Q D */

 Q D D D P */

 D D P Q D */

 Like left_symmetric, but P is before Q */

 P D D D Q */

 D D Q P D */

/*

 * There are cases where we want handle_stripe_dirtying() and

 * schedule_reconstruction() to delay towrite to some dev of a stripe.

 *

 * This function checks whether we want to delay the towrite. Specifically,

 * we delay the towrite when:

 *

 *   1. degraded stripe has a non-overwrite to the missing dev, AND this

 *      stripe has data in journal (for other devices).

 *

 *      In this case, when reading data for the non-overwrite dev, it is

 *      necessary to handle complex rmw of write back cache (prexor with

 *      orig_page, and xor with page). To keep read path simple, we would

 *      like to flush data in journal to RAID disks first, so complex rmw

 *      is handled in the write patch (handle_stripe_dirtying).

 *

 *   2. when journal space is critical (R5C_LOG_CRITICAL=1)

 *

 *      It is important to be able to flush all stripes in raid5-cache.

 *      Therefore, we need reserve some space on the journal device for

 *      these flushes. If flush operation includes pending writes to the

 *      stripe, we need to reserve (conf->raid_disk + 1) pages per stripe

 *      for the flush out. If we exclude these pending writes from flush

 *      operation, we only need (conf->max_degraded + 1) pages per stripe.

 *      Therefore, excluding pending writes in these cases enables more

 *      efficient use of the journal device.

 *

 *      Note: To make sure the stripe makes progress, we only delay

 *      towrite for stripes with data already in journal (injournal > 0).

 *      When LOG_CRITICAL, stripes with injournal == 0 will be sent to

 *      no_space_stripes list.

 *

 *   3. during journal failure

 *      In journal failure, we try to flush all cached data to raid disks

 *      based on data in stripe cache. The array is read-only to upper

 *      layers, so we would skip all pending writes.

 *

 case 1 above */

 case 2 above */

 case 3 above */

		/*

		 * In some cases, handle_stripe_dirtying initially decided to

		 * run rmw and allocates extra page for prexor. However, rcw is

		 * cheaper later on. We need to free the extra page now,

		 * because we won't be able to do that in ops_complete_prexor().

		/* if we are not expanding this is a proper write request, and

		 * there will be bios with new data to be drained into the

		 * stripe cache

 False alarm, nothing to do */

 False alarm - nothing to do */

	/* keep the parity disk(s) locked while asynchronous operations

	 * are in flight

/*

 * Each stripe/dev can have one or more bion attached.

 * toread/towrite point to the first in a chain.

 * The bi_next chain must be in order.

 Don't allow new IO added to stripes in batch list */

		/*

		 * With PPL only writes to consecutive data chunks within a

		 * stripe are allowed because for a single stripe_head we can

		 * only have one PPL entry at a time, which describes one data

		 * range. Not really an overlap, but wait_for_overlap can be

		 * used to handle this.

 check if page is covered */

		/* Cannot hold spinlock over bitmap_startwrite,

		 * but must ensure this isn't added to a batch until

		 * we have added to the bitmap and set bm_seq.

		 * So set STRIPE_BITMAP_PENDING to prevent

		 * batching.

		 * If multiple add_stripe_bio() calls race here they

		 * much all set STRIPE_BITMAP_PENDING.  So only the first one

		 * to complete "bitmap_startwrite" gets to set

		 * STRIPE_BIT_DELAY.  This is important as once a stripe

		 * is added to a batch, STRIPE_BIT_DELAY cannot be changed

		 * any more.

 fail all writes first */

 and fail all 'written' */

		/* fail any reads if this device is non-operational and

		 * the data has not reached the cache yet.

		/* If we were in the middle of a write the parity block might

		 * still be locked - so just clear all R5_LOCKED flags

	/* There is nothing more to do for sync/check/repair.

	 * Don't even need to abort as that is handled elsewhere

	 * if needed, and not always wanted e.g. if there is a known

	 * bad block here.

	 * For recover/replace we need to record a bad block on all

	 * non-sync devices, or abort the recovery

		/* During recovery devices cannot be removed, so

		 * locking and refcounting of rdevs is not needed

		/* No point reading this as we already have it or have

		 * decided to get it.

 We need this block to directly satisfy a request */

		/* When syncing, or expanding we read everything.

		 * When replacing, we need the replaced block.

		/* If we want to read from a failed device, then

		 * we need to actually read every other device.

	/* Sometimes neither read-modify-write nor reconstruct-write

	 * cycles can work.  In those cases we read every block we

	 * can.  Then the parity-update is certain to have enough to

	 * work with.

	 * This can only be a problem when we need to write something,

	 * and some device has failed.  If either of those tests

	 * fail we need look no further.

		/* Pre-reads at not permitted until after short delay

		 * to gather multiple requests.  However if this

		 * device is no Insync, the block could only be computed

		 * and there is no need to delay that.

			/* If we have a partial write to a failed

			 * device, then we will need to reconstruct

			 * the content of that device, so all other

			 * devices must be read.

			/* In max degraded raid6, If the failed disk is P, Q,

			 * or we want to read the failed disk, we need to do

			 * reconstruct-write.

	/* If we are forced to do a reconstruct-write, because parity

	 * cannot be trusted and we are currently recovering it, there

	 * is extra need to be careful.

	 * If one of the devices that we would need to read, because

	 * it is not being overwritten (and maybe not written at all)

	 * is missing/faulty, then we need to read everything we can.

 reconstruct-write isn't being forced */

/* fetch_block - checks the given member device to see if its data needs

 * to be read or computed to satisfy a request.

 *

 * Returns 1 when no more member devices need to be checked, otherwise returns

 * 0 to tell the loop in handle_stripe_fill to continue

 is the data in this block needed, and can we get it? */

		/* we would like to get this block, possibly by computing it,

		 * otherwise read it if the backing disk is insync

		/*

		 * In the raid6 case if the only non-uptodate disk is P

		 * then we already trusted P to compute the other failed

		 * drives. It is safe to compute rather than re-read P.

		 * In other cases we only compute blocks from failed

		 * devices, otherwise check/repair might fail to detect

		 * a real inconsistency.

			/* have disk failed, and we're requested to fetch it;

			 * do compute it

 no 2nd target */

			/* Careful: from this point on 'uptodate' is in the eye

			 * of raid_run_ops which services 'compute' operations

			 * before writes. R5_Wantcompute flags a block that will

			 * be R5_UPTODATE by the time it is needed for a

			 * subsequent operation.

			/* Computing 2-failure is *very* expensive; only

			 * do it if failed >= 2

/*

 * handle_stripe_fill - read or compute data to satisfy pending requests.

	/* look for blocks to read/compute, skip this if a compute

	 * is already in flight, or if the stripe contents are in the

	 * midst of changing due to a write

		/*

		 * For degraded stripe with data in journal, do not handle

		 * read requests yet, instead, flush the stripe to raid

		 * disks first, this avoids handling complex rmw of write

		 * back cache (prexor with orig_page, and then xor with

		 * page) in the read path

/* handle_stripe_clean_event

 * any written block on an uptodate or failed drive can be returned.

 * Note that if we 'wrote' to a failed drive, it will be UPTODATE, but

 * never LOCKED, so we don't need to test 'failed' directly.

 We can return any write requests */

 now that discard is done we can proceed with any sync */

		/*

		 * SCSI discard will change some bio fields and the stripe has

		 * no updated data, so remove it from hash list and the stripe

		 * will be reinitialized

/*

 * For RMW in write back cache, we need extra page in prexor to store the

 * old data. This page is stored in dev->orig_page.

 *

 * This function checks whether we have data for prexor. The exact logic

 * is:

 *       R5_UPTODATE && (!R5_InJournal || R5_OrigPageUPTDODATE)

	/* Check whether resync is now happening or should start.

	 * If yes, then the array is dirty (after unclean shutdown or

	 * initial creation), so parity in some stripes might be inconsistent.

	 * In this case, we need to always do reconstruct-write, to ensure

	 * that in case of drive failure or read-error correction, we

	 * generate correct data from the parity.

		/* Calculate the real rcw later - for now make it

		 * look like rcw is cheaper

 would I have to read this buffer for read_modify_write */

 cannot read it */

 Would I have to read this buffer for reconstruct_write */

 prefer read-modify-write, but need to get some data */

 alloc page for prexor */

				/*

				 * alloc_page() failed, try use

				 * disk_info->extra_page

 extra_page in use, add to delayed_list */

 want reconstruct write, but need to get some data */

	/* now if nothing is locked, and if we have enough data,

	 * we can start a write request

	/* since handle_stripe can be called at any time we need to handle the

	 * case where a compute block operation has been submitted and then a

	 * subsequent call wants to start a write request.  raid_run_ops only

	 * handles the case where compute block and reconstruct are requested

	 * simultaneously.  If this is not the case then new writes need to be

	 * held off until the compute completes.

 start a new check operation if there are no failures */

 check that a write has not made the stripe insync */

 either failed parity check, or recovery is happening */

 we will be called again upon completion */

		/* if a failure occurred during the check operation, leave

		 * STRIPE_INSYNC not set and let the stripe be handled again

		/* handle a successful check operation, if parity is correct

		 * we are done.  Otherwise update the mismatch count and repair

		 * parity if !MD_RECOVERY_CHECK

			/* parity is correct (on disc,

			 * not in buffer any more)

 don't try to repair!! */

	/* Want to check and possibly repair P and Q.

	 * However there could be one 'failed' device, in which

	 * case we can only check one of them, possibly using the

	 * other to generate missing data

 start a new check operation if there are < 2 failures */

			/* The only possible failed device holds Q, so it

			 * makes sense to check P (If anything else were failed,

			 * we would have used P to recreate it).

			/* Q is not failed, and we didn't use it to generate

			 * anything, so it makes sense to check it

 discard potentially stale zero_sum_result */

 async_xor_zero_sum destroys the contents of P */

			/* async_syndrome_zero_sum preserves P and Q, so

			 * no need to mark them !uptodate here

 we have 2-disk failure */

 check that a write has not made the stripe insync */

		/* now write out any block on a failed drive,

		 * or P or Q if they were recomputed

 we will be called again upon completion */

		/* handle a successful check operation, if parity is correct

		 * we are done.  Otherwise update the mismatch count and repair

		 * parity if !MD_RECOVERY_CHECK

 both parities are correct */

				/* in contrast to the raid5 case we can validate

				 * parity, but still have a failure to write

				 * back

				/* Returning at this point means that we may go

				 * off and bring p and/or q uptodate again so

				 * we make sure to check zero_sum_result again

				 * to verify if p or q need writeback

 don't try to repair!! */

	/* We have read all the blocks in this stripe and now we need to

	 * copy some of them into a target stripe for expand.

				/* so far only the early blocks of this stripe

				 * have been requested.  When later blocks

				 * get requested, we will try again

 must have already done this block */

 place all the copies on one channel */

 done submitting copies, wait for them to complete */

/*

 * handle_stripe - do things to a stripe.

 *

 * We lock the stripe by setting STRIPE_ACTIVE and then examine the

 * state of various bits to see what needs to be done.

 * Possible results:

 *    return some read requests which now have data

 *    return some write requests which are safely on storage

 *    schedule a read on some buffers

 *    schedule a write of some buffers

 *    return confirmation of parity correctness

 *

 Now to look around and see what can be done */

		/* maybe we can reply to a read

		 *

		 * new wantfill requests are only permitted while

		 * ops_complete_biofill is guaranteed to be inactive

 now count some things */

		/* Prefer to use the replacement for reads, but only

		 * if it is recovered enough and has no bad blocks.

 Not in-sync */;

 also not in-sync */

				/* treat as in-sync, but with a read error

				 * which we can now try to correct

 in sync if before recovery_offset */

			/* If we've reshaped into here, we assume it is Insync.

			 * We will shortly update recovery_offset to make

			 * it official.

			/* This flag does not apply to '.replacement'

			/* This flag does not apply to '.replacement'

 The ReadError flag will just be confusing now */

		/* If there is a failed device being replaced,

		 *     we must be recovering.

		 * else if we are after recovery_cp, we must be syncing

		 * else if MD_RECOVERY_REQUESTED is set, we also are syncing.

		 * else we can only be replacing

		 * sync and recovery both need to read all devices, and so

		 * use the same flag.

/*

 * Return '1' if this is a member of batch, or '0' if it is a lone stripe or

 * a head which can now be handled.

	/*

	 * this stripe could be added to a batch list before we check

	 * BATCH_READY, skips it

	/*

	 * BATCH_READY is cleared, no new stripes can be added.

	 * batch_list can be accessed without lock

	/*

	 * handle_stripe should not continue handle the batched stripe, only

	 * the head of batch list or lone stripe can continue. Otherwise we

	 * could see break_stripe_batch_list warns about the STRIPE_ACTIVE

	 * is set for the batched stripe.

		/* already being handled, ensure it gets handled

		/*

		 * Cannot process 'sync' concurrently with 'discard'.

		 * Flush data in r5cache before 'sync'.

 There is nothing for the blocked_rdev to block */

	/*

	 * check if the array has lost more than max_degraded devices and,

	 * if so, some requests might need to be failed.

	 *

	 * When journal device failed (log_failed), we will only process

	 * the stripe if there is data need write to raid disks

	/* Now we check to see if any write operations have recently

	 * completed

		/* All the 'written' buffers and the parity block are ready to

		 * be written back to disk

	/*

	 * might be able to return some write requests if the parity blocks

	 * are safe, or on a failed drive

	/* Now we might consider reading some blocks, either to check/generate

	 * parity, or to satisfy requests

	 * or to load a block that is being partially written.

	/*

	 * When the stripe finishes full journal write cycle (write to journal

	 * and raid disk), this is the clean up procedure so it is ready for

	 * next operation.

	/*

	 * Now to consider new write requests, cache write back and what else,

	 * if anything should be read.  We do not handle new writes when:

	 * 1/ A 'write' operation (copy+xor) is already in flight.

	 * 2/ A 'check' operation is in flight, as it may clobber the parity

	 *    block.

	 * 3/ A r5c cache log write is in flight.

 write back cache */

 First, try handle writes in caching phase */

			/*

			 * If caching phase failed: ret == -EAGAIN

			 *    OR

			 * stripe under reclaim: !caching && injournal

			 *

			 * fall back to handle_stripe_dirtying()

 stripe under reclaim: !caching && injournal */

	/* maybe we need to check and possibly fix the parity for this stripe

	 * Any reads will already have been scheduled, so we just see if enough

	 * data is available.  The parity check is held off while parity

	 * dependent operations are in flight.

 Write out to replacement devices where possible */

	/* If the failed drives are just a ReadError, then we might need

	 * to progress the repair/check process

 let's read it back */

 Finish reconstruct operations initiated by the expansion process */

			/* sh cannot be written until sh_src has been read.

			 * so arrange for sh to be delayed a little

 Need to write out all blocks after computing parity */

 wait for this device to become unblocked */

			/* Internal metadata will immediately

			 * be written by raid5d, so we don't

			 * need to wait here.

 We own a safe reference to the rdev */

 rdev have been moved down */

		/* We delay this until after ops_run_io so that if make_request

		 * is waiting on a flush, it won't continue until the writes

		 * have actually been submitted.

 device_lock is held */

/*

 *  add bio to the retry LIFO  ( in O(1) ... we are in interrupt )

 *  later sampled by raid5d.

/*

 *  The "raid5_align_endio" should check if the read succeeded and if it

 *  did, call bio_endio on the original bio (having bio_put the new bio

 *  first).

 *  If the read failed..

 No reshape active, so we can trust rdev->data_offset */

 need a memory barrier to detect the race with raid5_quiesce() */

		/* quiesce is in progress, so we need to undo io activation and wait

		 * for it to finish

/* __get_priority_stripe - get the next stripe to process

 *

 * Full stripe writes are allowed to pass preread active stripes up until

 * the bypass_threshold is exceeded.  In general the bypass_count

 * increments when the handle_list is handled before the hold_list; however, it

 * will not be incremented when STRIPE_IO_STARTED is sampled set signifying a

 * stripe with in flight i/o.  The bypass_count will be reset when the

 * head of the hold_list has changed, i.e. the head was promoted to the

 * handle_list.

			/*

			 * avoid race release_stripe_plug() sees

			 * STRIPE_ON_UNPLUG_LIST clear but the stripe

			 * is still in our list

			/*

			 * STRIPE_ON_RELEASE_LIST could be set here. In that

			 * case, the count is always > 1 here

 Skip discard while reshape is happening */

 ret == -EAGAIN, fallback */

		/*

		 * if r5l_handle_flush_request() didn't clear REQ_PREFLUSH,

		 * we need to flush journal device

	/*

	 * If array is degraded, better not do chunk aligned read because

	 * later we might have to read it again in order to reconstruct

	 * data on failed drives.

			/* spinlock is needed as reshape_progress may be

			 * 64bit on a 32bit platform, and so it might be

			 * possible to see a half-updated value

			 * Of course reshape_progress could change after

			 * the lock is dropped, so once we get a reference

			 * to the stripe that we think it is, we will have

			 * to check again.

				/* expansion might have moved on while waiting for a

				 * stripe, so we must do the range check again.

				 * Expansion could still move past after this

				 * test, but as we are holding a reference to

				 * 'sh', we know that if that happens,

				 *  STRIPE_EXPANDING will get set and the expansion

				 * won't proceed until we finish with the stripe.

 mismatch, need to try again */

				/* Might have got the wrong stripe_head

				 * by accident

				/* Stripe is busy expanding or

				 * add failed due to overlap.  Flush everything

				 * and wait a while

 we only need flush for one stripe */

 cannot get stripe for read-ahead, just give-up */

	/* reshaping is quite different to recovery/resync so it is

	 * handled quite separately ... here.

	 *

	 * On each call to sync_request, we gather one chunk worth of

	 * destination stripes and flag them as expanding.

	 * Then we find all the source stripes and request reads.

	 * As the reads complete, handle_stripe will copy the data

	 * into the destination stripe and release that stripe.

 If restarting in the middle, skip the initial sectors */

 shouldn't happen, but just in case, finish up.*/

	/* We need to process a full chunk at a time.

	 * If old and new chunk sizes differ, we need to process the

	 * largest of these

	/* We update the metadata at least every 10 seconds, or when

	 * the data about to be copied would over-write the source of

	 * the data at the front of the range.  i.e. one new_stripe

	 * along from reshape_progress new_maps to after where

	 * reshape_safe old_maps to

		/* readpos and safepos are worst-case calculations.

		 * A negative number is overly pessimistic, and causes

		 * obvious problems for unsigned storage.  So clip to 0.

	/* Having calculated the 'writepos' possibly use it

	 * to set 'stripe_addr' which is where we will write to.

	/* 'writepos' is the most advanced device address we might write.

	 * 'readpos' is the least advanced device address we might read.

	 * 'safepos' is the least address recorded in the metadata as having

	 *     been reshaped.

	 * If there is a min_offset_diff, these are adjusted either by

	 * increasing the safepos/readpos if diff is negative, or

	 * increasing writepos if diff is positive.

	 * If 'readpos' is then behind 'writepos', there is no way that we can

	 * ensure safety in the face of a crash - that must be done by userspace

	 * making a backup of the data.  So in that case there is no particular

	 * rush to update metadata.

	 * Otherwise if 'safepos' is behind 'writepos', then we really need to

	 * update the metadata to advance 'safepos' to match 'readpos' so that

	 * we can be safe in the event of a crash.

	 * So we insist on updating metadata if safepos is behind writepos and

	 * readpos is beyond writepos.

	 * In any case, update the metadata every 10 seconds.

	 * Maybe that number should be configurable, but I'm not sure it is

	 * worth it.... maybe it could be a multiple of safemode_delay???

 Cannot proceed until we've updated the superblock... */

 Can update recovery_offset */

		/* If any of this stripe is beyond the end of the old

		 * array, then we need to zero those blocks

	/* Ok, those stripe are ready. We can start scheduling

	 * reads on the source stripes.

	 * The source stripes are determined by mapping the first and last

	 * block on the destination stripes.

	/* Now that the sources are clearly marked, we can release

	 * the destination stripes

	/* If this takes us to the resync_max point where we have to pause,

	 * then we need to write out the superblock.

 Cannot proceed until we've updated the superblock... */

 Can update recovery_offset */

 just being told to finish up .. nothing much to do */

 aborted */

 completed sync */

 Allow raid5_quiesce to complete */

	/* No need to check resync_max as we never do more than one

	 * stripe, and as resync_max will always be on a chunk boundary,

	 * if the check in md_do_sync didn't fire, there is no chance

	 * of overstepping resync_max here

	/* if there is too many failed drives and we are trying

	 * to resync, then assert that we are finished, because there is

	 * nothing we can do.

 we can skip this block, and probably more */

 keep things rounded to whole stripes */

		/* make sure we don't swamp the stripe cache if someone else

		 * is trying to get access

	/* Need to check if array will still be degraded after recovery/resync

	 * Note in case of > 1 drive failures it's possible we're rebuilding

	 * one drive while leaving another faulty drive in array.

	/* We may not be able to submit a whole bio at once as there

	 * may not be enough stripe_heads available.

	 * We cannot pre-allocate enough stripe_heads as we may need

	 * more than exist in the cache (if we allow ever large chunks).

	 * So we do one stripe head at a time and record in

	 * ->bi_hw_segments how many have been done.

	 *

	 * We *know* that this entire raid_bio is in one chunk, so

	 * it will be only one 'dd_idx' and only need one call to raid5_compute_sector.

 already done this stripe */

 failed to get a stripe - must wait */

/*

 * This is our raid5 kernel thread.

 *

 * We scan the hash table for stripes which can be handled now.

 * During the scan, completed stripes are saved for us by the interrupt

 * handler, so that they will not have to wait for our next wakeup.

 Now is a good time to flush some bitmap updates */

		/* Set flag even if allocation failed.  This helps

		 * slow down allocation requests when mem is short

	/*

	 * The value should not be bigger than PAGE_SIZE. It requires to

	 * be multiple of DEFAULT_STRIPE_SIZE and the value should be power

	 * of two.

 8192 should be big enough */

 size is defined by the smallest of previous and new size */

 unlikely, but not impossible */

 Don't enable multi-threading by default*/

	/* We init hash_locks[0] separately to that it can be used

	 * as the reference lock in the spin_lock_nest_lock() call

	 * in lock_all_device_hash_locks_irq in order to convince

	 * lockdep that we know what we are doing.

 Cannot rely on bitmap to complete recovery */

	/*

	 * Losing a stripe head costs more than the time to refill it,

	 * it reduces the queue depth and so can hurt throughput.

	 * So set it rather large, scaled by number of devices.

		/* Check that we can continue the reshape.

		 * Difficulties arise if the stripe we would write to

		 * next is at or after the stripe we would read from next.

		 * For a reshape that changes the number of devices, this

		 * is only possible for a very short time, and mdadm makes

		 * sure that time appears to have past before assembling

		 * the array.  So we fail if that time hasn't passed.

		 * For a reshape that keeps the number of devices the same

		 * mdadm must be monitoring the reshape can keeping the

		 * critical areas read-only and backed up.  It will start

		 * the array in read-only mode, so we check for that.

		/* reshape_position must be on a new-stripe boundary, and one

		 * further up in new geometry must map after here in old

		 * geometry.

		 * If the chunk sizes are different, then as we perform reshape

		 * in units of the largest of the two, reshape_position needs

		 * be a multiple of the largest chunk size times new data disks.

 here_new is the stripe we will write to */

		/* here_old is the first stripe that we might need to read

			/* We cannot be sure it is safe to start an in-place

			 * reshape.  It is only safe if user-space is monitoring

			 * and taking constant backups.

			 * mdadm always starts a situation like this in

			 * readonly mode so it can take control before

			 * allowing any writes.  So just check for that.

 not really in-place - so OK */;

 Reading from the same stripe as writing to - bad */

 OK, we should be able to continue; */

 The replacement is all we have yet */

 replacements and reshape simply do not mix. */

		/* This disc is not fully in-sync.  However if it

		 * just stored parity (beyond the recovery_offset),

		 * when we don't need to be concerned about the

		 * array being dirty.

		 * When reshape goes 'backwards', we never have

		 * partially completed devices, so we only need

		 * to worry about reshape going forwards.

 Hack because v0.91 doesn't store recovery_offset properly. */

 We need to check old and new layout */

	/*

	 * 0 for a fully functional array, 1 or 2 for a degraded array.

 device size must be a multiple of chunk size */

 Ok, everything is just fine now */

		/* read-ahead size must cover two whole stripes, which

		 * is 2 * (datadisks) * chunksize where 'n' is the

		 * number of raid devices

		/*

		 * We can only discard a whole stripe. It doesn't make sense to

		 * discard data disk but write parity disk

		/*

		 * zeroing is required, otherwise data

		 * could be lost. Consider a scenario: discard a stripe

		 * (the stripe could be inconsistent if

		 * discard_zeroes_data is 0); write one disk of the

		 * stripe (the stripe could be inconsistent again

		 * depending on which disks are used to calculate

		 * parity); the disk is broken; The stripe data of this

		 * disk is lost.

		 *

		 * We only allow DISCARD if the sysadmin has confirmed that

		 * only safe devices are in use by setting a module parameter.

		 * A better idea might be to turn DISCARD into WRITE_ZEROES

		 * requests, as that is required to be safe.

 Replacement has just become active. */

				/* Replaced device not technically faulty,

				 * but we need to be sure it gets removed

				 * and never re-added.

		/*

		 * we can't wait pending write here, as this is called in

		 * raid5d, wait will deadlock.

		 * neilb: there is no locking about new writes here,

		 * so this cannot be safe.

	/* Only remove non-faulty devices if recovery

	 * isn't possible.

 lost the race, try later */

 We must have just cleared 'rdev' */

		smp_mb(); /* Make sure other CPUs may see both as identical

			   * but will never see neither - if they are careful

		/*

		 * The array is in readonly mode if journal is missing, so no

		 * write requests running. We should be safe

 no point adding a device */

	/*

	 * find the disk ... but prefer rdev->saved_raid_disk

	 * if possible.

	/* no resync is happening, and there is enough space

	 * on all devices, so we can resize.

	 * We need to make sure resync covers any new space.

	 * If the array is shrinking we should possibly wait until

	 * any io in the removed space completes, but it hardly seems

	 * worth it.

	/* Can only proceed if there are plenty of stripe_heads.

	 * We need a minimum of one full stripe,, and for sensible progress

	 * it is best to have about 4 times that.

	 * If we require 4 times, then the default 256 4K stripe_heads will

	 * allow for chunk sizes up to 256K, which is probably OK.

	 * If the chunk size is greater, user-space should request more

	 * stripe_heads first.

 nothing to do */

		/* We might be able to shrink, but the devices must

		 * be made bigger first.

		 * For raid6, 4 is the minimum size.

		 * Otherwise 2 is the minimum

 never bother to shrink */

		/* Not enough devices even to make a degraded array

		 * of that size

	/* Refuse to reduce size of the array.  Any reductions in

	 * array size must be through explicit setting of array_size

	 * attribute.

	/* Code that selects data_offset needs to see the generation update

	 * if reshape_progress has been set - so a memory barrier needed.

	/* Now make sure any requests that proceeded on the assumption

	 * the reshape wasn't running - like Discard or Read - have

	 * completed.

	/* Add some new drives, as many as will fit.

	 * We know there are enough to make the newly sized array work.

	 * Don't add devices if we are reducing the number of

	 * devices in the array.  This is because it is not possible

	 * to correctly record the "partially reconstructed" state of

	 * such devices during the reshape and confusion could result.

 Failure here is OK */

 This is a spare that was manually added */

		/* When a reshape changes the number of devices,

		 * ->degraded is measured against the larger of the

		 * pre and post number of devices.

/* This is called from the reshape thread and should make any

 * changes needed in 'conf'

/* This is called from the raid5d thread with mddev_lock held.

 * It makes config changes to the device.

 stop all writes */

		/* '2' tells resync/reshape to pause so that all

		 * active stripes can drain

		/* need a memory barrier to make sure read_one_chunk() sees

		 * quiesce started and reverts to slow (locked) path.

 allow reshape to continue */

 re-enable writes */

 for raid0 takeover only one zone is supported */

 make sure it will be not marked as dirty */

 Should check if there are write-behind devices? */

 64K by default */

 The array must be an exact multiple of chunksize */

 array size does not allow a suitable chunk size */

	/* For a 2-drive array, the layout and chunk size can be changed

	 * immediately as not restriping is needed.

	 * For larger arrays we record the new value - after validation

	 * to be used by a reshape pass.

 not factor of array size */

 They look valid */

 can make the change immediately */

 not factor of array size */

 They look valid */

	/* raid5 can take over:

	 *  raid0 - if there is only one strip zone - make it a raid4 layout

	 *  raid1 - if there are two drives.  We need to know the chunk size

	 *  raid4 - trivial - just use a raid4 layout.

	 *  raid6 - Providing it is a *_6 layout

	/* raid4 can take over:

	 *  raid0 - if there is only one strip zone

	 *  raid5 - if layout is right

	/* Currently can only take over a raid5.  We map the

	 * personality to an equivalent raid6 personality

	 * with the Q block at the end.

 ppl only works with RAID 5 */

 need remove journal device first */

 RAID5 */

 RAID6 */

 This used to be two separate modules, they were: */

/*

 * Copyright (C) 2004-2005 IBM Corp.  All Rights Reserved.

 * Copyright (C) 2006-2009 NEC Corporation.

 *

 * dm-queue-length.c

 *

 * Module Author: Stefan Bader, IBM

 * Modified by: Kiyoshi Ueda, NEC

 *

 * This file is released under the GPL.

 *

 * queue-length path selector - choose a path with the least number of

 * in-flight I/Os.

 the number of in-flight I/Os */

 When called with NULL path, return selector status/args. */

	/*

	 * Arguments: [<repeat_count>]

	 * 	<repeat_count>: The number of I/Os before switching path.

	 * 			If not given, default (QL_MIN_IO) is used.

 Allocate the path information structure */

/*

 * Select a path having the minimum number of in-flight I/Os

 Move most recently used to least preferred to evenly balance. */

/*

 * Copyright (C) 2003 Sistina Software.

 * Copyright (C) 2004-2005 Red Hat, Inc. All rights reserved.

 *

 * Module Author: Heinz Mauelshagen

 *

 * This file is released under the GPL.

 *

 * Round-robin path selector.

/*-----------------------------------------------------------------

 * Path-handling code, paths are held in lists

/*-----------------------------------------------------------------

 * Round-robin selector

/*

 * Called during initialisation to register each path with an

 * optional repeat_count.

 First path argument is number of I/Os before switching path */

 allocate the path */

 SPDX-License-Identifier: GPL-2.0-only

/*----------------------------------------------------------------

 * Writeset

	/*

	 * An in core copy of the bits to save constantly doing look ups on

	 * disk.

/*

 * This does not free off the on disk bitset as this will normally be done

 * after digesting into the era array.

/*

 * Allocates memory for the in core bitset.

/*

 * Wipes the in-core bitset, and creates a new on disk bitset.

	/*

	 * The bitset was flushed when it was archived, so we know there'll

	 * be no change to the root.

/*

 * Returns < 0 on error, 0 if the bit wasn't previously set, 1 if it was.

 FIXME: fail mode */

/*----------------------------------------------------------------

 * On disk metadata layout

	/*

	 * Only these two fields are valid within the metadata snapshot.

/*----------------------------------------------------------------

 * Superblock validation

/*----------------------------------------------------------------

 * Low level metadata handling

	/*

	 * We preallocate 2 writesets.  When an era rolls over we

	 * switch between them. This means the allocation is done at

	 * preresume time, rather than on the io path.

	/*

	 * A flag that is set whenever a writeset has been archived.

	/*

	 * Reading the space map root can fail, so we read it into this

	 * buffer before the superblock is locked and updated.

 FIXME: duplication with cache and thin */

	/*

	 * We can't use a validator here - it may be all zeroes.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Writes a superblock, including the static fields that don't get updated

 * with every commit (possible optimisation here).  'md' should be fully

 * constructed when this is called.

 FIXME: can't keep blanking the uuid (uuid is currently unused though) */

/*

 * Assumes block_size and the infos are set.

 Verify the data block size hasn't changed */

/*

 * This waits until all era_map threads have picked up the new filter.

/*----------------------------------------------------------------

 * Writesets get 'digested' into the main era array.

 *

 * We're using a coroutine here so the worker thread can do the digestion,

 * thus avoiding synchronisation of the metadata.  Digesting a whole

 * writeset in one go would cause too much latency.

	/*

	 * We initialise another bitset info to avoid any caching side effects

	 * with the previous one.

/*----------------------------------------------------------------

 * High level metadata interface.  Target methods should use these, and not

 * the lower level ones.

	/*

	 * dm_bitset restricts us to 2^32.  test_bit & co. restrict us

	 * further to 2^31 - 1

 FIXME: fail mode */

 FIXME: fail mode? */

 FIXME: fail mode */

	/*

	 * For now we just rollover, but later I want to put a check in to

	 * avoid this if the filter is still pretty fresh.

/*

 * Metadata snapshots allow userland to access era data.

	/*

	 * Whatever happens now we'll commit with no record of the metadata

	 * snap.

----------------------------------------------------------------*/

/*----------------------------------------------------------------

 * Remapping.

/*----------------------------------------------------------------

 * Worker thread

			/*

			 * This is bad news, we need to rollback.

			 * FIXME: finish.

			/*

			 * Only update the in-core writeset if the on-disk one

			 * was updated too.

/*

 * Make an rpc call to the worker to change the metadata.

/*----------------------------------------------------------------

 * Target methods

/*

 * <metadata dev> <data dev> <data block size (sectors)>

	/*

	 * All bios get remapped to the origin device.  We do this now, but

	 * it may not get issued until later.  Depending on whether the

	 * block is marked in this era.

	/*

	 * REQ_PREFLUSH bios carry no data, so we're not interested in them.

 FIXME: fail mode */

/*

 * Status format:

 *

 * <metadata block size> <#used metadata blocks>/<#total metadata blocks>

 * <current era> <held metadata root | '-'>

	/*

	 * If the system-determined stacked limits are compatible with the

	 * era device's blocksize (io_opt is a factor) do not override them.

----------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0

 Maximum size of each resync request */

/*

 * Number of guaranteed raid bios in case of extreme VM load:

/* when we get a read error on a read-only array, we redirect to another

 * device without failing the first device, or trying to over-write to

 * correct the read error.  To keep track of bad blocks on a per-bio

 * level, we store IO_BLOCKED in the appropriate 'bios' pointer

/* When we successfully write to a known bad-block, we need to remove the

 * bad-block marking which must be done from process context.  So we record

 * the success by setting devs[n].bio to IO_MADE_GOOD

/* When there are this many requests queue to be written by

 * the raid thread, we become 'congested' to provide back-pressure

 * for writeback.

 for managing resync I/O pages */

/*

 * 'strct resync_pages' stores actual pages used for doing the resync

 *  IO, and it is per-bio, so make .bi_private points to it.

 generally called after bio_reset() for reseting bvec */

 initialize bvec table again */

		/*

		 * won't fail because the vec table is big

		 * enough to hold all these pages

 SPDX-License-Identifier: GPL-2.0

/*

 * Using 64-bit values to avoid overflow (which is a

 * problem that block/genhd.c's IO accounting has).

/*

 * A typo on the command line could possibly make the kernel run out of memory

 * and crash. To prevent the crash we account all used memory. We fail if we

 * exhaust 1/4 of all memory or 1/2 of vmalloc space.

	/*

	 * Suspend/resume to make sure there is no i/o in flight,

	 * so that newly created statistics will be exact.

	 *

	 * (note: we couldn't suspend earlier because we must not

	 * allocate memory while suspended)

	/*

	 * vfree can't be called from RCU callback

	/*

	 * Output format:

	 *   <region_id>: <start_sector>+<length> <step> <program_id> <aux_data>

	/*

	 * This is racy, but so is part_round_stats_single.

	/*

	 * For strict correctness we should use local_irq_save/restore

	 * instead of preempt_disable/enable.

	 *

	 * preempt_disable/enable is racy if the driver finishes bios

	 * from non-interrupt context as well as from interrupt context

	 * or from more different interrupts.

	 *

	 * On 64-bit architectures the race only results in not counting some

	 * events, so it is acceptable.  On 32-bit architectures the race could

	 * cause the counter going off by 2^32, so we need to do proper locking

	 * there.

	 *

	 * part_stat_lock()/part_stat_unlock() have this race too.

		/*

		 * A race condition can at worst result in the merged flag being

		 * misrepresented, so we don't have to disable preemption here.

/*

 * This is like jiffies_to_msec, but works for 64-bit values.

	/*

	 * Output format:

	 *   <start_sector>+<length> counters

	/*

	 * Input format:

	 *   <range> <step> [<extra_parameters> <parameters>] [<program_id> [<aux_data>]]

	/*

	 * If a buffer overflow happens after we created the region,

	 * it's too late (the userspace would retry with a larger

	 * buffer, but the region id that caused the overflow is already

	 * leaked).  So we must detect buffer overflow in advance.

 All messages here must start with '@' */

 this wasn't a stats message */

 SPDX-License-Identifier: GPL-2.0

/*

 * dm-init.c

 * Copyright (C) 2017 The Chromium OS Authors <chromium-os-dev@chromium.org>

 *

 * This file is released under the GPLv2.

/*

 * Format: dm-mod.create=<name>,<uuid>,<minor>,<flags>,<table>[,<table>+][;<name>,<uuid>,<minor>,<flags>,<table>[,<table>+]+]

 * Table format: <start_sector> <num_sectors> <target_type> <target_args>

 *

 * See Documentation/admin-guide/device-mapper/dm-init.rst for dm-mod.create="..." format

 * details.

/**

 * str_field_delimit - delimit a string based on a separator char.

 * @str: the pointer to the string to delimit.

 * @separator: char that delimits the field

 *

 * Find a @separator and replace it by '\0'.

 * Remove leading and trailing spaces.

 * Return the remainder string after the @separator.

 TODO: add support for escaped characters */

 Delimit the field and remove trailing spaces */

/**

 * dm_parse_table_entry - parse a table entry

 * @dev: device to store the parsed information.

 * @str: the pointer to a string with the format:

 *	<start_sector> <num_sectors> <target_type> <target_args>[, ...]

 *

 * Return the remainder string after the table entry, i.e, after the comma which

 * delimits the entry or NULL if reached the end of the string.

 fields:  */

 Delimit first 3 fields that are separated by space */

 Delimit last field that can be terminated by comma */

 start_sector */

 num_sector */

 target_type */

 target_args */

/**

 * dm_parse_table - parse "dm-mod.create=" table field

 * @dev: device to store the parsed information.

 * @str: the pointer to a string with the format:

 *	<table>[,<table>+]

/**

 * dm_parse_device_entry - parse a device entry

 * @dev: device to store the parsed information.

 * @str: the pointer to a string with the format:

 *	name,uuid,minor,flags,table[; ...]

 *

 * Return the remainder string after the table entry, i.e, after the semi-colon

 * which delimits the entry or NULL if reached the end of the string.

 There are 5 fields: name,uuid,minor,flags,table; */

 Delimit first 4 fields that are separated by comma */

 Delimit last field that can be delimited by semi-colon */

 name */

 uuid */

 minor */

 flags */

 table */

/**

 * dm_parse_devices - parse "dm-mod.create=" argument

 * @devices: list of struct dm_device to store the parsed information.

 * @str: the pointer to a string with the format:

 *	<device>[;<device>+]

/**

 * dm_init_init - parse "dm-mod.create=" argument and configure drivers

/*

 * Copyright (C) 2011-2012 Red Hat UK.

 *

 * This file is released under the GPL.

/*

 * Tunable constants

/*

 * The block size of the device holding pool data must be

 * between 64KB and 1GB.

/*

 * Device id is restricted to 24 bits.

/*

 * How do we handle breaking sharing of data blocks?

 * =================================================

 *

 * We use a standard copy-on-write btree to store the mappings for the

 * devices (note I'm talking about copy-on-write of the metadata here, not

 * the data).  When you take an internal snapshot you clone the root node

 * of the origin btree.  After this there is no concept of an origin or a

 * snapshot.  They are just two device trees that happen to point to the

 * same data blocks.

 *

 * When we get a write in we decide if it's to a shared data block using

 * some timestamp magic.  If it is, we have to break sharing.

 *

 * Let's say we write to a shared block in what was the origin.  The

 * steps are:

 *

 * i) plug io further to this physical block. (see bio_prison code).

 *

 * ii) quiesce any read io to that shared data block.  Obviously

 * including all devices that share this block.  (see dm_deferred_set code)

 *

 * iii) copy the data block to a newly allocate block.  This step can be

 * missed out if the io covers the block. (schedule_copy).

 *

 * iv) insert the new mapping into the origin's btree

 * (process_prepared_mapping).  This act of inserting breaks some

 * sharing of btree nodes between the two devices.  Breaking sharing only

 * effects the btree of that specific device.  Btrees for the other

 * devices that share the block never change.  The btree for the origin

 * device as it was after the last commit is untouched, ie. we're using

 * persistent data structures in the functional programming sense.

 *

 * v) unplug io to this physical block, including the io that triggered

 * the breaking of sharing.

 *

 * Steps (ii) and (iii) occur in parallel.

 *

 * The metadata _doesn't_ need to be committed before the io continues.  We

 * get away with this because the io is always written to a _new_ block.

 * If there's a crash, then:

 *

 * - The origin mapping will point to the old origin block (the shared

 * one).  This will contain the data as it was before the io that triggered

 * the breaking of sharing came in.

 *

 * - The snap mapping still points to the old block.  As it would after

 * the commit.

 *

 * The downside of this scheme is the timestamp magic isn't perfect, and

 * will continue to think that data block in the snapshot device is shared

 * even after the write to the origin has broken sharing.  I suspect data

 * blocks will typically be shared by many different devices, so we're

 * breaking sharing n + 1 times, rather than n, where n is the number of

 * devices that reference this data block.  At the moment I think the

 * benefits far, far outweigh the disadvantages.

----------------------------------------------------------------*/

/*

 * Key building.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * A pool device ties together a metadata device and a data device.  It

 * also provides the interface for creating and destroying internal

 * devices.

/*

 * The pool runs in various modes.  Ordered in degraded order for comparisons.

 metadata may be changed */

 metadata may be changed, though data may not be allocated */

	/*

	 * Like READ_ONLY, except may switch back to WRITE on metadata resize. Reported as READ_ONLY.

 metadata may not be changed */

 all I/O fails */

 Only set if a pool target is bound */

 A dm event has been sent */

/*

 * Target context for a pool.

 Features requested during table load */

 Features used after adjusting for constituent devices */

/*

 * Target context for a thin.

 sorted list of deferred bios */

	/*

	 * Ensures the thin is not destroyed until the worker has finished

	 * iterating the active_thins list.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

		/*

		 * Even if one of the calls to issue_discard failed, we

		 * need to wait for the chain to complete.

	/*

	 * Even if r is set, there could be sub discards in flight that we

	 * need to wait for.

----------------------------------------------------------------*/

/*

 * wake_worker() is used when new work is queued and when pool_resume is

 * ready to continue deferred IO processing.

----------------------------------------------------------------*/

	/*

	 * Allocate a cell from the prison's mempool.

	 * This might block but it can't fail.

		/*

		 * We reused an old cell; we can get rid of

		 * the new one.

----------------------------------------------------------------*/

/*

 * A global list of pools that uses a struct mapped_device as a key.

----------------------------------------------------------------*/

/*

 * This section of code contains the logic for processing a thin device's IO.

 * Much of the code depends on pool object resources (lists, workqueues, etc)

 * but most is exclusively called from the thin target rather than the thin-pool

 * target.

/*

 * Returns the _complete_ blocks that this bio covers.

 so we round up */

 Can happen if the bio is within a single block. */

	/*

	 * Complete bio with an error if earlier I/O caused changes to

	 * the metadata that can't be committed e.g, due to I/O errors

	 * on the metadata device.

	/*

	 * Batch together any bios that trigger commits and then issue a

	 * single commit for them in process_deferred_bios().

----------------------------------------------------------------*/

/*

 * Bio endio functions.

	/*

	 * Track quiescing, copying and zeroing preparation actions.  When this

	 * counter hits zero the block is prepared and can be inserted into the

	 * btree.

	/*

	 * If the bio covers the whole area of a block then we can avoid

	 * zeroing or copying.  Instead this bio is hooked.  The bio will

	 * still be in the cell, so care has to be taken to avoid issuing

	 * the bio twice.

----------------------------------------------------------------*/

/*

 * Workqueue.

/*

 * Prepared mapping jobs.

/*

 * This sends the bios in the cell, except the original holder, back

 * to the deferred_bios list.

			/*

			 * We can't issue the bios with the bio prison lock

			 * held, so we add them to a list to issue on

			 * return from this function.

	/*

	 * We have to be careful to inc any bios we're about to issue

	 * before the cell is released, and avoid a race with new bios

	 * being added to the cell.

	/*

	 * If the bio has the REQ_FUA flag set we must commit the metadata

	 * before signaling its completion.

	/*

	 * Complete bio with an error if earlier I/O caused changes to the

	 * metadata that can't be committed, e.g, due to I/O errors on the

	 * metadata device.

	/*

	 * Batch together any bios that trigger commits and then issue a

	 * single commit for them in process_deferred_bios().

	/*

	 * Commit the prepared block into the mapping btree.

	 * Any I/O for this block arriving after this point will get

	 * remapped to it directly.

	/*

	 * Release any bios held while the block was being provisioned.

	 * If we are processing a write bio that completely covers the block,

	 * we already processed it so can ignore it now when processing

	 * the bios in the cell.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * We've already unmapped this range of blocks, but before we

	 * passdown we have to check that these blocks are now unused.

 find start of unmapped run */

 find end of run */

	/*

	 * It doesn't matter if the passdown discard failed, we still want

	 * to unmap (we ignore err).

	/*

	 * Only this thread allocates blocks, so we can be sure that the

	 * newly unmapped blocks will not be allocated before the end of

	 * the function.

	/*

	 * Increment the unmapped blocks.  This prevents a race between the

	 * passdown io and reallocation of freed blocks.

	/*

	 * The passdown has completed, so now we can decrement all those

	 * unmapped blocks.

/*

 * Deferred bio jobs.

/*

 * A partial copy also needs to zero the uncopied region.

	/*

	 * quiesce action + copy action + an extra reference held for the

	 * duration of this function (we may need to inc later for a

	 * partial zero).

 already quiesced */

	/*

	 * IO to pool_dev remaps to the pool target's data_dev.

	 *

	 * If the whole block of data is being overwritten, we can issue the

	 * bio immediately. Otherwise we use kcopyd to clone the data first.

		/*

		 * Do we need to zero a tail region?

 drop our ref */

 no need to quiesce */

	/*

	 * If the whole block of data is being overwritten or we are not

	 * zeroing pre-existing data, we can issue the bio immediately.

	 * Otherwise we use kcopyd to zero the data first.

/*

 * A non-zero return indicates read_only or fail_io mode.

 * Many callers don't care about the return value.

		/*

		 * Try to commit to see if that will free up some

		 * more space.

 Let's commit before we use up the metadata reserve. */

/*

 * If we have run out of space, queue bios until the device is

 * resumed, presumably after having been reloaded with more space.

 Shouldn't get here */

 Shouldn't get here */

	/*

	 * We don't need to lock the data blocks, since there's no

	 * passdown.  We only lock data blocks for allocation and breaking sharing.

 we did our best */

			/*

			 * Silently fail, letting any mappings we've

			 * created complete.

 contention, we'll give up with this range */

		/*

		 * IO may still be going to the destination block.  We must

		 * quiesce before we can do the removal.

		/*

		 * The parent bio must not complete before sub discard bios are

		 * chained to it (see end_discard's bio_chain)!

		 *

		 * This per-mapping bi_remaining increment is paired with

		 * the implicit decrement that occurs via bio_endio() in

		 * end_discard().

	/*

	 * The virt_cell will only get freed once the origin bio completes.

	 * This means it will remain locked while all the individual

	 * passdown bios are in flight.

	/*

	 * We complete the bio now, knowing that the bi_remaining field

	 * will prevent completion until the sub range discards have

	 * completed.

		/*

		 * The discard covers less than a block.

		/*

		 * Potential starvation issue: We're relying on the

		 * fs/application being well behaved, and not trying to

		 * send IO to a region at the same time as discarding it.

		 * If they do this persistently then it's possible this

		 * cell will never be granted.

	/*

	 * If cell is already occupied, then sharing is already in the process

	 * of being broken so we have nothing further to do here.

	/*

	 * Remap empty bios (flushes) immediately, without provisioning.

	/*

	 * Fill read bios with zeroes and complete them immediately.

	/*

	 * If cell is already occupied, then the block is already

	 * being provisioned so we have nothing further to do here.

/*

 * FIXME: should we also commit due to size of transaction, measured in

 * metadata blocks?

 Sort deferred_bio_list using rb-tree */

	/*

	 * Transfer the sorted bios in sort_bio_list back to

	 * deferred_bio_list to allow lockless submission of

	 * all bios.

		/*

		 * If we've got no free new_mapping structs, and processing

		 * this bio might require one, we pause until there are some

		 * prepared mappings to process.

			/*

			 * If we've got no free new_mapping structs, and processing

			 * this bio might require one, we pause until there are some

			 * prepared mappings to process.

/*

 * We can't hold rcu_read_lock() around code that can block.  So we

 * find a thin with the rcu lock held; bump a refcount; then drop

 * the lock.

	/*

	 * If there are any deferred flush bios, we must commit the metadata

	 * before issuing them or signaling their completion.

		/*

		 * The data device was flushed as part of metadata commit,

		 * so complete redundant flushes immediately.

/*

 * We want to commit periodically so that not too much

 * unwritten data builds up.

/*

 * We're holding onto IO to allow userland time to react.  After the

 * timeout either the pool will have been resized (and thus back in

 * PM_WRITE mode), or we degrade to PM_OUT_OF_DATA_SPACE w/ error_if_no_space.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * Never allow the pool to transition to PM_WRITE mode if user

	 * intervention is required to verify metadata and data consistency.

	/*

	 * If we were in PM_FAIL mode, rollback of metadata failed.  We're

	 * not going to recover without a thin_repair.	So we never let the

	 * pool move out of the old mode.

		/*

		 * Ideally we'd never hit this state; the low water mark

		 * would trigger userland to extend the pool before we

		 * completely run out of data space.  However, many small

		 * IOs to unprovisioned space can consume data space at an

		 * alarming rate.  Adjust your low water mark if you're

		 * frequently seeing this mode.

	/*

	 * The pool mode may have changed, sync it so bind_control_target()

	 * doesn't cause an unexpected mode transition on resume.

----------------------------------------------------------------*/

/*

 * Mapping functions.

/*

 * Called only while mapping a thin bio to hand it over to the workqueue.

/*

 * Non-blocking function called from the thin target's map function.

	/*

	 * We must hold the virtual cell before doing the lookup, otherwise

	 * there's a race with discard.

	/*

	 * Note that we defer readahead too.

			/*

			 * We have a race condition here between the

			 * result.shared value returned by the lookup and

			 * snapshot creation, which may cause new

			 * sharing.

			 *

			 * To avoid this always quiesce the origin before

			 * taking the snap.  You want to do this anyway to

			 * ensure a consistent application view

			 * (i.e. lockfs).

			 *

			 * More distant ancestors are irrelevant. The

			 * shared flag will be set in their case.

		/*

		 * Must always call bio_io_error on failure.

		 * dm_thin_find_block can fail with -EINVAL if the

		 * pool is switched to fail-io mode.

/*----------------------------------------------------------------

 * Binding of control targets to a pool object

/*

 * If discard_passdown was enabled verify that the data device

 * supports discards.  Disable discard_passdown if not.

	/*

	 * We want to make sure that a pool in PM_FAIL mode is never upgraded.

	/*

	 * Don't change the pool's mode until set_pool_mode() below.

	 * Otherwise the pool's process_* function pointers may

	 * not match the desired pool mode.

/*----------------------------------------------------------------

 * Pool creation

 Initialize pool features. */

	/*

	 * Create singlethreaded workqueue that will service all devices

	 * that use this metadata.

/*----------------------------------------------------------------

 * Pool target methods

	/*

	 * No feature arguments supplied.

/*

 * We need to flush the data device **before** committing the metadata.

 *

 * This ensures that the data blocks of any newly inserted mappings are

 * properly written to non-volatile storage and won't be lost in case of a

 * crash.

 *

 * Failure to do so can result in data corruption in the case of internal or

 * external snapshots and in the case of newly provisioned blocks, when block

 * zeroing is enabled.

/*

 * When a metadata threshold is crossed a dm event is triggered, and

 * userland should respond by growing the metadata device.  We could let

 * userland set the threshold, like we do with the data threshold, but I'm

 * not sure they know enough to do this well.

	/*

	 * 4M is ample for all ops with the possible exception of thin

	 * device deletion which is harmless if it fails (just retry the

	 * delete after you've grown the device).

 4M */, quarter);

/*

 * thin-pool <metadata dev> <data dev>

 *	     <data block size (sectors)>

 *	     <low water mark (blocks)>

 *	     [<#feature args> [<arg>]*]

 *

 * Optional feature arguments are:

 *	     skip_block_zeroing: skips the zeroing of newly-provisioned blocks.

 *	     ignore_discard: disable discard

 *	     no_discard_passdown: don't pass discards down to the data device

 *	     read_only: Don't allow any changes to be made to the pool metadata.

 *	     error_if_no_space: error IOs, instead of queueing, if no space.

	/*

	 * FIXME Remove validation from scope of lock.

 make sure metadata and data are different devices */

	/*

	 * Set default pool features.

	/*

	 * 'pool_created' reflects whether this is the first table load.

	 * Top level discard support is not allowed to be changed after

	 * initial load.  This would require a pool reload to trigger thin

	 * device changes.

	/*

	 * Only need to enable discards if the pool should pass

	 * them down to the data device.  The thin device's discard

	 * processing will cause mappings to be removed from the btree.

		/*

		 * Setting 'discards_supported' circumvents the normal

		 * stacking of discard limits (this keeps the pool and

		 * thin devices' discard limits consistent).

	/*

	 * As this is a singleton target, ti->begin is always zero.

/*

 * Retrieves the number of blocks of the data device from

 * the superblock and compares it to the actual device size,

 * thus resizing the data device in case it has grown.

 *

 * This both copes with opening preallocated data devices in the ctr

 * being followed by a resume

 * -and-

 * calling the resume method individually after userspace has

 * grown the data device in reaction to a table event.

	/*

	 * Take control of the pool object.

 Suspend all active thin devices */

 Resume all active thin devices */

	/*

	 * Must requeue active_thins' bios and then resume

	 * active_thins _before_ clearing 'suspend' flag.

/*

 * Messages supported:

 *   create_thin	<dev_id>

 *   create_snap	<dev_id> <origin_id>

 *   delete		<dev_id>

 *   set_transaction_id <current_trans_id> <new_trans_id>

 *   reserve_metadata_snap

 *   release_metadata_snap

/*

 * Status line is:

 *    <transaction id> <used metadata sectors>/<total metadata sectors>

 *    <used data sectors>/<total data sectors> <held metadata root>

 *    <pool mode> <discard config> <no space config> <needs_check>

 Commit to ensure statistics aren't out-of-date */

	/*

	 * If max_sectors is smaller than pool->sectors_per_block adjust it

	 * to the highest possible power-of-2 factor of pool->sectors_per_block.

	 * This is especially beneficial when the pool's data device is a RAID

	 * device that has a full stripe width that matches pool->sectors_per_block

	 * -- because even though partial RAID stripe-sized IOs will be issued to a

	 *    single RAID stripe; when aggregated they will end on a full RAID stripe

	 *    boundary.. which avoids additional partial RAID stripe writes cascading

	/*

	 * If the system-determined stacked limits are compatible with the

	 * pool's blocksize (io_opt is a factor) do not override them.

	/*

	 * pt->adjusted_pf is a staging area for the actual features to use.

	 * They get transferred to the live pool in bind_control_target()

	 * called from pool_preresume().

		/*

		 * Must explicitly disallow stacking discard limits otherwise the

		 * block layer will stack them if pool's data device has support.

		 * QUEUE_FLAG_DISCARD wouldn't be set but there is no way for the

		 * user to see that, so make sure to set all discard limits to 0.

	/*

	 * The pool uses the same discard limits as the underlying data

	 * device.  DM core has already set this up.

/*----------------------------------------------------------------

 * Thin target methods

/*

 * Thin target parameters:

 *

 * <pool_dev> <dev_id> [origin_dev]

 *

 * pool_dev: the path to the pool (eg, /dev/mapper/my_pool)

 * dev_id: the internal device identifier

 * origin_dev: a device external to the pool that should act as the origin

 *

 * If the pool device has discards disabled, they get disabled for the thin

 * device as well.

 In case the pool supports discards, pass them on. */

 reacquire for __pool_dec */

	/*

	 * This synchronize_rcu() call is needed here otherwise we risk a

	 * wake_worker() call finding no bios to process (because the newly

	 * added tc isn't yet visible).  So this reduces latency since we

	 * aren't then dependent on the periodic commit to wake_worker().

	/*

	 * The dm_noflush_suspending flag has been cleared by now, so

	 * unfortunately we must always run this.

/*

 * <nr mapped sectors> <highest mapped sector>

	/*

	 * We can't call dm_pool_get_data_dev_size() since that blocks.  So

	 * we follow a more convoluted path through to the pool's target.

 nothing is bound */

 16G */

----------------------------------------------------------------*/

/*

 * Copyright (C) 2003 Sistina Software

 * Copyright (C) 2006 Red Hat GmbH

 *

 * This file is released under the GPL.

/*

 * Aligning 'struct io' reduces the number of bits required to store

 * its address.  Refer to store_io_and_region_in_bio() below.

/*

 * Create a client with mempool and bioset.

/*-----------------------------------------------------------------

 * We need to keep track of which region a bio is doing io for.

 * To avoid a memory allocation to store just 5 or 6 bits, we

 * ensure the 'struct io' pointer is aligned so enough low bits are

 * always zero and then combine it with the region number directly in

 * bi_private.

/*-----------------------------------------------------------------

 * We need an io object to keep track of the number of bios that

 * have been dispatched for a particular io.

	/*

	 * The bio destructor in bio_put() may use the io object.

/*-----------------------------------------------------------------

 * These little objects provide an abstraction for getting a new

 * destination page for io.

/*

 * Functions for getting the pages from a list.

/*

 * Functions for getting the pages from a bvec.

 avoid figuring it out again in bio_next_page() */

	/*

	 * We just use bvec iterator to retrieve pages, so it is ok to

	 * access the bvec table directly here

/*

 * Functions for getting the pages from a VMA.

/*

 * Functions for getting the pages from kernel memory.

/*-----------------------------------------------------------------

 * IO routines that accept a list of pages.

	/*

	 * Reject unsupported discard and write same requests.

	/*

	 * where->count may be zero if op holds a flush and we need to

	 * send a zero-sized flush.

		/*

		 * Allocate a suitably sized-bio.

			/*

			 * WRITE SAME only uses a single page.

			/*

			 * Try and add as many pages as possible.

	/*

	 * For multiple regions we need to be careful to rewind

	 * the dp object for each call to do_region.

	/*

	 * Drop the extra reference that we were holding to avoid

	 * the io being completed too early.

 see dispatch_io() */

 see dispatch_io() */

 Set up dpages based on memory type */

/*

 * New collapsed (a)synchronous interface.

 *

 * If the IO is asynchronous (i.e. it has notify.fn), you must either unplug

 * the queue with blk_unplug() some time later or set REQ_SYNC in

 * io_req->bi_opf. If you fail to do one of these, the IO will be submitted to

 * the disk after q->unplug_delay, which defaults to 3ms in blk-settings.c.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * bitmap.c two-level bitmap (C) Peter T. Breuer (ptb@ot.uc3m.es) 2003

 *

 * bitmap_create  - sets up the bitmap structure

 * bitmap_destroy - destroys the bitmap structure

 *

 * additions, Copyright (C) 2003-2004, Paul Clements, SteelEye Technology, Inc.:

 * - added disk storage for bitmap

 * - changes to allow various bitmap chunk sizes

/*

 * Still to do:

 *

 * flush after percent set rather than just time based. (maybe both).

/*

 * check a page and, if necessary, allocate it (or hijack it if the alloc fails)

 *

 * 1) check to see if this page is allocated, if it's not then try to alloc

 * 2) if the alloc fails, set the page's hijacked flag so we'll use the

 *    page pointer directly as a counter

 *

 * if we find our page, we increment the page's refcount so that it stays

 * allocated while we're using it

		/* This can happen if bitmap_start_sync goes beyond

		 * End-of-device while looking for a whole page.

		 * It is harmless.

 it's hijacked, don't try to alloc */

 page is already allocated, just return */

 this page has not been allocated yet */

	/* It is possible that this is being called inside a

	 * prepare_to_wait/finish_wait loop from raid5c:make_request().

	 * In general it is not permitted to sleep in that context as it

	 * can cause the loop to spin freely.

	 * That doesn't apply here as we can only reach this point

	 * once with any loop.

	 * When this function completes, either bp[page].map or

	 * bp[page].hijacked.  In either case, this function will

	 * abort before getting to this point again.  So there is

	 * no risk of a free-spin, and so it is safe to assert

	 * that sleeping here is allowed.

 We don't support hijack for cluster raid */

		/* failed - set the hijacked flag so that we can use the

 somebody beat us to getting the page */

 no page was in place and we have one, so install it */

 if page is completely empty, put it back on the free list, or dealloc it */

 if page was hijacked, unmark the flag so it might get alloced next time */

 Note: lock should be held when calling this */

 page is still busy */

 page is no longer in use, it can be released */

 page was hijacked, undo this now */

 normal case, free the page */

/*

 * bitmap file handling - read and write the bitmap file and its superblock

/*

 * basic page I/O operations

 IO operations when bitmap is stored near all superblocks */

 choose a good rdev and read the page from there */

	/* Iterate the disks of an mddev, using rcu to protect access to the

	 * linked list, and raising the refcount of devices we return to ensure

	 * they don't disappear while in use.

	 * As devices are only added or removed when raid_disk is < 0 and

	 * nr_pending is 0 and In_sync is clear, the entries we return will

	 * still be in the same position on the list when we re-enter

	 * list_for_each_entry_continue_rcu.

	 *

	 * Note that if entered with 'rdev == NULL' to start at the

	 * beginning, we temporarily assign 'rdev' to an address which

	 * isn't really an rdev, but which can be used by

	 * list_for_each_entry_continue_rcu() to find the first entry.

 start at the beginning */

 release the previous rdev and start from there. */

 this is a usable devices */

		/* Just make sure we aren't corrupting data or

		 * metadata

 Bitmap could be anywhere. */

 DATA  BITMAP METADATA  */

 bitmap runs in to metadata */

 data runs in to bitmap */

 METADATA BITMAP DATA */

 bitmap runs in to data */

 DATA METADATA BITMAP - no problems */

/*

 * write out a page to a file

/* read a page from a file.

 * We both read the page, and attach buffers to the page to record the

 * address of each block (using bmap).  These addresses will be used

 * to write the block later, completely bypassing the filesystem.

 * This usage is similar to how swap files are handled, and allows us

 * to write to a file with no concerns of memory allocation failing.

/*

 * bitmap file superblock operations

/*

 * md_bitmap_wait_writes() should be called before writing any bitmap

 * blocks, to ensure previous writes, particularly from

 * md_bitmap_daemon_work(), have completed.

		/* Note that we ignore the return value.  The writes

		 * might have failed, but that would just mean that

		 * some bits which should be cleared haven't been,

		 * which is safe.  The relevant bitmap blocks will

		 * probably get written again, but there is no great

		 * loss if they aren't.

 update the event counter and sync the superblock to disk */

 no bitmap for this array */

 no superblock */

 rocking back to read-only */

	/*

	 * clear BITMAP_WRITE_ERROR bit to protect against the case that

	 * a bitmap write error occurred but the later writes succeeded.

 Just in case these have been changed via sysfs: */

 This might have been changed by a reshape */

 print out the bitmap file superblock */

/*

 * bitmap_new_disk_sb

 * @bitmap

 *

 * This function is somewhat the reverse of bitmap_read_sb.  bitmap_read_sb

 * reads and verifies the on-disk bitmap superblock and populates bitmap_info.

 * This function verifies 'bitmap_info' and populates the on-disk bitmap

 * structure, which is to be written to disk.

 *

 * Returns: 0 on success, -Exxx on error

	/*

	 * FIXME: write_behind for RAID1.  If not specified, what

	 * is a good choice?  We choose COUNTER_MAX / 2 arbitrarily.

 keep the array size field of the bitmap superblock up to date */

 read the superblock from the bitmap file and initialize some bitmap fields */

 page 0 is the superblock, read it... */

 If cluster_slot is set, the cluster is setup */

 bits to bytes */

 to 4k blocks */

	/* Setup nodes/clustername only if bitmap version is

	 * cluster-compatible

 verify that the bitmap-specific fields are valid */

 keep the array size field of the bitmap superblock up to date */

		/*

		 * We have a persistent array superblock, so compare the

		 * bitmap's UUID and event counter to the mddev's

 assign fields using values from superblock */

 Assigning chunksize is required for "re_read" */

/*

 * general bitmap file operations

/*

 * on-disk bitmap:

 *

 * Use one bit per "chunk" (block set). We do the disk I/O on the bitmap

 * file a page at a time. There's a superblock at the start of the file.

 calculate the index of the page that contains this bit */

 calculate the (bit) offset of this bit within a page */

/*

 * return a pointer to the page in the filemap that contains the given bit

 *

	/* We need 4 bits per page, rounded up to a multiple

 0 is sb_page, release it below */

/*

 * bitmap_file_kick - if an error occurs while manipulating the bitmap file

 * then it is no longer reliable, so we stop using it and we mark the file

 * as failed in the superblock

 there are set bits that need to be synced */

	BITMAP_PAGE_PENDING = 1,   /* there are bits that are being cleaned.

 there are cleared bits that need to be synced */

/*

 * bitmap_file_set_bit -- called before performing a write to the md device

 * to set (and eventually sync) a particular bit in the bitmap file

 *

 * we set the bit immediately, then we record the page number so that

 * when an unplug occurs, we can flush the dirty pages out to disk

 set the bit */

 record page number so it gets flushed to disk when unplug occurs */

/* this gets called when the md device is ready to unplug its underlying

 * (slave) device queues -- before we let any writes go down, we need to

	/* look at each page to see if there are any set bits that need to be

/* * bitmap_init_from_disk -- called at bitmap_create time to initialize

 * the in-memory bitmap from the on-disk bitmap -- also, sets up the

 * memory mapping of the bitmap file

 * Special cases:

 *   if there's no bitmap file, or if the bitmap file had been

 *   previously kicked from the array, we mark all the bits as

 *   1's in order to cause a full resync.

 *

 * We ignore all bits for sectors that end earlier than 'start'.

 * This is used when reading an out-of-date bitmap...

 No permanent bitmap - fill with '1s'. */

 if the disk bit is set, set the memory bit */

 this is a new page, read it in */

 unmap the old page, we're done with it */

				/*

				 * if bitmap is out of date, dirty the

				 * whole page and write it out

 if the disk bit is set, set the memory bit */

	/* We don't actually write all bitmap blocks here,

	 * just flag them as needing to be written

 Only one copy, so nothing needed */

/*

 * bitmap daemon -- periodically wakes up to clean bits and flush pages

 *			out to disk

	/* Use a mutex to guard daemon_work against

	 * bitmap_destroy.

	/* Any file-page which is PENDING now needs to be written.

	 * So set NEEDWRITE now, then after we make any last-minute changes

	 * we will write it.

		/* Arrange for superblock update as well as

	/* Now look at the bitmap counters and if any are '2' or '1',

	 * decrement and handle accordingly.

 We can clear the bit */

	/* Now start writeout on any page in NEEDWRITE that isn't DIRTY.

	 * DIRTY pages need to be written by bitmap_unplug so it can wait

	 * for them.

	 * If we find any DIRTY page we stop there and let bitmap_unplug

	 * handle all the rest.  This is important in the case where

	 * the first blocking holds the superblock and it has been updated.

	 * We mustn't write any other blocks before the superblock.

 bitmap_unplug will handle the rest */

	/* If 'create', we might release the lock and reclaim it.

	 * The lock must have been taken with interrupts enabled.

	 * If !create, we don't release the lock.

 now locked ... */

 hijacked pointer */

		/* should we use the first or second counter field

 page is allocated */

			/* note that it is safe to do the prepare_to_wait

			 * after the test as long as we do it before dropping

			 * the spinlock.

 FIXME or bitmap set as 'failed' */

 always resync if no bitmap */

 locked */

 don't set/clear bits if degraded */

	/* bitmap_start_sync must always report on multiples of whole

	 * pages, otherwise resync (which is very PAGE_SIZE based) will

	 * get confused.

	 * So call __bitmap_start_sync repeatedly (if needed) until

	 * At least PAGE_SIZE>>9 blocks are covered.

	 * Return the 'or' of the result.

 locked */

	/* Sync has finished, and any bitmap chunks that weren't synced

	 * properly have been aborted.  It remains to us to clear the

	 * RESYNC bit wherever it is still on

	/* For each chunk covered by any of these sectors, set the

	 * counter to 2 and possibly set resync_needed.  They should all

	 * be 0 at this point

 dirty the memory and file bits for bitmap chunks "s" to "e" */

			/* We are asserting that the array is dirty,

			 * so move the recovery_cp address back so

			 * that it is obvious that it is dirty

/*

 * flush out any pending updates

 there was no bitmap */

	/* run the daemon_work three time to ensure everything is flushed

	 * that can be

/*

 * free memory that was allocated

 there was no bitmap */

 Shouldn't be needed - but just in case.... */

 release the bitmap file  */

 free all allocated memory */

 deallocate the page memory */

 wait for behind writes to complete */

 need to kick something here to make sure I/O goes? */

 there was no bitmap */

 disconnect from the md device */

/*

 * initialize the bitmap structure

 * if this returns an error, bitmap_destroy must be called to do clean up

 * once mddev->bitmap is set

		/* As future accesses to this file will use bmap,

		 * and bypass the page cache, we must sync the file

		 * first.

 read superblock from bitmap file (this sets mddev->bitmap_info.chunksize) */

		/*

		 * If 'MD_ARRAY_FIRST_USE' is set, then device-mapper is

		 * instructing us to create a new on-disk bitmap instance.

			/* chunksize and time_base need to be

	/* Clear out old bitmap info first:  Either there is none, or we

	 * are resuming after someone else has possibly changed things,

	 * so we should forget old cached info.

	 * All chunks should be clean, but some might need_sync.

		/* no need to keep dirty bits to optimise a

 Kick recovery in case any bits were set */

 caller need to free returned bitmap with md_bitmap_free() */

/* Loads the bitmap associated with slot and copies the resync information

 * to our bitmap

		/* BITMAP_PAGE_PENDING is set, but bitmap_unplug needs

	/* If chunk_size is 0, choose an appropriate chunk size.

	 * Then possibly allocate new storage space.

	 * Then quiesce, copy bits, replace bitmap, and re-start

	 *

	 * This function is called both to set up the initial bitmap

	 * and to resize the bitmap while the array is active.

	 * If this happens as a result of the array being resized,

	 * chunksize will be zero, and we need to choose a suitable

	 * chunksize, otherwise we use what we are given.

		/* If there is enough space, leave the chunk size unchanged,

		 * else increase by factor of two until there is enough space.

			/* We don't know how much space there is, so limit

			 * to current size - in sectors.

 'chunkshift' is shift from block size to chunk size */

 For cluster raid, need to pre-allocate bitmap */

 deallocate the page memory */

 restore some fields from old_counts */

 need to set on-disk bits too. */

				/* new space.  It needs to be resynced, so

				 * we set NEEDED_MASK.

 bitmap already configured.  Only option is to clear it */

 No bitmap, OK to set a location */

 nothing to be done */;

 Not supported yet */

		/* Ensure new bitmap info is stored in

		 * metadata promptly.

/* 'bitmap/space' is the space available at 'location' for the

 * bitmap.  This allows the kernel to know when it is safe to

 * resize the bitmap to match a resized array.

 Bitmap is too big for this small space */

	/* could make sure it isn't too big, but that isn't really

	 * needed - user-space should be careful.

 timeout can be set at any time */

 just to make sure we don't overflow... */

		/* if thread->timeout is MAX_SCHEDULE_TIMEOUT, then

		 * the bitmap is all clean and we don't need to

		 * adjust the timeout right now

	/*

	 * Without write mostly device, it doesn't make sense to set

	 * backlog for max_write_behind.

 serial_info_pool is not needed if backlog is zero */

 serial_info_pool is needed since backlog is not zero */

 Can only be changed when no bitmap is active */

/*

 * Copyright (C) 2015 Red Hat. All rights reserved.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * Safe division functions that return zero on divide by zero.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * An entry_space manages a set of entries that we use for the queues.

 * The clean and dirty queues share entries, so this object is separate

 * from the queue itself.

----------------------------------------------------------------*/

 excluding sentinel entries */

----------------------------------------------------------------*/

/*

 * The stochastic-multi-queue is a set of lru lists stacked into levels.

 * Entries are moved up levels when they are used, which loosely orders the

 * most accessed entries in the top levels and least in the bottom.  This

 * structure is *much* better than a single lru list.

	/*

	 * We maintain a count of the number of entries we would like in each

	 * level.

/*

 * Insert an entry to the back of the given level.

/*

 * Return the oldest entry of the lowest populated level.

/*

 * This function assumes there is a non-sentinel entry to pop.  It's only

 * used by redistribute, so we know this is true.  It also doesn't adjust

 * the q->nr_elts count.

/*

 * Typically we have fewer elements in the top few levels which allows us

 * to adjust the promote threshold nicely.

		/*

		 * Pull down some entries from the level above.

 bug in nr_elts */

		/*

		 * Push some entries up.

 bug in nr_elts */

 try and find an entry to swap with */

----------------------------------------------------------------*/

/*

 * There are times when we don't have any confidence in the hotspot queue.

 * Such as when a fresh cache is created and the blocks have been spread

 * out across the levels, or if an io load changes.  We detect this by

 * seeing how often a lookup is in the top levels of the hotspot queue.

----------------------------------------------------------------*/

/*

 * All cache entries are stored in a chained hash table.  To save space we

 * use indexing again, and only store indexes to the next entry.

/*

 * Also moves each entry to the front of the bucket.

		/*

		 * Move to the front because this entry is likely

		 * to be hit again.

	/*

	 * The down side of using a singly linked list is we have to

	 * iterate the bucket to remove an item.

----------------------------------------------------------------*/

	/*

	 * We can't memset because that would clear the hotspot and

	 * sentinel bits which remain constant.

 FIXME: audit */

/*

 * This assumes the cblock hasn't already been allocated.

----------------------------------------------------------------*/

 protects everything */

	/*

	 * We maintain three queues of entries.  The cache proper,

	 * consisting of a clean and dirty queue, containing the currently

	 * active mappings.  The hotspot queue uses a larger block size to

	 * track blocks that are being hit frequently and potential

	 * candidates for promotion to the cache.

	/*

	 * Keeps track of time, incremented by the core.  We use this to

	 * avoid attributing multiple hits within the same tick.

	/*

	 * The hash tables allows us to quickly find an entry by origin

	 * block.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

 !h, !q, a -> h, q, a

	/*

	 * Pending work has temporarily been taken out of the queues.

	/*

	 * The promote level depends on the current performance of the

	 * cache.

	 *

	 * If the cache is performing badly, then we can't afford

	 * to promote much without causing performance to drop below that

	 * of the origin device.

	 *

	 * If the cache is performing well, then we don't need to promote

	 * much.  If it isn't broken, don't fix it.

	 *

	 * If the cache is middling then we promote more.

	 *

	 * This scheme reminds me of a graph of entropy vs probability of a

	 * binary variable.

	/*

	 * If there are unused cache entries then we want to be really

	 * eager to promote.

	/*

	 * If the hotspot queue is performing badly then we have little

	 * confidence that we know which blocks to promote.  So we cut down

	 * the amount of promotions.

/*

 * If the hotspot queue is performing badly, then we try and move entries

 * around more quickly.

----------------------------------------------------------------*/

/*

 * Targets are given as a percentage.

	/*

	 * Cache entries may not be populated.  So we cannot rely on the

	 * size of the clean queue.

		/*

		 * We'd like to clean everything.

	/*

	 * If we're busy we don't worry about cleaning at all.

----------------------------------------------------------------*/

		/*

		 * We always claim to be 'idle' to ensure some demotions happen

		 * with continuous loads.

	/*

	 * We allocate the entry now to reserve the cblock.  If the

	 * background work is aborted we must remember to free it.

----------------------------------------------------------------*/

/*

 * Converts a boolean into a promote result.

----------------------------------------------------------------*/

/*

 * Public interface, via the policy struct.  See dm-cache-policy.h for a

 * description of these.

----------------------------------------------------------------*/

		/*

		 * The hotspot queue only gets updated with misses.

/*

 * We need to clear any pending work flags that have been set, and in the

 * case of promotion free the entry for the destination cblock.

 !h, !q, a

 h, q, a

 !h, !q, !a

 h, !q, a

 !h, !q, !a

 h, q, a

 h, !q, a

 h, q, a

 in_hash(oblock) -> in_hash(oblock)

	/*

	 * When we load mappings we push ahead of both sentinels in order to

	 * allow demotions and cleaning to occur immediately.

 FIXME: what if this block has pending background work?

/*

 * smq has no config values, but the old mq policy did.  To avoid breaking

 * software we continue to accept these configurables for the mq policy,

 * but they have no effect.

 Init the policy plugin interface function pointers. */

 FIXME: hard coded value */

----------------------------------------------------------------*/

/*

 * Copyright (C) 2002 Sistina Software (UK) Limited.

 * Copyright (C) 2006 Red Hat GmbH

 *

 * This file is released under the GPL.

 *

 * Kcopyd provides a simple interface for copying an area of one

 * block-device to one or more other block-devices, with an asynchronous

 * completion notification.

/*-----------------------------------------------------------------

 * Each kcopyd client has its own little pool of preallocated

 * pages for kcopyd io.

/*

 * We maintain four lists of jobs:

 *

 * i)   jobs waiting for pages

 * ii)  jobs that have pages, and are waiting for the io to be issued.

 * iii) jobs that don't need to do any IO and just run a callback

 * iv) jobs that have completed.

 *

 * All four of these are protected by job_lock.

/*

 * IO/IDLE accounting slowly decays after (1 << ACCOUNT_INTERVAL_SHIFT) period.

 * When total_period >= (1 << ACCOUNT_INTERVAL_SHIFT) the counters are divided

 * by 2.

/*

 * Sleep this number of milliseconds.

 *

 * The value was decided experimentally.

 * Smaller values seem to cause an increased copy rate above the limit.

 * The reason for this is unknown but possibly due to jiffies rounding errors

 * or read/write cache inside the disk.

/*

 * Maximum number of sleep events. There is a theoretical livelock if more

 * kcopyd clients do work simultaneously which this limit avoids.

	/*

	 * Maintain sane values if we got a temporary overflow.

		/*

		 * Maintain sane values if we got a temporary overflow.

/*

 * Obtain one page for the use of kcopyd.

/*

 * Add the provided pages to a client's free page list, releasing

 * back to the system any beyond the reserved_pages limit.

 Use reserved pages */

/*

 * These three functions resize the page pool.

/*

 * Allocate and reserve nr_pages for the use of a specific client.

/*-----------------------------------------------------------------

 * kcopyd_jobs need to be allocated by the *clients* of kcopyd,

 * for this reason we use a mempool to prevent the client from

 * ever having to do io (which could cause a deadlock).

	/*

	 * Error state of the job.

	/*

	 * Either READ or WRITE

	/*

	 * The destinations for the transfer.

	/*

	 * Set this to ensure you are notified when the job has

	 * completed.  'context' is for callback to use.

	/*

	 * These fields are only used if the job has been split

	 * into more manageable parts.

/*

 * Functions to push and pop a job onto the head of a given job

 * list.

	/*

	 * For I/O jobs, pop any read, any write without sequential write

	 * constraint and sequential writes that are at the right position.

/*

 * These three functions process 1 item from the corresponding

 * job list.

 *

 * They return:

 * < 0: error

 *   0: success

 * > 0: can't process yet.

	/*

	 * If this is the master job, the sub jobs have already

	 * completed so we can free everything.

/*

 * Request io on as many buffer heads as we can currently get for

 * a particular job.

	/*

	 * If we need to write sequentially and some reads or writes failed,

	 * no point in continuing.

 this job is ready for io */

 can't complete now */

/*

 * Run through a list for as long as possible.  Returns the count

 * of successful jobs.

 error this rogue job */

			/*

			 * We couldn't service this job ATM, so

			 * push this job back onto the list.

/*

 * kcopyd does this every time it's woken up.

	/*

	 * The order that these are called is *very* important.

	 * complete jobs can free some pages for pages jobs.

	 * Pages jobs when successful will jump onto the io jobs

	 * list.  io jobs call wake when they complete and it all

	 * starts again.

/*

 * If we are copying a small region we just dispatch a single job

 * to do the copy, otherwise the io has to be split up into many

 * jobs.

 FIXME: tidy this function */

 update the error */

	/*

	 * Only dispatch more work if there hasn't been an error.

 get the next chunk of work */

		/*

		 * Queue the completion callback to the kcopyd thread.

		 *

		 * Some callers assume that all the completions are called

		 * from a single thread and don't race with each other.

		 *

		 * We must not call the callback directly here because this

		 * code may not be executing in the thread.

/*

 * Create some sub jobs to share the work between them.

	/*

	 * Allocate an array of jobs consisting of one master job

	 * followed by SPLIT_COUNT sub jobs.

	/*

	 * set up for the read.

	/*

	 * If one of the destination is a host-managed zoned block device,

	 * we need to write sequentially. If one of the destination is a

	 * host-aware device, then leave it to the caller to choose what to do.

	/*

	 * If we need to write sequentially, errors cannot be ignored.

		/*

		 * Use WRITE ZEROES to optimize zeroing if all dests support it.

/*

 * Cancels a kcopyd job, eg. someone might be deactivating a

 * mirror.

 FIXME: finish */

  0  */

/*-----------------------------------------------------------------

 * Client setup

 Wait for completion of all jobs submitted by this client. */

/*

 * Copyright (C) 2012-2017 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * @nr_cells should be the number of cells you want in use _concurrently_.

 * Don't confuse it with the number of distinct keys.

/*

 * Returns true if node found, otherwise it inserts a new one.

 FIXME: shared locks granted above the lock level could starve this

 FIXME: we don't yet know what level these shared locks

 were taken at, so have to quiesce them all.

----------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0

/*

 * When md (and any require personalities) are compiled into the kernel

 * (not a module), arrays can be assembles are boot time using with AUTODETECT

 * where specially marked partitions are registered with md_autodetect_dev(),

 * and with MD_BOOT where devices to be collected are given on the boot line

 * with md=.....

 * The code for that is here.

/*

 * Parse the command-line parameters given our kernel, but do not

 * actually try to invoke the MD device now; that is handled by

 * md_setup_drive after the low-level disk drivers have initialised.

 *

 * 27/11/1999: Fixed to work correctly with the 2.3 kernel (which

 *             assigns the task of parsing integer arguments to the

 *             invoked program now).  Added ability to initialise all

 *             the MD devices (by specifying multiple "md=" lines)

 *             instead of just one.  -- KTK

 * 18May2000: Added support for persistent-superblock arrays:

 *             md=n,0,factor,fault,device-list   uses RAID0 for device n

 *             md=n,-1,factor,fault,device-list  uses LINEAR for device n

 *             md=n,device-list      reads a RAID superblock from the devices

 *             elements in device-list are read by name_to_kdev_t so can be

 *             a hex number or something like /dev/hda1 /dev/sdb

 * 2001-06-03: Dave Cinege <dcinege@psychosis.com>

 *		Shifted name_to_kdev_t() and related operations to md_set_drive()

 *		for later execution. Rewrote section to make devfs compatible.

 MD Number */

 RAID level */

 could be 0 or -1.. */

 Chunk Size */

 the first device is numeric */

 non-persistent */

	/*

	 * Since we don't want to detect and use half a raid array, we need to

	 * wait for the known devices to complete their probing

/*

 * Copyright (C) 2003 Sistina Software Limited.

 * Copyright (C) 2005-2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

 Maximum number of regions recovered in parallel. */

/*-----------------------------------------------------------------

 * Mirror set structures.

 protects the lists */

 bios are waiting until suspend */

 recovery */

 Default mirror */

 if details->bi_bdev == NULL, details were not saved */

/*

 * Every mirror should look like this one.

/*

 * This is yucky.  We squirrel the mirror struct away inside

 * bi_next for read/write buffers.  This is safe since the bh

 * doesn't get submitted to the lower levels of block layer.

/* fail_mirror

 * @m: mirror device to fail

 * @error_type: one of the enum's, DM_RAID1_*_ERROR

 *

 * If errors are being handled, record the type of

 * error encountered for this device.  If this type

 * of error has already been recorded, we can return;

 * otherwise, we must signal userspace by triggering

 * an event.  Additionally, if the device is the

 * primary device, we must choose a new primary, but

 * only if the mirror is in-sync.

 *

 * This function must not block.

	/*

	 * error_count is used for nothing more than a

	 * simple way to tell if a device has encountered

	 * errors.

		/*

		 * Better to issue requests to same failing device

		 * than to risk returning corrupt data.

/*-----------------------------------------------------------------

 * Recovery.

 *

 * When a mirror is first activated we may find that some regions

 * are in the no-sync state.  We have to recover these by

 * recopying from the default mirror to all the others.

 Read error means the failure of default mirror. */

		/*

		 * Bits correspond to devices (excluding default mirror).

		 * The default mirror cannot change during recovery.

 fill in the source */

		/*

		 * The final region may be smaller than

		 * region_size.

 fill in the destinations */

 hand to kcopyd */

	/*

	 * Start quiescing some regions.

	/*

	 * Copy any already quiesced regions.

	/*

	 * Update the in sync flag.

 the sync is complete */

/*-----------------------------------------------------------------

 * Reads

/*

 * remap a buffer to a particular mirror.

	/*

	 * Lock is required to avoid race condition during suspend

	 * process.

		/*

		 * If device is suspended, complete the bio.

	/*

	 * Hold bio until the suspend is complete.

/*-----------------------------------------------------------------

 * Reads

 Asynchronous read. */

		/*

		 * We can only read balance if the region is in sync.

/*-----------------------------------------------------------------

 * Writes.

 *

 * We do different things with the write io depending on the

 * state of the region that it's in:

 *

 * SYNC: 	increment pending, use kcopyd to write to *all* mirrors

 * RECOVERING:	delay the io until recovery completes

 * NOSYNC:	increment pending, just write to the default mirror

	/*

	 * NOTE: We don't decrement the pending count here,

	 * instead it is done by the targets endio function.

	 * This way we handle both writes to SYNC and NOSYNC

	 * regions with the same code.

	/*

	 * If the bio is discard, return an error, but do not

	 * degrade the array.

	/*

	 * Need to raise event.  Since raising

	 * events can block, we need to do it in

	 * the main thread.

	/*

	 * Use default mirror because we only need it to retrieve the reference

	 * to the mirror set in write_callback().

	/*

	 * Classify each write.

	/*

	 * Add bios that are delayed due to remote recovery

	 * back on to the write queue

	/*

	 * Increment the pending counts for any regions that will

	 * be written to (writes to recover regions are going to

	 * be delayed).

	/*

	 * If the flush fails on a previous call and succeeds here,

	 * we must not reset the log_failure variable.  We need

	 * userspace interaction to do that.

	/*

	 * Dispatch io.

	/*

	 * If the log has failed, unattempted writes are being

	 * put on the holds list.  We can't issue those writes

	 * until a log has been marked, so we must store them.

	 *

	 * If a 'noflush' suspend is in progress, we can requeue

	 * the I/O's to the core.  This give userspace a chance

	 * to reconfigure the mirror, at which point the core

	 * will reissue the writes.  If the 'noflush' flag is

	 * not set, we have no choice but to return errors.

	 *

	 * Some writes on the failures list may have been

	 * submitted before the log failure and represent a

	 * failure to write to one of the devices.  It is ok

	 * for us to treat them the same and requeue them

	 * as well.

		/*

		 * If all the legs are dead, fail the I/O.

		 * If the device has failed and keep_log is enabled,

		 * fail the I/O.

		 *

		 * If we have been told to handle errors, and keep_log

		 * isn't enabled, hold the bio and wait for userspace to

		 * deal with the problem.

		 *

		 * Otherwise pretend that the I/O succeeded. (This would

		 * be wrong if the failed leg returned after reboot and

		 * got replicated back to the good legs.)

/*-----------------------------------------------------------------

 * kmirrord

/*-----------------------------------------------------------------

 * Target functions

/*

 * Create dirty log: log_type #log_params <log_params>

/*

 * Construct a mirror mapping:

 *

 * log_type #log_params <log_params>

 * #mirrors [mirror_path offset]{2,}

 * [#features <features>]

 *

 * log_type is "core" or "disk"

 * #log_params is between 1 and 3

 *

 * If present, supported features are "handle_errors" and "keep_log".

 Get the mirror parameter sets */

	/*

	 * Any read-balancing addition depends on the

	 * DM_RAID1_HANDLE_ERRORS flag being present.

	 * This is because the decision to balance depends

	 * on the sync state of a region.  If the above

	 * flag is not present, we ignore errors; and

	 * the sync state may be inaccurate.

/*

 * Mirror mapping function

 Save region for mirror_end_io() handler */

	/*

	 * If region is not in-sync queue the bio.

	/*

	 * The region is in-sync and we can perform reads directly.

	 * Store enough information so we can retry if it fails.

	/*

	 * We need to dec pending if this was a write.

			/*

			 * There wasn't enough memory to record necessary

			 * information for a retry or there was no other

			 * mirror in-sync.

		/*

		 * A failed read is requeued for another attempt using an intact

		 * mirror.

	/*

	 * Process bios in the hold list to start recovery waiting

	 * for bios in the hold list. After the process, no bio has

	 * a chance to be added in the hold list because ms->suspend

	 * is set.

	/*

	 * We must finish up all the work that we've

	 * generated (i.e. recovery work).

 FIXME: need better error handling */

	/*

	 * Now that recovery is complete/stopped and the

	 * delayed bios are queued, we need to wait for

	 * the worker thread to complete.  This way,

	 * we know that all of our I/O has been pushed.

 FIXME: need better error handling */

 FIXME: need better error handling */

/*

 * device_status_char

 * @m: mirror device/leg we want the status of

 *

 * We return one character representing the most severe error

 * we have encountered.

 *    A => Alive - No failures

 *    D => Dead - A write failure occurred leaving mirror out-of-sync

 *    S => Sync - A sychronization failure occurred, mirror out-of-sync

 *    R => Read - A read failure occurred, mirror data unaffected

 *

 * Returns: <char>

 Module hooks */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * multipath.c : Multiple Devices driver for Linux

 *

 * Copyright (C) 1999, 2000, 2001 Ingo Molnar, Red Hat

 *

 * Copyright (C) 1996, 1997, 1998 Ingo Molnar, Miguel de Icaza, Gadi Oxman

 *

 * MULTIPATH management functions.

 *

 * derived from raid1.c.

	/*

	 * Later we do read balancing on the read side

	 * now we use the first available disk.

/*

 * multipath_end_bh_io() is called when we have finished servicing a multipathed

 * operation and are ready to return a success/failure code to the buffer

 * cache layer.

		/*

		 * oops, IO error:

/*

 * Careful, this can execute in IRQ contexts as well!

		/*

		 * Uh oh, we can do nothing if this is our last path, but

		 * first check if this is a queued request for a device

		 * which has just failed.

 leave it active... it's all we have */

	/*

	 * Mark disk as unusable

 lost the race, try later */

/*

 * This is a kernel thread which:

 *

 *	1.	Retries failed read operations on working multipaths.

 *	2.	Updates the raid superblock when problems encounter.

 *	3.	Performs writes following reads for array syncronising.

	/*

	 * copy the already verified devices into our private MULTIPATH

	 * bookkeeping area. [whatever we allocate in multipath_run(),

	 * should be freed in multipath_free()]

	/*

	 * Ok, everything is just fine now

 MULTIPATH */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Red Hat. All rights reserved.

 *

 * This file is released under the GPL.

 writing via async dm-io (implied by notify.fn above) won't return an error */

	/*

	 * Catch incorrect access to these values while the device is suspended.

 Free another committed entry with lower seq-count */

	/*

	 * clflushopt performs better with block size 1024, 2048, 4096

	 * non-temporal stores perform better with block size 512

	 *

	 * block size   512             1024            2048            4096

	 * movnti       496 MB/s        642 MB/s        725 MB/s        744 MB/s

	 * clflushopt   373 MB/s        688 MB/s        1.1 GB/s        1.2 GB/s

	 *

	 * We see that movnti performs better for 512-byte blocks, and

	 * clflushopt performs better for 1024-byte and larger blocks. So, we

	 * prefer clflushopt for sizes >= 768.

	 *

	 * NOTE: this happens to be the case now (with dm-writecache's single

	 * threaded model) but re-evaluate this once memcpy_flushcache() is

	 * enabled to use movdir64b which might invalidate this performance

	 * advantage seen with cache-allocating-writes plus flushing.

 SSD: */

 SSD: */

 make sure that writecache_end_io decrements bio_in_progress: */

 Wait for any active kcopyd work on behalf of ssd writeback */

if (unlikely(n_walked > WRITEBACK_LATENCY) && likely(!wc->writeback_all))

	break;

		/*

		 * If we didn't do any progress, we must wait until some

		 * writeback finishes to avoid burning CPU in a loop

 Verify the following entries[n_blocks] won't overflow */

 check if the bit field overflows */

	/*

	 * Parse the mode (pmem or ssd)

		/*

		 * If the architecture doesn't support persistent memory or

		 * the kernel doesn't support any DAX drivers, this driver can

		 * only be used in SSD-only mode.

	/*

	 * Parse the origin data device

	/*

	 * Parse cache data device (be it pmem or ssd)

	/*

	 * Parse the cache block size

	/*

	 * Parse optional arguments

 this is limitation of test_bit functions */

/*

 * Copyright (C) 2003 Sistina Software (UK) Limited.

 * Copyright (C) 2004, 2010-2011 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

/*

 * Flakey: Used for testing only, simulates intermittent,

 * catastrophic device failure.

 No feature arguments supplied. */

		/*

		 * drop_writes

		/*

		 * error_writes

		/*

		 * corrupt_bio_byte <Nth_byte> <direction> <value> <bio_flags>

			/*

			 * Direction r or w?

			/*

			 * Value of byte (0-255) to write in place of correct one.

			/*

			 * Only corrupt bios with these flags set.

/*

 * Construct a flakey mapping:

 * <dev_path> <offset> <up interval> <down interval> [<#feature args> [<arg>]*]

 *

 *   Feature args:

 *     [drop_writes]

 *     [corrupt_bio_byte <Nth_byte> <direction> <value> <bio_flags>]

 *

 *   Nth_byte starts from 1 for the first byte.

 *   Direction is r for READ or w for WRITE.

 *   bio_flags is ignored if 0.

	/*

	 * Overwrite the Nth byte of the bio's data, on whichever page

	 * it falls.

 Are we alive ? */

		/*

		 * Flag this bio as submitted while down.

		/*

		 * Error reads if neither corrupt_bio_byte or drop_writes or error_writes are set.

		 * Otherwise, flakey_end_io() will decide if the reads should be modified.

		/*

		 * Drop or error writes?

		/*

		 * Corrupt matching writes.

		/*

		 * By default, error all I/O.

			/*

			 * Corrupt successful matching READs while in down state.

			/*

			 * Error read during the down_interval if drop_writes

			 * and error_writes were not configured.

	/*

	 * Only pass ioctls through if the device sizes match exactly.

 Module hooks */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Device Mapper Uevent Support (dm-uevent)

 *

 * Copyright IBM Corporation, 2007

 * 	Author: Mike Anderson <andmike@linux.vnet.ibm.com>

/**

 * dm_send_uevents - send uevents for given list

 *

 * @events:	list of events to send

 * @kobj:	kobject generating event

 *

		/*

		 * When a device is being removed this copy fails and we

		 * discard these unsent events.

/**

 * dm_path_uevent - called to create a new path event and queue it

 *

 * @event_type:	path event type enum

 * @ti:			pointer to a dm_target

 * @path:		string containing pathname

 * @nr_valid_paths:	number of valid paths remaining

 *

/*

 * Copyright (C) 2003 Jana Saout <jana@saout.de>

 * Copyright (C) 2004 Clemens Fruhwirth <clemens@endorphin.org>

 * Copyright (C) 2006-2020 Red Hat, Inc. All rights reserved.

 * Copyright (C) 2013-2020 Milan Broz <gmazyland@gmail.com>

 *

 * This file is released under the GPL.

 for struct rtattr and RTA macros only */

/*

 * context holding the current state of a multi-part conversion

/*

 * per bio private data

 hash + 0 */

/*

 * Crypt: maps a linear range of a block device

 * and encrypts / decrypts at the same time.

 Use authenticated mode for cipher */

 Calculate IV from sector_size, not 512B sectors */

 Must preprocess data for encryption (elephant) */

/*

 * The fields in here must be read only after initialization.

	/*

	 * Layout of each crypto request:

	 *

	 *   struct skcipher_request

	 *      context

	 *      padding

	 *   struct dm_crypt_request

	 *      padding

	 *   IV

	 *

	 * The padding is added so that dm_crypt_request and the IV are

	 * correctly aligned.

 independent parts in key buffer */

 additional keys length */

 MAC key size for authenc(...) */

	/*

	 * pool for per bio private data, crypto requests,

	 * encryption requeusts/buffer pages and integrity tags

 space for keys in authenc() format (if used) */

/*

 * Use this to access cipher attributes that are independent of the key.

/*

 * Different IV generation algorithms:

 *

 * plain: the initial vector is the 32-bit little-endian version of the sector

 *        number, padded with zeros if necessary.

 *

 * plain64: the initial vector is the 64-bit little-endian version of the sector

 *        number, padded with zeros if necessary.

 *

 * plain64be: the initial vector is the 64-bit big-endian version of the sector

 *        number, padded with zeros if necessary.

 *

 * essiv: "encrypted sector|salt initial vector", the sector number is

 *        encrypted with the bulk cipher using a salt as key. The salt

 *        should be derived from the bulk cipher's key via hashing.

 *

 * benbi: the 64-bit "big-endian 'narrow block'-count", starting at 1

 *        (needed for LRW-32-AES and possible other narrow block modes)

 *

 * null: the initial vector is always zero.  Provides compatibility with

 *       obsolete loop_fish2 devices.  Do not use for new devices.

 *

 * lmk:  Compatible implementation of the block chaining mode used

 *       by the Loop-AES block device encryption system

 *       designed by Jari Ruusu. See http://loop-aes.sourceforge.net/

 *       It operates on full 512 byte sectors and uses CBC

 *       with an IV derived from the sector number, the data and

 *       optionally extra IV seed.

 *       This means that after decryption the first block

 *       of sector must be tweaked according to decrypted data.

 *       Loop-AES can use three encryption schemes:

 *         version 1: is plain aes-cbc mode

 *         version 2: uses 64 multikey scheme with lmk IV generator

 *         version 3: the same as version 2 with additional IV seed

 *                   (it uses 65 keys, last key is used as IV seed)

 *

 * tcw:  Compatible implementation of the block chaining mode used

 *       by the TrueCrypt device encryption system (prior to version 4.1).

 *       For more info see: https://gitlab.com/cryptsetup/cryptsetup/wikis/TrueCryptOnDiskFormat

 *       It operates on full 512 byte sectors and uses CBC

 *       with an IV derived from initial key and the sector number.

 *       In addition, whitening value is applied on every sector, whitening

 *       is calculated from initial key, sector number and mixed using CRC32.

 *       Note that this encryption scheme is vulnerable to watermarking attacks

 *       and should be used for old compatible containers access only.

 *

 * eboiv: Encrypted byte-offset IV (used in Bitlocker in CBC mode)

 *        The IV is encrypted little-endian byte-offset (with the same key

 *        and cipher as the volume).

 *

 * elephant: The extended version of eboiv with additional Elephant diffuser

 *           used with Bitlocker CBC mode.

 *           This mode was used in older Windows systems

 *           https://download.microsoft.com/download/0/2/3/0238acaf-d3bf-4a6d-b3d6-0a0be4bbb36e/bitlockercipher200608.pdf

 iv_size is at least of size u64; usually it is 16 bytes */

	/*

	 * ESSIV encryption of the IV is now handled by the crypto API,

	 * so just pass the plain sector number here.

	/* we need to calculate how far we must shift the sector count

 rest is cleared below */

 No seed in LMK version 2 */

 LMK seed is on the position of LMK_KEYS + 1 key */

 Sector is always 512B, block size 16, add data of blocks 1-31 */

 Sector is cropped to 56 bits here */

 No MD5 padding here */

 Tweak the first block of plaintext sector */

 xor whitening with sector number */

 calculate crc32 for every 32bit part and xor it */

 apply whitening (8 bytes) to whole sector */

 Remove whitening from ciphertext */

 Calculate IV */

 Apply whitening on ciphertext */

 Used only for writes, there must be an additional space to store IV */

 Key for AES */

 Elephant sector key */

 E(Ks, e(s)) */

 E(Ks, e'(s)) */

 Cannot modify original bio, copy to sg_out and apply Elephant to it */

/*

 * Integrity extensions

 Get sg containing data */

 From now we require underlying device with our integrity profile */

 Reject unexpected unaligned bio. */

	/* AEAD request:

	 *  |----- AAD -------|------ DATA -------|-- AUTH TAG --|

	 *  | (authenticated) | (auth+encryption) |              |

	 *  | sector_LE |  IV |  sector in/out    |  tag in/out  |

 For READs use IV stored in integrity metadata */

 Store generated IV in integrity metadata */

 Working copy of IV, to be modified in crypto API */

 Reject unexpected unaligned bio. */

 For skcipher we use only the first sg item */

 For READs use IV stored in integrity metadata */

 Data can be already preprocessed in generator */

 Store generated IV in integrity metadata */

 Working copy of IV, to be modified in crypto API */

	/*

	 * Use REQ_MAY_BACKLOG so a cipher driver internally backlogs

	 * requests if driver request queue is full.

	/*

	 * Use REQ_MAY_BACKLOG so a cipher driver internally backlogs

	 * requests if driver request queue is full.

/*

 * Encrypt / decrypt data from one bio to another one (can be the same one)

	/*

	 * if reset_pending is set we are dealing with the bio for the first time,

	 * else we're continuing to work on the previous bio, so don't mess with

	 * the cc_pending counter

		/*

		 * The request was queued by a crypto driver

		 * but the driver request queue is full, let's wait.

					/*

					 * we don't have to block to wait for completion,

					 * so proceed

					/*

					 * we can't wait for completion without blocking

					 * exit and continue processing in a workqueue

		/*

		 * The request is queued and processed asynchronously,

		 * completion function kcryptd_async_done() will be called.

		/*

		 * The request was already processed (synchronously).

		/*

		 * There was a data integrity error.

		/*

		 * There was an error while processing the request.

/*

 * Generate a new unfragmented bio with the given size

 * This should never violate the device limitations (but only because

 * max_segment_size is being constrained to PAGE_SIZE).

 *

 * This function may be called concurrently. If we allocate from the mempool

 * concurrently, there is a possibility of deadlock. For example, if we have

 * mempool of 256 pages, two processes, each wanting 256, pages allocate from

 * the mempool concurrently, it may deadlock in a situation where both processes

 * have allocated 128 pages and the mempool is exhausted.

 *

 * In order to avoid this scenario we allocate the pages under a mutex.

 *

 * In order to not degrade performance with excessive locking, we try

 * non-blocking allocations without a mutex first but on failure we fallback

 * to blocking allocations with a mutex.

 Allocate space for integrity tags */

/*

 * One of the bios was finished. Check for completion of

 * the whole request and correctly clean up the buffer.

	/*

	 * If we are running this function from our tasklet,

	 * we can't call bio_endio() here, because it will call

	 * clone_endio() from dm.c, which in turn will

	 * free the current struct dm_crypt_io structure with

	 * our tasklet. In this case we need to delay bio_endio()

	 * execution to after the tasklet is done and dequeued.

/*

 * kcryptd/kcryptd_io:

 *

 * Needed because it would be very unwise to do decryption in an

 * interrupt context.

 *

 * kcryptd performs the actual encryption or decryption.

 *

 * kcryptd_io performs the IO submission.

 *

 * They must be separated as otherwise the final stages could be

 * starved by new requests which can block in the first stages due

 * to memory allocation.

 *

 * The work is done per CPU global for all dm-crypt instances.

 * They should not depend on each other and do not block.

	/*

	 * free the processed pages

	/*

	 * We need the original biovec array in order to decrypt

	 * the whole bio data *afterwards* -- thanks to immutable

	 * biovecs we don't need to worry about the block layer

	 * modifying the biovec array; so leverage bio_clone_fast().

		/*

		 * Note: we cannot walk the tree here with rb_next because

		 * the structures may be freed when kcryptd_io_write is called.

 crypt_convert should have filled the clone bio */

	/*

	 * Note: zone append writes (REQ_OP_ZONE_APPEND) do not have ordering

	 * constraints so they do not need to be issued inline by

	 * kcryptd_crypt_write_convert().

 Wait for completion signaled by kcryptd_async_done() */

 Encryption was already finished, submit io now */

	/*

	 * Prevent io from disappearing until this function completes.

	/*

	 * Crypto API backlogged the request, because its queue was full

	 * and we're in softirq context, so continue from a workqueue

	 * (TODO: is it actually possible to be in softirq in the write path?)

 Wait for completion signaled by kcryptd_async_done() */

 Encryption was already finished, submit io now */

	/*

	 * Crypto API backlogged the request, because its queue was full

	 * and we're in softirq context, so continue from a workqueue

	/*

	 * A request from crypto driver backlog is going to be processed now,

	 * finish the completion and continue in crypt_convert().

	 * (Callback will be called for the second time for this request.)

	/*

	 * The request is fully completed: for inline writes, let

	 * kcryptd_crypt_write_convert() do the IO submission.

		/*

		 * in_hardirq(): Crypto API's skcipher_walk_first() refuses to work in hard IRQ context.

		 * irqs_disabled(): the kernel may run some IO completion from the idle thread, but

		 * it is being executed with irqs disabled.

	/*

	 * dm-crypt performance can vary greatly depending on which crypto

	 * algorithm implementation is used.  Help people debug performance

	 * problems by logging the ->cra_driver_name.

/*

 * If AEAD is composed like authenc(hmac(sha256),xts(aes)),

 * the key must be for some reason in special format.

 * This funcion converts cc->key to this special format.

 Ignore extra keys (which are used for IV etc) */

	/*

	 * Reject key_string with whitespace. dm core currently lacks code for

	 * proper whitespace escaping in arguments on DM_TABLE_STATUS path.

 look for next ':' separating key_type from key_description */

 clear the flag since following operations may invalidate previously valid key */

 look for next ':' in key string */

 remaining key string should be :<logon|user>:<key_desc> */

 CONFIG_KEYS */

 Hyphen (which gives a key_size of zero) means there is no key. */

 ':' means the key is in kernel keyring, short-circuit normal key processing */

 clear the flag since following operations may invalidate previously valid key */

 wipe references to any kernel keyring key */

 Decode key from its hex representation. */

 Hex key string not needed after here, so wipe it. */

 Wipe IV private keys */

	/*

	 * Note, percpu_counter_read_positive() may over (and under) estimate

	 * the current usage by at most (batch - 1) * num_online_cpus() pages,

	 * but avoids potential spinlock contention of an exact result.

 Must zero key material before freeing */

 at least a 64 bit sector number should fit in our buffer */

 Choose ivmode, see comments at iv code. */

		/*

		 * Version 2 and 3 is recognised according

		 * to length of provided multi-key string.

		 * If present (version 3), last key is used as IV seed.

		 * All keys (including IV seed) are always the same size.

 IV + whitening */

 Need storage space in integrity fields. */

/*

 * Workaround to parse HMAC algorithm from AEAD crypto API spec.

 * The HMAC is needed to calculate tag size (HMAC digest size).

 * This should be probably done by crypto-api calls (once available...)

	/*

	 * New format (capi: prefix)

	 * capi:cipher_api_spec-iv:ivopts

 Separate IV options if present, it can contain another '-' in hash name */

 Parse IV mode */

 The rest is crypto API spec */

 Alloc AEAD, can be used only in new format. */

 Allocate cipher */

	/*

	 * Legacy dm-crypt cipher specification

	 * cipher[:keycount]-mode-iv:ivopts

	/*

	 * For compatibility with the original dm-crypt mapping format, if

	 * only the cipher name is supplied, use cbc-plain.

 Allocate cipher */

 Initialize IV */

 Initialize and set key */

 Allocate IV */

 Initialize IV (set keys for ESSIV etc) */

 wipe the kernel key payload copy */

 Optional parameters */

/*

 * Construct an encryption mapping:

 * <cipher> [<key>|:<key_size>:<user|logon>:<key_description>] <iv_offset> <dev_path> <start>

 Optional parameters need to be read before cipher constructor */

 Allocate the padding exactly */

		/*

		 * If the cipher requires greater alignment than kmalloc

		 * alignment, we don't know the exact position of the

		 * initialization vector. We must assume worst case.

  ...| IV + padding | original IV | original sec. number | bio tag offset | */

		/*

		 * For zoned block devices, we need to preserve the issuer write

		 * ordering. To do so, disable write workqueues and force inline

		 * encryption completion.

		/*

		 * All zone append writes to a zone of a zoned block device will

		 * have the same BIO sector, the start of the zone. When the

		 * cypher IV mode uses sector values, all data targeting a

		 * zone will be encrypted using the first sector numbers of the

		 * zone. This will not result in write errors but will

		 * cause most reads to fail as reads will use the sector values

		 * for the actual data locations, resulting in IV mismatch.

		 * To avoid this problem, ask DM core to emulate zone append

		 * operations with regular writes.

	/*

	 * If bio is REQ_PREFLUSH or REQ_OP_DISCARD, just bypass crypt queues.

	 * - for REQ_PREFLUSH device-mapper core ensures that no IO is in-flight

	 * - for REQ_OP_DISCARD caller must use flush if IO ordering matters

	/*

	 * Check if bio is too large, split as needed.

	/*

	 * Ensure that bio is a multiple of internal sector encryption size

	 * and is aligned to this size as defined in IO hints.

/* Message interface

 *	key set <key>

 *	key wipe

 The key size may not be changed. */

 wipe the kernel key payload copy */

	/*

	 * Unfortunate constraint that is required to avoid the potential

	 * for exceeding underlying device's max_segments limits -- due to

	 * crypt_alloc_buffer() possibly allocating pages for the encryption

	 * bio that are not as physically contiguous as the original bio.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

   linear.c : Multiple Devices driver for Linux

	      Copyright (C) 1994-96 Marc ZYNGIER

	      <zyngier@ufr-info-p7.ibp.fr> or

	      <maz@gloups.fdn.fr>



   Linear mode management functions.



/*

 * find which device holds a particular offset

	/*

	 * Binary Search

	/*

	 * Here we calculate the device offsets.

	/*

	 * conf->raid_disks is copy of mddev->raid_disks. The reason to

	 * keep a copy of mddev->raid_disks in struct linear_conf is,

	 * mddev->raid_disks may not be consistent with pointers number of

	 * conf->disks[] when it is updated in linear_add() and used to

	 * iterate old conf->disks[] earray in linear_congested().

	 * Here conf->raid_disks is always consitent with number of

	 * pointers in conf->disks[] array, and mddev->private is updated

	 * with rcu_assign_pointer() in linear_addr(), such race can be

	 * avoided.

	/* Adding a drive to a linear array allows the array to grow.

	 * It is permitted if the new drive has a matching superblock

	 * already on it, with raid_disk equal to raid_disks.

	 * It is achieved by creating a new linear_private_data structure

	 * and swapping it in in-place of the current one.

	 * The current one is never freed until the array is stopped.

	 * This avoids races.

	/* newconf->raid_disks already keeps a copy of * the increased

	 * value of mddev->raid_disks, WARN_ONCE() is just used to make

	 * sure of this. It is possible that oldconf is still referenced

	 * in linear_congested(), therefore kfree_rcu() is used to free

	 * oldconf until no one uses it anymore.

 This bio crosses a device boundary, so we have to split it */

 Just ignore it */

 LINEAR - deprecated*/

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * raid1.c : Multiple Devices driver for Linux

 *

 * Copyright (C) 1999, 2000, 2001 Ingo Molnar, Red Hat

 *

 * Copyright (C) 1996, 1997, 1998 Ingo Molnar, Miguel de Icaza, Gadi Oxman

 *

 * RAID-1 management functions.

 *

 * Better read-balancing code written by Mika Kuoppala <miku@iki.fi>, 2000

 *

 * Fixes to reconstruction by Jakob Østergaard" <jakob@ostenfeld.dk>

 * Various fixes by Neil Brown <neilb@cse.unsw.edu.au>

 *

 * Changes by Peter T. Breuer <ptb@it.uc3m.es> 31/1/2003 to support

 * bitmapped intelligence in resync:

 *

 *      - bitmap marked during normal i/o

 *      - bitmap used to skip nondirty blocks during sync

 *

 * Additions to bitmap code, (C) 2003-2004 Paul Clements, SteelEye Technology:

 * - persistent bitmap code

 collision happened */

/*

 * for resync bio, r1bio pointer can be retrieved from the per-bio

 * 'struct resync_pages'.

 allocate a r1bio with room for raid_disks entries in the bios array */

	/*

	 * Allocate bios : 1 for reading, n-1 for writing

	/*

	 * Allocate RESYNC_PAGES data pages and attach them to

	 * the first bio.

	 * If this is a user-requested check/repair, allocate

	 * RESYNC_PAGES for each bio.

 resync pages array stored in the 1st bio's .bi_private */

/*

 * raid_end_bio_io() is called when we have finished servicing a mirrored

 * operation and are ready to return a success/failure code to the buffer

 * cache layer.

 if nobody has done the final endio yet, do it now */

	/*

	 * Wake up any possible resync thread that waits for the device

	 * to go idle.  All I/Os, even write-behind writes, are done.

/*

 * Update disk head position estimator based on IRQ completion info.

/*

 * Find the disk number which triggered given bio

	/*

	 * this branch is our 'one mirror IO has finished' event handler:

		/* This was a fail-fast read so we definitely

		/* If all other devices have failed, we want to return

		 * the error upwards rather than fail the last device.

		 * Here we redefine "uptodate" to mean "Don't want to retry"

		/*

		 * oops, read error:

 don't drop the reference on read_disk yet */

 it really is the end of this request */

 clear the bitmap if all writes complete successfully */

	/*

	 * 'one mirror IO has finished' event handler:

 We never try FailFast to WriteMostly devices */

		/*

		 * When the device is faulty, it is not necessary to

		 * handle write error.

 Fail the request */

 Finished with this branch */

		/*

		 * Set R1BIO_Uptodate in our master bio, so that we

		 * will return a good error code for to the higher

		 * levels even if IO on some other mirrored buffer

		 * fails.

		 *

		 * The 'master' represents the composite IO operation

		 * to user-side. So if something waits for IO, then it

		 * will wait for the 'master' bio.

		/*

		 * Do not set R1BIO_Uptodate if the current device is

		 * rebuilding or Faulty. This is because we cannot use

		 * such device for properly reading the data back (we could

		 * potentially use it, if the current write would have felt

		 * before rdev->recovery_offset, but for simplicity we don't

		 * check this here.

 Maybe we can clear some bad blocks. */

		/*

		 * In behind mode, we ACK the master bio once the I/O

		 * has safely reached all non-writemostly

		 * disks. Setting the Returned bit ensures that this

		 * gets done only once -- we don't ever want to return

		 * -EIO here, instead we'll wait

 Maybe we can return now */

	/*

	 * Let's see if all mirrored write operations have finished

	 * already.

	/*

	 * len is the number of sectors from start_sector to end of the

	 * barrier unit which start_sector belongs to.

/*

 * This routine returns the disk from which the requested read should

 * be done. There is a per-array 'next expected sequential IO' sector

 * number - if this matches on the next IO then we use the last disk.

 * There is also a per-disk 'last know head position' sector that is

 * maintained from IRQ contexts, both the normal and the resync IO

 * completion handlers update this position correctly. If there is no

 * perfect sequential match then we pick the disk whose head is closest.

 *

 * If there are 2 mirrors in the same 2 devices, performance degrades

 * because position is mirror, not device based.

 *

 * The rdev for the device selected will have nr_pending incremented.

	/*

	 * Check if we can balance. We can balance on the whole

	 * device if no resync is going on, or below the resync window.

	 * We take the first readable disk when above the resync window.

			/* Don't balance among write-mostly, just

 Cannot use this */

		/* This is a reasonable device to use.  It might

		 * even be best.

 already have a better device */

				/* cannot read here. If this is the 'primary'

				 * device, then we must not read beyond

				 * bad_sectors from another device..

 At least two disks to choose from so failfast is OK */

 Don't change to another disk for sequential reads */

			/*

			 * If buffered sequential IO size exceeds optimal

			 * iosize, check if there is idle disk. If yes, choose

			 * the idle disk. read_balance could already choose an

			 * idle disk before noticing it's a sequential IO in

			 * this disk. This doesn't matter because this disk

			 * will idle, next time it will be utilized after the

			 * first disk has IO size exceeds optimal iosize. In

			 * this way, iosize of the first disk will be optimal

			 * iosize at least. iosize of the second disk might be

			 * small, but not a big deal since when the second disk

			 * starts IO, the first disk is likely still busy.

	/*

	 * If all disks are rotational, choose the closest disk. If any disk is

	 * non-rotational, choose the disk with less pending request even the

	 * disk is rotational, which might/might not be optimal for raids with

	 * mixed ratation/non-rotational disks depending on workload.

 flush any pending bitmap writes to disk before proceeding w/ I/O */

 submit pending writes */

 Just ignore it */

	/* Any writes that have been queued but are awaiting

	 * bitmap updates get flushed here.

		/*

		 * As this is called in a wait_event() loop (see freeze_array),

		 * current->state might be TASK_UNINTERRUPTIBLE which will

		 * cause a warning when we prepare to wait again.  As it is

		 * rare that this path is taken, it is perfectly safe to force

		 * us to go around the wait_event() loop again, so the warning

		 * is a false-positive.  Silence the warning by resetting

		 * thread state

/* Barriers....

 * Sometimes we need to suspend IO while we do something else,

 * either some resync/recovery, or reconfigure the array.

 * To do this we raise a 'barrier'.

 * The 'barrier' is a counter that can be raised multiple times

 * to count how many activities are happening which preclude

 * normal IO.

 * We can only raise the barrier if there is no pending IO.

 * i.e. if nr_pending == 0.

 * We choose only to raise the barrier if no-one is waiting for the

 * barrier to go down.  This means that as soon as an IO request

 * is ready, no other operations which require a barrier will start

 * until the IO request has had a chance.

 *

 * So: regular IO calls 'wait_barrier'.  When that returns there

 *    is no backgroup IO happening,  It must arrange to call

 *    allow_barrier when it has finished its IO.

 * backgroup IO calls must call raise_barrier.  Once that returns

 *    there is no normal IO happeing.  It must arrange to call

 *    lower_barrier when the particular background IO completes.

 *

 * If resync/recovery is interrupted, returns -EINTR;

 * Otherwise, returns 0.

 Wait until no block IO is waiting */

 block any new IO from starting */

	/*

	 * In raise_barrier() we firstly increase conf->barrier[idx] then

	 * check conf->nr_pending[idx]. In _wait_barrier() we firstly

	 * increase conf->nr_pending[idx] then check conf->barrier[idx].

	 * A memory barrier here to make sure conf->nr_pending[idx] won't

	 * be fetched before conf->barrier[idx] is increased. Otherwise

	 * there will be a race between raise_barrier() and _wait_barrier().

	/* For these conditions we must wait:

	 * A: while the array is in frozen state

	 * B: while conf->nr_pending[idx] is not 0, meaning regular I/O

	 *    existing in corresponding I/O barrier bucket.

	 * C: while conf->barrier[idx] >= RESYNC_DEPTH, meaning reaches

	 *    max resync count which allowed on current I/O barrier bucket.

	/*

	 * We need to increase conf->nr_pending[idx] very early here,

	 * then raise_barrier() can be blocked when it waits for

	 * conf->nr_pending[idx] to be 0. Then we can avoid holding

	 * conf->resync_lock when there is no barrier raised in same

	 * barrier unit bucket. Also if the array is frozen, I/O

	 * should be blocked until array is unfrozen.

	/*

	 * In _wait_barrier() we firstly increase conf->nr_pending[idx], then

	 * check conf->barrier[idx]. In raise_barrier() we firstly increase

	 * conf->barrier[idx], then check conf->nr_pending[idx]. A memory

	 * barrier is necessary here to make sure conf->barrier[idx] won't be

	 * fetched before conf->nr_pending[idx] is increased. Otherwise there

	 * will be a race between _wait_barrier() and raise_barrier().

	/*

	 * Don't worry about checking two atomic_t variables at same time

	 * here. If during we check conf->barrier[idx], the array is

	 * frozen (conf->array_frozen is 1), and chonf->barrier[idx] is

	 * 0, it is safe to return and make the I/O continue. Because the

	 * array is frozen, all I/O returned here will eventually complete

	 * or be queued, no race will happen. See code comment in

	 * frozen_array().

	/*

	 * After holding conf->resync_lock, conf->nr_pending[idx]

	 * should be decreased before waiting for barrier to drop.

	 * Otherwise, we may encounter a race condition because

	 * raise_barrer() might be waiting for conf->nr_pending[idx]

	 * to be 0 at same time.

	/*

	 * In case freeze_array() is waiting for

	 * get_unqueued_pending() == extra

 Wait for the barrier in same barrier unit bucket to drop. */

	/*

	 * Very similar to _wait_barrier(). The difference is, for read

	 * I/O we don't need wait for sync I/O, but if the whole array

	 * is frozen, the read I/O still has to wait until the array is

	 * unfrozen. Since there is no ordering requirement with

	 * conf->barrier[idx] here, memory barrier is unnecessary as well.

	/*

	 * In case freeze_array() is waiting for

	 * get_unqueued_pending() == extra

 Wait for array to be unfrozen */

 conf->resync_lock should be held */

	/* Stop sync I/O and normal I/O and wait for everything to

	 * go quiet.

	 * This is called in two situations:

	 * 1) management command handlers (reshape, remove disk, quiesce).

	 * 2) one normal I/O request failed.



	 * After array_frozen is set to 1, new sync IO will be blocked at

	 * raise_barrier(), and new normal I/O will blocked at _wait_barrier()

	 * or wait_read_barrier(). The flying I/Os will either complete or be

	 * queued. When everything goes quite, there are only queued I/Os left.



	 * Every flying I/O contributes to a conf->nr_pending[idx], idx is the

	 * barrier bucket index which this I/O request hits. When all sync and

	 * normal I/O are queued, sum of all conf->nr_pending[] will match sum

	 * of all conf->nr_queued[]. But normal I/O failure is an exception,

	 * in handle_read_error(), we may call freeze_array() before trying to

	 * fix the read error. In this case, the error read I/O is not queued,

	 * so get_unqueued_pending() == 1.

	 *

	 * Therefore before this function returns, we need to wait until

	 * get_unqueued_pendings(conf) gets equal to extra. For

	 * normal I/O context, extra is 1, in rested situations extra is 0.

 reverse the effect of the freeze */

 discard op, we don't support writezero/writesame yet */

 we aren't scheduling, so we can do the write-out directly. */

 Ensure no bio records IO_BLOCKED */

	/*

	 * If r1_bio is set, we are blocking the raid1d thread

	 * so there is a tiny risk of deadlock.  So ask for

	 * emergency memory if needed.

 Need to get the block device name carefully */

	/*

	 * Still need barrier for READ in case that whole

	 * array is frozen.

	/*

	 * make_request() can abort the operation when read-ahead is being

	 * used and no empty request is available.

 couldn't find anywhere to read from */

		/*

		 * Reading from a write-mostly device must take care not to

		 * over-take any writes that are 'behind'

	/*

	 * Register the new request and wait if the reconstruction

	 * thread has put up a bar for new requests.

	 * Continue immediately if no resync is active currently.

	/* first select target devices under rcu_lock and

	 * inc refcount on their rdev.  Record them by setting

	 * bios[x] to bio

	 * If there are known/acknowledged bad blocks on any device on

	 * which we have seen a write error, we want to avoid writing those

	 * blocks.

	 * This potentially requires several writes to write around

	 * the bad blocks.  Each set of writes gets it's own r1bio

	 * with a set of bios attached.

		/*

		 * The write-behind io is only attempted on drives marked as

		 * write-mostly, which means we could allocate write behind

		 * bio later.

				/* mustn't write here until the bad block is

 Cannot write here at all */

					/* mustn't write more than bad_sectors

					 * to other devices yet

				/* We don't set R1BIO_Degraded as that

				 * only applies if the disk is

				 * missing, so it might be re-added,

				 * and we want to know to recover this

				 * chunk.

				 * In this case the device is here,

				 * and the fact that this chunk is not

				 * in-sync is recorded in the bad

				 * block log

 Wait for this device to become unblocked */

	/*

	 * When using a bitmap, we may call alloc_behind_master_bio below.

	 * alloc_behind_master_bio allocates a copy of the data payload a page

	 * at a time and thus needs a new bio that can fit the whole payload

	 * this bio in page sized chunks.

			/* do behind I/O ?

			 * Not if there are too many, or cannot

			 * allocate memory, or a reader on WriteMostly

 flush_pending_writes() needs access to the rdev so...*/

 In case raid1d snuck in to freeze_array */

	/*

	 * There is a limit to the maximum size, but

	 * the read/write handler might find a lower limit

	 * due to bad blocks.  To avoid multiple splits,

	 * we pass the maximum number of sectors down

	 * and let the lower level perform the split.

	/*

	 * If it is not operational, then we have already marked it as dead

	 * else if it is the last working disks with "fail_last_dev == false",

	 * ignore the error, let the next level up know.

	 * else mark the drive as failed

		/*

		 * Don't fail the drive, act as though we were just a

		 * normal single drive.

		 * However don't try a recovery from this drive as

		 * it is very likely to fail.

	/*

	 * if recovery is running, make sure it aborts.

	/*

	 * Find all failed disks within the RAID1 configuration

	 * and mark them readable.

	 * Called under mddev lock, so rcu protection not needed.

	 * device_lock used to avoid races with raid1_end_read_request

	 * which expects 'In_sync' flags and ->degraded to be consistent.

 replacement has just become active */

				/* Replaced device not technically

				 * faulty, but we need to be sure

				 * it gets removed and never re-added

	/*

	 * find the disk ... but prefer rdev->saved_raid_disk

	 * if possible.

			/* As all devices are equivalent, we don't need a full recovery

			 * if this was recently any drive of the array

 Add this device as a replacement */

		/* Only remove non-faulty devices if recovery

		 * is not possible.

 lost the race, try later */

			/* We just removed a device that is being replaced.

			 * Move down the replacement.  We drain all IO before

			 * doing this to avoid confusion.

				/* It means that some queued IO of retry_list

				 * hold repl. Thus, we cannot set replacement

				 * as NULL, avoiding rdev NULL pointer

				 * dereference in sync_request_write and

				 * handle_write_finished.

	/*

	 * we have read a block, now it needs to be re-written,

	 * or re-read if the read failed.

	 * We don't do much here, just schedule handling by raid1d

 make sure these bits don't get cleared. */

 success */

 need to record an error - either for the block or the device */

	/* Try some synchronous reads of other devices to get

	 * good data, much like with normal read errors.  Only

	 * read into the pages we already have so we don't

	 * need to re-issue the read request.

	 * We don't need to freeze the array, because being in an

	 * active sync request, there is no normal IO, and

	 * no overlapping syncs.

	 * We don't need to check is_badblock() again as we

	 * made sure that anything with a bad block in range

	 * will have bi_end_io clear.

		/* Don't try recovering from here - just fail it

			/* Don't try to read from here, but make sure

			 * put_buf does it's thing

				/* No rcu protection needed here devices

				 * can only be removed when no resync is

				 * active, and resync is currently active

			/* Cannot read from anywhere, this block is lost.

			 * Record a bad block on each device.  If that doesn't

			 * work just disable and interrupt the recovery.

			 * Don't fail devices as that won't really help.

 Try next page */

 write it back and re-read */

	/* We have read all readable devices.  If we haven't

	 * got the block, then there is no hope left.

	 * If we have, then we want to do a comparison

	 * and skip the write if everything is the same.

	 * If any blocks failed to read, then we need to

	 * attempt an over-write

 Fix variable parts of all bios */

 fixup the bio for reuse, but preserve errno */

 initialize bvec table again */

 Now we can 'fixup' the error value */

 No need to write to this device. */

 ouch - failed to read all of that. */

	/*

	 * schedule writes

/*

 * This is a kernel thread which:

 *

 *	1.	Retries failed read operations on working mirrors.

 *	2.	Updates the raid superblock when problems encounter.

 *	3.	Performs writes following reads for array synchronising.

 Cannot read from anywhere - mark it bad */

 write it back and re-read */

	/* bio has the data to be written to device 'i' where

	 * we just recently had a write error.

	 * We repeatedly clone the bio and trim down to one block,

	 * then try the write.  Where the write fails we record

	 * a bad block.

	 * It is conceivable that the bio doesn't exactly align with

	 * blocks.  We must handle this somehow.

	 *

	 * We currently own a reference on the rdev.

 Write at 'sector' for 'sectors'*/

 failure! */

			/* This drive got a write error.  We need to

			 * narrow down and record precise write

			 * errors.

 an I/O failed, we can't clear the bitmap */

		/*

		 * In case freeze_array() is waiting for condition

		 * get_unqueued_pending() == extra to be true.

	/* we got a read error. Maybe the drive is bad.  Maybe just

	 * the block and we can fix it.

	 * We freeze all other IO, and try reading the block from

	 * other devices.  When we find one, we re-write

	 * and check it that fixes the read error.

	 * This is all done synchronously while the array is

	 * frozen

 Reuse the old r1_bio so that the IO_BLOCKED settings are preserved */

/*

 * perform a "sync" on one "block"

 *

 * We need to make sure that no normal I/O request - particularly write

 * requests - conflict with active sync requests.

 *

 * This is achieved by tracking pending requests and a 'barrier' concept

 * that can be installed to exclude normal IO requests.

 number of sectors that are bad in all devices */

		/* If we aborted, we need to abort the

		 * sync on the 'current' bitmap chunk (there will

		 * only be one in raid1 resync.

		 * We can find the current addess in mddev->curr_resync

 aborted */

 completed sync */

	/* before building a request, check if we can skip these blocks..

	 * This call the bitmap_start_sync doesn't actually record anything

 We can skip this block, and probably several more */

	/*

	 * If there is non-resync activity waiting for a turn, then let it

	 * though before starting on this new sync request.

	/* we are incrementing sector_nr below. To be safe, we check against

	 * sector_nr + two times RESYNC_SECTORS

	/*

	 * If we get a correctably read error during resync or recovery,

	 * we might want to read from a different device.  So we

	 * flag all drives that could conceivably be read from for READ,

	 * and any others (which will be non-In_sync devices) for WRITE.

	 * If a read fails, we try reading from something else for which READ

	 * is OK.

 make sure good_sectors won't go across barrier unit boundary */

 may need to read from here */

				/*

				 * The device is suitable for reading (InSync),

				 * but has bad block(s) here. Let's try to correct them,

				 * if we are doing resync or repair. Otherwise, leave

				 * this device alone for this sync request.

		/* These sectors are bad on all InSync devices, so we

		 * need to mark them bad on all write targets

			/* Cannot record the badblocks, so need to

			 * abort the resync.

			 * If there are multiple read targets, could just

			 * fail the really bad ones ???

		/* only resync enough to reach the next bad->good

 extra read targets are also write targets */

		/* There is nowhere to write, so all non-sync

		 * drives must be failed - so we are finished

 Don't do IO beyond here */

				/*

				 * won't fail because the vec table is big

				 * enough to hold all these pages

 Send resync message */

	/* For a user-requested sync, we read all readable devices and do a

	 * compare

 This slot has a replacement. */

				/* No original, just make the replacement

				 * a recovering spare

 Original is not in_sync - bad */

	/*

	 * copy the already verified devices into our private RAID1

	 * bookkeeping area. [whatever we allocate in run(),

	 * should be freed in raid1_free()]

	/*

	 * RAID1 needs at least one disk in active

	/*

	 * Ok, everything is just fine now

	/* no resync is happening, and there is enough space

	 * on all devices, so we can resize.

	 * We need to make sure resync covers any new space.

	 * If the array is shrinking we should possibly wait until

	 * any io in the removed space completes, but it hardly seems

	 * worth it.

	/* We need to:

	 * 1/ resize the r1bio_pool

	 * 2/ resize conf->mirrors

	 *

	 * We allocate a new r1bio_pool if we can.

	 * Then raise a device barrier and wait until all IO stops.

	 * Then resize conf->mirrors and swap in the new r1bio pool.

	 *

	 * At the same time, we "pack" the devices so that all the missing

	 * devices have the higher raid_disk numbers.

 Cannot change chunk_size, layout, or level */

 ok, everything is stopped */

	/* raid1 can take over:

	 *  raid5 with 2 devices, any layout or chunk size

 Array must appear to be quiesced */

 RAID1 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Oracle Corporation

 *

 * Module Author: Mike Christie

	/*

	 * Perf is not optimal, but we at least try the local node then just

	 * try not to fail.

/*

 * Copyright (C) 2001-2002 Sistina Software (UK) Limited.

 * Copyright (C) 2006-2008 Red Hat GmbH

 *

 * This file is released under the GPL.

/*-----------------------------------------------------------------

 * Implementation of the store for non-persistent snapshots.

 Just succeed */

/*

 * Copyright (C) 2009-2011 Red Hat, Inc.

 *

 * Author: Mikulas Patocka <mpatocka@redhat.com>

 *

 * This file is released under the GPL.

/*

 * Memory management policy:

 *	Limit the number of buffers to DM_BUFIO_MEMORY_PERCENT of main memory

 *	or DM_BUFIO_VMALLOC_PERCENT of vmalloc memory (whichever is lower).

 *	Always allocate at least DM_BUFIO_MIN_BUFFERS buffers.

 *	Start background writeback when there are DM_BUFIO_WRITEBACK_PERCENT

 *	dirty buffers.

/*

 * Check buffer ages in this interval (seconds)

/*

 * Free buffers when they are older than this (seconds)

/*

 * The nr of bytes of cached data to keep around.

/*

 * Align buffer writes to this boundary.

 * Tests show that SSDs have the highest IOPS when using 4k writes.

/*

 * dm_buffer->list_mode

/*

 * Linking of buffers:

 *	All buffers are linked to buffer_tree with their node field.

 *

 *	Clean buffers that are not being written (B_WRITING not set)

 *	are linked to lru[LIST_CLEAN] with their lru_list field.

 *

 *	Dirty and clean buffers that are being written are linked to

 *	lru[LIST_DIRTY] with their lru_list field. When the write

 *	finishes, the buffer cannot be relinked immediately (because we

 *	are in an interrupt context and relinking requires process

 *	context), so some clean-not-writing buffers can be held on

 *	dirty_lru too.  They are later added to lru in the process

 *	context.

/*

 * Buffer state bits.

/*

 * Describes how the block was allocated:

 * kmem_cache_alloc(), __get_free_pages() or vmalloc().

 * See the comment at alloc_buffer_data.

 DATA_MODE_* */

 LIST_* */

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Default cache size: available memory divided by the ratio.

/*

 * Total cache size set by the user.

/*

 * A copy of dm_bufio_cache_size because dm_bufio_cache_size can change

 * at any time.  If it disagrees, the user has changed cache size.

/*

 * Buffers are freed after this timeout

----------------------------------------------------------------*/

/*

 * The current number of clients.

/*

 * The list of all clients.

/*

 * This mutex protects dm_bufio_cache_size_latch and dm_bufio_client_count

/*----------------------------------------------------------------

 * A red/black tree acts as an index for all the buffers.

----------------------------------------------------------------*/

/*

 * Change the number of clients and recalculate per-client limit.

	/*

	 * Use default if set to 0 and report the actual cache size used.

/*

 * Allocating buffer data.

 *

 * Small buffers are allocated with kmem_cache, to use space optimally.

 *

 * For large buffers, we choose between get_free_pages and vmalloc.

 * Each has advantages and disadvantages.

 *

 * __get_free_pages can randomly fail if the memory is fragmented.

 * __vmalloc won't randomly fail, but vmalloc space is limited (it may be

 * as low as 128M) so using it for caching is not appropriate.

 *

 * If the allocation may fail we use __get_free_pages. Memory fragmentation

 * won't have a fatal effect here, but it just causes flushes of some other

 * buffers and more I/O will be performed. Don't use __get_free_pages if it

 * always fails (i.e. order >= MAX_ORDER).

 *

 * If the allocation shouldn't fail we use __vmalloc. This is only for the

 * initial reserve allocation, so there's no risk of wasting all vmalloc

 * space.

	/*

	 * __vmalloc allocates the data pages and auxiliary structures with

	 * gfp_flags that were specified, but pagetables are always allocated

	 * with GFP_KERNEL, no matter what was specified as gfp_mask.

	 *

	 * Consequently, we must set per-process flag PF_MEMALLOC_NOIO so that

	 * all allocations done by this process (including pagetables) are done

	 * as if GFP_NOIO was specified.

/*

 * Free buffer's data.

/*

 * Allocate buffer and its data.

/*

 * Free buffer and its data.

/*

 * Link buffer to the buffer tree and clean or dirty queue.

/*

 * Unlink buffer from the buffer tree and dirty or clean queue.

/*

 * Place the buffer to the head of dirty or clean LRU queue.

/*----------------------------------------------------------------

 * Submit I/O on the buffer.

 *

 * Bio interface is faster but it has some problems:

 *	the vector list is limited (increasing this limit increases

 *	memory-consumption per buffer, so it is not viable);

 *

 *	the memory must be direct-mapped, not vmalloced;

 *

 * If the buffer is small enough (up to DM_BUFIO_INLINE_VECS pages) and

 * it is not vmalloced, try using the bio interface.

 *

 * If the buffer is big, if it is vmalloced or if the underlying device

 * rejects the bio because it is too large, use dm-io layer to do the I/O.

 * The dm-io layer splits the I/O into multiple requests, avoiding the above

 * shortcomings.

/*

 * dm-io completion routine. It just calls b->bio.bi_end_io, pretending

 * that the request was handled directly with bio interface.

/*----------------------------------------------------------------

 * Writing dirty buffers

/*

 * The endio routine for write.

 *

 * Set the error, clear B_WRITING bit and wake anyone who was waiting on

 * it.

/*

 * Initiate a write on a dirty buffer, but don't wait for it.

 *

 * - If the buffer is not dirty, exit.

 * - If there some previous write going on, wait for it to finish (we can't

 *   have two writes on the same buffer simultaneously).

 * - Submit our write and don't wait on it. We set B_WRITING indicating

 *   that there is a write in progress.

/*

 * Wait until any activity on the buffer finishes.  Possibly write the

 * buffer if it is dirty.  When this function finishes, there is no I/O

 * running on the buffer and the buffer is not dirty.

 fast case */

/*

 * Find some buffer that is not held by anybody, clean it, unlink it and

 * return it.

/*

 * Wait until some other threads free some buffer or release hold count on

 * some buffer.

 *

 * This function is entered with c->lock held, drops it and regains it

 * before exiting.

/*

 * Allocate a new buffer. If the allocation is not possible, wait until

 * some other thread frees a buffer.

 *

 * May drop the lock and regain it.

	/*

	 * dm-bufio is resistant to allocation failures (it just keeps

	 * one buffer reserved in cases all the allocations fail).

	 * So set flags to not try too hard:

	 *	GFP_NOWAIT: don't wait; if we need to sleep we'll release our

	 *		    mutex and wait ourselves.

	 *	__GFP_NORETRY: don't retry and rather return failure

	 *	__GFP_NOMEMALLOC: don't use emergency reserves

	 *	__GFP_NOWARN: don't print a warning in case of failure

	 *

	 * For debugging, if we set the cache size to 1, no new buffers will

	 * be allocated.

/*

 * Free a buffer and wake other threads waiting for free buffers.

/*

 * Check if we're over watermark.

 * If we are over threshold_buffers, start freeing buffers.

 * If we're over "limit_buffers", block until we get under the limit.

/*----------------------------------------------------------------

 * Getting a buffer

	/*

	 * We've had a period where the mutex was unlocked, so need to

	 * recheck the buffer tree.

	/*

	 * Note: it is essential that we don't wait for the buffer to be

	 * read if dm_bufio_get function is used. Both dm_bufio_get and

	 * dm_bufio_prefetch can be used in the driver request routine.

	 * If the user called both dm_bufio_prefetch and dm_bufio_get on

	 * the same buffer, it would deadlock if we waited.

/*

 * The endio routine for reading: set the error, clear the bit and wake up

 * anyone waiting on the buffer.

/*

 * A common routine for dm_bufio_new and dm_bufio_read.  Operation of these

 * functions is similar except that dm_bufio_new doesn't read the

 * buffer from the disk (assuming that the caller overwrites all the data

 * and uses dm_bufio_mark_buffer_dirty to write new data back).

		/*

		 * If there were errors on the buffer, and the buffer is not

		 * to be written, free the buffer. There is no point in caching

		 * invalid buffer.

/*

 * For performance, it is essential that the buffers are written asynchronously

 * and simultaneously (so that the block layer can merge the writes) and then

 * waited upon.

 *

 * Finally, we flush hardware disk cache.

		/*

		 * If we dropped the lock, the list is no longer consistent,

		 * so we must restart the search.

		 *

		 * In the most common case, the buffer just processed is

		 * relinked to the clean list, so we won't loop scanning the

		 * same buffer again and again.

		 *

		 * This may livelock if there is another thread simultaneously

		 * dirtying buffers, so we count the number of buffers walked

		 * and if it exceeds the total number of buffers, it means that

		 * someone is doing some writes simultaneously with us.  In

		 * this case, stop, dropping the lock.

/*

 * Use dm-io to send an empty barrier to flush the device.

/*

 * Use dm-io to send a discard request to flush the device.

/*

 * We first delete any other buffer that may be at that new location.

 *

 * Then, we write the buffer to the original location if it was dirty.

 *

 * Then, if we are the only one who is holding the buffer, relink the buffer

 * in the buffer tree for the new location.

 *

 * If there was someone else holding the buffer, we write it to the new

 * location but not relink it, because that other user needs to have the buffer

 * at the same place.

		/*

		 * FIXME: Is there any point waiting for a write that's going

		 * to be overwritten in a bit?

		/*

		 * Relink buffer to "new_block" so that write_callback

		 * sees "new_block" as a block number.

		 * After the write, link the buffer back to old_block.

		 * All this must be done in bufio lock, so that block number

		 * change isn't visible to other threads.

/*

 * Free the given buffer.

 *

 * This is just a hint, if the buffer is in use or dirty, this function

 * does nothing.

	/*

	 * An optimization so that the buffers are not written one-by-one.

 mark unclaimed to avoid BUG_ON below */

/*

 * We may not be able to evict this buffer if IO pending or the client

 * is still using it.  Caller is expected to know buffer is too old.

 *

 * And if GFP_NOFS is used, we must not do any I/O because we hold

 * dm_bufio_clients_lock and we would risk deadlock if the I/O gets

 * rerouted to different bufio client.

/*

 * Create the buffering interface

/*

 * Free the buffering interface.

 * It is required that there are no references on any buffers.

/*----------------------------------------------------------------

 * Module setup

/*

 * This is called only once for the whole dm_bufio module.

 * It initializes memory limit.

/*

 * This is called once when unloading the dm_bufio module.

/*

 * Copyright (C) 2005-2007 Red Hat GmbH

 *

 * A target that delays reads and/or writes and can send

 * them to different devices.

 *

 * This file is released under the GPL.

/*

 * Mapping parameters:

 *    <device> <offset> <delay> [<write_device> <write_offset> <write_delay>]

 *

 * With separate write parameters, the first set is only used for reads.

 * Offsets are specified in sectors.

 * Delays are specified in milliseconds.

 Module hooks */

/*

 * Copyright (C) 2001-2002 Sistina Software (UK) Limited.

 * Copyright (C) 2006-2008 Red Hat GmbH

 *

 * This file is released under the GPL.

 16KB */

/*-----------------------------------------------------------------

 * Persistent snapshots, by persistent we mean that the snapshot

 * will survive a reboot.

/*

 * We need to store a record of which parts of the origin have

 * been copied to the snapshot device.  The snapshot code

 * requires that we copy exception chunks to chunk aligned areas

 * of the COW store.  It makes sense therefore, to store the

 * metadata in chunk size blocks.

 *

 * There is no backward or forward compatibility implemented,

 * snapshots with different disk versions than the kernel will

 * not be usable.  It is expected that "lvcreate" will blank out

 * the start of a fresh COW device before calling the snapshot

 * constructor.

 *

 * The first chunk of the COW device just contains the header.

 * After this there is a chunk filled with exception metadata,

 * followed by as many exception chunks as can fit in the

 * metadata areas.

 *

 * All on disk structures are in little-endian format.  The end

 * of the exceptions info is indicated by an exception with a

 * new_chunk of 0, which is invalid since it would point to the

 * header chunk.

/*

 * Magic for persistent snapshots: "SnAp" - Feeble isn't it.

/*

 * The on-disk version of the metadata.

	/*

	 * Is this snapshot valid.  There is no way of recovering

	 * an invalid snapshot.

	/*

	 * Simple, incrementing version. no backward

	 * compatibility.

 In sectors */

/*

 * The top level structure for a persistent exception store.

	/*

	 * Now that we have an asynchronous kcopyd there is no

	 * need for large chunk sizes, so it wont hurt to have a

	 * whole chunks worth of metadata in memory at once.

	/*

	 * An area of zeros used to clear the next area.

	/*

	 * An area used for header. The header can be written

	 * concurrently with metadata (when invalidating the snapshot),

	 * so it needs a separate buffer.

	/*

	 * Used to keep track of which metadata area the data in

	 * 'chunk' refers to.

	/*

	 * The next free chunk for an exception.

	 *

	 * When creating exceptions, all the chunks here and above are

	 * free.  It holds the next chunk to be allocated.  On rare

	 * occasions (e.g. after a system crash) holes can be left in

	 * the exception store because chunks can be committed out of

	 * order.

	 *

	 * When merging exceptions, it does not necessarily mean all the

	 * chunks here and above are free.  It holds the value it would

	 * have held if all chunks had been committed in order of

	 * allocation.  Consequently the value may occasionally be

	 * slightly too low, but since it's only used for 'status' and

	 * it can never reach its minimum value too early this doesn't

	 * matter.

	/*

	 * The index of next free exception in the current

	 * metadata area.

	/*

	 * Allocate the chunk_size block of memory that will hold

	 * a single metadata area.

/*

 * Read or write a chunk aligned and sized block of data from a device.

	/*

	 * Issue the synchronous I/O from a different thread

	 * to avoid submit_bio_noacct recursion.

/*

 * Convert a metadata area index to a chunk index.

/*

 * Read or write a metadata area.  Remembering to skip the first

 * chunk which holds the header.

	/*

	 * Use default chunk size (or logical_block_size, if larger)

	 * if none supplied

 We had a bogus chunk_size. Fix stuff up. */

/*

 * Access functions for the disk exceptions, these do the endian conversions.

 copy it */

 copy it */

 clear it */

/*

 * Registers the exceptions that are present in the current area.

 * 'full' is filled in to indicate if the area has been

 * filled.

 presume the area is full */

		/*

		 * If the new_chunk is pointing at the start of

		 * the COW device, where the first metadata area

		 * is we know that we've hit the end of the

		 * exceptions.  Therefore the area is not full.

		/*

		 * Keep track of the start of the free chunks.

		/*

		 * Otherwise we add the exception to the snapshot.

	/*

	 * Setup for one current buffer + desired readahead buffers.

	/*

	 * Keeping reading chunks and inserting exceptions until

	 * we find a partially full area.

	/*

	 * First chunk is the fixed header.

	 * Then there are (ps->current_area + 1) metadata chunks, each one

	 * separated from the next by ps->exceptions_per_area data chunks.

 Created in read_header */

 Allocated in persistent_read_metadata */

	/*

	 * Read the snapshot header.

	/*

	 * Now we know correct chunk_size, complete the initialisation.

	/*

	 * Do we need to setup a new snapshot ?

	/*

	 * Sanity checks.

	/*

	 * Metadata are valid, but snapshot is invalidated

	/*

	 * Read the metadata.

 Is there enough room ? */

	/*

	 * Move onto the next free pending, making sure to take

	 * into account the location of the metadata chunks.

	/*

	 * Add the callback to the back of the array.  This code

	 * is the only place where the callback array is

	 * manipulated, and we know that it will never be called

	 * multiple times concurrently.

	/*

	 * If there are exceptions in flight and we have not yet

	 * filled this metadata area there's nothing more to do.

	/*

	 * If we completely filled the current area, then wipe the next one.

	/*

	 * Commit exceptions to disk.

	/*

	 * Advance to the next area if this one is full.

	/*

	 * When current area is empty, move back to preceding area.

		/*

		 * Have we finished?

	/*

	 * Find number of consecutive chunks within the current area,

	 * working backwards.

	/*

	 * At this stage, only persistent_usage() uses ps->next_free, so

	 * we make no attempt to keep ps->next_free strictly accurate

	 * as exceptions may have been committed out-of-order originally.

	 * Once a snapshot has become merging, we set it to the value it

	 * would have held had all the exceptions been committed in order.

	 *

	 * ps->current_area does not get reduced by prepare_merge() until

	 * after commit_merge() has removed the nr_merged previous exceptions.

 allocate the pstore */

 header and 1st area */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Microsoft Corporation

 *

 * Author: Tushar Sugandhi <tusharsu@linux.microsoft.com>

 *

 * File: dm-ima.c

 *       Enables IMA measurements for DM targets

/*

 * Internal function to prefix separator characters in input buffer with escape

 * character, so that they don't interfere with the construction of key-value pairs,

 * and clients can split the key1=val1,key2=val2,key3=val3; pairs properly.

/*

 * Internal function to allocate memory for IMA measurements.

/*

 * Internal function to allocate and copy name and uuid for IMA measurements.

/*

 * Internal function to allocate and copy device data for IMA measurements.

/*

 * Internal wrapper function to call IMA to measure DM data.

/*

 * Internal function to allocate and copy current device capacity for IMA measurements.

/*

 * Initialize/reset the dm ima related data structure variables.

/*

 * Build up the IMA data for each target, and finally measure.

	/*

	 * In below hash_alg_prefix_len assignment +1 is for the additional char (':'),

	 * when prefixing the hash value with the hash algorithm name. e.g. sha256:<hash_value>.

		/*

		 * First retrieve the target metadata.

		/*

		 * Then retrieve the actual target data.

		/*

		 * Check if the total data can fit into the IMA buffer.

		/*

		 * IMA measurements for DM targets are best-effort.

		 * If the total data buffered so far, including the current target,

		 * is too large to fit into DM_IMA_MEASUREMENT_BUF_LEN, measure what

		 * we have in the current buffer, and continue measuring the remaining

		 * targets by prefixing the device metadata again.

			/*

			 * Each new "dm_table_load" entry in IMA log should have device data

			 * prefix, so that multiple records from the same "dm_table_load" for

			 * a given device can be linked together.

			/*

			 * If this iteration of the for loop turns out to be the last target

			 * in the table, dm_ima_measure_data("dm_table_load", ...) doesn't need

			 * to be called again, just the hash needs to be finalized.

			 * "last_target_measured" tracks this state.

		/*

		 * Fill-in all the target metadata, so that multiple targets for the same

		 * device can be linked together.

	/*

	 * Finalize the table hash, and store it in table->md->ima.inactive_table.hash,

	 * so that the table data can be verified against the future device state change

	 * events, e.g. resume, rename, remove, table-clear etc.

/*

 * Measure IMA data on device resume.

/*

 * Measure IMA data on remove.

	/*

	 * In case both active and inactive tables, and corresponding

	 * device metadata is cleared/missing - record the name and uuid

	 * in IMA measurements.

/*

 * Measure ima data on table clear.

/*

 * Measure IMA data on device rename.

/*

 * Copyright (C) 2012 Red Hat. All rights reserved.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

 One size fits all for now */

 if t->real is set then an alias was used (e.g. "default") */

----------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0

/*

 * Historical Service Time

 *

 *  Keeps a time-weighted exponential moving average of the historical

 *  service time. Estimates future service time based on the historical

 *  service time and the number of outstanding requests.

 *

 *  Marks paths stale if they have not finished within hst *

 *  num_paths. If a path is stale and unused, we will send a single

 *  request to probe in case the path has improved. This situation

 *  generally arises if the path is so much worse than others that it

 *  will never have the best estimated service time, or if the entire

 *  multipath device is unused. If a path is stale and in use, limit the

 *  number of requests it can receive with the assumption that the path

 *  has become degraded.

 *

 *  To avoid repeatedly calculating exponents for time weighting, times

 *  are split into HST_WEIGHT_COUNT buckets each (1 >> HST_BUCKET_SHIFT)

 *  ns, and the weighting is pre-calculated.

 *

 10 bits of decimal precision */

 Buckets are ~ 16ms */

 Fixed point */

/**

 * fixed_power - compute: x^n, in O(log n) time

 *

 * @x:         base of the power

 * @frac_bits: fractional bits of @x

 * @n:         power to raise @x to.

 *

 * By exploiting the relation between the definition of the natural power

 * function: x^n := x*x*...*x (x multiplied by itself for n times), and

 * the binary encoding of numbers used by computers: n := \Sum n_i * 2^i,

 * (where: n_i \elem {0, 1}, the binary vector representing n),

 * we find: x^n := x^(\Sum n_i * 2^i) := \Prod x^(n_i * 2^i), which is

 * of course trivially computable in O(log_2 n), the length of our binary

 * vector.

 *

 * (see: kernel/sched/loadavg.c)

/*

 * Calculate the next value of an exponential moving average

 * a_1 = a_0 * e + a * (1 - e)

 *

 * @last: [0, ULLONG_MAX >> HST_FIXED_SHIFT]

 * @next: [0, ULLONG_MAX >> HST_FIXED_SHIFT]

 * @weight: [0, HST_FIXED_1]

 *

 * Note:

 *   To account for multiple periods in the same calculation,

 *   a_n = a_0 * e^n + a * (1 - e^n),

 *   so call fixed_ema(last, next, pow(weight, N))

/*

 * Get the weight for a given time span.

/*

 * Set up the weights array.

 *

 * weights[len-1] = 0

 * weights[n] = base ^ (n + 1)

	/*

	 * Arguments: [<base_weight> [<threshold_multiplier>]]

	 *   <base_weight>: Base weight for ema [0, 1024) 10-bit fixed point. A

	 *                  value of 0 will completely ignore any history.

	 *                  If not given, default (HST_FIXED_95) is used.

	 *   <threshold_multiplier>: Minimum threshold multiplier for paths to

	 *                  be considered different. That is, a path is

	 *                  considered different iff (p1 > N * p2) where p1

	 *                  is the path with higher service time. A threshold

	 *                  of 1 or 0 has no effect. Defaults to 0.

	/*

	 * Arguments: [<repeat_count>]

	 *   <repeat_count>: The number of I/Os before switching path.

	 *                   If not given, default (HST_MIN_IO) is used.

 allocate the path */

/*

 * Compare the estimated service time of 2 paths, pi1 and pi2,

 * for the incoming I/O.

 *

 * Returns:

 * < 0 : pi1 is better

 * 0   : no difference between pi1 and pi2

 * > 0 : pi2 is better

 *

	/* Check here if estimated latency for two paths are too similar.

	 * If this is the case, we skip extra calculation and just compare

	 * outstanding requests. In this case, any unloaded paths will

	 * be preferred.

	/*

	 * If an unloaded path is stale, choose it. If both paths are unloaded,

	 * choose path that is the most stale.

	 * (If one path is loaded, choose the other)

	/* Compare estimated service time. If outstanding is the same, we

	 * don't need to multiply

 Potential overflow with out >= 1024 */

			/* If over 1023 in-flights, we may overflow if hst

			 * is at max. (With this shift we still overflow at

			 * 1048576 in-flights, which is high enough).

 In the case that the 'winner' is stale, limit to equal usage. */

 Move last used path to end (least preferred in case of ties) */

	/* if a previous disk request has finished after this IO was

	 * sent to the hardware, pretend the submission happened

	 * serially.

	/*

	 * On request end, mark path as fresh. If a path hasn't

	 * finished any requests within the fresh period, the estimated

	 * service time is considered too optimistic and we limit the

	 * maximum requests on that path.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 Arrikto, Inc. All Rights Reserved.

 Min and max dm-clone metadata versions supported */

/*

 * On-disk metadata layout

/*

 * Region and Dirty bitmaps.

 *

 * dm-clone logically splits the source and destination devices in regions of

 * fixed size. The destination device's regions are gradually hydrated, i.e.,

 * we copy (clone) the source's regions to the destination device. Eventually,

 * all regions will get hydrated and all I/O will be served from the

 * destination device.

 *

 * We maintain an on-disk bitmap which tracks the state of each of the

 * destination device's regions, i.e., whether they are hydrated or not.

 *

 * To save constantly doing look ups on disk we keep an in core copy of the

 * on-disk bitmap, the region_map.

 *

 * In order to track which regions are hydrated during a metadata transaction,

 * we use a second set of bitmaps, the dmap (dirty bitmap), which includes two

 * bitmaps, namely dirty_regions and dirty_words. The dirty_regions bitmap

 * tracks the regions that got hydrated during the current metadata

 * transaction. The dirty_words bitmap tracks the dirty words, i.e. longs, of

 * the dirty_regions bitmap.

 *

 * This allows us to precisely track the regions that were hydrated during the

 * current metadata transaction and update the metadata accordingly, when we

 * commit the current transaction. This is important because dm-clone should

 * only commit the metadata of regions that were properly flushed to the

 * destination device beforehand. Otherwise, in case of a crash, we could end

 * up with a corrupted dm-clone device.

 *

 * When a region finishes hydrating dm-clone calls

 * dm_clone_set_region_hydrated(), or for discard requests

 * dm_clone_cond_set_range(), which sets the corresponding bits in region_map

 * and dmap.

 *

 * During a metadata commit we scan dmap->dirty_words and dmap->dirty_regions

 * and update the on-disk metadata accordingly. Thus, we don't have to flush to

 * disk the whole region_map. We can just flush the dirty region_map bits.

 *

 * We use the helper dmap->dirty_words bitmap, which is smaller than the

 * original region_map, to reduce the amount of memory accesses during a

 * metadata commit. Moreover, as dm-bitset also accesses the on-disk bitmap in

 * 64-bit word granularity, the dirty_words bitmap helps us avoid useless disk

 * accesses.

 *

 * We could update directly the on-disk bitmap, when dm-clone calls either

 * dm_clone_set_region_hydrated() or dm_clone_cond_set_range(), buts this

 * inserts significant metadata I/O overhead in dm-clone's I/O path. Also, as

 * these two functions don't block, we can call them in interrupt context,

 * e.g., in a hooked overwrite bio's completion routine, and further reduce the

 * I/O completion latency.

 *

 * We maintain two dirty bitmap sets. During a metadata commit we atomically

 * swap the currently used dmap with the unused one. This allows the metadata

 * update functions to run concurrently with an ongoing commit.

 The metadata block device */

 Spinlock protecting the region and dirty bitmaps. */

 Protected by lock */

	/*

	 * In core copy of the on-disk bitmap to save constantly doing look ups

	 * on disk.

 Protected by bitmap_lock */

	/*

	 * Reading the space map root can fail, so we read it into this

	 * buffer before the superblock is locked and updated.

---------------------------------------------------------------------------*/

/*

 * Superblock validation.

 Check metadata version */

/*

 * Check if the superblock is formatted or not. We consider the superblock to

 * be formatted in case we find non-zero bytes in it.

	/*

	 * We don't use a validator here because the superblock could be all

	 * zeroes.

 This assumes that the block size is a multiple of 8 bytes */

---------------------------------------------------------------------------*/

/*

 * Low-level metadata handling.

 Save dm-clone metadata in superblock */

 FIXME: UUID is currently unused */

 Save the metadata space_map root */

 Verify that target_size and region_size haven't changed. */

 Flush to disk all blocks, except the superblock */

 Create block manager */

---------------------------------------------------------------------------*/

 Flush bitset cache */

 Flush bitset cache */

 Flush to disk all blocks, except the superblock */

 Save the space map root in cmd->metadata_space_map_root */

 Lock the superblock */

 Save the metadata in superblock */

 Unlock superblock and commit it to disk */

	/*

	 * FIXME: Find a more efficient way to check if the hydration is done.

 Update the changed flag */

 Get current dirty bitmap */

 Get next dirty bitmap */

	/*

	 * The last commit failed, so we don't have a clean dirty-bitmap to

	 * use.

 Swap dirty bitmaps */

 Set old dirty bitmap as currently committing */

 Clear committing dmap */

/*

 * WARNING: This must not be called concurrently with either

 * dm_clone_set_region_hydrated() or dm_clone_cond_set_range(), as it changes

 * cmd->region_map without taking the cmd->bitmap_lock spinlock. The only

 * exception is after setting the metadata to read-only mode, using

 * dm_clone_metadata_set_read_only().

 *

 * We don't take the spinlock because __load_bitset_in_core() does I/O, so it

 * may block.

 If something went wrong we can neither write nor read the metadata */

/*

 * Copyright (C) 2001, 2002 Sistina Software (UK) Limited.

 * Copyright (C) 2004 - 2006 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

	/*

	 * poll will wait until the global event number is greater than

	 * this value.

/*-----------------------------------------------------------------

 * The ioctl interface needs to be able to look up devices by

 * name or uuid.

/*

 * Guards access to both hash tables.

/*

 * Protects use of mdptr to obtain hash cell name and uuid from mapped device.

/*-----------------------------------------------------------------

 * Code for looking up a device by name

/*-----------------------------------------------------------------

 * Inserting, removing and renaming a device.

/*

 * The kdev_t and uuid of a device can never change once it is

 * initially inserted.

	/*

	 * Allocate the new cells.

	/*

	 * Insert the cell into both hash tables.

 remove from the dev trees */

		/*

		 * Some mapped devices may be using other mapped

		 * devices, so repeat until we make no further

		 * progress.  If a new mapped device is created

		 * here it will also get removed.

/*

 * Set the uuid of a hash_cell that isn't already set.

/*

 * Changes the name of a hash_cell and returns the old name for

 * the caller to free.

	/*

	 * Rename and move the name cell.

	/*

	 * duplicate new.

	/*

	 * Is new free ?

	/*

	 * Is there such a device as 'old' ?

	/*

	 * Does this device already have a uuid?

	/*

	 * Wake up any dm event waiters.

/*-----------------------------------------------------------------

 * Implementation of the ioctl commands

/*

 * All the ioctl commands get dispatched to functions with this

 * prototype.

/*

 * Round up the ptr to an 8-byte boundary.

/*

 * Retrieves the data payload buffer from an already allocated

 * struct dm_ioctl.

	/*

	 * Loop through all the devices working out how much

	 * space we need.

	/*

	 * Grab our output buffer.

 Flags no data */

	/*

	 * Now loop through filling out the names.

	/*

	 * If mismatch happens, security may be compromised due to buffer

	 * overflow, so it's better to crash.

 Check space - it might have changed since the first iteration */

	/*

	 * Loop through all the devices working out how much

	 * space we need.

	/*

	 * Grab our output buffer.

	/*

	 * Now loop through filling out the names & versions.

/*

 * On successful return, the caller must not attempt to acquire

 * _hash_lock without first calling dm_put_live_table, because dm_table_destroy

 * waits for this dm_put_live_table and could be called under this lock.

 increment rcu count, we don't care about the table pointer */

/*

 * Fills in a dm_ioctl structure, ready for sending back to

 * userland.

	/*

	 * Yes, this will be out of date by the time it gets back

	 * to userland, but it is still very useful for

	 * debugging.

/*

 * Always use UUID for lookups if it's present, otherwise use name or dev.

	/*

	 * Sneakily write in both the name and the uuid

	 * while we have the cell.

	/*

	 * Ensure the device is not open and nothing further can open it.

/*

 * Check a string doesn't overrun the chunk of

 * memory we copied from userland.

 Do we need to load a new map ? */

 Suspend if it isn't already suspended */

	/*

	 * Since dm_swap_table synchronizes RCU, nobody should be in

	 * read-side critical section already.

/*

 * Set or unset the suspension state of a device.

 * If the device already is in the requested state we just return its status.

/*

 * Copies device info back to user space, used by

 * the create and info ioctls.

/*

 * Build up the status struct for each target

 Get all the target info */

 Get the status/table string from the target driver */

/*

 * Wait for a device to report an event

	/*

	 * Wait for a notification event

	/*

	 * The userland program is going to want to know what

	 * changed to trigger the event, so we may as well tell

	 * him and save an ioctl.

/*

 * Remember the global event number and make it possible to poll

 * for further events.

 Protect md->type and md->queue against concurrent table loads. */

 setup md->queue to reflect md's type (may block) */

 stage inactive table */

/*

 * Retrieves a list of devices used by a particular dm device.

	/*

	 * Count the devices.

	/*

	 * Check we have enough space.

	/*

	 * Fill in the devices.

/*

 * Return the status of a device as a text string for each

 * target.

/*

 * Process device-mapper dependent messages.  Messages prefixed with '@'

 * are processed by the DM core.  All others are delivered to the target.

 * Returns a number <= 1 if message was processed by device mapper.

 * Returns 2 if message should be delivered to the target.

 no '@' prefix, deliver to target */

/*

 * Pass a message to the target that's at the supplied device offset.

/*

 * The ioctl parameter block consists of two parts, a dm_ioctl struct

 * followed by a data buffer.  This flag is set if the second part,

 * which has a variable size, is not used by the function processing

 * the ioctl.

/*-----------------------------------------------------------------

 * Implementation of open/close/ioctl on the special char

 * device.

 version is dealt with elsewhere */

/*

 * As well as checking the version compatibility this always

 * copies the kernel interface version out.

	/*

	 * Fill in the kernel version.

 Params allocated with kvmalloc() */

 Wipe input buffer before returning from ioctl */

	/*

	 * Use __GFP_HIGH to avoid low memory issues when a device is

	 * suspended and the ioctl is needed to resume it.

	 * Use kmalloc() rather than vmalloc() when we can.

 Copy from param_kernel (which was already copied from user) */

 Wipe the user buffer so we do not return it to userspace */

 Always clear this flag */

 Ignores parameters */

 Ensure strings are terminated */

 only root can play with this */

	/*

	 * Check the interface version passed in.  This also

	 * writes out the kernel's interface version.

	/*

	 * Nothing more to do for the version command.

	/*

	 * Copy the parameters into kernel space.

	/*

	 * Copy the results back to userland.

/*

 * Create misc character device and link to DM_DIR/control.

/**

 * dm_copy_name_and_uuid - Copy mapped device name & uuid into supplied buffers

 * @md: Pointer to mapped_device

 * @name: Buffer (size DM_NAME_LEN) for name

 * @uuid: Buffer (size DM_UUID_LEN) for uuid or empty string if uuid not defined

/**

 * dm_early_create - create a mapped device in early boot.

 *

 * @dmi: Contains main information of the device mapping to be created.

 * @spec_array: array of pointers to struct dm_target_spec. Describes the

 * mapping table of the device.

 * @target_params_array: array of strings with the parameters to a specific

 * target.

 *

 * Instead of having the struct dm_target_spec and the parameters for every

 * target embedded at the end of struct dm_ioctl (as performed in a normal

 * ioctl), pass them as arguments, so the caller doesn't need to serialize them.

 * The size of the spec_array and target_params_array is given by

 * @dmi->target_count.

 * This function is supposed to be called in early boot, so locking mechanisms

 * to protect against concurrent loads are not required.

 alloc dm device */

 hash insert */

 alloc table */

 add targets */

 finish table */

 setup md->queue to reflect md's type (may block) */

 Set new map */

 resume device */

 release reference from __get_name_cell */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

   raid0.c : Multiple Devices driver for Linux

	     Copyright (C) 1994-96 Marc ZYNGIER

	     <zyngier@ufr-info-p7.ibp.fr> or

	     <maz@gloups.fdn.fr>

	     Copyright (C) 1999, 2000 Ingo Molnar, Red Hat



   RAID-0 management functions.



/*

 * inform the user of the raid configuration

 round size to chunk_size */

				/*

				 * Not unique, don't count it as a new

				 * group

	/*

	 * now since we have the hard sector sizes, we can make sure

	 * chunk size is a multiple of that sector size

	/* The first zone must contain all devices, so here we check that

	 * there is a proper alignment of slots to devices and find them all

 taking over a raid10-n2 array */

			/* taiking over a raid1 array-

			 * we have only one active disk

 now do the other zones */

/* Find the zone which holds a particular offset

 * Update *sectorp to be an offset in that zone

/*

 * remaps the bio to the target device. we separate two flows.

 * power 2 flow and a general flow for the sake of performance

 find the sector offset inside the chunk */

 chunk in zone */

 quotient is the chunk in real device*/

	/*

	*  position the bio over the real device

	*  real sector = chunk in device + starting of zone

	*	+ the position in the chunk

 if private is not null, we are here after takeover */

 calculate array device size */

 Now start and end is the offset in zone */

 Restore due to sector_div */

 check slot number for a disk */

 Set new parameters */

 make sure it will be not marked as dirty */

	/* Check layout:

	 *  - far_copies must be 1

	 *  - near_copies must be 2

	 *  - disks number must be even

	 *  - all mirrors must be already degraded

 Set new parameters */

 make sure it will be not marked as dirty */

	/* Check layout:

	 *  - (N - 1) mirror drives must be already faulty

	/*

	 * a raid1 doesn't have the notion of chunk size, so

	 * figure out the largest suitable size we can use.

 64K by default */

 The array must be an exact multiple of chunksize */

 array size does not allow a suitable chunk size */

 Set new parameters */

 make sure it will be not marked as dirty */

	/* raid0 can take over:

	 *  raid4 - if all data disks are active.

	 *  raid5 - providing it is Raid4 layout and one disk is faulty

	 *  raid10 - assuming we have all necessary active disks

	 *  raid1 - with (N -1) mirror drives faulty

 RAID0 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Western Digital Corporation or its affiliates.

 *

 * This file is released under the GPL.

/*

 * Metadata version.

/*

 * On-disk super block magic.

/*

 * On disk super block.

 * This uses only 512 B but uses on disk a full 4KB block. This block is

 * followed on disk by the mapping table of chunks to zones and the bitmap

 * blocks indicating zone block validity.

 * The overall resulting metadata format is:

 *    (1) Super block (1 block)

 *    (2) Chunk mapping table (nr_map_blocks)

 *    (3) Bitmap blocks (nr_bitmap_blocks)

 * All metadata blocks are stored in conventional zones, starting from

 * the first conventional zone found on disk.

 Magic number */

   4 */

 Metadata version number */

   8 */

 Generation number */

  16 */

 This block number */

  24 */

 The number of metadata blocks, including this super block */

  28 */

 The number of sequential zones reserved for reclaim */

  32 */

 The number of entries in the mapping table */

  36 */

 The number of blocks used for the chunk mapping table */

  40 */

 The number of blocks used for the block bitmaps */

  44 */

 Checksum */

  48 */

 DM-Zoned label */

  80 */

 DM-Zoned UUID */

  96 */

 Device UUID */

 112 */

 Padding to full 512B sector */

 512 */

/*

 * Chunk mapping entry: entries are indexed by chunk number

 * and give the zone ID (dzone_id) mapping the chunk on disk.

 * This zone may be sequential or random. If it is a sequential

 * zone, a second zone (bzone_id) used as a write buffer may

 * also be specified. This second zone will always be a randomly

 * writeable zone.

/*

 * Chunk mapping table metadata: 512 8-bytes entries per 4KB block.

/*

 * Meta data block descriptor (for cached metadata blocks).

/*

 * Metadata block state flags.

/*

 * Super block information (one per metadata set).

/*

 * In-memory metadata.

 Zone information array */

 Zone allocation management */

/*

 * Various accessors

/*

 * Lock/unlock mapping table.

 * The map lock also protects all the zone lists.

/*

 * Lock/unlock metadata access. This is a "read" lock on a semaphore

 * that prevents metadata flush from running while metadata are being

 * modified. The actual metadata write mutual exclusion is achieved with

 * the map lock and zone state management (active and reclaim state are

 * mutually exclusive).

/*

 * Lock/unlock flush: prevent concurrent executions

 * of dmz_flush_metadata as well as metadata modification in reclaim

 * while flush is being executed.

/*

 * Allocate a metadata block.

 See if we can reuse cached blocks */

 Allocate a new block */

/*

 * Free a metadata block.

/*

 * Insert a metadata block in the rbtree.

 Figure out where to put the new node */

 Add new node and rebalance tree */

/*

 * Lookup a metadata block in the rbtree. If the block is found, increment

 * its reference count.

			/*

			 * If this is the first reference to the block,

			 * remove it from the LRU list.

/*

 * Metadata block BIO end callback.

/*

 * Read an uncached metadata block from disk and add it to the cache.

 Get a new block and a BIO to read it */

	/*

	 * Make sure that another context did not start reading

	 * the block already.

 Submit read BIO */

/*

 * Free metadata blocks.

/*

 * For mblock shrinker: get the number of unused metadata blocks in the cache.

/*

 * For mblock shrinker: scan unused metadata blocks and shrink the cache.

/*

 * Release a metadata block.

/*

 * Get a metadata block from the rbtree. If the block

 * is not present, read it from disk.

 Check rbtree */

 Cache miss: read the block from disk */

 Wait for on-going read I/O and check for error */

/*

 * Mark a metadata block dirty.

/*

 * Issue a metadata block write BIO.

/*

 * Read/write a metadata block.

/*

 * Write super block of the specified metadata set.

	/*

	 * The metadata always references the absolute block address,

	 * ie relative to the entire block range, not the per-device

	 * block address.

/*

 * Write dirty metadata blocks to the specified set.

 Issue writes */

 Wait for completion */

 Flush drive cache (this will also sync data) */

/*

 * Log dirty metadata blocks.

 Write dirty blocks to the log */

	/*

	 * No error so far: now validate the log by updating the

	 * log index super block generation.

/*

 * Flush dirty metadata blocks.

	/*

	 * Make sure that metadata blocks are stable before logging: take

	 * the write lock on the metadata semaphore to prevent target BIOs

	 * from modifying metadata.

	/*

	 * This is called from the target flush work and reclaim work.

	 * Concurrent execution is not allowed.

 Get dirty blocks */

 If there are no dirty metadata blocks, just flush the device cache */

	/*

	 * The primary metadata set is still clean. Keep it this way until

	 * all updates are successful in the secondary set. That is, use

	 * the secondary set as a log.

	/*

	 * The log is on disk. It is now safe to update in place

	 * in the primary metadata set.

/*

 * Check super block.

			/*

			 * Generation number should be 0, but it doesn't

			 * really matter if it isn't.

 OK */

/*

 * Read the first or second super block from disk.

/*

 * Determine the position of the secondary super blocks on disk.

 * This is used only if a corruption of the primary super block

 * is detected.

 Allocate a block */

 Bad first super block: search for the second one */

/*

 * Read a super block from disk.

 Allocate a block */

 Read super block */

/*

 * Recover a metadata set.

 Copy metadata blocks */

 Finalize with the super block */

/*

 * Get super block from disk.

 Read and check the primary super block */

 Read and check secondary super block */

 Use highest generation sb first */

/*

 * Initialize a zone descriptor.

 Ignore the eventual runt (smaller) zone */

	/*

	 * Devices that have zones with a capacity smaller than the zone size

	 * (e.g. NVMe zoned namespaces) are not supported.

 Primary super block zone */

			/*

			 * Tertiary superblock zones are always at the

			 * start of the zoned devices, so mark them

			 * as metadata zone.

 Disable runt zone */

/*

 * Free zones descriptors.

/*

 * Allocate and initialize zone descriptors using the zone

 * information from disk.

 Init */

 Allocate zone array */

		/*

		 * Primary superblock zone is always at zone 0 when multiple

		 * drives are present.

	/*

	 * Get zone information and initialize zone descriptors.  At the same

	 * time, determine where the super block should be: first block of the

	 * first randomly writable zone.

/*

 * Update a zone information.

	/*

	 * Get zone information from disk. Since blkdev_report_zones() uses

	 * GFP_KERNEL by default for memory allocations, set the per-task

	 * PF_MEMALLOC_NOIO flag so that all allocations are done as if

	 * GFP_NOIO was specified.

/*

 * Check a zone write pointer position when the zone is marked

 * with the sequential write error flag.

/*

 * Reset a zone write pointer.

	/*

	 * Ignore offline zones, read only zones,

	 * and conventional zones.

 Clear write error bit and rewind write pointer position */

/*

 * Initialize chunk mapping.

 Metadata block array for the chunk mapping table */

 Get chunk mapping table blocks and initialize zone mapping */

 Get mapping block */

 Check data zone */

 Check buffer zone */

	/*

	 * At this point, only meta zones and mapped data zones were

	 * fully initialized. All remaining zones are unmapped data

	 * zones. Finish initializing those here.

 Already initialized */

 Unmapped data zone */

/*

 * Set a data chunk mapping.

/*

 * The list of mapped zones is maintained in LRU order.

 * This rotates a zone at the end of its map list.

 LRU rotate sequential zone */

 LRU rotate cache zone */

 LRU rotate random zone */

/*

 * The list of mapped random zones is maintained

 * in LRU order. This rotates a zone at the end of the list.

/*

 * Wait for any zone to be freed.

/*

 * Lock a zone for reclaim (set the zone RECLAIM bit).

 * Returns false if the zone cannot be locked or if it is already locked

 * and 1 otherwise.

 Active zones cannot be reclaimed */

/*

 * Clear a zone reclaim flag.

/*

 * Wait for a zone reclaim to complete.

/*

 * Select a cache or random write zone for reclaim.

 If we have cache zones select from the cache zone list */

 Try to relaim random zones, too, when idle */

	/*

	 * Find the buffer zone with the heaviest weight or the first (oldest)

	 * data zone that can be reclaimed.

	/*

	 * If we come here, none of the zones inspected could be locked for

	 * reclaim. Try again, being more aggressive, that is, find the

	 * first zone that can be reclaimed regardless of its weitght.

/*

 * Select a buffered sequential zone for reclaim.

/*

 * Select a zone for reclaim.

	/*

	 * Search for a zone candidate to reclaim: 2 cases are possible.

	 * (1) There is no free sequential zones. Then a random data zone

	 *     cannot be reclaimed. So choose a sequential zone to reclaim so

	 *     that afterward a random zone can be reclaimed.

	 * (2) At least one free sequential zone is available, then choose

	 *     the oldest random zone (data or buffer) that can be locked.

/*

 * Get the zone mapping a chunk, if the chunk is mapped already.

 * If no mapping exist and the operation is WRITE, a zone is

 * allocated and used to map the chunk.

 * The zone returned will be set to the active state.

 Get the chunk mapping */

		/*

		 * Read or discard in unmapped chunks are fine. But for

		 * writes, we need a mapping, so get one.

 Allocate a random zone */

 The chunk is already mapped: get the mapping zone */

 Repair write pointer if the sequential dzone has error */

	/*

	 * If the zone is being reclaimed, the chunk mapping may change

	 * to a different zone. So wait for reclaim and retry. Otherwise,

	 * activate the zone (this will prevent reclaim from touching it).

/*

 * Write and discard change the block validity of data zones and their buffer

 * zones. Check here that valid blocks are still present. If all blocks are

 * invalid, the zones can be unmapped on the fly without waiting for reclaim

 * to do it.

 Empty buffer zone: reclaim it */

 Deactivate the data zone */

 Unbuffered inactive empty data zone: reclaim it */

/*

 * Allocate and map a random zone to buffer a chunk

 * already mapped to a sequential zone.

 Allocate a random zone */

 Update the chunk mapping */

/*

 * Get an unmapped (free) zone.

 * This must be called with the mapping lock held.

 Schedule reclaim to ensure free zones are available */

		/*

		 * No free zone: return NULL if this is for not reclaim.

		/*

		 * Try to allocate from other devices

		/*

		 * Fallback to the reserved sequential zones

/*

 * Free a zone.

 * This must be called with the mapping lock held.

 If this is a sequential zone, reset it */

 Return the zone to its type unmap list */

/*

 * Map a chunk to a zone.

 * This must be called with the mapping lock held.

 Set the chunk mapping */

/*

 * Unmap a zone.

 * This must be called with the mapping lock held.

 Already unmapped */

		/*

		 * Unmapping the chunk buffer zone: clear only

		 * the chunk buffer mapping

		/*

		 * Unmapping the chunk data zone: the zone must

		 * not be buffered.

/*

 * Set @nr_bits bits in @bitmap starting from @bit.

 * Return the number of bits changed from 0 to 1.

 Try to set the whole word at once */

/*

 * Get the bitmap block storing the bit for chunk_block in zone.

/*

 * Copy the valid blocks bitmap of from_zone to the bitmap of to_zone.

 Get the zones bitmap blocks */

/*

 * Merge the valid blocks bitmap of from_zone into the bitmap of to_zone,

 * starting from chunk_block.

 Get the zones bitmap blocks */

 Get a valid region from the source zone */

/*

 * Validate all the blocks in the range [block..block+nr_blocks-1].

 Get bitmap block */

 Set bits */

/*

 * Clear nr_bits bits in bitmap starting from bit.

 * Return the number of bits cleared.

 Try to clear whole word at once */

/*

 * Invalidate all the blocks in the range [block..block+nr_blocks-1].

 Get bitmap block */

 Clear bits */

/*

 * Get a block bit value.

 Get bitmap block */

 Get offset */

/*

 * Return the number of blocks from chunk_block to the first block with a bit

 * value specified by set. Search at most nr_blocks blocks from chunk_block.

 Get bitmap block */

 Get offset */

/*

 * Test if chunk_block is valid. If it is, the number of consecutive

 * valid blocks from chunk_block will be returned.

 The block is valid: get the number of valid blocks from block */

/*

 * Find the first valid block from @chunk_block in @zone.

 * If such a block is found, its number is returned using

 * @chunk_block and the total number of valid blocks from @chunk_block

 * is returned.

/*

 * Count the number of bits set starting from bit up to bit + nr_bits - 1.

/*

 * Get a zone weight.

 Get bitmap block */

 Count bits in this block */

/*

 * Cleanup the zoned metadata resources.

 Release zone mapping resources */

 Release super blocks */

 Free cached blocks */

 Sanity checks: the mblock rbtree should now be empty */

 Free the zone descriptors */

/*

 * Initialize the zoned metadata.

 Initialize zone descriptors */

 Get super block */

 Set metadata zones starting from sb_zone */

 Load mapping table */

	/*

	 * Cache size boundaries: allow at least 2 super blocks, the chunk map

	 * blocks and enough blocks to be able to cache the bitmap blocks of

	 * up to 16 zones when idle (min_nr_mblks). Otherwise, if busy, allow

	 * the cache to add 512 more metadata blocks.

 Metadata cache shrinker */

/*

 * Cleanup the zoned metadata resources.

/*

 * Check zone information on resume.

 Check zones */

 Check write pointer */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Partial Parity Log for closing the RAID5 write hole

 * Copyright (c) 2017, Intel Corporation.

/*

 * PPL consists of a 4KB header (struct ppl_header) and at least 128KB for

 * partial parity data. The header contains an array of entries

 * (struct ppl_header_entry) which describe the logged write requests.

 * Partial parity for the entries comes after the header, written in the same

 * sequence as the entries:

 *

 * Header

 *   entry0

 *   ...

 *   entryN

 * PP data

 *   PP for entry0

 *   ...

 *   PP for entryN

 *

 * An entry describes one or more consecutive stripe_heads, up to a full

 * stripe. The modifed raid data chunks form an m-by-n matrix, where m is the

 * number of stripe_heads in the entry and n is the number of modified data

 * disks. Every stripe_head in the entry must write to the same data disks.

 * An example of a valid case described by a single entry (writes to the first

 * stripe of a 4 disk array, 16k chunk size):

 *

 * sh->sector   dd0   dd1   dd2    ppl

 *            +-----+-----+-----+

 * 0          | --- | --- | --- | +----+

 * 8          | -W- | -W- | --- | | pp |   data_sector = 8

 * 16         | -W- | -W- | --- | | pp |   data_size = 3 * 2 * 4k

 * 24         | -W- | -W- | --- | | pp |   pp_size = 3 * 4k

 *            +-----+-----+-----+ +----+

 *

 * data_sector is the first raid sector of the modified data, data_size is the

 * total size of modified data and pp_size is the size of partial parity for

 * this entry. Entries for full stripe writes contain no partial parity

 * (pp_size = 0), they only mark the stripes for which parity should be

 * recalculated after an unclean shutdown. Every entry holds a checksum of its

 * partial parity, the header also has a checksum of the header itself.

 *

 * A write request is always logged to the PPL instance stored on the parity

 * disk of the corresponding stripe. For each member disk there is one ppl_log

 * used to handle logging for this disk, independently from others. They are

 * grouped in child_logs array in struct ppl_conf, which is assigned to

 * r5conf->log_private.

 *

 * ppl_io_unit represents a full PPL write, header_page contains the ppl_header.

 * PPL entries for logged stripes are added in ppl_log_stripe(). A stripe_head

 * can be appended to the last entry if it meets the conditions for a valid

 * entry described above, otherwise a new entry is added. Checksums of entries

 * are calculated incrementally as stripes containing partial parity are being

 * added. ppl_submit_iounit() calculates the checksum of the header and submits

 * a bio containing the header page and partial parity pages (sh->ppl_page) for

 * all stripes of the io_unit. When the PPL write completes, the stripes

 * associated with the io_unit are released and raid5d starts writing their data

 * and parity. When all stripes are written, the io_unit is freed and the next

 * can be submitted.

 *

 * An io_unit is used to gather stripes until it is submitted or becomes full

 * (if the maximum number of entries or size of PPL is reached). Another io_unit

 * can't be submitted until the previous has completed (PPL and stripe

 * data+parity is written). The log->io_list tracks all io_units of a log

 * (for a single member disk). New io_units are added to the end of the list

 * and the first io_unit is submitted, if it is not submitted already.

 * The current io_unit accepting new stripes is always at the end of the list.

 *

 * If write-back cache is enabled for any of the disks in the array, its data

 * must be flushed before next io_unit is submitted.

 array of child logs, one for each raid disk */

	int block_size;		/* the logical block size used for data_sector

 raid array identifier */

 current log write sequence number */

 used only for recovery */

 stripes to retry if failed to allocate io_unit */

 shared between all log instances */

	struct md_rdev *rdev;		/* array member disk associated with

	struct ppl_io_unit *current_io;	/* current io_unit accepting new data

 all io_units of this log */

 for ppl_header */

 number of entries in ppl_header */

 total size current of partial parity */

 sequence number of this log write */

 log->io_list */

 stripes added to the io_unit */

 how many stripes not written to raid */

 how many disk flushes are in progress */

 true if write to log started */

 inline bio and its biovec for submitting the iounit */

	/*

	 * Partial parity is the XOR of stripe data chunks that are not changed

	 * during the write request. Depending on available data

	 * (read-modify-write vs. reconstruct-write case) we calculate it

	 * differently.

		/*

		 * rmw: xor old data and parity from updated disks

		 * This is calculated earlier by ops_run_prexor5() so just copy

		 * the parity dev page.

 rcw: xor data from all not updated disks */

 check if current io_unit is full */

 add a new unit if there is none or the current is full */

		/*

		 * Check if we can append the stripe to the last entry. It must

		 * be just after the last logged stripe and write to the same

		 * disks. Use bit shift and logarithm to avoid 64-bit division.

 don't write any PP if full stripe write */

 Rewind the buffer if current PPL is larger then remaining space */

 entries for full stripe writes have no partial parity */

/*

 * PPL recovery strategy: xor partial parity and data from all modified data

 * disks within a stripe and write the result as the new stripe parity. If all

 * stripe data disks are modified (full stripe write), no partial parity is

 * available, so just xor the data disks.

 *

 * Recovery of a PPL entry shall occur only if all modified data disks are

 * available and read from all of them succeeds.

 *

 * A PPL entry applies to a stripe, partial parity size for an entry is at most

 * the size of the chunk. Examples of possible cases for a single entry:

 *

 * case 0: single data disk write:

 *   data0    data1    data2     ppl        parity

 * +--------+--------+--------+           +--------------------+

 * | ------ | ------ | ------ | +----+    | (no change)        |

 * | ------ | -data- | ------ | | pp | -> | data1 ^ pp         |

 * | ------ | -data- | ------ | | pp | -> | data1 ^ pp         |

 * | ------ | ------ | ------ | +----+    | (no change)        |

 * +--------+--------+--------+           +--------------------+

 * pp_size = data_size

 *

 * case 1: more than one data disk write:

 *   data0    data1    data2     ppl        parity

 * +--------+--------+--------+           +--------------------+

 * | ------ | ------ | ------ | +----+    | (no change)        |

 * | -data- | -data- | ------ | | pp | -> | data0 ^ data1 ^ pp |

 * | -data- | -data- | ------ | | pp | -> | data0 ^ data1 ^ pp |

 * | ------ | ------ | ------ | +----+    | (no change)        |

 * +--------+--------+--------+           +--------------------+

 * pp_size = data_size / modified_data_disks

 *

 * case 2: write to all data disks (also full stripe write):

 *   data0    data1    data2                parity

 * +--------+--------+--------+           +--------------------+

 * | ------ | ------ | ------ |           | (no change)        |

 * | -data- | -data- | -data- | --------> | xor all data       |

 * | ------ | ------ | ------ | --------> | (no change)        |

 * | ------ | ------ | ------ |           | (no change)        |

 * +--------+--------+--------+           +--------------------+

 * pp_size = 0

 *

 * The following cases are possible only in other implementations. The recovery

 * code can handle them, but they are not generated at runtime because they can

 * be reduced to cases 0, 1 and 2:

 *

 * case 3:

 *   data0    data1    data2     ppl        parity

 * +--------+--------+--------+ +----+    +--------------------+

 * | ------ | -data- | -data- | | pp |    | data1 ^ data2 ^ pp |

 * | ------ | -data- | -data- | | pp | -> | data1 ^ data2 ^ pp |

 * | -data- | -data- | -data- | | -- | -> | xor all data       |

 * | -data- | -data- | ------ | | pp |    | data0 ^ data1 ^ pp |

 * +--------+--------+--------+ +----+    +--------------------+

 * pp_size = chunk_size

 *

 * case 4:

 *   data0    data1    data2     ppl        parity

 * +--------+--------+--------+ +----+    +--------------------+

 * | ------ | -data- | ------ | | pp |    | data1 ^ pp         |

 * | ------ | ------ | ------ | | -- | -> | (no change)        |

 * | ------ | ------ | ------ | | -- | -> | (no change)        |

 * | -data- | ------ | ------ | | pp |    | data0 ^ pp         |

 * +--------+--------+--------+ +----+    +--------------------+

 * pp_size = chunk_size

 if start and end is 4k aligned, use a 4k block */

 iterate through blocks in strip */

 iterate through data member disks */

 map raid sector to member disk */

 map raid sector to parity disk */

 iterate through all PPL entries saved */

 read parial parity for this entry and calculate its checksum */

			/*

			 * Don't recover this entry if the checksum does not

			 * match, but keep going and try to recover other

			 * entries.

 flush the disk cache after recovery if necessary */

 zero out PPL space to avoid collision with old PPLs */

 read PPL headers, find the recent one */

 searching ppl area for latest ppl */

 if not able to read - don't recover any PPL */

 check header validity */

			/*

			 * For external metadata the header signature is set and

			 * validated in userspace.

 previous was newest */

 calculate next potential ppl offset */

 no valid ppl found */

 attempt to recover from log if we are starting a dirty array */

 write empty header if we are starting the array */

 skip missing drive */

		/*

		 * For external metadata we can't check if the signature is

		 * correct on a single drive, but we can check if it is the same

		 * on all drives.

	/*

	 * The configured PPL size must be enough to store

	 * the header and (at the very least) partial parity

	 * for one stripe. Round it down to ensure the data

	 * space is cleanly divisible by stripe size.

 load and possibly recover the logs from the member disks */

		/*

		 * If we are starting a dirty array and the recovery succeeds

		 * without any issues, set the array as clean.

 no mismatch allowed when enabling PPL for a running array */

/*

 * Copyright (C) 2012 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * @nr_cells should be the number of cells you want in use _concurrently_.

 * Don't confuse it with the number of distinct keys.

/*

 * @inmates must have been initialised prior to this call

/*

 * Sometimes we don't want the holder, just the additional bios.

----------------------------------------------------------------*/

/*

 * Returns 1 if deferred or 0 if no pending items to delay job.

----------------------------------------------------------------*/

/*

 * module hooks

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015, SUSE

 lock name. */

 flags to pass to dlm_lock() */

 wait queue for synchronized locking */

 blocking AST function pointer*/

 pointing back to mddev. */

 md_cluster_info flags */

/* Lock the send communication. This is done through

 * bit manipulation as opposed to a mutex in order to

 * accomodate lock and hold. See next comment.

/* If cluster operations (such as adding a disk) must lock the

 * communication channel, so as to perform extra operations

 * (update metadata) and no other operation is allowed on the

 * MD. Token needs to be locked and held until the operation

 * completes witha md_update_sb(), which would eventually release

 * the lock.

/* We should receive message after node joined cluster and

 the md device which md_cluster_info belongs to */

 dlm lock space and resources for clustered raid. */

 record the region which write should be suspended */

 the slot which broadcast suspend_lo/hi */

 communication loc resources */

 record the region in RESYNCING message */

 TODO: Unionize this for smaller footprint */

/*

 * An variation of dlm_lock_sync, which make lock request could

 * be interrupted

		/*

		 * the convert queue contains the lock request when request is

		 * interrupted, and sync_ast could still be run, so need to

		 * cancel the request and reset completion

	/*

	 * use FORCEUNLOCK flag, so we can unlock even the lock is on the

	 * waiting or convert queue

 Clear suspend_area associated with the bitmap */

 Kick off a reshape if needed */

			/* wake up thread to continue resync in case resync

				/*

				 * clear the REMOTE flag since we will launch

				 * resync thread in current node.

	/* deduct one since dlm slot starts from one while the num of

	/* completion is only need to be complete when node join cluster,

/* the ops is called when node join the cluster, and do lock recovery

/*

 * The BAST function for the ack lock resource

 * This function wakes up the receive thread in

 * order to receive and process the message.

		/*

		 * clear the REMOTE flag since resync or recovery is finished

		 * in remote node.

	/*

	 * The bitmaps are not same for different nodes

	 * if RESYNCING is happening in one node, then

	 * the node which received the RESYNCING message

	 * probably will perform resync with the region

	 * [lo, hi] again, so we could reduce resync time

	 * a lot if we can ensure that the bitmaps among

	 * different nodes are match up well.

	 *

	 * sync_low/hi is used to record the region which

	 * arrived in the previous RESYNCING message,

	 *

	 * Call md_bitmap_sync_with_cluster to clear NEEDED_MASK

	 * and set RESYNC_MASK since  resync thread is running

	 * in another node, so we don't need to do the resync

	 * again with the same section.

	 *

	 * Skip md_bitmap_sync_with_cluster in case reshape

	 * happening, because reshaping region is small and

	 * we don't want to trigger lots of WARN.

/*

 * thread for receiving message

get CR on Message*/

 read lvb and wake up thread to process this message_lockres */

release CR on ack_lockres*/

up-convert to PR on message_lockres*/

get CR on ack_lockres again*/

release CR on message_lockres*/

/* lock_token()

 * Takes the lock on the TOKEN lock resource so no other

 * node can communicate while the operation is underway.

 Lock the receive sequence */

/* lock_comm()

 * Sets the MD_CLUSTER_SEND_LOCK bit to lock the send channel.

	/*

	 * If resync thread run after raid1d thread, then process_metadata_update

	 * could not continue if raid1d held reconfig_mutex (and raid1d is blocked

	 * since another node already got EX on Token and waitting the EX of Ack),

	 * so let resync wake up thread in case flag is set.

/* __sendmsg()

 * This function performs the actual sending of the message. This function is

 * usually called after performing the encompassing operation

 * The function:

 * 1. Grabs the message lockresource in EX mode

 * 2. Copies the message to the message LVB

 * 3. Downconverts message lockresource to CW

 * 4. Upconverts ack lock resource from CR to EX. This forces the BAST on other nodes

 *    and the other nodes read the message. The thread will wait here until all other

 *    nodes have released ack lock resource.

 * 5. Downconvert ack lockresource to CR

get EX on Message*/

down-convert EX to CW on Message*/

up-convert CR to EX on Ack*/

down-convert EX to CR on Ack*/

 in case the message can't be released due to some reason */

 Read the disk bitmap sb and check if it needs recovery */

 Initiate the communication resources */

 get sync CR lock on ACK. */

 get sync CR lock on no-new-dev. */

 load all the node's bitmap info for resync */

 wake up recv thread in case something need to be handled */

	/*

	 * BITMAP_NEEDS_SYNC message should be sent when node

	 * is leaving the cluster with dirty bitmap, also we

	 * can only deliver it when dlm connection is available.

	 *

	 * Also, we should send BITMAP_NEEDS_SYNC message in

	 * case reshaping is interrupted.

/* slot_number(): Returns the MD slot number to use

 * DLM starts the slot numbers from 1, wheras cluster-md

 * wants the number to be from zero, so we deduct one

/*

 * Check if the communication is already locked, else lock the communication

 * channel.

 * If it is already locked, token is in EX mode, and hence lock_token()

 * should not be called.

	/*

	 * metadata_update_start is always called with the protection of

	 * reconfig_mutex, so set WAITING_FOR_TOKEN here.

 If token is already locked, return 0 */

	/* Pick up a good active device number to send.

	/*

	 * We need to ensure all the nodes can grow to a larger

	 * bitmap size before make the reshaping.

		/*

		 * If we can hold the bitmap lock of one node then

		 * the slot is not occupied, update the pages.

			/*

			 * Let's revert the bitmap size if one node

			 * can't resize bitmap

/*

 * return 0 if all the bitmaps have the same sync_size

		/*

		 * If we can hold the bitmap lock of one node then

		 * the slot is not occupied, update the sb.

/*

 * Update the size for cluster raid is a little more complex, we perform it

 * by the steps:

 * 1. hold token lock and update superblock in initiator node.

 * 2. send METADATA_UPDATED msg to other nodes.

 * 3. The initiator node continues to check each bitmap's sync_size, if all

 *    bitmaps have the same value of sync_size, then we can set capacity and

 *    let other nodes to perform it. If one node can't update sync_size

 *    accordingly, we need to revert to previous value.

		/*

		 * We can only change capiticy after all the nodes can do it,

		 * so need to wait after other nodes already received the msg

		 * and handled the change

	/*

	 * check the sync_size from other node's bitmap, if sync_size

	 * have already updated in other nodes as expected, send an

	 * empty metadata msg to permit the change of capacity

 revert to previous sectors */

 do not send zero again, if we have sent before */

 Re-acquire the lock to refresh LVB */

	/*

	 * mddev_lock is held if resync_info_update is called from

	 * resync_finish (md_reap_sync_thread -> resync_finish)

	/*

	 * If resync thread is interrupted so we can't say resync is finished,

	 * another node will launch resync thread to continue.

/* add_new_disk() - initiates a disk add

 * However, if this fails before writing md_update_sb(),

 * add_new_disk_cancel() must be called to release token lock

 Some node does not "see" the device */

		/* Since MD_CHANGE_DEVS will be set in add_bound_rdev which

		 * will run soon after add_new_disk, the below path will be

		 * invoked:

		 *   md_wakeup_thread(mddev->thread)

		 *	-> conf->thread (raid1d)

		 *	-> md_check_recovery -> md_update_sb

		 *	-> metadata_update_start/finish

		 * MD_CLUSTER_SEND_LOCKED_ALREADY will be cleared eventually.

		 *

		 * For other failure cases, metadata_update_cancel and

		 * add_new_disk_cancel also clear below bit as well.

 release other node's bitmap lock if they are existed */

 SPDX-License-Identifier: GPL-2.0

/*

 * Creating audit records for mapped devices.

 *

 * Copyright (C) 2021 Fraunhofer AISEC. All rights reserved.

 *

 * Authors: Michael Weiß <michael.weiss@aisec.fraunhofer.de>

 unintended use */

/*

 * Copyright (C) 2006-2009 Red Hat, Inc.

 *

 * This file is released under the LGPL.

/*

 * This limit on the number of mark and clear request is, to a degree,

 * arbitrary.  However, there is some basis for the choice in the limits

 * imposed on the size of data payload by dm-log-userspace-transfer.c:

 * dm_consult_userspace().

	/*

	 * Mark and clear requests are held until a flush is issued

	 * so that we can group, and thereby limit, the amount of

	 * network traffic between kernel and userspace.  The 'flush_lock'

	 * is used to protect these lists.

	/*

	 * in_sync_hint gets set when doing is_remote_recovering.  It

	 * represents the first region that needs recovery.  IOW, the

	 * first zero bit of sync_bits.  This can be useful for to limit

	 * traffic for calls like is_remote_recovering and get_resync_work,

	 * but be take care in its use for anything else.

	/*

	 * Workqueue for flush of clear region requests.

	/*

	 * Combine userspace flush and mark requests for efficiency.

	/*

	 * If the server isn't there, -ESRCH is returned,

	 * and we must keep trying until the server is

	 * restored.

	/*

	 * Determine overall size of the string.

 +1 for space between args */

 Max number of chars in a printed u64 number */

/*

 * userspace_ctr

 *

 * argv contains:

 *	<UUID> [integrated_flush] <other args>

 * Where 'other args' are the userspace implementation-specific log

 * arguments.

 *

 * Example:

 *	<UUID> [integrated_flush] clustered-disk <arg count> <log dev>

 *	<region_size> [[no]sync]

 *

 * This module strips off the <UUID> and uses it for identification

 * purposes when communicating with userspace about a log.

 *

 * If integrated_flush is defined, the kernel combines flush

 * and mark requests.

 *

 * The rest of the line, beginning with 'clustered-disk', is passed

 * to the userspace ctr function.

 The ptr value is sufficient for local unique id */

	/*

	 * Send table string and get back any opened device.

 Since the region size does not change, get it now */

 flush workqueue */

	/*

	 * Run planned flush earlier.

/*

 * userspace_is_clean

 *

 * Check whether a region is clean.  If there is any sort of

 * failure when consulting the server, we return not clean.

 *

 * Returns: 1 if clean, 0 otherwise

/*

 * userspace_in_sync

 *

 * Check if the region is in-sync.  If there is any sort

 * of failure when consulting the server, we assume that

 * the region is not in sync.

 *

 * If 'can_block' is set, return immediately

 *

 * Returns: 1 if in-sync, 0 if not-in-sync, -EWOULDBLOCK

	/*

	 * We can never respond directly - even if in_sync_hint is

	 * set.  This is because another machine could see a device

	 * failure and mark the region out-of-sync.  If we don't go

	 * to userspace to ask, we might think the region is in-sync

	 * and allow a read to pick up data that is stale.  (This is

	 * very unlikely if a device actually fails; but it is very

	 * likely if a connection to one device from one machine fails.)

	 *

	 * There still might be a problem if the mirror caches the region

	 * state as in-sync... but then this call would not be made.  So,

	 * that is a mirror problem.

	/*

	 * Group process the requests

			/*

			 * Integrated flush failed.

				/*

				 * Group send failed.  Attempt one-by-one.

	/*

	 * Must collect flush_entrys that were successfully processed

	 * as a group so that they will be free'd by the caller.

/*

 * userspace_flush

 *

 * This function is ok to block.

 * The flush happens in two stages.  First, it sends all

 * clear/mark requests that are on the list.  Then it

 * tells the server to commit them.  This gives the

 * server a chance to optimise the commit, instead of

 * doing it for every request.

 *

 * Additionally, we could implement another thread that

 * sends the requests up to the server - reducing the

 * load on flush.  Then the flush would have less in

 * the list and be responsible for the finishing commit.

 *

 * Returns: 0 on success, < 0 on failure

	/*

	 * Send integrated flush request with mark_list as payload.

		/*

		 * When there are only clear region requests,

		 * we schedule a flush in the future.

		/*

		 * Cancel pending flush because we

		 * have already flushed in mark_region.

	/*

	 * We can safely remove these entries, even after failure.

	 * Calling code will receive an error and will know that

	 * the log facility has failed.

/*

 * userspace_mark_region

 *

 * This function should avoid blocking unless absolutely required.

 * (Memory allocation is valid for blocking.)

 Wait for an allocation, but _never_ fail */

/*

 * userspace_clear_region

 *

 * This function must not block.

 * So, the alloc can't block.  In the worst case, it is ok to

 * fail.  It would simply mean we can't clear the region.

 * Does nothing to current sync context, but does mean

 * the region will be re-sync'ed on a reload of the mirror

 * even though it is in-sync.

	/*

	 * If we fail to allocate, we skip the clearing of

	 * the region.  This doesn't hurt us in any way, except

	 * to cause the region to be resync'ed when the

	 * device is activated next time.

/*

 * userspace_get_resync_work

 *

 * Get a region that needs recovery.  It is valid to return

 * an error for this function.

 *

 * Returns: 1 if region filled, 0 if no work, <0 on error

 64-bit for mix arch compatibility */

/*

 * userspace_set_region_sync

 *

 * Set the sync status of a given region.  This function

 * must not fail.

	/*

	 * It would be nice to be able to report failures.

	 * However, it is easy enough to detect and resolve.

/*

 * userspace_get_sync_count

 *

 * If there is any sort of failure when consulting the server,

 * we assume that the sync count is zero.

 *

 * Returns: sync count on success, 0 on failure

/*

 * userspace_status

 *

 * Returns: amount of space consumed

 There will always be a ' ' */

/*

 * userspace_is_remote_recovering

 *

 * Returns: 1 if region recovering, 0 otherwise

	/*

	 * Once the mirror has been reported to be in-sync,

	 * it will never again ask for recovery work.  So,

	 * we can safely say there is not a remote machine

	 * recovering if the device is in-sync.  (in_sync_hint

	 * must be reset at resume time.)

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * faulty.c : Multiple Devices driver for Linux

 *

 * Copyright (C) 2004 Neil Brown

 *

 * fautly-device-simulator personality for md

/*

 * The "faulty" personality causes some requests to fail.

 *

 * Possible failure modes are:

 *   reads fail "randomly" but succeed on retry

 *   writes fail "randomly" but succeed on retry

 *   reads for some address fail and then persist until a write

 *   reads for some address fail and then persist irrespective of write

 *   writes for some address fail and persist

 *   all writes fail

 *

 * Different modes can be active at a time, but only

 * one can be set at array creation.  Others can be added later.

 * A mode can be one-shot or recurrent with the recurrence being

 * once in every N requests.

 * The bottom 5 bits of the "layout" indicate the mode.  The

 * remainder indicate a period, or 0 for one-shot.

 *

 * There is an implementation limit on the number of concurrently

 * persisting-faulty blocks. When a new fault is requested that would

 * exceed the limit, it is ignored.

 * All current faults can be clear using a layout of "0".

 *

 * Requests are always sent to the device.  If they are to fail,

 * we clone the bio and insert a new b_end_io into the chain.

 doesn't go to device */

 internal use only */

 no failure, no decrement */

 If we find a ReadFixable sector, we fix it ... */

 found it ... */

 write request */

			/* special case - don't decrement, don't submit_bio_noacct,

			 * just fail immediately

 read request */

 new layout */

 makes sure further changes come through */

 faulty */

/*

 * Copyright (C) 2016-2017 Red Hat, Inc. All rights reserved.

 * Copyright (C) 2016-2017 Milan Broz

 * Copyright (C) 2016-2017 Mikulas Patocka

 *

 * This file is released under the GPL.

 don't change it */

/*

 * Warning - DEBUG_PRINT prints security-sensitive data to the log,

 * so it should not be enabled in the official kernel

#define DEBUG_PRINT

#define INTERNAL_VERIFY

/*

 * On disk structures

 userspace uses this value */

 __u8 tag[0]; */

/*

 * In-memory structures

 these variables are locked with endio_wait.lock */

/*

 * DM Integrity profile, protection is performed layer above (dm-crypt)

	/*

	 * Xor the number with section and sector, so that if a piece of

	 * journal is written at wrong place, it is detected.

 e.g.: op == TAG_CMP */

 this shouldn't happen anyway, the hash functions have no reason to fail */

				/*if (bi_size < this_step_blocks << (SECTOR_SHIFT + ic->sb->log2_sectors_per_block)) {

					printk("BUGG: bi_sector: %llx, bi_size: %u\n", bi_sector, bi_size);

					printk("BUGG: this_step_blocks: %u\n", this_step_blocks);

					BUG();

		/*

		 * Don't pass down the FUA flag because we have to flush

		 * disk cache anyway.

		/*

		 * We must not sleep in the request routine because it could

		 * stall bios on current->bio_list.

		 * So, we offload the bio to a workqueue if we have to sleep.

		/*

		 * wait_and_add_new_range drops the spinlock, so the journal

		 * may have been changed arbitrarily. We need to recheck.

		 * To simplify the code, we restrict I/O size to just one block.

 don't write if there is newer committed sector */

 the following test is not needed, but it tests the replay code */

DEBUG_print("zeroing journal\n");*/

				/*

				 * This could be caused by crash during writing.

				 * We won't replay the inconsistent part of the

				 * journal.

 set to 0 to test bitmap replay code */

 be notified after md and before hardware drivers */

 set to 1 to stress test synchronous mode */

 we have to maintain excessive padding for compatibility with existing volumes */

/*

 * Construct a integrity mapping

 *

 * Arguments:

 *	device

 *	offset from the start of the device

 *	tag size

 *	D - direct writes, J - journal writes, B - bitmap mode, R - recovery mode

 *	number of optional arguments

 *	optional arguments:

 *		journal_sectors

 *		interleave_sectors

 *		buffer_sectors

 *		journal_watermark

 *		commit_time

 *		meta_device

 *		block_size

 *		sectors_per_bit

 *		bitmap_flush_interval

 *		internal_hash

 *		journal_crypt

 *		journal_mac

 *		recalculate

	/*

	 * If this workqueue were percpu, it would cause bio reordering

	 * and reduced performance.

 make sure that ti->max_io_len doesn't overflow */

/*

 * Copyright (C) 2001-2003 Sistina Software (UK) Limited.

 *

 * This file is released under the GPL.

 The size of this target / num. stripes */

 Needed for handling events */

 Work struct used for triggering events*/

/*

 * An event is triggered whenever a drive

 * drops out of a stripe volume.

/*

 * Parse a single <dev> <sector> pair

/*

 * Construct a striped mapping.

 * <number of stripes> <chunk size> [<dev_path> <offset>]+

	/*

	 * Do we have enough arguments for that many stripes ?

 Set pointer to dm target; used in trigger_event */

	/*

	 * Get the stripe destinations.

 round down */

 next chunk */

 The range doesn't map to the target stripe */

/*

 * Stripe status:

 *

 * INFO

 * #stripes [stripe_name <stripe_name>] [group word count]

 * [error count 'A|D' <error count 'A|D'>]

 *

 * TABLE

 * #stripes [stripe chunk size]

 * [stripe_name physical_start <stripe_name physical_start>]

 *

 I/O complete */

	/*

	 * Test to see which stripe drive triggered the event

	 * and increment error count for all stripes on that device.

	 * If the error count for a given device exceeds the threshold

	 * value we will no longer trigger any further events.

/*

 * Copyright (C) 2001-2002 Sistina Software (UK) Limited.

 *

 * This file is released under the GPL.

/*

 * The size of the mempool used to track chunks in use.

 List of snapshots per Origin */

	/*

	 * You can't use a snapshot if this is 0 (e.g. if full).

	 * A snapshot-merge target never clears this.

	/*

	 * The snapshot overflowed because of a write to the snapshot device.

	 * We don't have to invalidate the snapshot in this case, but we need

	 * to prevent further writes.

 Origin writes don't trigger exceptions until this is set */

 Protected by "pe_allocation_lock" */

 Protected by kcopyd single-threaded callback */

	/*

	 * A list of pending exceptions that completed out of order.

	 * Protected by kcopyd single-threaded callback.

	/*

	 * pe_lock protects all pending_exception operations and access

	 * as well as the snapshot_bios list.

 Chunks with outstanding reads */

 The on disk metadata handler */

 Wait for events based on state_bits */

 Range of chunks currently being merged. */

	/*

	 * The merge operation failed if this flag is set.

	 * Failure modes are handled as follows:

	 * - I/O error reading the header

	 *   	=> don't load the target; abort.

	 * - Header does not have "valid" flag set

	 *   	=> use the origin; forget about the snapshot.

	 * - I/O error when reading exceptions

	 *   	=> don't load the target; abort.

	 *         (We can't use the intermediate origin state.)

	 * - I/O error while merging

	 *	=> stop merging; set merge_failed; process I/O normally.

	/*

	 * Incoming bios that overlap with chunks being merged must wait

	 * for them to be committed.

	/*

	 * Flush data after merge.

/*

 * state_bits:

 *   RUNNING_MERGE  - Merge operation is in progress.

 *   SHUTDOWN_MERGE - Set to signal that merge needs to be stopped;

 *                    cleared afterwards.

/*

 * Maximum number of chunks being copied on write.

 *

 * The value was decided experimentally as a trade-off between memory

 * consumption, stalling the kernel's workqueues and maintaining a high enough

 * throughput.

	/*

	 * There is only ever one instance of a particular block

	 * device so we can compare pointers safely.

	/*

	 * Origin buffers waiting for this to complete are held

	 * in a bio list

 Pointer back to snapshot context */

	/*

	 * 1 indicates the exception has already been sent to

	 * kcopyd.

 There was copying error. */

 A sequence number, it is used for in-order completion. */

	/*

	 * For writing a complete chunk, bypassing the copy.

/*

 * Hash table mapping origin volumes to lists of snapshots and

 * a lock to protect it

/*

 * This conflicting I/O is extremely improbable in the caller,

 * so msleep(1) is sufficient and there is no need for a wait queue.

/*

 * One of these per registered origin, held in the snapshot_origins hash

 The origin device */

 List of snapshots for this origin */

/*

 * This structure is allocated for each origin target

/*

 * Size of the hash table for origin volumes. If we make this

 * the size of the minors list then it should be nearly perfect

/*

 * _origins_lock must be held when calling this function.

 * Returns number of snapshots registered using the supplied cow device, plus:

 * snap_src - a snapshot suitable for use as a source of exception handover

 * snap_dest - a snapshot capable of receiving exception handover.

 * snap_merge - an existing snapshot-merge target linked to the same origin.

 *   There can be at most one snapshot-merge target. The parameter is optional.

 *

 * Possible return values and states of snap_src and snap_dest.

 *   0: NULL, NULL  - first new snapshot

 *   1: snap_src, NULL - normal snapshot

 *   2: snap_src, snap_dest  - waiting for handover

 *   2: snap_src, NULL - handed over, waiting for old to be deleted

 *   1: NULL, snap_dest - source got destroyed without handover

/*

 * On success, returns 1 if this snapshot is a handover destination,

 * otherwise returns 0.

 Does snapshot need exceptions handed over to it? */

	/*

	 * If no snap_src was found, snap cannot become a handover

	 * destination.

	/*

	 * Non-snapshot-merge handover?

	/*

	 * Do not allow more than one merging snapshot.

 Sort the list according to chunk size, largest-first smallest-last */

/*

 * Make a note of the snapshot and its origin so we can look it

 * up when the origin has a write on it.

 *

 * Also validate snapshot exception store handovers.

 * On success, returns 1 if this registration is a handover destination,

 * otherwise returns 0.

 New origin */

 Initialise the struct */

/*

 * Move snapshot to correct place in list according to chunk size.

/*

 * Implementation of the exception hash tables.

 * The lowest hash_shift bits of the chunk number are ignored, allowing

 * some consecutive chunks to be grouped together.

 Lock to protect access to the completed and pending exception hash tables. */

/*

 * Return the exception data for a sector, or NULL if not

 * remapped.

 Add immediately if this table doesn't support consecutive chunks */

 List is ordered by old_chunk */

 Insert after an existing chunk? */

 Insert before an existing chunk? */

		/*

		 * Either the table doesn't support consecutive chunks or slot

		 * l is empty.

 Add before an existing exception */

 Add to l's tail: e is the last exception in this slot */

/*

 * Callback used by the exception stores to load exceptions when

 * initialising.

 Consecutive_count is implicitly initialised to zero */

	/*

	 * Although there is no need to lock access to the exception tables

	 * here, if we don't then hlist_bl_add_head(), called by

	 * dm_insert_exception(), will complain about accessing the

	 * corresponding list without locking it first.

/*

 * Return a minimum chunk size of all snapshots that have the specified origin.

 * Return zero if the origin has no snapshots.

/*

 * Hard coded magic.

 use a fixed size of 2MB */

/*

 * Allocate room for a suitable hash table.

	/*

	 * Calculate based on the size of the original volume or

	 * the COW volume...

	/*

	 * Allocate hash table for in-flight exceptions

	 * Make this smaller than the real hash table

/*

 * Remove one chunk from the index of completed exceptions.

	/*

	 * If this is the only chunk using this exception, remove exception.

	/*

	 * The chunk may be either at the beginning or the end of a

	 * group of consecutive chunks - never in the middle.  We are

	 * removing chunks in the opposite order to that in which they

	 * were added, so this should always be true.

	 * Decrement the consecutive chunk counter and adjust the

	 * starting point if necessary.

	/*

	 * Process chunks (and associated exceptions) in reverse order

	 * so that dm_consecutive_chunk_count_dec() accounting works.

	/*

	 * valid flag never changes during merge, so no lock required.

 Adjust old_chunk and new_chunk to reflect start of linear region */

	/*

	 * Use one (potentially large) I/O to copy all 'linear_chunks'

	 * from the exception store to the origin

	/*

	 * Reallocate any exceptions needed in other snapshots then

	 * wait for the pending exceptions to complete.

	 * Each time any pending exception (globally on the system)

	 * completes we are woken and repeat the process to find out

	 * if we can proceed.  While this may not seem a particularly

	 * efficient algorithm, it is not expected to have any

	 * significant impact on performance.

 Retry after the wait, until all exceptions are done. */

 Wait until writes to all 'linear_chunks' drain */

/*

 * Stop the merging process and wait until it finishes.

	/*

	 * No feature arguments supplied.

		/*

		 * TODO: really these are disjoint.. but ti->num_discard_bios

		 * and dm_bio_get_target_bio_nr() require rigid constraints.

/*

 * Construct a snapshot mapping:

 * <origin_dev> <COW-dev> <p|po|n> <chunk-size> [<# feature args> [<arg>]*]

 Allocate hash table for COW data */

 Add snapshot to the list of snapshots for this origin */

 Exceptions aren't triggered till snapshot_resume() is called */

 invalid handover, register_snapshot has set ti->error */

	/*

	 * Metadata must only be loaded into one table at once, so skip this

	 * if metadata will be handed over during resume.

	 * Chunk size will be set during the handover - set it to zero to

	 * ensure it's ignored.

	/*

	 * Swap all snapshot context information between the two instances.

	/*

	 * Set source invalid to ensure it receives no further I/O.

 Check whether exception handover must be cancelled */

 Prevent further origin writes from using this snapshot. */

 After this returns there can be no new kcopyd jobs. */

	/*

	 * Ensure instructions in mempool_exit aren't reordered

	 * before atomic_read.

			/*

			 * NOTE: this throttle doesn't account for whether

			 * the caller is servicing an IO that will trigger a COW

			 * so excess throttling may result for chunks not required

			 * to be COW'd.  But if cow_threshold was reached, extra

			 * throttling is unlikely to negatively impact performance.

/*

 * Flush a list of buffers.

/*

 * Flush a list of buffers.

/*

 * Error a list of buffers.

 Read/write error - snapshot is unusable */

	/*

	 * Add a proper exception. After inserting the completed exception all

	 * subsequent snapshot reads to this chunk will be redirected to the

	 * COW device.  This ensures that we do not starve. Moreover, as long

	 * as the pending exception exists, neither origin writes nor snapshot

	 * merging can overwrite the chunk in origin.

 Wait for conflicting reads to drain */

 Remove the in-flight exception from the list */

 Submit any pending write bios */

 Update the metadata if we are persistent */

/*

 * Called when the copy I/O has finished.  kcopyd actually runs

 * this code so don't block.

/*

 * Dispatches the copy operation to kcopyd.

 Hand over to kcopyd */

/*

 * Inserts a pending exception into the pending table.

 *

 * NOTE: a write lock must be held on the chunk's pending exception table slot

 * before calling this.

/*

 * Looks to see if this snapshot already has a pending exception

 * for this chunk, otherwise it allocates a new one and inserts

 * it into the pending table.

 *

 * NOTE: a write lock must be held on the chunk's pending exception table slot

 * before calling this.

 Full snapshots are not usable */

 To get here the table must be live so s->active is always set. */

 wait_for_in_progress() has slept */

			/*

			 * passdown discard to origin (without triggering

			 * snapshot exceptions via do_origin; doing so would

			 * defeat the goal of freeing space in origin that is

			 * implied by the "discard_passdown_origin" feature)

 discard to snapshot (target_bio_nr == 0) zeroes exceptions */

 If the block is already remapped - use that, else remap it */

 discard is not issued */

		/*

		 * If no exception exists, complete discard immediately

		 * otherwise it'll trigger copy-out.

	/*

	 * Write to snapshot - higher level takes care of RW/RO

	 * flags so we should only get this if we are

	 * writeable.

 this is protected by the exception table lock */

/*

 * A snapshot-merge target behaves like a combination of a snapshot

 * target and a snapshot-origin target.  It only generates new

 * exceptions in other snapshots and not in the one that is being

 * merged.

 *

 * For each chunk, if there is an existing exception, it is used to

 * redirect I/O to the cow device.  Otherwise I/O is sent to the origin,

 * which in turn might generate exceptions in other snapshots.

 * If merging is currently taking place on the chunk in question, the

 * I/O is deferred by adding it to s->bios_queued_during_merge.

 Once merging, discards no longer effect change */

 Full merging snapshots are redirected to the origin */

 If the block is already remapped - use that */

 Queue writes overlapping with chunks being merged */

 Now we have correct chunk size, reregister */

	/*

	 * Handover exceptions from existing snapshot.

	/*

	 * snapshot-merge acts as an origin, so set ti->max_io_len

		/*

		 * kdevname returns a static pointer so we need

		 * to make private copies if the output is to

		 * make sense.

 All discards are split on chunk_size boundary */

/*-----------------------------------------------------------------

 * Origin methods

/*

 * If no exceptions need creating, DM_MAPIO_REMAPPED is returned and any

 * supplied bio was ignored.  The caller may submit it immediately.

 * (No remapping actually occurs as the origin is always a direct linear

 * map.)

 *

 * If further exceptions are required, DM_MAPIO_SUBMITTED is returned

 * and any supplied bio is added to a list to be submitted once all

 * the necessary exceptions exist.

 Do all the snapshots on this origin */

		/*

		 * Don't make new exceptions in a merging snapshot

		 * because it has effectively been deleted

 Nothing to do if writing beyond end of snapshot */

		/*

		 * Remember, different snapshots can have

		 * different chunk sizes.

 Only deal with valid and active snapshots */

			/*

			 * Check exception table to see if block is already

			 * remapped in this snapshot and trigger an exception

			 * if not.

		/*

		 * If an origin bio was supplied, queue it to wait for the

		 * completion of this exception, and start this one last,

		 * at the end of the function.

	/*

	 * Submit the exception against which the bio is queued last,

	 * to give the other exceptions a head start.

/*

 * Called on a write from the origin driver.

/*

 * Trigger exceptions in all non-merging snapshots.

 *

 * The chunk size of the merging snapshot may be larger than the chunk

 * size of some other snapshot so we may need to reallocate multiple

 * chunks in other snapshots.

 *

 * We scan all the overlapping exceptions in the other snapshots.

 * Returns 1 if anything was reallocated and must be waited for,

 * otherwise returns 0.

 *

 * size must be a multiple of merging_snap's chunk_size.

	/*

	 * The origin's __minimum_chunk_size() got stored in max_io_len

	 * by snapshot_merge_resume().

/*

 * Origin: maps a linear range of a device, with hooks for snapshotting.

/*

 * Construct an origin mapping: <dev_path>

 * The context for an origin is merely a 'struct dm_dev *'

 * pointing to the real device.

 Only tell snapshots if this is a write */

/*

 * Set the target "max_io_len" field to the minimum of all the snapshots'

 * chunk sizes.

 Module hooks */

/*

 * Copyright (C) 2014 Facebook. All rights reserved.

 *

 * This file is released under the GPL.

/*

 * This target will sequentially log all writes to the target device onto the

 * log device.  This is helpful for replaying writes to check for fs consistency

 * at all times.  This target provides a mechanism to mark specific events to

 * check data at a later time.  So for example you would:

 *

 * write data

 * fsync

 * dmsetup message /dev/whatever mark mymark

 * unmount /mnt/test

 *

 * Then replay the log up to mymark and check the contents of the replay to

 * verify it matches what was written.

 *

 * We log writes only after they have been flushed, this makes the log describe

 * close to the order in which the data hits the actual disk, not its cache.  So

 * for example the following sequence (W means write, C means complete)

 *

 * Wa,Wb,Wc,Cc,Ca,FLUSH,FUAd,Cb,CFLUSH,CFUAd

 *

 * Would result in the log looking like this:

 *

 * c,a,b,flush,fuad,<other writes>,<next flush>

 *

 * This is meant to help expose problems where file systems do not properly wait

 * on data being written before invoking a FLUSH.  FUA bypasses cache so once it

 * completes it is added to the log as it should be on disk.

 *

 * We treat DISCARDs as if they don't bypass cache so that they are logged in

 * order of completion along with the normal writes.  If we didn't do it this

 * way we would process all the discards first and then write all the data, when

 * in fact we want to do the data and the discard in the order that they

 * completed.

/*

 * The disk format for this is braindead simple.

 *

 * At byte 0 we have our super, followed by the following sequence for

 * nr_entries:

 *

 * [   1 sector    ][  entry->nr_sectors ]

 * [log_write_entry][    data written    ]

 *

 * The log_write_entry takes up a full sector so we can have arbitrary length

 * marks and it leaves us room for extra content in the future.

/*

 * Basic info about the log for userspace.

/*

 * sector - the sector we wrote.

 * nr_sectors - the number of sectors we wrote.

 * flags - flags for this log entry.

 * data_len - the size of the data in this log entry, this is for private log

 * entry stuff, the MARK data provided by userspace for example.

/*

 * Meant to be called if there is an error, it will free all the pages

 * associated with the block.

 we don't support both inline data & bio data */

		/*

		 * The page offset is always 0 because we allocate a new page

		 * for every bvec in the original bio for simplicity sake.

	/*

	 * Super sector should be writen in-order, otherwise the

	 * nr_entries could be rewritten incorrectly by an old bio.

			/*

			 * Apparently the size of the device may not be known

			 * right away, so handle this properly.

/*

 * Construct a log-writes mapping:

 * log-writes <dev_path> <log_dev_path>

	/*

	 * next_sector is in 512b sectors to correspond to what bi_sector expects.

	 * The super starts at sector 0, and the next_sector is the next logical

	 * one based on the sectorsize of the device.

	/*

	 * This is just nice to have since it'll update the super to include the

	 * unflushed blocks, if it fails we don't really care.

 Don't bother doing anything if logging has been disabled */

	/*

	 * Map reads as normal.

 No sectors and not a flush?  Don't care */

	/*

	 * Discards will have bi_size set but there's no actual data, so just

	 * allocate the size of the pending block.

 We don't need the data, just submit */

 Flush bio, splice the unflushed blocks onto this list and submit */

	/*

	 * We will write this bio somewhere else way later so we need to copy

	 * the actual contents into new pages so we know the data will always be

	 * there.

	 *

	 * We do this because this could be a bio from O_DIRECT in which case we

	 * can't just hold onto the page until some later point, we have to

	 * manually copy the contents.

 Had a flush with data in it, weird */

/*

 * INFO format: <logged entries> <highest allocated sector>

	/*

	 * Only pass ioctls through if the device sizes match exactly.

/*

 * Messages supported:

 *   mark <mark data> - specify the marked data.

 write data provided via the iterator */

 rewind the iterator so that the block driver can use it */

 Don't bother doing anything if logging has been disabled */

/*

 * Copyright (C) 2011-2012 Red Hat, Inc.

 *

 * This file is released under the GPL.

/*--------------------------------------------------------------------------

 * As far as the metadata goes, there is:

 *

 * - A superblock in block zero, taking up fewer than 512 bytes for

 *   atomic writes.

 *

 * - A space map managing the metadata blocks.

 *

 * - A space map managing the data blocks.

 *

 * - A btree mapping our internal thin dev ids onto struct disk_device_details.

 *

 * - A hierarchical btree, with 2 levels which effectively maps (thin

 *   dev id, virtual block) -> block_time.  Block time is a 64-bit

 *   field holding the time in the low 24 bits, and block in the top 40

 *   bits.

 *

 * BTrees consist solely of btree_nodes, that fill a block.  Some are

 * internal nodes, as such their values are a __le64 pointing to other

 * nodes.  Leaf nodes can store data of any reasonable size (ie. much

 * smaller than the block size).  The nodes consist of the header,

 * followed by an array of keys, followed by an array of values.  We have

 * to binary search on the keys so they're all held together to help the

 * cpu cache.

 *

 * Space maps have 2 btrees:

 *

 * - One maps a uint64_t onto a struct index_entry.  Which points to a

 *   bitmap block, and has some details about how many free entries there

 *   are etc.

 *

 * - The bitmap blocks have a header (for the checksum).  Then the rest

 *   of the block is pairs of bits.  With the meaning being:

 *

 *   0 - ref count is 0

 *   1 - ref count is 1

 *   2 - ref count is 2

 *   3 - ref count is higher than 2

 *

 * - If the count is higher than 2 then the ref count is entered in a

 *   second btree that directly maps the block_address to a uint32_t ref

 *   count.

 *

 * The space map metadata variant doesn't have a bitmaps btree.  Instead

 * it has one single blocks worth of index_entries.  This avoids

 * recursive issues with the bitmap btree needing to allocate space in

 * order to insert.  With a small data block size such as 64k the

 * metadata support data devices that are hundreds of terrabytes.

 *

 * The space maps allocate space linearly from front to back.  Space that

 * is freed in a transaction is never recycled within that transaction.

 * To try and avoid fragmenting _free_ space the allocator always goes

 * back and fills in gaps.

 *

 * All metadata io is in THIN_METADATA_BLOCK_SIZE sized/aligned chunks

 * from the block manager.

/*

 * For btree insert:

 *  3 for btree insert +

 *  2 for btree lookup used within space map

 * For btree remove:

 *  2 for shadow spine +

 *  4 for rebalance 3 child node

 This should be plenty */

/*

 * Little endian on-disk superblock and device details.

 Checksum of superblock except for this field. */

 This block number, dm_block_t. */

	/*

	 * Root held by userspace transactions.

	/*

	 * 2-level btree mapping (dev_id, (dev block, time)) -> data block

	/*

	 * Device detail root mapping dev_id -> device_details

 In 512-byte sectors. */

 In 512-byte sectors. */

 When created. */

	/*

	 * Two-level btree.

	 * First level holds thin_dev_t.

	 * Second level holds mappings.

	/*

	 * Non-blocking version of the above.

	/*

	 * Just the top level for deleting whole devices.

	/*

	 * Just the bottom level for creating new devices.

	/*

	 * Describes the device details btree.

	/*

	 * Pre-commit callback.

	 *

	 * This allows the thin provisioning target to run a callback before

	 * the metadata are committed.

	/*

	 * We reserve a section of the metadata for commit overhead.

	 * All reported space does *not* include this.

	/*

	 * Set if a transaction has to be aborted but the attempt to roll back

	 * to the previous (good) transaction failed.  The only pool metadata

	 * operation possible in this state is the closing of the device.

	/*

	 * Set once a thin-pool has been accessed through one of the interfaces

	 * that imply the pool is in-service (e.g. thin devices created/deleted,

	 * thin-pool message, metadata snapshots, etc).

	/*

	 * Reading the space map roots can fail, so we read it into these

	 * buffers before the superblock is locked and updated.

/*----------------------------------------------------------------

 * superblock validator

/*----------------------------------------------------------------

 * Methods for the btree value types

/*

 * It's more efficient to call dm_sm_{inc,dec}_blocks as few times as

 * possible.  'with_runs' reads contiguous runs of blocks, and calls the

 * given sm function.

 We know value_le is 8 byte aligned */

----------------------------------------------------------------*/

/*

 * Variant that is used for in-core only changes or code that

 * shouldn't put the pool in service on its own (e.g. commit).

----------------------------------------------------------------*/

	/*

	 * We can't use a validator here - it may be all zeroes.

	/*

	 * Check for read-only metadata to skip the following RDWR checks.

 Verify the data block size hasn't changed */

	/*

	 * We re-read the superblock every time.  Shouldn't need to do this

	 * really.

	/*

	 * We need to know if the thin_disk_superblock exceeds a 512-byte sector.

 16M */

/*

 * __open_device: Returns @td corresponding to device with id @dev,

 * creating it if @create is set and incrementing @td->open_count.

 * On failure, @td is undefined.

	/*

	 * If the device is already open, return it.

			/*

			 * May not create an already-open device.

	/*

	 * Check the device exists.

		/*

		 * Create new device.

	/*

	 * Create an empty btree for the mappings.

	/*

	 * Insert it into the main mapping tree.

 check this device is unused */

 find the mapping tree for the origin */

 clone the origin, an inc will do */

 insert into the main mapping tree */

 TODO: failure should mark the transaction invalid */

	/*

	 * We commit to ensure the btree roots which we increment in a

	 * moment are up to date.

	/*

	 * Copy the superblock.

	/*

	 * Wipe the spacemap since we're not publishing this.

	/*

	 * Increment the data structures that need to be preserved.

	/*

	 * Write the held root into the superblock.

/*

 * Check whether @time (of block creation) is older than @td's last snapshot.

 * If so then the associated block is shared with the last snapshot device.

 * Any block on a device created *after* the device last got snapshotted is

 * necessarily not shared.

	/*

	 * Find the mapping tree

	/*

	 * Remove from the mapping tree, taking care to inc the

	 * ref count so it doesn't get deleted.

	/*

	 * Remove leaves stops at the first unmapped entry, so we have to

	 * loop round finding mapped ranges.

	/*

	 * Reinsert the mapping tree.

	/*

	 * Care is taken to not have commit be what

	 * triggers putting the thin-pool in-service.

	/*

	 * Open the next transaction.

/*

 * Copyright (C) 2020 Red Hat GmbH

 *

 * This file is released under the GPL.

 *

 * Device-mapper target to emulate smaller logical block

 * size on backing devices exposing (natively) larger ones.

 *

 * E.g. 512 byte sector emulation on 4K native disks.

 Emulated block size context. */

 Underlying device to emulate block size on. */

 Use dm-bufio for read and read-modify-write processing. */

 Workqueue for ^ processing of bios. */

 Work item used for ^. */

 Worker bios input list. */

 Guard bios input list above. */

 <start> table line argument, see ebs_ctr below. */

 Emulated block size in sectors exposed to upper layer. */

 Underlying block size in sectors retrieved from/set on lower layer device. */

 bitshift sectors -> blocks used in dm-bufio API. */

 Flag to indicate underlying block size is set on table line. */

 Return number of blocks for a bio, accounting for misalignment of start and end sectors. */

/*

 * READ/WRITE:

 *

 * copy blocks between bufio blocks and bio vector's (partial/overlapping) pages.

 Handle overlapping page <-> blocks */

 Avoid reading for writes in case bio vector's page overwrites block completely. */

			/*

			 * Carry on with next buffer, if any, to issue all possible

			 * data but return error.

 Copy data to/from bio to buffer if read/new was successful above. */

 READ/WRITE: iterate bio vector's copying between (partial) pages and bufio blocks. */

/*

 * Discard bio's blocks, i.e. pass discards down.

 *

 * Avoid discarding partial blocks at beginning and end;

 * return 0 in case no blocks can be discarded as a result.

	/*

	 * Partial first underlying block (__nr_blocks() may have

	 * resulted in one block).

 Partial last underlying block if any. */

 Release blocks them from the bufio cache. */

 Worker function to process incoming bios. */

 Prefetch all read and any mis-aligned write buffers */

	/*

	 * We write dirty buffers after processing I/O on them

	 * but before we endio thus addressing REQ_FUA/REQ_SYNC.

 Any other request is endioed. */

/*

 * Construct an emulated block size mapping: <dev_path> <offset> <ebs> [<ubs>]

 *

 * <dev_path>: path of the underlying device

 * <offset>: offset in 512 bytes sectors into <dev_path>

 * <ebs>: emulated block size in units of 512 bytes exposed to the upper layer

 * [<ubs>]: underlying block size in units of 512 bytes imposed on the lower layer;

 * 	    optional, if not supplied, retrieve logical block size from underlying device

	/*

	 * Only queue for bufio processing in case of partial or overlapping buffers

	 * -or-

	 * emulation with ebs == ubs aiming for tests of dm-bufio overhead.

 Forget any buffer content relative to this direct backing device I/O. */

	/*

	 * Only pass ioctls through if the device sizes match exactly.

 SPDX-License-Identifier: GPL-2.0

/*

 * The kobject release method must not be placed in the module itself,

 * otherwise we are subject to module unload races.

 *

 * The release method is called when the last reference to the kobject is

 * dropped. It may be called by any other kernel code that drops the last

 * reference.

 *

 * The release method suffers from module unload race. We may prevent the

 * module from being unloaded at the start of the release method (using

 * increased module reference count or synchronizing against the release

 * method), however there is no way to prevent the module from being

 * unloaded at the end of the release method.

 *

 * If this code were placed in the dm module, the following race may

 * happen:

 *  1. Some other process takes a reference to dm kobject

 *  2. The user issues ioctl function to unload the dm device

 *  3. dm_sysfs_exit calls kobject_put, however the object is not released

 *     because of the other reference taken at step 1

 *  4. dm_sysfs_exit waits on the completion

 *  5. The other process that took the reference in step 1 drops it,

 *     dm_kobject_release is called from this process

 *  6. dm_kobject_release calls complete()

 *  7. a reschedule happens before dm_kobject_release returns

 *  8. dm_sysfs_exit continues, the dm device is unloaded, module reference

 *     count is decremented

 *  9. The user unloads the dm module

 * 10. The other process that was rescheduled in step 7 continues to run,

 *     it is now executing code in unloaded module, so it crashes

 *

 * Note that if the process that takes the foreign reference to dm kobject

 * has a low priority and the system is sufficiently loaded with

 * higher-priority processes that prevent the low-priority process from

 * being scheduled long enough, this bug may really happen.

 *

 * In order to fix this module unload race, we place the release method

 * into a helper code that is compiled directly into the kernel.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Google, Inc.

 *

 * Author: Sami Tolvanen <samitolvanen@google.com>

/*

 * If error correction has been configured, returns true.

/*

 * Return a pointer to dm_verity_fec_io after dm_verity_io and its variable

 * length fields.

/*

 * Return an interleaved offset for a byte in RS block.

/*

 * Decode an RS block using Reed-Solomon.

/*

 * Read error-correcting codes for the requested RS block. Returns a pointer

 * to the data block. Caller is responsible for releasing buf.

 Loop over each preallocated buffer slot. */

 Loop over each extra buffer slot. */

 Loop over each allocated buffer. */

 Loop over each RS block in each allocated buffer. */

/*

 * Return a pointer to the current RS block when called inside

 * fec_for_each_buffer_rs_block.

/*

 * Return an index to the current RS block when called inside

 * fec_for_each_buffer_rs_block.

/*

 * Decode all RS blocks from buffers and copy corrected bytes into fio->output

 * starting from block_offset.

	/*

	 * Decode the RS blocks we have in bufs. Each RS block results in

	 * one corrected target byte and consumes fec->roots parity bytes.

 read the next block when we run out of parity bytes */

/*

 * Locate data block erasures using verity hashes.

/*

 * Read data blocks that are part of the RS block and deinterleave as much as

 * fits into buffers. Check for erasure locations if @neras is non-NULL.

	/*

	 * read each of the rsn data blocks that are part of the RS block, and

	 * interleave contents to available bufs

		/*

		 * target is the data block we want to correct, target_index is

		 * the index of this block within the rsn RS blocks

			/*

			 * blocks outside the area were assumed to contain

			 * zeros when encoding data was generated

 assume the block is corrupted */

 locate erasures if the block is on the data device */

 skip known zero blocks entirely */

			/*

			 * skip if we have already found the theoretical

			 * maximum number (i.e. fec->roots) of erasures

		/*

		 * deinterleave and copy the bytes that fit into bufs,

		 * starting from block_offset

/*

 * Allocate RS control structure and FEC buffers from preallocated mempools,

 * and attempt to allocate as many extra buffers as available.

 try to allocate the maximum number of buffers */

 we can manage with even one buffer if necessary */

/*

 * Initialize buffers and clear erasures. fec_read_bufs() assumes buffers are

 * zeroed before deinterleaving.

/*

 * Decode all RS blocks in a single data block and return the target block

 * (indicated by @offset) in fio->output. If @use_erasures is non-zero, uses

 * hashes to locate erasures.

 Always re-validate the corrected block against the expected hash */

/*

 * Correct errors in a block. Copies corrected block to dest if non-NULL,

 * otherwise to a bio_vec starting from iter.

	/*

	 * For RS(M, N), the continuous FEC data is divided into blocks of N

	 * bytes. Since block size may not be divisible by N, the last block

	 * is zero padded when decoding.

	 *

	 * Each byte of the block is covered by a different RS(M, N) code,

	 * and each code is interleaved over N blocks to make it less likely

	 * that bursty corruption will leave us in unrecoverable state.

	/*

	 * The base RS block we can feed to the interleaver to find out all

	 * blocks required for decoding.

	/*

	 * Locating erasures is slow, so attempt to recover the block without

	 * them first. Do a second attempt with erasures if the corruption is

	 * bad enough.

/*

 * Clean up per-bio data.

/*

 * Initialize per-bio data.

/*

 * Append feature arguments and values to the status table.

/*

 * Allocate dm_verity_fec for v->fec. Must be called before verity_fec_ctr.

/*

 * Validate arguments and preallocate memory. Must be called after arguments

 * have been parsed using verity_fec_parse_opt_args.

	/*

	 * FEC is computed over data blocks, possible metadata, and

	 * hash blocks. In other words, FEC covers total of fec_blocks

	 * blocks consisting of the following:

	 *

	 *  data blocks | hash blocks | metadata (optional)

	 *

	 * We allow metadata after hash blocks to support a use case

	 * where all data is stored on the same device and FEC covers

	 * the entire area.

	 *

	 * If metadata is included, we require it to be available on the

	 * hash device after the hash blocks.

	/*

	 * Require matching block sizes for data and hash devices for

	 * simplicity.

	/*

	 * Due to optional metadata, f->blocks can be larger than

	 * data_blocks and hash_blocks combined.

	/*

	 * Metadata is accessed through the hash device, so we require

	 * it to be large enough.

 Preallocate an rs_control structure for each worker thread */

 Preallocate DM_VERITY_FEC_BUF_PREALLOC buffers for each thread */

 Preallocate an output buffer for each thread */

 Reserve space for our per-bio data */

/*

 * Copyright (C) 2003 Sistina Software Limited.

 * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

/*-----------------------------------------------------------------

 * Region hash

 *

 * The mirror splits itself up into discrete regions.  Each

 * region can be in one of three states: clean, dirty,

 * nosync.  There is no need to put clean regions in the hash.

 *

 * In addition to being present in the hash table a region _may_

 * be present on one of three lists.

 *

 *   clean_regions: Regions on this list have no io pending to

 *   them, they are in sync, we are no longer interested in them,

 *   they are dull.  dm_rh_update_states() will remove them from the

 *   hash table.

 *

 *   quiesced_regions: These regions have been spun down, ready

 *   for recovery.  rh_recovery_start() will remove regions from

 *   this list and hand them to kmirrord, which will schedule the

 *   recovery io with kcopyd.

 *

 *   recovered_regions: Regions that kcopyd has successfully

 *   recovered.  dm_rh_update_states() will now schedule any delayed

 *   io, up the recovery_count, and remove the region from the

 *   hash.

 *

 * There are 2 locks:

 *   A rw spin lock 'hash_lock' protects just the hash table,

 *   this is never held in write mode from interrupt context,

 *   which I believe means that we only have to disable irqs when

 *   doing a write lock.

 *

 *   An ordinary spin lock 'region_lock' that protects the three

 *   lists in the region_hash, with the 'state', 'list' and

 *   'delayed_bios' fields of the regions.  This is used from irq

 *   context, so all other uses will have to suspend local irqs.

 holds persistent region state */

 hash table */

	/*

	 * If there was a flush failure no regions can be marked clean.

 Max # of regions to recover in parallel */

 Callback function to schedule bios writes */

 Callback function to wakeup callers worker thread. */

 Callback function to wakeup callers recovery waiters. */

 FIXME: can we get rid of this ? */

/*

 * Conversion fns

/*

 * FIXME: shall we pass in a structure instead of all these args to

 * dm_region_hash_create()????

	/*

	 * Calculate a suitable number of buckets for our hash

	 * table.

 We lost the race. */

	/*

	 * The region wasn't in the hash, so we fall back to the

	 * dirty log.

	/*

	 * Any error from the dirty log (eg. -EWOULDBLOCK) gets

	 * taken as a DM_RH_NOSYNC

	/*

	 * Dispatch the bios before we call 'wake_up_all'.

	 * This is important because if we are suspending,

	 * we want to know that recovery is complete and

	 * the work queue is flushed.  If we wake_up_all

	 * before we dispatch_bios (queue bios and call wake()),

	 * then we risk suspending before the work queue

	 * has been properly flushed.

/* dm_rh_mark_nosync

 * @ms

 * @bio

 *

 * The bio was written on some mirror(s) but failed on other mirror(s).

 * We can successfully endio the bio but should avoid the region being

 * marked clean by setting the state DM_RH_NOSYNC.

 *

 * This function is _not_ safe in interrupt context!

 We must inform the log that the sync count has changed. */

 region hash entry should exist because write was in-flight */

	/*

	 * Possible cases:

	 *   1) DM_RH_DIRTY

	 *   2) DM_RH_NOSYNC: was dirty, other preceding writes failed

	 *   3) DM_RH_RECOVERING: flushing pending writes

	 * Either case, the region should have not been connected to list.

	/*

	 * Quickly grab the lists.

	/*

	 * All the regions on the recovered and clean lists have

	 * now been pulled out of the system, so no need to do

	 * any more locking.

 take off the clean list */

		/*

		 * There is no pending I/O for this region.

		 * We can move the region to corresponding list for next action.

		 * At this point, the region is not yet connected to any list.

		 *

		 * If the state is DM_RH_NOSYNC, the region should be kept off

		 * from clean list.

		 * The hash entry for DM_RH_NOSYNC will remain in memory

		 * until the region is recovered or the map is reloaded.

 do nothing for DM_RH_NOSYNC */

			/*

			 * If a write flush failed some time ago, we

			 * don't know whether or not this write made it

			 * to the disk, so we must resync the device.

/*

 * Starts quiescing a region in preparation for recovery.

	/*

	 * Ask the dirty log what's next.

	/*

	 * Get this region, and start it quiescing by setting the

	 * recovering flag.

 Already quiesced ? */

 Extra reference to avoid race with dm_rh_stop_recovery */

 Drop the extra reference */

/*

 * Returns any quiesced regions.

 remove from the quiesced list */

 Return recovery in flight count. */

 wait for any recovering regions */

/*

 * Copyright (C) 2003 Jana Saout <jana@saout.de>

 *

 * This file is released under the GPL.

/*

 * Construct a dummy mapping that only returns zeros

	/*

	 * Silently drop discards, avoiding -EOPNOTSUPP.

/*

 * Return zeros only on reads

 readahead of null bytes only wastes buffer cache */

 writes get silently dropped */

 accepted bio, don't make new request */

/*

 * Copyright (C) 2001-2002 Sistina Software (UK) Limited.

 * Copyright (C) 2006-2008 Red Hat GmbH

 *

 * This file is released under the GPL.

/*

 * get_type

 * @type_name

 *

 * Attempt to retrieve the dm_exception_store_type by name.  If not already

 * available, attempt to load the appropriate module.

 *

 * Exstore modules are named "dm-exstore-" followed by the 'type_name'.

 * Modules may contain multiple types.

 * This function will first try the module "dm-exstore-<type_name>",

 * then truncate 'type_name' on the last '-' and try again.

 *

 * For example, if type_name was "clustered-shared", it would search

 * 'dm-exstore-clustered-shared' then 'dm-exstore-clustered'.

 *

 * 'dm-exception-store-<type_name>' is too long of a name in my

 * opinion, which is why I've chosen to have the files

 * containing exception store implementations be 'dm-exstore-<type_name>'.

 * If you want your module to be autoloaded, you will follow this

 * naming convention.

 *

 * Returns: dm_exception_store_type* on success, NULL on failure

 Check chunk_size is a power of 2 */

 Validate the chunk size against the device block size */

/*

 * Copyright (C) 2001, 2002 Sistina Software (UK) Limited.

 * Copyright (C) 2004-2008 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

/*

 * Cookies are numeric values sent with CHANGE and REMOVE

 * uevents while resuming, removing or renaming the device.

/*

 * One of these is allocated (on-stack) per original bio.

/*

 * For mempools pre-allocation at the table loading time.

/*

 * Bio-based DM's mempools' reserved IOs set by the user.

	/*

	 * Should be empty by this point.

/*

 * Block device functions

/*

 * Guarantees nothing is using the device before it's deleted.

 We only support devices that have a single target */

		/*

		 * Target determined this ioctl is being issued against a

		 * subset of the parent bdev; require extra privileges.

 nudge anyone waiting on suspend queue */

 the dm_target_io embedded in ci->io is available */

/*

 * Add the bio to the list of deferred io.

/*

 * Everyone (including functions in this file), should use this

 * function to access the md->map field, and make sure they call

 * dm_put_live_table() when finished.

/*

 * A fast alternative to dm_get_live_table/dm_put_live_table.

 * The caller must not block between these two functions.

/*

 * Open a table device so we can use it as a map destination.

/*

 * Close a table device that we've been using.

/*

 * Get the geometry associated with a dm device

/*

 * Set the geometry of a device.

/*

 * Decrements the number of outstanding ios that a bio has been

 * cloned into, completing the original io if necc.

 Push-back supersedes any I/O errors */

			/*

			 * Target requested pushing back the I/O.

 NOTE early return due to BLK_STS_DM_REQUEUE below */

				/*

				 * noflush suspend was interrupted or this is

				 * a write to a zoned target.

			/*

			 * Preflush done for flush with data, reissue

			 * without REQ_PREFLUSH.

 done with normal IO or empty flush */

 device doesn't really support DISCARD, disable it */

 device doesn't really support WRITE SAME, disable it */

 device doesn't really support WRITE ZEROES, disable it */

			/*

			 * Requeuing writes to a sequential zone of a zoned

			 * target will break the sequential write pattern:

			 * fail such IO.

 The target will handle the io */

/*

 * Return maximum size of I/O possible at the supplied sector up to the current

 * target boundary.

	/*

	 * Does the target need to split IO even further?

	 * - varied (per target) IO splitting is a tenet of DM; this

	 *   explains why stacked chunk_sectors based splitting via

	 *   blk_max_size_offset() isn't possible here. So pass in

	 *   ti->max_io_len to override stacked chunk_sectors.

		/*

		 * ->zero_page_range() is mandatory dax operation. If we are

		 *  here, something is wrong.

/*

 * A target may call dm_accept_partial_bio only from the map routine.  It is

 * allowed for all bio types except REQ_PREFLUSH, REQ_OP_ZONE_* zone management

 * operations and REQ_OP_ZONE_APPEND (zone append writes).

 *

 * dm_accept_partial_bio informs the dm that the target only wants to process

 * additional n_sectors sectors of the bio and the rest of the data should be

 * sent in a next bio.

 *

 * A diagram that explains the arithmetics:

 * +--------------------+---------------+-------+

 * |         1          |       2       |   3   |

 * +--------------------+---------------+-------+

 *

 * <-------------- *tio->len_ptr --------------->

 *                      <------- bi_size ------->

 *                      <-- n_sectors -->

 *

 * Region 1 was already iterated over with bio_advance or similar function.

 *	(it may be empty if the target doesn't use bio_advance)

 * Region 2 is the remaining bio size that the target wants to process.

 *	(it may be empty if region 1 is non-empty, although there is no reason

 *	 to make it empty)

 * The target requires that region 3 is to be sent in the next bio.

 *

 * If the target wants to receive multiple copies of the bio (via num_*bios, etc),

 * the partially processed part (the sum of regions 1+2) must be the same for all

 * copies of the bio.

	/*

	 * Map the clone.  If r == 0 we don't need to do

	 * anything, the target has assumed ownership of

	 * this io.

	/*

	 * Check if the IO needs a special mapping due to zone append emulation

	 * on zoned target. In this case, dm_zone_map_bio() calls the target

	 * map operation.

 the bio has been remapped so dispatch it */

/*

 * Creates a bio that consists of range of complete bvecs.

	/*

	 * Use an on-stack bio for this, it's safe since we don't

	 * need to reference it after submit. It's just used as

	 * the basis for the clone(s).

	/*

	 * Even though the device advertised support for this type of

	 * request, that does not mean every target supports it, and

	 * reconfiguration might also have changed that since the

	 * check was performed.

/*

 * Select the correct strategy for processing a non-flush bio.

/*

 * Entry point to split a bio into clones and submit them to the targets.

 dm_io_dec_pending submits any data associated with flush */

			/*

			 * Remainder must be passed to submit_bio_noacct()

			 * so that it gets handled *after* bios already submitted

			 * have been completely processed.

			 * We take a clone of the original to store in

			 * ci.io->orig_bio to be used by end_io_acct() and

			 * for dec_pending to use for completion handling.

			/*

			 * Adjust IO stats for each split, otherwise upon queue

			 * reentry there will be redundant IO accounting.

			 * NOTE: this is a stop-gap fix, a proper fix involves

			 * significant refactoring of DM core's bio splitting

			 * (by eliminating DM's splitting and just using bio_split)

 drop the extra reference count */

 If suspended, queue this IO for later */

	/*

	 * Use blk_queue_split() for abnormal IO (e.g. discard, writesame, etc)

	 * otherwise associated queue_limits won't be imposed.

/*-----------------------------------------------------------------

 * An IDR is used to keep track of allocated minor numbers.

/*

 * See if the device with a specific minor # is free.

 CONFIG_BLK_INLINE_ENCRYPTION */

 !CONFIG_BLK_INLINE_ENCRYPTION */

/*

 * Allocate and initialise a blank device with a given minor.

 get a minor number for the dev */

	/*

	 * default to bio-based until DM table is loaded and md->type

	 * established. If request-based table is loaded: blk-mq will

	 * override accordingly.

 Populate the mapping, nobody knows we exist yet */

		/*

		 * The md may already have mempools that need changing.

		 * If so, reload bioset because front_pad may have changed

		 * because a different table was loaded.

		/*

		 * There's no need to reload with request-based dm

		 * because the size of front_pad doesn't change.

		 * Note for future: If you are to reload bioset,

		 * prep-ed requests in the queue may refer

		 * to bio from the old bioset, so you must walk

		 * through the queue to unprep.

 mempool bind completed, no longer need any mempools in the table */

/*

 * Bind a table to the device.

/*

 * Returns old map, which caller must destroy.

	/*

	 * Wipe any geometry if the size of the table changed.

		/*

		 * Leverage the fact that request-based DM targets are

		 * immutable singletons - used to optimize dm_mq_queue_rq.

/*

 * Returns unbound table for the caller to free.

/*

 * Constructor for a new device.

/*

 * Functions to manage md->type.

 * All are required to hold md->type_lock.

/*

 * The queue_limits are only valid as long as you have a reference

 * count on 'md'.

/*

 * Setup the DM device's queue based on md's type

	/*

	 * Take suspend_lock so that presuspend and postsuspend methods

	 * do not race with internal suspend.

 dm_put_live_table must be before msleep, otherwise deadlock is possible */

	/*

	 * Rare, but there may be I/O requests still going to complete,

	 * for example.  Wait for all references to disappear.

	 * No one should increment the reference count of the mapped_device,

	 * after the mapped_device state becomes DMF_FREEING.

/*

 * Process the deferred bios

/*

 * Swap in a new table, returning the old one for the caller to destroy.

 device must be suspended */

	/*

	 * If the new table has no data devices, retain the existing limits.

	 * This helps multipath with queue_if_no_path if all paths disappear,

	 * then new I/O is queued based on these limits, and then some paths

	 * reappear.

/*

 * Functions to lock and unlock any filesystem running on the

 * device.

/*

 * @suspend_flags: DM_SUSPEND_LOCKFS_FLAG and/or DM_SUSPEND_NOFLUSH_FLAG

 * @task_state: e.g. TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE

 * @dmf_suspended_flag: DMF_SUSPENDED or DMF_SUSPENDED_INTERNALLY

 *

 * If __dm_suspend returns 0, the device is completely quiescent

 * now. There is no request-processing activity. All new requests

 * are being added to md->deferred list.

	/*

	 * DMF_NOFLUSH_SUSPENDING must be set before presuspend.

	 * This flag is cleared before dm_suspend returns.

	/*

	 * This gets reverted if there's an error later and the targets

	 * provide the .presuspend_undo hook.

	/*

	 * Flush I/O to the device.

	 * Any I/O submitted after lock_fs() may not be flushed.

	 * noflush takes precedence over do_lockfs.

	 * (lock_fs() flushes I/Os and waits for them to complete.)

	/*

	 * Here we must make sure that no processes are submitting requests

	 * to target drivers i.e. no one may be executing

	 * __split_and_process_bio from dm_submit_bio.

	 *

	 * To get all processes out of __split_and_process_bio in dm_submit_bio,

	 * we take the write lock. To prevent any process from reentering

	 * __split_and_process_bio from dm_submit_bio and quiesce the thread

	 * (dm_wq_work), we set DMF_BLOCK_IO_FOR_SUSPEND and call

	 * flush_workqueue(md->wq).

	/*

	 * Stop md->queue before flushing md->wq in case request-based

	 * dm defers requests to md->wq from md->queue.

	/*

	 * At this point no more requests are entering target request routines.

	 * We call dm_wait_for_completion to wait for all existing requests

	 * to finish.

 were we interrupted ? */

 pushback list is already flushed, so skip flush */

/*

 * We need to be able to change a mapping table under a mounted

 * filesystem.  For example we might want to move some data in

 * the background.  Before the table can be swapped with

 * dm_bind_table, dm_suspend must be called to flush any in

 * flight bios and ensure that any further io gets deferred.

/*

 * Suspend mechanism in request-based dm.

 *

 * 1. Flush all I/Os by lock_fs() if needed.

 * 2. Stop dispatching any I/O by stopping the request_queue.

 * 3. Wait for all in-flight I/Os to be completed or requeued.

 *

 * To abort suspend, start the request_queue.

 already internally suspended, wait for internal resume */

	/*

	 * Flushing deferred I/Os must be done after targets are resumed

	 * so that mapping of targets can work correctly.

	 * Request-based dm is queueing the deferred I/Os in its request_queue.

 already internally suspended, wait for internal resume */

/*

 * Internal suspend/resume works like userspace-driven suspend. It waits

 * until all bios finish and prevents issuing new bios to the target drivers.

 * It may be used only from the kernel.

 nested internal suspend */

 nest suspend */

	/*

	 * Using TASK_UNINTERRUPTIBLE because only NOFLUSH internal suspend is

	 * supported.  Properly supporting a TASK_INTERRUPTIBLE internal suspend

	 * would require changing .presuspend to return an error -- avoid this

	 * until there is a need for more elaborate variants of internal suspend.

 resume from nested internal suspend */

 resume from nested suspend */

	/*

	 * NOTE: existing callers don't need to call dm_table_resume_targets

	 * (which may fail -- so best to avoid it for now by passing NULL map)

/*

 * Fast variants of internal suspend/resume hold md->suspend_lock,

 * which prevents interaction with userspace-driven suspend.

/*-----------------------------------------------------------------

 * Event notification.

/*

 * The gendisk is only valid as long as you have a reference

 * count on 'md'.

 per_io_data_size is used for blk-mq pdu at queue allocation */

 We only support devices that have a single target */

/*

 * For register / unregister we need to manually call out to every path.

 unregister all paths if we failed to register any path */

/*

 * module hooks

 SPDX-License-Identifier: GPL-2.0-or-later

/*

   md.c : Multiple Devices driver for Linux

     Copyright (C) 1998, 1999, 2000 Ingo Molnar



     completely rewritten, based on the MD driver code from Marc Zyngier



   Changes:



   - RAID-1/RAID-5 extensions by Miguel de Icaza, Gadi Oxman, Ingo Molnar

   - RAID-6 extensions by H. Peter Anvin <hpa@zytor.com>

   - boot support for linear and striped mode by Harald Hoyer <HarryH@Royal.Net>

   - kerneld support by Boris Tobotras <boris@xtalk.msk.su>

   - kmod support by: Cyrus Durgin

   - RAID0 bugfixes: Mark Anthony Lisher <markal@iname.com>

   - Devfs support by Richard Gooch <rgooch@atnf.csiro.au>



   - lots of fixes and improvements to the RAID1/RAID5 and generic

     RAID code (such as request based resynchronization):



     Neil Brown <neilb@cse.unsw.edu.au>.



   - persistent bitmap code

     Copyright (C) 2003-2004, Paul Clements, SteelEye Technology, Inc.





   Errors, Warnings, etc.

   Please use:

     pr_crit() for error conditions that risk data loss

     pr_err() for error conditions that are unexpected, like an IO error

         or internal inconsistency

     pr_warn() for error conditions that could have been predicated, like

         adding a device to an array when it has incompatible metadata

     pr_info() for every interesting, very rare events, like an array starting

         or stopping, or resync starting or stopping

     pr_debug() for everything else.



/* pers_list is a list of registered personalities protected

 * by pers_lock.

 * pers_lock does extra service to protect accesses to

 * mddev->thread when the mutex cannot be held.

/*

 * Default number of read corrections we'll attempt on an rdev

 * before ejecting it from the array. We divide the read error

 * count by 2 for every hour elapsed between read errors.

 Default safemode delay: 200 msec */

/*

 * Current RAID-1,4,5 parallel reconstruction 'guaranteed speed limit'

 * is 1000 KB/sec, so the extra system load does not show up that much.

 * Increase it if you want to have more _guaranteed_ speed. Note that

 * the RAID driver will use the maximum available bandwidth if the IO

 * subsystem is idle. There is also an 'absolute maximum' reconstruction

 * speed limit - in case reconstruction slows down your system despite

 * idle IO detection.

 *

 * you can change it via /proc/sys/dev/raid/speed_limit_min and _max.

 * or /sys/block/mdX/md/sync_speed_{min,max}

 serial_nums equals with BARRIER_BUCKETS_NR */

 Free all resources if pool is not existed */

/*

 * rdev needs to enable serial stuffs if it meets the conditions:

 * 1. it is multi-queue device flaged with writemostly.

 * 2. the write-behind mode is enabled.

/*

 * Init resource for rdev(s), then create serial_info_pool if:

 * 1. rdev is the first device which return true from rdev_enable_serial.

 * 2. rdev is NULL, means we want to enable serialization for all rdevs.

		/*

		 * already in memalloc noio context by

		 * mddev_suspend()

/*

 * Free resource from rdev(s), and destroy serial_info_pool under conditions:

 * 1. rdev is the last device flaged with CollisionCheck.

 * 2. when bitmap is destroyed while policy is not enabled.

 * 3. for disable policy, the pool is destroyed only when no rdev needs it.

 used to track if other rdevs need the pool */

/*

 * The original mechanism for creating an md device is to create

 * a device node in /dev and to open it.  This causes races with device-close.

 * The preferred method is to write to the "new_array" module parameter.

 * This can avoid races.

 * Setting create_on_open to false disables the original mechanism

 * so all the races disappear.

/*

 * We have a system wide 'event count' that is incremented

 * on any 'interesting' event, and readers of /proc/mdstat

 * can use 'poll' or 'select' to find out when the event

 * count increases.

 *

 * Events are:

 *  start array, stop array, error, add device, remove device,

 *  start build, activate spare

/*

 * Enables to iterate over all existing md arrays

 * all_mddevs_lock protects this list.

/*

 * iterates through all used mddevs in the system.

 * We take care to grab the all_mddevs_lock whenever navigating

 * the list, and to always hold a refcount when unlocked.

 * Any code which breaks out of this loop while own

 * a reference to the current mddev and must mddev_put it.

/* Rather than calling directly into the personality make_request function,

 * IO requests come here first so that we can check if the device is

 * being suspended pending a reconfiguration.

 * We hold a refcount over the call to ->make_request.  By the time that

 * call has finished, the bio has been linked into some internal structure

 * and so is visible to ->quiesce(), so we don't need the refcount any more.

 bio could be mergeable after passing to underlayer */

/* mddev_suspend makes sure no new requests are submitted

 * to the device, and that any requests that have been submitted

 * are completely handled.

 * Once mddev_detach() is called and completes, the module will be

 * completely unused.

 restrict memory reclaim I/O during raid array is suspend */

 entred the memalloc scope from mddev_suspend() */

 possibly kick off a reshape */

/*

 * Generic flush handling for md

 The pre-request flush has finished */

			/* Take two references, one is dropped

			 * when request finishes, one after

			 * we reclaim rcu_read_lock

	/*

	 * must reset flush_bio before calling into md_handle_request to avoid a

	 * deadlock, because other bios passed md_handle_request suspend check

	 * could wait for this and below md_handle_request could wait for those

	 * bios because of suspend check

 an empty barrier - all done */

/*

 * Manages consolidation of flushes and submitting any flushes needed for

 * a bio with REQ_PREFLUSH.  Returns true if the bio is finished or is

 * being finished in another context.  Returns false if the flushing is

 * complete but still needs the I/O portion of the bio to be processed.

	/* flush requests wait until ongoing flush completes,

	 * hence coalescing all the pending requests.

 new request after previous flush is completed */

 flush was performed for some other bio while we waited. */

 an empty barrier - all done */

		/* Array is not configured at all, and not held active,

		/*

		 * Call queue_work inside the spinlock so that

		 * flush_workqueue() after mddev_find will succeed in waiting

		 * for the work to be done.

 find an unused unit number */

 Oh dear, all in use. */

		/* These cannot be removed under reconfig_mutex as

		 * an access to the files will try to take reconfig_mutex

		 * while holding the file unremovable, which leads to

		 * a deadlock.

		 * So hold set sysfs_active while the remove in happeing,

		 * and anything else which might set ->to_remove or my

		 * otherwise change the sysfs namespace will fail with

		 * -EBUSY if sysfs_active is still set.

		 * We set sysfs_active under reconfig_mutex and elsewhere

		 * test it under the same mutex to ensure its correct value

		 * is seen.

	/* As we've dropped the mutex we need a spinlock to

	 * make sure the thread doesn't disappear

 return the offset of the super block in 512byte sectors */

	/* write first size bytes of page to sector of rdev

	 * Increment mddev->pending_writes before returning

	 * and decrement it on completion, waking up sb_wait

	 * if zero is reached.

	 * If an error occurred, call md_error

 wait for all superblock writes that were scheduled to complete */

	/*

	 * nr_disks is not constant

	/* This used to use csum_partial, which was wrong for several

	 * reasons including that different results are returned on

	 * different architectures.  It isn't critical that we get exactly

	 * the same return value as before (we always csum_fold before

	 * testing, and that removes any differences).  However as we

	 * know that csum_partial always returned a 16bit value on

	 * alphas, do a fold to maximise conformity to previous behaviour.

/*

 * Handle superblock details.

 * We want to be able to handle multiple superblock formats

 * so we have a common interface to them all, and an array of

 * different handlers.

 * We rely on user-space to write the initial superblock, and support

 * reading and updating of superblocks.

 * Interface methods are:

 *   int load_super(struct md_rdev *dev, struct md_rdev *refdev, int minor_version)

 *      loads and validates a superblock on dev.

 *      if refdev != NULL, compare superblocks on both devices

 *    Return:

 *      0 - dev has a superblock that is compatible with refdev

 *      1 - dev has a superblock that is compatible and newer than refdev

 *          so dev should be used as the refdev in future

 *     -EINVAL superblock incompatible or invalid

 *     -othererror e.g. -EIO

 *

 *   int validate_super(struct mddev *mddev, struct md_rdev *dev)

 *      Verify that dev is acceptable into mddev.

 *       The first time, mddev->raid_disks will be 0, and data from

 *       dev should be merged in.  Subsequent calls check that dev

 *       is new enough.  Return 0 or -EINVAL

 *

 *   void sync_super(struct mddev *mddev, struct md_rdev *dev)

 *     Update the superblock for rdev with data in mddev

 *     This does not write to disc.

 *

/*

 * Check that the given mddev has no bitmap.

 *

 * This function is called from the run method of all personalities that do not

 * support bitmaps. It prints an error message and returns non-zero if mddev

 * has a bitmap. Otherwise, it returns 0.

 *

/*

 * load_super for 0.90.0

	/*

	 * Calculate the position of the superblock (512byte sectors),

	 * it's at the end of the disk.

	 *

	 * It also happens to be a multiple of 4Kb.

 not spare disk, or LEVEL_MULTIPATH */

	/* Limit to 4TB as metadata cannot record more than that.

	 * (not needed for Linear and RAID0 as metadata doesn't

	 * record this size)

 "this cannot possibly happen" ... */

/*

 * validate_super for 0.90.0

 bitmap can use 60 K after the 4K superblocks */

		/* Insist on good event counter while assembling, except

		/* if adding to array with a bitmap, then we can accept an

		 * older device ... but not too old.

 just a hot-add of a new device, leave raid_disk at -1 */

		else if (desc->state & (1<<MD_DISK_SYNC) /* &&

			/* active but not in sync implies recovery up to

			 * reshape position.  We don't know exactly where

 MULTIPATH are always insync */

/*

 * sync_super for 0.90.0

	/* make rdev->sb match mddev data..

	 *

	 * 1/ zero out disks

	 * 2/ Add info for each disk, keeping track of highest desc_nr (next_spare);

	 * 3/ any empty disks < next_spare become removed

	 *

	 * disks[0] gets initialised to REMOVED because

	 * we cannot be sure from other fields if it has

	 * been initialised or not.

 ignored */

			/* we have nowhere to store the recovery_offset,

			 * but if it is not below the reshape_position,

			 * we can piggy-back on that.

 compatibility */

 now set the "removed" and "faulty" bits on any missing devices */

/*

 * rdev_size_change for 0.90.0

 component must fit device */

 can't move bitmap */

	/* Limit to 4TB as metadata cannot record more than that.

	 * 4TB == 2^32 KB, or 2*2^32 sectors.

 non-zero offset changes not possible with v0.90 */

/*

 * version 1 superblock

	/*

	 * Calculate the position of the superblock in 512byte sectors.

	 * It is always aligned to a 4K boundary and

	 * depeding on minor_version, it can be:

	 * 0: At least 8K, but less than 12K, from end of device

	 * 1: At start of device

	 * 2: 4K from start of device.

	/* superblock is rarely larger than 1K, but it can be larger,

	 * and it is safe to read 4k, so we do that

 Some padding is non-zero, might be a new feature */

		/* need to load the bad block list.

		 * Currently we limit it to one page.

 not spare disk, or LEVEL_MULTIPATH */

		/* Default location for bitmap is 1K after superblock

		 * using 3K - total of 4K

			/* Metadata doesn't record how much space is available.

			 * For 1.0, we assume we can use up to the superblock

			 * if before, else to 4K beyond superblock.

			 * For others, assume no change is possible.

		/* Insist of good event counter while assembling, except for

		/* If adding to array with a bitmap, then we can accept an

		 * older device, but not too old.

 just a hot-add of a new device, leave raid_disk at -1 */

 spare */

 faulty */

 journal device */

 journal device without journal feature */

				/*

				 * If the array is FROZEN, then the device can't

				 * be in_sync with rest of array.

 MULTIPATH are always insync */

 make rdev->sb match mddev and rdev data. */

 Note: recovery_offset and journal_tail share space  */

 Nothing to do for bad blocks*/ ;

 Cannot record bad blocks on this device */

	/* if the device is bigger than 8Gig, save 64k for bitmap

	 * usage, if bigger than 200Gig, save 128k

 component must fit device */

 too confusing */

 minor versions 1 and 2; superblock before data */

 minor version 0 with bitmap we can't move */

 minor version 0; superblock after data */

 8K is for superblock */

		/* Space that can be used to store date needs to decrease

		 * superblock bitmap space and bad block space(4K)

 All necessary checks on new >= old have been done */

	/* with 1.0 metadata, there is no metadata to tread on

	/* otherwise we must be sure not to step on

	 * any metadata, so stay:

	 * 36K beyond start of superblock

	 * beyond end of badblocks

	 * beyond write-intent bitmap

/*

 * Try to register data integrity profile for an mddev

 *

 * This is called when an array is started and after a disk has been kicked

 * from the array. It only succeeds if all working and active component devices

 * are integrity capable with matching profiles.

 nothing to do */

 shouldn't register, or already is */

 skip spares and non-functional disks */

 Use the first rdev as the reference */

 does this rdev's profile match the reference profile? */

	/*

	 * All component devices are integrity capable and have matching

	 * profiles, register the common profile for the md device.

		/*

		 * No need to handle the failure of bioset_integrity_create,

		 * because the function is called by md_run() -> pers->run(),

		 * md_run calls bioset_exit -> bioset_integrity_free in case

		 * of failure case.

/*

 * Attempt to add an rdev, but only if it is consistent with the current

 * integrity profile

 nothing to do */

 prevent duplicates */

 make sure rdev->sectors exceeds mddev->dev_sectors */

			/* Cannot change size, so fail

			 * If mddev->level <= 0, then we don't care

			 * about aligning sizes (e.g. linear)

	/* Verify rdev->desc_nr is unique.

	 * If it is -1, assign a free number, else

	 * check number is not in use

 failure here is OK */

 May as well allow recovery to be retried once */

	/* We need to delay this, otherwise we can deadlock when

	 * writing to 'remove' to "dev/state".  We also need

	 * to delay it due to rcu usage.

/*

 * prevent the device from being mounted, repartitioned or

 * otherwise reused by a RAID array (or any other kernel

 * subsystem), by bd_claiming the device.

			/*

			 * Ensure ->in_sync is visible before we clear

			 * ->sync_checkers.

	/* Update each superblock (in-memory image), but

	 * if we are allowed to, skip spares which already

	 * have the right event counter, or have one earlier

	 * (which would mean they aren't being marked as dirty

	 * with the rest of the array)

 Don't update this superblock */

 Find a good rdev */

 No good device found. */

 Check if a device has become faulty or a spare become active */

 Device activated? */

 Device turned faulty? */

 Check if any mddev parameters have changed */

 Has someone else has updated the sb */

	/*

	 * First make sure individual recovery_offsets are correct

	 * curr_resync_completed can only be used during recovery.

	 * During reshape/resync it might use array-addresses rather

	 * that device addresses.

		/* just a clean<-> dirty transition, possibly leave spares alone,

		 * though if events isn't the right even/odd, we will have to do

		 * spares after all

		/* If the array is degraded, then skipping spares is both

		 * dangerous and fairly pointless.

		 * Dangerous because a device that was removed from the array

		 * might have a event_count that still looks up-to-date,

		 * so it can be re-added without a resync.

		 * Pointless because if there are any spares to skip,

		 * then a recovery will happen and soon that array won't

		 * be degraded any more and the spare can go back to sleep then.

	/* If this is just a dirty<->clean transition, and the array is clean

 otherwise we have to go forward and ... */

	/*

	 * This 64-bit counter should never wrap.

	 * Either we are in around ~1 trillion A.C., assuming

	 * 1 reboot per second, or we have a bug...

 no noise on spare devices */

 only need to write one superblock... */

 if there was a failure, MD_SB_CHANGE_DEVS was set, and we re-write super */

 have to write it out again */

		/* If there is hot_add_disk but no hot_remove_disk

		 * then added disks for geometry changes,

		 * and should be added immediately.

/* words written to sysfs files may, or may not, be \n terminated.

 * We want to accept with case. For this we use cmd_match.

	/* See if cmd, written into a sysfs file, matches

	 * str.  They must either be the same, or cmd can

	 * have a trailing newline

	/* can write

	 *  faulty  - simulates an error

	 *  remove  - disconnects the device

	 *  writemostly - sets write_mostly

	 *  -writemostly - clears write_mostly

	 *  blocked - sets the Blocked flags

	 *  -blocked - clears the Blocked and possibly simulates an error

	 *  insync - sets Insync providing device isn't active

	 *  -insync - clear Insync for a device with a slot assigned,

	 *            so that it gets rebuilt based on bitmap

	 *  write_error - sets WriteErrorSeen

	 *  -write_error - clears WriteErrorSeen

	 *  {,-}failfast - set/clear FailFast

			/* metadata handler doesn't understand badblocks,

			 * so we need to fail the device

		/* Any non-spare device that is not a replacement can

		 * become want_replacement at any time, but we then need to

		 * check if recovery is needed.

		/* Clearing 'want_replacement' is always allowed.

		 * Once replacements starts it is too late though.

		/* Can only set a device as a replacement when array has not

		 * yet been started.  Once running, replacement is automatic

		 * from spares, or by assigning 'slot'.

 Similarly, can only clear Replacement before start */

			/* clear_bit is performed _after_ all the devices

			 * have their local Faulty bit cleared. If any writes

			 * happen in the meantime in the local node, they

			 * will land in the local bitmap, which will be synced

			 * by this node eventually

		/* Setting 'slot' on an active array requires also

		 * updating the 'rd%d' link, and communicating

		 * with the personality with ->hot_*_disk.

		 * For now we only support removing

		 * failed/spare devices.  This normally happens automatically,

		 * but not when the metadata is externally managed.

 personality does all needed checks */

		/* Activating a spare .. or possibly reactivating

		 * if we ever get bitmaps working here.

 failure here is OK */;

 don't wakeup anyone, leave that to userspace. */

 assume it is working */

		/* Must set offset before size, so overlap checks

 reset is always permitted */

 must not push array size beyond rdev_sectors */

 Metadata worries about other space details. */

	/* decreasing the offset is inconsistent with a backwards

	 * reshape.

	/* Increasing offset is inconsistent with forwards

	 * reshape.  reshape_direction should be set to

	 * 'backwards' first.

 check if two start/length pairs overlap */

 sector conversion overflow */

 unsigned long long to sector_t overflow */

 too confusing */

 Cannot change size for RAID0 or Linear etc */

 component must fit device */

		/* Need to check that all other rdevs with the same

		 * ->bdev do not overlap.  'rcu' is sufficient to walk

		 * the rdev lists safely.

		 * This check does not provide a hard guarantee, it

		 * just helps avoid dangerous mistakes.

			/* Someone else could have slipped in a size

			 * change here, but doing so is just silly.

			 * We put oldsectors back because we *know* it is

			 * safe, and trust userspace not to race with

			 * itself

/* sysfs access to bad-blocks list.

 * We present two files.

 * 'bad-blocks' lists sector numbers and lengths of ranges that

 *    are recorded as bad.  The list is truncated to fit within

 *    the one-page limit of sysfs.

 *    Writing "sector length" to this file adds an acknowledged

 *    bad block list.

 * 'unacknowledged-bad-blocks' lists bad blocks that have not yet

 *    been acknowledged.  Writing to this file adds bad blocks

 *    without acknowledging them.  This is largely for testing.

 Maybe that ack was all we needed */

	/* Add space to store bad block list.

	 * This reserves the space even on arrays where it cannot

	 * be used - I wonder if that matters

/*

 * Import a device. If 'super_format' >= 0, then sanity check the superblock

 *

 * mark the device faulty if:

 *

 *   - the device is nonexistent (zero size)

 *   - the device has no valid superblock

 *

 * a faulty rdev _never_ has rdev->sb set.

/*

 * Check a full RAID array for plausibility

 Cannot find a valid fresh disk */

/* Read a fixed-point number.

 * Numbers in sysfs attributes should be in "standard" units where

 * possible, so time should be in seconds.

 * However we internally use a a much smaller unit such as

 * milliseconds or jiffies.

 * This function takes a decimal number with a possible fractional

 * component, and produces an integer which is the result of

 * multiplying that number by 10^'scale'.

 * all without any floating-point arithmetic.

	/* request to change the personality.  Need to ensure:

	 *  - array is not engaged in resync/recovery/reshape

	 *  - old personality can be suspended

	 *  - new personality will access other array.

 Now find the new personality */

 Nothing to do! */

	/* ->takeover must set new_* and/or delta_disks

	 * if it succeeds, and may set them when it fails.

 Looks like we have a winner */

		/* We are converting from a no-redundancy array

		 * to a redundancy array and metadata is managed

		 * externally so we need to be sure that writes

		 * won't block due to a need to transition

		 *      clean->dirty

		 * until external management is started.

 need to add the md_redundancy_group */

 need to remove the md_redundancy_group */

		/* this is now an array without redundancy, so

		 * it must always be in_sync

 just a number, not meaningful for all levels */

/*

 * The array state can be:

 *

 * clear

 *     No devices, no size, no level

 *     Equivalent to STOP_ARRAY ioctl

 * inactive

 *     May have some settings, but array is not active

 *        all IO results in error

 *     When written, doesn't tear down array, but just stops it

 * suspended (not supported yet)

 *     All IO requests will block. The array can be reconfigured.

 *     Writing this, if accepted, will block until array is quiescent

 * readonly

 *     no resync can happen.  no superblocks get written.

 *     write requests fail

 * read-auto

 *     like readonly, but behaves like 'clean' on a write request.

 *

 * clean - no pending writes, but otherwise active.

 *     When written to inactive array, starts without resync

 *     If a write request arrives then

 *       if metadata is known, mark 'dirty' and switch to 'active'.

 *       if not known, block and switch to write-pending

 *     If written to an active array that has pending writes, then fails.

 * active

 *     fully active: IO and resync can be happening.

 *     When written to inactive array, starts with resync

 *

 * write-pending

 *     clean, but writes are blocked waiting for 'active' to be written.

 *

 * active-idle

 *     like active, but no writes have been seen for a while (100msec).

 *

 * broken

 *     RAID0/LINEAR-only: same as clean, but array is missing a member.

 *     It's useful because RAID0/LINEAR mounted-arrays aren't stopped

 *     when a member is gone, so this state will at least alert the

 *     user that something is wrong.

		/* don't take reconfig_mutex when toggling between

		 * clean and active

 st == clean */ {

 stopping an active array */

 stopping an active array */

 already inactive */

 not supported yet */

 these cannot be set */

 need to ensure rdev_delayed_delete() has completed */

 buf must be %d:%d\n? giving major and minor numbers */

	/* The new device is added to the array.

	 * If the array has a persistent superblock, we read the

	 * superblock to initialise info and check validity.

	 * Otherwise, only checking done is that in bind_rdev_to_array,

	 * which mainly checks size.

 buf should be <chunk> <chunk> ... or <chunk>-<chunk> ... (range) */

 range */

 flush the bits to disk */

	/* If array is inactive, we can reduce the component size, but

	 * not increase it (except from 0).

	 * If array is active, we can try an on-line resize

/* Metadata version.

 * This is one of

 *   'none' for arrays with no metadata (good luck...)

 *   'external' for arrays with externally managed metadata,

 * or N.M for internally known formats

	/* Changing the details of 'external' metadata is

	 * always permitted.  Otherwise there must be

	 * no devices attached to the array.

		/* A write to sync_action is enough to justify

		 * canceling read-auto mode

 force parallel resync, even with shared block devices */

 K/sec */

 Round down to multiple of 4K for safety */

 Must be a multiple of chunk_size */

 check if we are allowed to change */

 cluster raid doesn't support change array_sectors */

/*

 * Setting fail_last_dev to true to allow last device to be forcibly removed

 * from RAID1/RAID10.

/*

 * Setting serialize_policy to true to enforce write IO is not reordered

 * for raid1.

 We want to start with the refcount at zero */

	/*

	 * If dev is zero, name is the name of a device to allocate with

	 * an arbitrary minor number.  It will be "md_???"

	 * If dev is non-zero it must be a device number with a MAJOR of

	 * MD_MAJOR or mdp_major.  In this case, if "name" is NULL, then

	 * the device is being created by opening a node in /dev.

	 * If "name" is not NULL, the device is being created by

	 * writing to /sys/module/md_mod/parameters/new_array.

	/*

	 * Wait for any previous instance of this device to be completely

	 * removed (mddev_delayed_delete).

		/* Need to ensure that 'name' is not a duplicate.

		/*

		 * Creating /dev/mdNNN via "newarray", so adjust hold_active.

	/* Allow extended partitions.  This makes the

	 * 'mdp' device redundant, but we can't really

	 * remove it now.

	/*

	 * val must be "md_*" or "mdNNN".

	 * For "md_*" we allocate an array with a large free minor number, and

	 * set the name to val.  val must not already be an active name.

	 * For "mdNNN" we allocate an array with the minor number NNN

	 * which must not already be in use.

 cannot run an array with no devices.. */

 Cannot run until previous stop completes properly */

	/*

	 * Analyze all RAID superblock(s)

	/*

	 * Drop all container device buffers, from now on

	 * the only valid external interface is through the md

	 * device.

		/* perform some consistency tests on the device.

		 * We don't want the data to overlap the metadata,

		 * Internal Bitmap issues have been handled elsewhere.

 Nothing to check */;

 This personality cannot handle reshaping... */

		/* Warn if this is a potentially silly

		 * configuration.

 may be over-ridden by personality */

 read-only, but switch on first write */

 auto-readonly not meaningful */

 failure here is OK */

		/* This ensures that recovering status is reported immediately

		 * via sysfs - until a lack of spares is confirmed.

 run start up tasks that require md_thread */

 possibly kick off a reshape */

 Complain if it has no devices */

 Don't restart rw with journal missing/faulty */

 Kick recovery or resync if necessary */

 mark array as shutdown cleanly */

 disable policy to guarantee rdevs free resources for serialization */

 the unplug fn references 'conf'*/

 Ensure ->event_work is done */

	/* stop the array and free an attached data structures.

	 * This is called from dm-raid

		/* Thread might be blocked waiting for metadata update

/* mode:

 *   0 - completely stop and dis-assemble array

 *   2 - stop but do not disassemble array

		/* Thread might be blocked waiting for metadata update

 tell userspace to handle 'inactive' */

	/*

	 * Free resources if final stop

/*

 * lets try to run arrays based on all disks that have arrived

 * until now. (those are in pending_raid_disks)

 *

 * the method: pick the first pending disk, collect all disks with

 * the same UUID, remove all from the pending list and put them into

 * the 'same_array' list. Then order this list based on superblock

 * update time (freshest comes first), kick out 'old' disks and

 * compare superblocks. If everything's fine then run it.

 *

 * If "unit" is allocated, then bump its reference count

		/*

		 * now we have a set of devices, with all of them having

		 * mostly sane superblocks. It's time to allocate the

		 * mddev.

		/* on success, candidates will be empty, on error

		 * it won't...

 !MODULE */

 TODO: add journal count to md_u.h */

 overflow */

 too big for stack allocation */

 bitmap enabled */

 expecting a device which has a superblock */

	/*

	 * md_add_new_disk can be used once the array is assembled

	 * to add "hot spares".  They must already have a superblock

	 * written

 set saved_raid_disk if appropriate */

			/* This was a hot-add request, but events doesn't

			 * match, so reject it.

 just to be sure */

 make sure no existing journal disk */

		/*

		 * check whether the device shows up in other nodes

 --add initiated by this node */

	/* otherwise, md_add_new_disk is only allowed

	 * for major_version==0 superblocks

	/*

	 * The rest should better be atomic, we can have disk failures

	 * noticed in interrupt contexts ...

	/*

	 * Kick recovery, maybe this spare has to be added to the

	 * array immediately.

 we should be able to change the bitmap.. */

 cannot add when bitmap is present */

 file overrides offset */

 cannot remove what isn't there */

/*

 * md_set_array_info is used two different ways

 * The original usage is when creating a new array.

 * In this usage, raid_disks is > 0 and it together with

 *  level, size, not_persistent,layout,chunksize determine the

 *  shape of the array.

 *  This will always create an array with a type-0.90.0 superblock.

 * The newer usage is when assembling an array.

 *  In this case raid_disks will be 0, and the major_version field is

 *  use to determine which style super-blocks are to be found on the devices.

 *  The minor and patch _version numbers are also kept incase the

 *  super_block handler wishes to interpret them.

 just setting version number for superblock loading */

 maybe try to auto-load a module? */

		/* ensure mddev_put doesn't delete this now that there

		 * is some minimal configuration.

	/* don't set md_minor, it is determined by which /dev/md* was

	 * openned

 Cannot trust RAID0 layout info here */

	/*

	 * Generate a 128 bit UUID

	/* The "num_sectors" is the number of sectors of each device that

	 * is used.  This can only make sense for arrays with redundancy.

	 * linear and raid0 always use whatever space is available. We can only

	 * consider changing this number if no resync or reconstruction is

	 * happening, and if the new size is acceptable. It must fit before the

	 * sb_start or, if that is <data_offset, it must fit before the size

	 * of each device.  If num_sectors is zero, we find the largest size

	 * that fits.

 change the number of raid disks */

/*

 * update_array_info is used to change the configuration of an

 * on-line array.

 * The version, ctime,level,size,raid_disks,not_persistent, layout,chunk_size

 * fields in the info are checked against the array.

 * Any differences that cannot be handled will cause an error.

 * Normally, only one change can be managed at a time.

 calculate expected state,ignoring low bits */

	    mddev->patch_version != info->patch_version || */

	    mddev->layout        != info->layout        || */

 ignore bottom 8 bits of state, and allow SB_BITMAP_PRESENT to change */

 Check there is only one change */

		/* Change layout

		 * we don't need to do anything at the md level, the

		 * personality will take care of it all.

 add the bitmap */

 remove the bitmap */

 hold PW on all the bitmap lock */

/*

 * We have a problem here : there is no easy way to give a CHS

 * virtual geometry. We currently pretend that we have a 2 heads

 * 4 sectors (with a BIG number of cylinders...). This drives

 * dosfs just mad... ;-)

	/*

	 * Commands dealing with the RAID driver but not any

	 * particular array:

	/*

	 * Commands creating/starting a new array:

 Some actions do not requires the mutex */

 need to ensure recovery thread has run */

		/* Need to flush page cache, and ensure no-one else opens

		 * and writes

	/*

	 * Commands querying/configuring an existing array:

	/* if we are not initialised yet, only ADD_NEW_DISK, STOP_ARRAY,

	/*

	 * Commands even a read-only array can execute:

		/* We can support ADD_NEW_DISK on read-only arrays

		 * only if we are re-adding a preexisting device.

		 * So require mddev->pers and MD_DISK_SYNC.

 Need to clear read-only for this */

	/*

	 * The remaining ioctls are changing the state of the

	 * superblock, so we do not allow them on read-only arrays.

 mddev_unlock will wake thread */

			/* If a device failed while we were read-only, we

			 * need to make sure the metadata is updated now.

 These take in integer arg, do not convert */

 CONFIG_COMPAT */

	/*

	 * Transitioning to read-auto need only happen for arrays that call

	 * md_write_start and which are not ready for writes yet.

	/*

	 * Succeed if we can lock the mddev, which confirms that

	 * it isn't being stopped right now.

		/* we are racing with mddev_put which is discarding this

		 * bd_disk.

 Wait until bdev->bd_disk is definitely gone */

	/*

	 * md_thread is a 'system-thread', it's priority should be very

	 * high. We avoid resource deadlocks individually in each

	 * raid personality. (RAID5 does preallocation) We also use RR and

	 * the very same RT priority as kswapd, thus we will never get

	 * into a priority inversion deadlock.

	 *

	 * we definitely have to have equal or higher priority than

	 * bdflush, otherwise bdflush will deadlock if there are too

	 * many dirty RAID5 blocks.

		/* We need to wait INTERRUPTIBLE so that

		 * we don't add to the load-average.

		 * That means we need to be sure no signals are

		 * pending

	/* Locking ensures that mddev_unlock does not wake_up a

	 * non-existent thread

 seq_file implementation /proc/mdstat */

 Still cleaning up */

	/* Pick 'scale' such that (resync>>scale)*1000 will fit

	 * in a sector_t, and (max_sectors>>scale) will fit in a

	 * u32, as those are the requirements for sector_div.

	 * Thus 'scale' must be at least 10

	/*

	 * dt: time from mark until now

	 * db: blocks written from mark until now

	 * rt: remaining time

	 *

	 * rt is a sector_t, which is always 64bit now. We are keeping

	 * the original algorithm, but it is not really necessary.

	 *

	 * Original algorithm:

	 *   So we divide before multiply in case it is 32bit and close

	 *   to the limit.

	 *   We scale the divisor (db) by 32 to avoid losing precision

	 *   near the end of resync when the number of remaining sectors

	 *   is close to 'db'.

	 *   We then divide rt by 32 after multiplying by db to compensate.

	 *   The '+1' avoids division by zero if db is very small.

 number of remaining sectors */

 header */

 tail */

 spare */

 always allow read */

 ensure module won't be unloaded */

		/* sync IO will cause sync_io to increase before the disk_stats

		 * as sync_io is counted when a request starts, and

		 * disk_stats is counted when it completes.

		 * So resync activity will cause curr_events to be smaller than

		 * when there was no such activity.

		 * non-sync IO will cause disk_stat to increase without

		 * increasing sync_io so curr_events will (eventually)

		 * be larger than it was before.  Once it becomes

		 * substantially larger, the test below will cause

		 * the array to appear non-idle, and resync will slow

		 * down.

		 * If there is a lot of outstanding resync activity when

		 * we set last_event to curr_events, then all that activity

		 * completing might cause the array to appear non-idle

		 * and resync will be slowed down even though there might

		 * not have been non-resync activity.  This will only

		 * happen once though.  'last_events' will soon reflect

		 * the state where there is little or no outstanding

		 * resync requests, and further resync activity will

		 * always make curr_events less than last_events.

		 *

 another "blocks" (512byte) blocks have been synced */

 stop recovery, signal do_sync ....

/* md_write_start(mddev, bi)

 * If we need to update some array metadata (e.g. 'active' flag

 * in superblock) before writing, schedule a superblock update

 * and wait for it to complete.

 * A return value of 'false' means that the write wasn't recorded

 * and cannot proceed as the array is being suspend.

 need to switch to read/write */

 Match smp_mb in set_in_sync() */

 sync_checkers is always 0 when writes_pending is in per-cpu mode */

/* md_write_inc can only be called when md_write_start() has

 * already been called at least once of the current request.

 * It increments the counter and is useful when a single request

 * is split into several parts.  Each part causes an increment and

 * so needs a matching md_write_end().

 * Unlike md_write_start(), it is safe to call md_write_inc() inside

 * a spinlocked region.

		/* The roundup() ensures this only performs locking once

		 * every ->safemode_delay jiffies

 This is used by raid0 and raid10 */

/*

 * Used by personalities that don't already clone the bio and thus can't

 * easily add the timestamp to their extended bio structure.

/* md_allow_write(mddev)

 * Calling this ensures that the array is marked 'active' so that writes

 * may proceed without blocking.  It is important to call this before

 * attempting a GFP_KERNEL allocation while holding the mddev lock.

 * Must be called with mddev_lock held.

 wait for the dirty state to be recorded in the metadata */

 just incase thread restarts... */

 never try to sync a read-only array */

	/* we overload curr_resync somewhat here.

	 * 0 == not engaged in resync at all

	 * 2 == checking that there is no conflict with another sync

	 * 1 == like 2, but have yielded to allow conflicting resync to

	 *		commence

	 * other == active in resync - this many blocks

	 *

	 * Before starting a resync we must have set curr_resync to

	 * 2, and then checked that every "conflicting" array has curr_resync

	 * less than ours.  When we find one that is the same or higher

	 * we wait on resync_wait.  To avoid deadlock, we reduce curr_resync

	 * to 1 if we choose to yield (based arbitrarily on address of mddev structure).

	 * This will mean we have to start checking from the beginning again.

	 *

 arbitrarily yield */

					/* no need to wait here, we can wait the next

					 * time 'round when curr_resync == 2

				/* We need to wait 'interruptible' so as not to

				 * contribute to the load average, and not to

				 * be caught by 'softlockup'

		/* resync follows the size requested by the personality,

		 * which defaults to physical size, but can be virtual size

 we don't use the checkpoint if there's a bitmap */

		/*

		 * If the original node aborts reshaping then we continue the

		 * reshaping, so set j again to avoid restart reshape from the

		 * first beginning

 recovery follows the physical size of devices */

		/* If there is a bitmap, we need to make sure all

		 * writes that started before we added a spare

		 * complete before we start doing a recovery.

		 * Otherwise the write might complete and (via

		 * bitmap_endwrite) set a bit in the bitmap after the

		 * recovery has checked that bit and skipped that

		 * region.

 this initializes IO event counters */

	/*

	 * Tune reconstruction:

 no longer delayed */

 time to update curr_resync_completed */

			/* As this condition is controlled by user-space,

			 * we can block indefinitely, so use '_interruptible'

			 * to avoid triggering warnings.

 just in case */

 actual IO requested */

 when skipping, extra large numbers can be returned. */

			/* this is the earliest that rebuild will be

			 * visible in /proc/mdstat

 step marks */

		/*

		 * this loop exits only if either when we are slower than

		 * the 'hard' speed limit, or the system was IO-idle for

		 * a jiffy.

		 * the system might be non-idle CPU-wise, but we only care

		 * about not overloading the IO subsystem. (things like an

		 * e2fsck being done on the RAID array should execute fast)

				/*

				 * Give other IO more of a chance.

				 * The faster the devices, the less we wait.

	/*

	 * this also signals 'finished resyncing' to md_stop

	/* set CHANGE_PENDING here since maybe another update is needed,

	 * so other nodes are informed. It should be harmless for normal

 We completed so min/max setting can be forgotten if used. */

 Mustn't remove devices when resync thread is running */

			/* Faulty non-Blocked devices with nr_pending == 0

			 * never get nr_pending incremented,

			 * never get Faulty cleared, and never get Blocked set.

			 * So we can synchronize_rcu now rather than once per device

 failure here is OK */

 leave the spares where they are, it shouldn't hurt */

/*

 * This routine is regularly called by all per-raid-array threads to

 * deal with generic issues like resync and super-block update.

 * Raid personalities that don't have a thread (linear/raid0) do not

 * need this as they never do any recovery or update the superblock.

 *

 * It does not do any resync itself, but rather "forks" off other threads

 * to do that as needed.

 * When it is determined that resync is needed, we set MD_RECOVERY_RUNNING in

 * "->recovery" and create a thread at ->sync_thread.

 * When the thread finishes it sets MD_RECOVERY_DONE

 * and wakeups up this thread which will reap the thread and finish up.

 * This thread also removes any faulty devices (with nr_pending == 0).

 *

 * The overall approach is:

 *  1/ if the superblock needs updating, update it.

 *  2/ If a recovery thread is running, don't do anything else.

 *  3/ If recovery has finished, clean up, possibly marking spares active.

 *  4/ If there are any faulty devices, remove them.

 *  5/ If array is degraded, try to add spares devices

 *  6/ If array has spares or is not in-sync, start a resync thread.

		/* Write superblock - thread that called mddev_suspend()

		 * holds reconfig_mutex for us.

				/* 'Blocked' flag not needed as failed devices

				 * will be recorded if array switched to read/write.

				 * Leaving it set will prevent the device

				 * from being removed.

			/* On a read-only array we can:

			 * - remove failed devices

			 * - add already-in_sync devices if the array itself

			 *   is in-sync.

			 * As we only add devices that are already in-sync,

			 * we can activate the spares immediately.

			/* There is no thread, but we need to call

			 * ->spare_active and clear saved_raid_disk

			/* kick the device if another node issued a

			 * remove disk.

 resync/recovery still happening */

		/* Set RUNNING before clearing NEEDED to avoid

		 * any transients in the value of "sync_action".

		/* Clear some bits that don't mean anything, but

		 * might be left set

		/* no recovery is running.

		 * remove any failed drives, then

		 * add spares if possible.

		 * Spares are also removed and re-added, to allow

		 * the personality to fail the re-add.

 Cannot proceed */

 nothing to be done ... */

				/* We are adding a device or devices to an array

				 * which has the bitmap stored on all devices.

				 * So make sure all bitmap pages get written

 resync has finished, collect result */

 success...*/

 activate any spares */

	/* If array is no-longer degraded, then any saved_raid_disk

	 * information must be scrapped.

	/* MD_SB_CHANGE_PENDING should be cleared by md_update_sb, so we can

	 * call resync_finish here if MD_CLUSTER_RESYNC_LOCKED is set by

	/*

	 * We call md_cluster_ops->update_size here because sync_size could

	 * be changed by md_update_sb, and MD_RECOVERY_RESHAPE is cleared,

	 * so it is time to update size across cluster.

 flag recovery needed just to double check */

 called be personality module when reshape completes. */

 Bad block management */

 Returns 1 on success, 0 on failure */

 Make sure they get written out promptly */

	/*

	 * certain more exotic SCSI devices are known to be

	 * volatile wrt too early system reboots. While the

	 * right place to handle this issue is the given

	 * driver, we do want to have a safe RAID driver ...

 before any real devices */

	/*

	 * If size is changed in another node then we need to

	 * do resize as well.

 Check for change of roles in the active devices */

 Check if the roles changed */

			/*

			 * got activated except reshape is happening.

				/* wakeup mddev->thread here, so array could

			/* device faulty

			 * We just want to do the minimum to mark the disk

			 * as faulty. The recovery is performed by the

			 * one who initiated the error.

	/*

	 * Since mddev->delta_disks has already updated in update_raid_disks,

	 * so it is time to check reshape.

		/*

		 * reshape is happening in the remote node, we need to

		 * update reshape_position and call start_reshape.

 reshape is just done in another node. */

 Finally set the event to be up to date */

	/* Store the sb page of the rdev in the swapout temporary

	 * variable in case we err in the future

	/* Read the offset unconditionally, even if MD_FEATURE_RECOVERY_OFFSET

	 * is not set

	/* The other node finished recovery, call spare_active to set

	 * device In_sync and mddev->degraded

 Find the rdev */

 Read all rdev's to update recovery_offset */

/*

 * Searches all registered partitions for autorun RAID arrays

 * at boot time.

 !MODULE */

	/* We cannot unload the modules while some process is

	 * waiting for us in select() or poll() - wake them up

 not safe to leave yet */

		/*

		 * for_each_mddev() will call mddev_put() at the end of each

		 * iteration.  As the mddev is now fully clear, this will

		 * schedule the mddev for destruction by a workqueue, and the

		 * destroy_workqueue() below will wait for that to complete.

/*

 * Copyright (C) 2001 Sistina Software (UK) Limited

 *

 * This file is released under the GPL.

/*

 * io-err: always fails an io, useful for bringing

 * up LVs that have holes in them.

	/*

	 * Return error for discards instead of -EOPNOTSUPP

 empty */

/*

 * Copyright (C) 2010-2011 Neil Brown

 * Copyright (C) 2010-2018 Red Hat, Inc. All rights reserved.

 *

 * This file is released under the GPL.

 md-raid kernel limit */

/*

 * Minimum sectors of free reshape space per raid device

/*

 * Minimum journal space 4 MiB in sectors.

/*

 * The following flags are used by dm-raid.c to set up the array state.

 * They must be cleared before md_run is called.

 rdev flag */

	/*

	 * Two DM devices, one to hold metadata and one to hold the

	 * actual data/parity.	The reason for this is to not confuse

	 * ti->len and give more flexibility in altering size and

	 * characteristics.

	 *

	 * While it is possible for this device to be associated

	 * with a different physical device than the data_dev, it

	 * is intended for it to be the same.

	 *    |--------- Physical Device ---------|

	 *    |- meta_dev -|------ data_dev ------|

/*

 * Bits for establishing rs->ctr_flags

 *

 * 1 = no flag value

 * 2 = flag with value

 1 */ 
 1 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 2 */ 
 New for v1.9.0 */

 2 */ 
 2 */ 
 2 */ 
 New for v1.10.0 */

 2 */ 
 New for v1.11.1 */

 2 */ 
/*

 * Flags for rs->ctr_flags field.

/*

 * Definitions of various constructor flags to

 * be used in checks of valid / invalid flags

 * per raid level.

 Define all any sync flags */

 Define flags for options without argument (e.g. 'nosync') */

 Define flags for options with one argument (e.g. 'delta_disks +2') */

 Valid options definitions per raid level... */

 "raid0" does only accept data offset */

 "raid1" does not accept stripe cache, data offset, delta_disks or any raid10 options */

 "raid10" does not accept any raid1 or stripe cache options */

/*

 * "raid4/5/6" do not accept any raid1 or raid10 specific options

 *

 * "raid6" does not accept "nosync", because it is not guaranteed

 * that both parity and q-syndrome are being written properly with

 * any writes

 ...valid options definitions per raid level */

/*

 * Flags for rs->runtime_flags field

 * (RT_FLAG prefix meaning "runtime flag")

 *

 * These are all internal and used to define runtime state,

 * e.g. to prevent another resume from preresume processing

 * the raid set all over again.

 Array elements of 64 bit needed for rebuild/failed disk bits */

/*

 * raid set level, layout and chunk sectors backup/restore

 Optional raid4/5/6 journal device */

 raid10 algorithms (i.e. formats) */

 Supported raid types and properties. */

 RAID algorithm. */

 Descriptor text for logging. */

 # of parity devices. */

 minimal # of devices in set. */

 RAID level. */

 RAID algorithm. */

 NONE */},

 NONE */},

 raid4 layout = raid5_0 */

 True, if @v is in inclusive range [@min, @max] */

 All table line arguments are defined here */

 Return argument name string for given @flag */

 Define correlation of raid456 journal cache modes and dm-raid target line parameters */

 Return MD raid4/5/6 journal mode for dm @journal_mode one */

 Return dm-raid raid4/5/6 journal mode string for @mode */

/*

 * Bool helpers to test for various raid levels of a raid set.

 * It's level as reported by the superblock rather than

 * the requested raid_type passed to the constructor.

 Return true, if raid set in @rs is raid0 */

 Return true, if raid set in @rs is raid1 */

 Return true, if raid set in @rs is raid10 */

 Return true, if raid set in @rs is level 6 */

 Return true, if raid set in @rs is level 4, 5 or 6 */

 Return true, if raid set in @rs is reshapable */

 Return true, if raid set in @rs is recovering */

 Return true, if raid set in @rs is reshaping */

/*

 * bool helpers to test for various raid levels of a raid type @rt

 Return true, if raid type in @rt is raid0 */

 Return true, if raid type in @rt is raid1 */

 Return true, if raid type in @rt is raid10 */

 Return true, if raid type in @rt is raid4/5 */

 Return true, if raid type in @rt is raid6 */

 Return true, if raid type in @rt is raid4/5/6 */

 END: raid level bools */

 Return valid ctr flags for the raid level of @rs */

/*

 * Check for valid flags set on @rs

 *

 * Has to be called after parsing of the ctr flags!

 MD raid10 bit definitions and helpers */

 stripes with data copies area adjacent on devices */

 Broken in raid10.c: use sets instead of whole stripe rotation */

 Use sets instead of whole stripe rotation */

 raid10 # far copies shift (2nd byte of layout) */

 Return md raid10 near copies for @layout */

 Return md raid10 far copies for @layout */

 Return true if md raid10 offset for @layout */

 Return true if md raid10 near for @layout */

 Return true if md raid10 far for @layout */

 Return md raid10 layout string for @layout */

	/*

	 * Bit 16 stands for "offset"

	 * (i.e. adjacent stripes hold copies)

	 *

	 * Refer to MD's raid10.c for details

 Return md raid10 algorithm for @name */

 Return md raid10 copies for @layout */

 Return md raid10 format id for @format string */

	/*

	 * MD resilienece flaw:

	 *

	 * enabling use_far_sets for far/offset formats causes copies

	 * to be colocated on the same devs together with their origins!

	 *

	 * -> disable it for now in the definition above

 END: MD raid10 bit definitions and helpers */

 Check for any of the raid10 algorithms */

 Return raid_type for @name */

 Return raid_type for @name based derived from @level and @layout */

 RAID10 special checks based on @layout flags/properties */

 Adjust rdev sectors */

	/*

	 * raid10 sets rdev->sector to the device size, which

	 * is unintended in case of out-of-place reshaping

/*

 * Change bdev capacity of @rs in case of a disk add/remove reshape

/*

 * Set the mddev properties in @rs to the current

 * ones retrieved from the freshest superblock

/*

 * Set the mddev properties in @rs to the new

 * ones requested by the ctr

	/*

	 * Remaining items to be initialized by further RAID params:

	 *  rs->md.persistent

	 *  rs->md.external

	 *  rs->md.chunk_sectors

	 *  rs->md.new_chunk_sectors

	 *  rs->md.dev_sectors

 Free all @rs allocations */

/*

 * For every device we have two words

 *  <meta_dev>: meta device name or '-' if missing

 *  <data_dev>: data device name or '-' if missing

 *

 * The following are permitted:

 *    - -

 *    - <data_dev>

 *    <meta_dev> <data_dev>

 *

 * The following is not allowed:

 *    <meta_dev> -

 *

 * This code parses those words.  If there is a failure,

 * the caller must use raid_set_free() to unwind the operations.

 Put off the number of raid devices argument to get to dev pairs */

		/*

		 * There are no offsets initially.

		 * Out of place reshape will set them accordingly.

		/*

		 * Without metadata, we will not be able to tell if the array

		 * is in-sync or not - we must assume it is not.  Therefore,

		 * it is impossible to rebuild a drive.

		 *

		 * Even if there is metadata, the on-disk information may

		 * indicate that the array is not in-sync and it will then

		 * fail at that time.

		 *

		 * User could specify 'nosync' option if desperate.

/*

 * validate_region_size

 * @rs

 * @region_size:  region size in sectors.  If 0, pick a size (4MiB default).

 *

 * Set rs->md.bitmap_info.chunksize (which really refers to 'region size').

 * Ensure that (ti->len/region_size < 2^21) - required by MD bitmap.

 *

 * Returns: 0 on success, -EINVAL on failure.

		/*

		 * Choose a reasonable default.	 All figures in sectors.

 If not a power of 2, make it the next power of 2 */

 sectors */

		/*

		 * Validate user-supplied value.

	/*

	 * Convert sectors to bytes.

/*

 * validate_raid_redundancy

 * @rs

 *

 * Determine if there are enough devices in the array that haven't

 * failed (or are being rebuilt) to form a usable array.

 *

 * Returns: 0 on success, -EINVAL on failure.

		/*

		 * It is possible to have a higher rebuild count for RAID10,

		 * as long as the failed devices occur in different mirror

		 * groups (i.e. different stripes).

		 *

		 * When checking "near" format, make sure no adjacent devices

		 * have failed beyond what can be handled.  In addition to the

		 * simple case where the number of devices is a multiple of the

		 * number of copies, we must also handle cases where the number

		 * of devices is not a multiple of the number of copies.

		 * E.g.	   dev1 dev2 dev3 dev4 dev5

		 *	    A	 A    B	   B	C

		 *	    C	 D    D	   E	E

		/*

		 * When checking "far" and "offset" formats, we need to ensure

		 * that the device that holds its copy is not also dead or

		 * being rebuilt.  (Note that "far" and "offset" formats only

		 * support two copies right now.  These formats also only ever

		 * use the 'use_far_sets' variant.)

		 *

		 * This check is somewhat complicated by the need to account

		 * for arrays that are not a multiple of (far) copies.	This

		 * results in the need to treat the last (potentially larger)

		 * set differently.

/*

 * Possible arguments are...

 *	<chunk_size> [optional_args]

 *

 * Argument definitions

 *    <chunk_size>			The number of sectors per disk that

 *					will form the "stripe"

 *    [[no]sync]			Force or prevent recovery of the

 *					entire array

 *    [rebuild <idx>]			Rebuild the drive indicated by the index

 *    [daemon_sleep <ms>]		Time between bitmap daemon work to

 *					clear bits

 *    [min_recovery_rate <kB/sec/disk>]	Throttle RAID initialization

 *    [max_recovery_rate <kB/sec/disk>]	Throttle RAID initialization

 *    [write_mostly <idx>]		Indicate a write mostly drive via index

 *    [max_write_behind <sectors>]	See '-write-behind=' (man mdadm)

 *    [stripe_cache <sectors>]		Stripe cache size for higher RAIDs

 *    [region_size <sectors>]		Defines granularity of bitmap

 *    [journal_dev <dev>]		raid4/5/6 journaling deviice

 *    					(i.e. write hole closing log)

 *

 * RAID10-only options:

 *    [raid10_copies <# copies>]	Number of copies.  (Default: 2)

 *    [raid10_format <near|far|offset>] Layout algorithm.  (Default: near)

 Account for chunk_size argument */

	/*

	 * First, parse the in-order required arguments

	 * "chunk_size" is the only argument of this type.

	/*

	 * We set each individual device as In_sync with a completed

	 * 'recovery_offset'.  If there has been a device failure or

	 * replacement then one of the following cases applies:

	 *

	 *   1) User specifies 'rebuild'.

	 *	- Device is reset when param is read.

	 *   2) A new device is supplied.

	 *	- No matching superblock found, resets device.

	 *   3) Device failure was transient and returns on reload.

	 *	- Failure noticed, resets device for bitmap replay.

	 *   4) Device hadn't completed recovery after previous failure.

	 *	- Superblock is read and overrides recovery_offset.

	 *

	 * What is found in the superblocks of the devices is always

	 * authoritative, unless 'rebuild' or '[no]sync' was specified.

	/*

	 * Second, parse the unordered optional arguments

 Account for the argument pairs */

		/*

		 * Parameters that take a string value are checked here.

 "raid10_format {near|offset|far} */

 "journal_dev <dev>" */

 "journal_mode <mode>" ("journal_dev" mandatory!) */

		/*

		 * Parameters with number values from here on.

			/*

			 * "rebuild" is being passed in by userspace to provide

			 * indexes of replaced devices and to set up additional

			 * devices on raid level takeover.

			/*

			 * In device-mapper, we specify things in sectors, but

			 * MD records this value in kB

 Userspace passes new data_offset after having extended the the data image LV */

 Ensure sensible data offset */

 Define the +/-# of disks to add to/remove from the given raid set */

 Ensure MAX_RAID_DEVICES and raid type minimal_devs! */

 Assume there are no metadata devices until the drives are parsed */

 Check, if any invalid ctr arguments have been passed in for the raid level */

 Set raid4/5/6 cache size */

 Try setting number of stripes in raid456 stripe cache */

 Return # of data stripes as kept in mddev as of @rs (i.e. as of superblock) */

 Return # of data stripes of @rs (i.e. as of ctr) */

/*

 * Retrieve rdev->sectors from any valid raid device of @rs

 * to allow userpace to pass in arbitray "- -" device tupples.

 Check that calculated dev_sectors fits all component devices. */

 Calculate the sectors per device and per array used for @rs */

 Special raid1 case w/o delta_disks support (yet) */

 Striped layouts */

 Setup recovery on @rs */

 raid0 does not recover */

	/*

	 * A raid6 set has to be recovered either

	 * completely or for the grown part to

	 * ensure proper parity and Q-Syndrome

	/*

	 * Other raid set types may skip recovery

	 * depending on the 'nosync' flag.

 Make sure we access most actual mddev properties */

/*

 * Make sure a valid takover (level switch) is being requested on @rs

 *

 * Conversions of raid sets from one MD personality to another

 * have to conform to restrictions which are enforced here.

 raid0 -> raid1/5 with one disk */

 raid0 -> raid10 */

 raid0 with multiple disks -> raid4/5/6 */

 Can't takeover raid10_offset! */

 raid10* -> raid0 */

 Can takeover raid10_near with raid disks divisable by data copies! */

 Can takeover raid10_far */

 raid10_{near,far} -> raid1 */

 raid10_{near,far} with 2 disks -> raid4/5 */

 raid1 with 2 disks -> raid4/5 */

 raid1 -> raid0 */

 raid1 -> raid10 */

 raid4 -> raid0 */

 raid4 -> raid1/5 with 2 disks */

 raid4 -> raid5/6 with parity N */

 raid5 with parity N -> raid0 */

 raid5 with parity N -> raid4 */

 raid5 with 2 disks -> raid1/4/10 */

 raid5_* ->  raid6_*_6 with Q-Syndrome N (e.g. raid5_ra -> raid6_ra_6 */

 raid6 with parity N -> raid0 */

 raid6 with parity N -> raid4 */

 raid6_*_n with Q-Syndrome N -> raid5_* */

 True if @rs requested to be taken over */

 True if layout is set to reshape. */

 True if @rs is requested to reshape by ctr */

 Historical case to support raid1 reshape without delta disks */

  Features */

 Supports extended superblock */

 State flags for sb->flags */

/*

 * This structure is never routinely used by userspace, unlike md superblocks.

 * Devices with this superblock should only ever be accessed via device-mapper.

 "DmRd" */

 Used to indicate compatible features (like 1.9.0 ondisk metadata extension) */

 Number of devices in this raid set. (Max 64) */

 The position of this drive in the raid set */

 Incremented by md when superblock updated */

 Pre 1.9.0 part of bit field of devices to */

 indicate failures (see extension below) */

	/*

	 * This offset tracks the progress of the repair or replacement of

	 * an individual drive.

	/*

	 * This offset tracks the progress of the initial raid set

	 * synchronisation/parity calculation.

	/*

	 * raid characteristics

	/********************************************************************

	 * BELOW FOLLOW V1.9.0 EXTENSIONS TO THE PRISTINE SUPERBLOCK FORMAT!!!

	 *

	 * FEATURE_FLAG_SUPPORTS_V190 in the compat_features member indicates that those exist

 Flags defining array states for reshaping */

	/*

	 * This offset tracks the progress of a raid

	 * set reshape in order to be able to restart it

	/*

	 * These define the properties of the array in case of an interrupted reshape

 Array size in sectors */

	/*

	 * Sector offsets to data on devices (reshaping).

	 * Needed to support out of place reshaping, thus

	 * not writing over any stripes whilst converting

	 * them from old to new layout

 Used device size in sectors */

	/*

	 * Additonal Bit field of devices indicating failures to support

	 * up to 256 devices with the 1.9.0 on-disk metadata format

 Used to indicate any incompatible features */

 Always set rest up to logical block size to 0 when writing (see get_metadata_device() below). */

/*

 * Check for reshape constraints on raid set @rs:

 *

 * - reshape function non-existent

 * - degraded set

 * - ongoing recovery

 * - ongoing reshape

 *

 * Returns 0 if none or -EPERM if given constraint

 * and error message reference in @errmsg

/*

 * Synchronize the superblock members with the raid set properties

 *

 * All superblock data is little endian.

 No metadata device, no superblock */

	/********************************************************************

	 * BELOW FOLLOW V1.9.0 EXTENSIONS TO THE PRISTINE SUPERBLOCK FORMAT!!!

	 *

	 * FEATURE_FLAG_SUPPORTS_V190 in the compat_features member indicates that those exist

 Make sure we access most recent reshape position */

 Flag ongoing reshape */

 Clear reshape flags */

 Zero out the rest of the payload after the size of the superblock */

/*

 * super_load

 *

 * This function creates a superblock if one is not found on the device

 * and will decide which superblock to use if there's a choice.

 *

 * Return: 1 if use rdev, 0 if use refdev, -Exxx otherwise

	/*

	 * Two cases that we want to write new superblocks and rebuild:

	 * 1) New device (no matching magic number)

	 * 2) Device specified for rebuild (!In_sync w/ offset == 0)

 Force writing of superblocks to disk */

 Any superblock is better than none, choose that if given */

	/*

	 * Initialise to 1 if this is a new superblock.

	/*

	 * Reshaping is supported, e.g. reshape_position is valid

	 * in superblock and superblock content is authoritative.

 Superblock is authoritative wrt given raid set layout! */

 raid was reshaping and got interrupted */

		/*

		 * No takeover/reshaping, because we don't have the extended v1.9.0 metadata

	/*

	 * During load, we set FirstUse if a new superblock was written.

	 * There are two reasons we might not have a superblock:

	 * 1) The raid set is brand new - in which case, all of the

	 *    devices must have their In_sync bit set.	Also,

	 *    recovery_cp must be 0, unless forced.

	 * 2) This is a new device being added to an old raid set

	 *    and the new device needs to be rebuilt - in which

	 *    case the In_sync bit will /not/ be set and

	 *    recovery_cp must be MaxSector.

	 * 3) This is/are a new device(s) being added to an old

	 *    raid set during takeover to a higher raid level

	 *    to provide capacity for redundancy or during reshape

	 *    to add capacity to grow the raid set.

 Replace a broken device */

	/*

	 * Now we set the Faulty bit for those devices that are

	 * recorded in the superblock as failed.

		/*

		 * Check for any device re-ordering.

			/*

			 * Partial recovery is performed on

			 * returning failed devices.

	/*

	 * If mddev->events is not set, we know we have not yet initialized

	 * the array.

 Enable bitmap creation on @rs unless no metadevs or raid0 or journaled raid4/5/6 set. */

		/*

		 * Retrieve rdev size stored in superblock to be prepared for shrink.

		 * Check extended superblock members are present otherwise the size

		 * will not be set!

		/*

		 * If no reshape in progress -> we're recovering single

		 * disk(s) and have to set the device(s) to out-of-sync

 Mandatory for recovery */

	/*

	 * If a device comes back, set it as not In_sync and no longer faulty.

 Reshape support -> restore repective data offsets */

/*

 * Analyse superblocks and select the freshest.

 Set superblock offset/size for metadata device. */

		/*

		 * Skipping super_load due to CTR_FLAG_SYNC will cause

		 * the array to undergo initialization again as

		 * though it were new.	This is the intended effect

		 * of the "sync" directive.

		 *

		 * With reshaping capability added, we must ensure that

		 * that the "sync" directive is disallowed during the reshape.

 This is a failure to read the superblock from the metadata device. */

			/*

			 * We have to keep any raid0 data/metadata device pairs or

			 * the MD raid0 personality will fail to start the array.

			/*

			 * We keep the dm_devs to be able to emit the device tuple

			 * properly on the table line in raid_status() (rather than

			 * mistakenly acting as if '- -' got passed into the constructor).

			 *

			 * The rdev has to stay on the same_set list to allow for

			 * the attempt to restore faulty devices on second resume.

	/*

	 * Validation of the freshest device provides the source of

	 * validation for the remaining devices.

/*

 * Adjust data_offset and new_data_offset on all disk members of @rs

 * for out of place reshaping if requested by contructor

 *

 * We need free space at the beginning of each raid disk for forward

 * and at the end for backward reshapes which userspace has to provide

 * via remapping/reordering of space.

 Constructor did not request data offset change */

 HM FIXME: get In_Sync raid_dev? */

		/*

		 * Removing disks (reshaping backwards):

		 *

		 * - before reshape: data is at offset 0 and free space

		 *		     is at end of each component LV

		 *

		 * - after reshape: data is at offset rs->data_offset != 0 on each component LV

		/*

		 * Adding disks (reshaping forwards):

		 *

		 * - before reshape: data is at offset rs->data_offset != 0 and

		 *		     free space is at begin of each component LV

		 *

		 * - after reshape: data is at offset 0 on each component LV

		/*

		 * User space passes in 0 for data offset after having removed reshape space

		 *

		 * - or - (data offset != 0)

		 *

		 * Changing RAID layout or chunk size -> toggle offsets

		 *

		 * - before reshape: data is at offset rs->data_offset 0 and

		 *		     free space is at end of each component LV

		 *		     -or-

		 *                   data is at offset rs->data_offset != 0 and

		 *		     free space is at begin of each component LV

		 *

		 * - after reshape: data is at offset 0 if it was at offset != 0

		 *                  or at offset != 0 if it was at offset 0

		 *                  on each component LV

		 *

	/*

	 * Make sure we got a minimum amount of free sectors per device

	/*

	 * Raise recovery_cp in case data_offset != 0 to

	 * avoid false recovery positives in the constructor.

 Adjust data offsets on all rdevs but on any raid4/5/6 journal device */

 Userpace reordered disks -> adjust raid_disk indexes in @rs */

/*

 * Setup @rs for takeover by a different raid level

 Userpace reordered disks -> adjust raid_disk indexes */

 raid0 -> raid10_far layout */

 raid1 -> raid10_near layout */

 Bitmap has to be created when we do an "up" takeover */

 Prepare @rs for reshape */

			/*

			 * raid disk have to be multiple of data copies to allow this conversion,

			 *

			 * This is actually not a reshape it is a

			 * rebuild of any additional mirrors per group

 Userpace reordered disks to add/remove mirrors -> adjust raid_disk indexes */

 Process raid1 via delta_disks */

 Process raid1 without delta_disks */

 Create new superblocks and bitmaps, if any new disks */

 Get reshape sectors from data_offsets or raid set */

/*

 * Reshape:

 * - change raid layout

 * - change chunk size

 * - add disks

 * - remove disks

 Ignore impossible layout change whilst adding/removing disks */

	/*

	 * Adjust array size:

	 *

	 * - in case of adding disk(s), array size has

	 *   to grow after the disk adding reshape,

	 *   which'll hapen in the event handler;

	 *   reshape will happen forward, so space has to

	 *   be available at the beginning of each disk

	 *

	 * - in case of removing disk(s), array size

	 *   has to shrink before starting the reshape,

	 *   which'll happen here;

	 *   reshape will happen backward, so space has to

	 *   be available at the end of each disk

	 *

	 * - data_offset and new_data_offset are

	 *   adjusted for aforementioned out of place

	 *   reshaping based on userspace passing in

	 *   the "data_offset <sectors>" key/value

	 *   pair via the constructor

 Add disk(s) */

 Prepare disks for check in raid4/5/6/10 {check|start}_reshape */

			/*

			 * save_raid_disk needs to be -1, or recovery_offset will be set to 0

			 * by md, which'll store that erroneously in the superblock on reshape

 adding disk(s) -> forward reshape */

 Remove disk(s) */

 removing disk(s) -> backward reshape */

 Change layout and/or chunk size */

		/*

		 * Reshape layout (e.g. raid5_ls -> raid5_n) and/or chunk size:

		 *

		 * keeping number of disks and do layout change ->

		 *

		 * toggle reshape_backward depending on data_offset:

		 *

		 * - free space upfront -> reshape forward

		 *

		 * - free space at the end -> reshape backward

		 *

		 *

		 * This utilizes free reshape space avoiding the need

		 * for userspace to move (parts of) LV segments in

		 * case of layout/chunksize change  (for disk

		 * adding/removing reshape space has to be at

		 * the proper address (see above with delta_disks):

		 *

		 * add disk(s)   -> begin

		 * remove disk(s)-> end

	/*

	 * Adjust device size for forward reshape

	 * because md_finish_reshape() reduces it.

/*

 * If the md resync thread has updated superblock with max reshape position

 * at the end of a reshape but not (yet) reset the layout configuration

 * changes -> reset the latter.

/*

 * Enable/disable discard support on RAID set depending on

 * RAID level and discard properties of underlying RAID members.

	/*

	 * XXX: RAID level 4,5,6 require zeroing for safety.

/*

 * Construct a RAID0/1/10/4/5/6 mapping:

 * Args:

 *	<raid_type> <#raid_params> <raid_params>{0,}	\

 *	<#raid_devs> [<meta_dev1> <dev1>]{1,}

 *

 * <raid_params> varies by <raid_type>.	 See 'parse_raid_params' for

 * details on possible <raid_params>.

 *

 * Userspace is free to initialize the metadata devices, hence the superblocks to

 * enforce recreation based on the passed in table parameters.

 *

 Must have <#raid_params> */

 number of raid device tupples <meta_dev data_dev> */

	/*

	 * Calculate ctr requested array and device sizes to allow

	 * for superblock analysis needing device sizes defined.

	 *

	 * Any existing superblock will overwrite the array and device sizes

 Memorize just calculated, potentially larger sizes to grow the raid set in preresume */

	/*

	 * Backup any new raid set level, layout, ...

	 * requested to be able to compare to superblock

	 * members for conversion decisions.

 All in-core metadata now as of current superblocks after calling analyse_superblocks() */

 Restore any requested new layout for conversion decision */

	/*

	 * Now that we have any superblock metadata available,

	 * check for new, recovering, reshaping, to be taken over,

	 * to be reshaped or an existing, unchanged raid set to

	 * run in sequence.

 A new raid6 set has to be recovered to ensure proper parity and Q-Syndrome */

 A recovering raid set may be resized */

 Have to reject size change request during reshape */

 skip setup rs */

 We can't takeover a journaled raid4/5/6 */

		/*

		 * If a takeover is needed, userspace sets any additional

		 * devices to rebuild and we can check for a valid request here.

		 *

		 * If acceptible, set the level to the new requested

		 * one, prohibit requesting recovery, allow the raid

		 * set to run and store superblocks during resume.

 Takeover ain't recovery, so disable recovery */

 Only request grow on raid set size extensions, not on reshapes. */

		/*

		 * No need to check for 'ongoing' takeover here, because takeover

		 * is an instant operation as oposed to an ongoing reshape.

 We can't reshape a journaled raid4/5/6 */

 Out-of-place space has to be available to allow for a reshape unless raid1! */

			/*

			  * We can only prepare for a reshape here, because the

			  * raid set needs to run to provide the repective reshape

			  * check functions via its MD personality instance.

			  *

			  * So do the reshape check after md_run() succeeded.

 Reshaping ain't recovery, so disable recovery */

 May not set recovery when a device rebuild is requested */

			/*

			 * Set raid set to current size, i.e. size as of

			 * superblocks to grow to larger size in preresume.

 This is no size change or it is shrinking, update size and record in superblocks */

 If constructor requested it, change data and new_data offsets */

 Catch any inconclusive reshape superblock content. */

 Start raid set read-only and assumed clean to change in raid_resume() */

 Keep array frozen until resume. */

 Has to be held on running the array */

 Assume already marked dirty */

 If raid4/5/6 journal mode explicitly requested (only possible with journal dev) -> set it */

 Try to adjust the raid4/5/6 stripe cache size to the stripe size */

 Now do an early reshape check */

 Restore new, ctr requested layout to perform check */

 Disable/enable discard support on raid set. */

	/*

	 * If we're reshaping to add disk(s)), ti->len and

	 * mddev->array_sectors will differ during the process

	 * (ti->len > mddev->array_sectors), so we have to requeue

	 * bios with addresses > mddev->array_sectors here or

	 * there will occur accesses past EOD of the component

	 * data images thus erroring the raid set.

 Return sync state string for @state */

 Has to be in above sync_state order! */

 Return enum sync_state for @mddev derived from @recovery flags */

 The MD sync thread can be done with io or be interrupted but still be running */

/*

 * Return status string for @rdev

 *

 * Status characters:

 *

 *  'D' = Dead/Failed raid set component or raid4/5/6 journal device

 *  'a' = Alive but not in-sync raid set component _or_ alive raid4/5/6 'write_back' journal device

 *  'A' = Alive and in-sync raid set component _or_ alive raid4/5/6 'write_through' journal device

 *  '-' = Non-existing device (i.e. uspace passed '- -' into the ctr)

 Helper to return resync/reshape progress for @rs and runtime flags for raid set in sync / resynching */

			/*

			 * Sync complete.

 In case we have finished recovering, the array is in sync. */

			/*

			 * In case we are recovering, the array is not in sync

			 * and health chars should show the recovering legs.

			 *

			 * Already retrieved recovery offset from curr_resync_completed above.

			/*

			 * If "resync/reshape" is occurring, the raid set

			 * is or may be out of sync hence the health

			 * characters shall be 'a'.

			/*

			 * If "check" or "repair" is occurring, the raid set has

			 * undergone an initial sync and the health characters

			 * should not be 'a' anymore.

			/*

			 * We are idle and recovery is needed, prevent 'A' chars race

			 * caused by components still set to in-sync by constructor.

			/*

			 * We are idle and the raid set may be doing an initial

			 * sync, or it may be rebuilding individual components.

			 * If all the devices are In_sync, then it is the raid set

			 * that is being initialized.

 Helper to return @dev name or "-" if !@dev */

 at least 1 for chunksize */

 *Should* always succeed */

 Access most recent mddev properties for status output */

 Get sensible max sectors even if raid set not yet started */

 HM FIXME: do we want another state char for raid0? It shows 'D'/'A'/'-' now */

		/*

		 * In-sync/Reshape ratio:

		 *  The in-sync ratio shows the progress of:

		 *   - Initializing the raid set

		 *   - Rebuilding a subset of devices of the raid set

		 *  The user can distinguish between the two by referring

		 *  to the status characters.

		 *

		 *  The reshape ratio shows the progress of

		 *  changing the raid layout or the number of

		 *  disks of a raid set

		/*

		 * v1.5.0+:

		 *

		 * Sync action:

		 *   See Documentation/admin-guide/device-mapper/dm-raid.rst for

		 *   information on each of these states.

		/*

		 * v1.5.0+:

		 *

		 * resync_mismatches/mismatch_cnt

		 *   This field shows the number of discrepancies found when

		 *   performing a "check" of the raid set.

		/*

		 * v1.9.0+:

		 *

		 * data_offset (needed for out of space reshaping)

		 *   This field shows the data offset into the data

		 *   image LV where the first stripes data starts.

		 *

		 * We keep data_offset equal on all raid disks of the set,

		 * so retrieving it from the first raid disk is sufficient.

		/*

		 * v1.10.0+:

 Report the table line string you would use to construct this raid set */

		/*

		 * Count any rebuild or writemostly argument pairs and subtract the

		 * hweight count being added below of any rebuild and writemostly ctr flags.

 Calculate raid parameter count based on ^ rebuild/writemostly argument counts and ctr flags set. */

 Emit table line */

 This has to be in the documented order for userspace! */

 Access most recent mddev properties for status output */

 MD_RECOVERY_NEEDED set below */

		/* A write to sync_action is enough to justify

		 * canceling read-auto mode

 No offset on data devs */

 Writes have to be stopped before suspending to avoid deadlocks. */

 RAID personalities have to provide hot add/remove methods or we need to bail out. */

 HM FIXME: enhance journal device recovery processing */

			/*

			 * Faulty bit may be set, but sometimes the array can

			 * be suspended before the personalities can respond

			 * by removing the device from the array (i.e. calling

			 * 'hot_remove_disk').	If they haven't yet removed

			 * the failed device, its 'raid_disk' number will be

			 * '>= 0' - meaning we must call this function

			 * ourselves.

 Mandatory for hot remove. */

 Failed to revive this device, try next */

 Failed to revive this device, try next */

 If any failed devices could be cleared, update all sbs failed_devices bits */

 Try loading the bitmap unless "raid0", which does not have one */

 Enforce updating all superblocks */

/*

 * Reshape changes raid algorithm of @rs to new one within personality

 * (e.g. raid6_zr -> raid6_nc), changes stripe size, adds/removes

 * disks from a raid set thus growing/shrinking it or resizes the set

 *

 * Call mddev_lock_nointr() before!

 Don't allow the sync thread to work until the table gets reloaded. */

	/*

	 * Check any reshape constraints enforced by the personalility

	 *

	 * May as well already kick the reshape off so that * pers->start_reshape() becomes optional.

	/*

	 * Personality may not provide start reshape method in which

	 * case check_reshape above has already covered everything

	/*

	 * Now reshape got set up, update superblocks to

	 * reflect the fact so that a table reload will

	 * access proper superblock content in the ctr.

 This is a resume after a suspend of the set -> it's already started. */

	/*

	 * The superblocks need to be updated on disk if the

	 * array is new or new devices got added (thus zeroed

	 * out by userspace) or __load_dirty_region_bitmap

	 * will overwrite them in core with old data or fail.

 Load the bitmap from disk unless raid0 */

 We are extending the raid set size, adjust mddev/md_rdev sizes and set capacity. */

 Resize bitmap to adjust to changed region size (aka MD bitmap chunksize) or grown device size */

 Check for any resize/reshape on @rs and adjust/initiate */

 Be prepared for mddev_resume() in raid_resume() */

 Check for any reshape request unless new raid set */

 Initiate a reshape. */

		/*

		 * A secondary resume while the device is active.

		 * Take this opportunity to check whether any failed

		 * devices are reachable again.

 Only reduce raid set size before running a disk removing reshape. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * raid10.c : Multiple Devices driver for Linux

 *

 * Copyright (C) 2000-2004 Neil Brown

 *

 * RAID-10 support for md.

 *

 * Base on code in raid1.c.  See raid1.c for further copyright information.

/*

 * RAID10 provides a combination of RAID0 and RAID1 functionality.

 * The layout of data is defined by

 *    chunk_size

 *    raid_disks

 *    near_copies (stored in low byte of layout)

 *    far_copies (stored in second byte of layout)

 *    far_offset (stored in bit 16 of layout )

 *    use_far_sets (stored in bit 17 of layout )

 *    use_far_sets_bugfixed (stored in bit 18 of layout )

 *

 * The data to be stored is divided into chunks using chunksize.  Each device

 * is divided into far_copies sections.   In each section, chunks are laid out

 * in a style similar to raid0, but near_copies copies of each chunk is stored

 * (each on a different drive).  The starting device for each section is offset

 * near_copies from the starting device of the previous section.  Thus there

 * are (near_copies * far_copies) of each chunk, and each is on a different

 * drive.  near_copies and far_copies must be at least one, and their product

 * is at most raid_disks.

 *

 * If far_offset is true, then the far_copies are handled a bit differently.

 * The copies are still in different stripes, but instead of being very far

 * apart on disk, there are adjacent stripes.

 *

 * The far and offset algorithms are handled slightly differently if

 * 'use_far_sets' is true.  In this case, the array's devices are grouped into

 * sets that are (near_copies * far_copies) in size.  The far copied stripes

 * are still shifted by 'near_copies' devices, but this shifting stays confined

 * to the set rather than the entire array.  This is done to improve the number

 * of device combinations that can fail without causing the array to fail.

 * Example 'far' algorithm w/o 'use_far_sets' (each letter represents a chunk

 * on a device):

 *    A B C D    A B C D E

 *      ...         ...

 *    D A B C    E A B C D

 * Example 'far' algorithm w/ 'use_far_sets' enabled (sets illustrated w/ []'s):

 *    [A B] [C D]    [A B] [C D E]

 *    |...| |...|    |...| | ... |

 *    [B A] [D C]    [B A] [E C D]

/*

 * for resync bio, r10bio pointer can be retrieved from the per-bio

 * 'struct resync_pages'.

	/* allocate a r10bio with room for raid_disks entries in the

 amount of memory to reserve for resync requests */

 maximum number of concurrent requests, memory permitting */

/*

 * When performing a resync, we need to read and compare, so

 * we need as many pages are there are copies.

 * When performing a recovery, we need 2 bios, one for read,

 * one for write (we recover only one drive per r10buf)

 *

 resync */

 recovery */

 allocate once for all bios */

	/*

	 * Allocate bios.

	/*

	 * Allocate RESYNC_PAGES data pages and attach them

	 * where needed.

 resync pages array stored in the 1st bio's .bi_private */

 wake up frozen array... */

/*

 * raid_end_bio_io() is called when we have finished servicing a mirrored

 * operation and are ready to return a success/failure code to the buffer

 * cache layer.

	/*

	 * Wake up any possible resync thread that waits for the device

	 * to go idle.

/*

 * Update disk head position estimator based on IRQ completion info.

/*

 * Find the disk number which triggered given bio

	/*

	 * this branch is our 'one mirror IO has finished' event handler:

		/*

		 * Set R10BIO_Uptodate in our master bio, so that

		 * we will return a good error code to the higher

		 * levels even if IO on some other mirrored buffer fails.

		 *

		 * The 'master' represents the composite IO operation to

		 * user-side. So if something waits for IO, then it will

		 * wait for the 'master' bio.

		/* If all other devices that store this block have

		 * failed, we want to return the error upwards rather

		 * than fail the last device.  Here we redefine

		 * "uptodate" to mean "Don't want to retry"

		/*

		 * oops, read error - keep the refcount on the rdev

 clear the bitmap if all writes complete successfully */

	/*

	 * this branch is our 'one mirror IO has finished' event handler:

			/* Never record new bad blocks to replacement,

			 * just fail it.

			/*

			 * When the device is faulty, it is not necessary to

			 * handle write error.

 Fail the request */

		/*

		 * Set R10BIO_Uptodate in our master bio, so that

		 * we will return a good error code for to the higher

		 * levels even if IO on some other mirrored buffer fails.

		 *

		 * The 'master' represents the composite IO operation to

		 * user-side. So if something waits for IO, then it will

		 * wait for the 'master' bio.

		/*

		 * Do not set R10BIO_Uptodate if the current device is

		 * rebuilding or Faulty. This is because we cannot use

		 * such device for properly reading the data back (we could

		 * potentially use it, if the current write would have felt

		 * before rdev->recovery_offset, but for simplicity we don't

		 * check this here.

 Maybe we can clear some bad blocks. */

	/*

	 *

	 * Let's see if all mirrored write operations have finished

	 * already.

/*

 * RAID10 layout manager

 * As well as the chunksize and raid_disks count, there are two

 * parameters: near_copies and far_copies.

 * near_copies * far_copies must be <= raid_disks.

 * Normally one of these will be 1.

 * If both are 1, we get raid0.

 * If near_copies == raid_disks, we get raid1.

 *

 * Chunks are laid out in raid0 style with near_copies copies of the

 * first chunk, followed by near_copies copies of the next chunk and

 * so on.

 * If far_copies > 1, then after 1/far_copies of the array has been assigned

 * as described above, we start again with a device offset of near_copies.

 * So we effectively have another copy of the whole array further down all

 * the drives, but with blocks on different drives.

 * With this layout, and block is never stored twice on the one device.

 *

 * raid10_find_phys finds the sector offset of a given virtual sector

 * on each device that it is on.

 *

 * raid10_find_virt does the reverse mapping, from a device and a

 * sector offset to a virtual address

 now calculate first sector/dev */

 and calculate all the others */

	/* Never use conf->prev as this is only called during resync

	 * or recovery, so reshape isn't happening

/*

 * This routine returns the disk from which the requested read should

 * be done. There is a per-array 'next expected sequential IO' sector

 * number - if this matches on the next IO then we use the last disk.

 * There is also a per-disk 'last know head position' sector that is

 * maintained from IRQ contexts, both the normal and the resync IO

 * completion handlers update this position correctly. If there is no

 * perfect sequential match then we pick the disk whose head is closest.

 *

 * If there are 2 mirrors in the same 2 devices, performance degrades

 * because position is mirror, not device based.

 *

 * The rdev for the device selected will have nr_pending incremented.

/*

 * FIXME: possibly should rethink readbalancing and do it differently

 * depending on near_copies / far_copies geometry.

	/*

	 * Check if we can balance. We can balance on the whole

	 * device if no resync is going on (recovery is ok), or below

	 * the resync window. We take the first readable disk when

	 * above the resync window.

 Already have a better slot */

				/* Cannot read here.  If this is the

				 * 'primary' device, then we must not read

				 * beyond 'bad_sectors' from another device.

 Must read from here */

 At least 2 disks to choose from so failfast is OK */

		/* This optimisation is debatable, and completely destroys

		 * sequential read speed for 'far copies' arrays.  So only

		 * keep it for 'near' arrays, and review those later.

 for far > 1 always use the lowest address */

	/* Any writes that have been queued but are awaiting

	 * bitmap updates get flushed here.

		/*

		 * As this is called in a wait_event() loop (see freeze_array),

		 * current->state might be TASK_UNINTERRUPTIBLE which will

		 * cause a warning when we prepare to wait again.  As it is

		 * rare that this path is taken, it is perfectly safe to force

		 * us to go around the wait_event() loop again, so the warning

		 * is a false-positive. Silence the warning by resetting

		 * thread state

		/* flush any pending bitmap writes to disk

 submit pending writes */

 Just ignore it */

/* Barriers....

 * Sometimes we need to suspend IO while we do something else,

 * either some resync/recovery, or reconfigure the array.

 * To do this we raise a 'barrier'.

 * The 'barrier' is a counter that can be raised multiple times

 * to count how many activities are happening which preclude

 * normal IO.

 * We can only raise the barrier if there is no pending IO.

 * i.e. if nr_pending == 0.

 * We choose only to raise the barrier if no-one is waiting for the

 * barrier to go down.  This means that as soon as an IO request

 * is ready, no other operations which require a barrier will start

 * until the IO request has had a chance.

 *

 * So: regular IO calls 'wait_barrier'.  When that returns there

 *    is no backgroup IO happening,  It must arrange to call

 *    allow_barrier when it has finished its IO.

 * backgroup IO calls must call raise_barrier.  Once that returns

 *    there is no normal IO happeing.  It must arrange to call

 *    lower_barrier when the particular background IO completes.

 Wait until no block IO is waiting (unless 'force') */

 block any new IO from starting */

 Now wait for all pending IO to complete */

		/* Wait for the barrier to drop.

		 * However if there are already pending

		 * requests (preventing the barrier from

		 * rising completely), and the

		 * pre-process bio queue isn't empty,

		 * then don't wait, as we need to empty

		 * that queue to get the nr_pending

		 * count down.

				     /* move on if recovery thread is

				      * blocked by us

	/* stop syncio and normal IO and wait for everything to

	 * go quiet.

	 * We increment barrier and nr_waiting, and then

	 * wait until nr_pending match nr_queued+extra

	 * This is called in the context of one normal IO request

	 * that has failed. Thus any sync request that might be pending

	 * will be blocked by nr_pending, and we need to wait for

	 * pending IO requests to complete or be queued for re-try.

	 * Thus the number queued (nr_queued) plus this request (extra)

	 * must match the number of pending IOs (nr_pending) before

	 * we continue.

 reverse the effect of the freeze */

 we aren't scheduling, so we can do the write-out directly. */

 submit pending writes */

 Just ignore it */

/*

 * 1. Register the new request and wait if the reconstruction thread has put

 * up a bar for new requests. Continue immediately if no resync is active

 * currently.

 * 2. If IO spans the reshape position.  Need to wait for reshape to pass.

		/*

		 * This is an error retry, but we cannot

		 * safely dereference the rdev in the r10_bio,

		 * we must use the one in conf.

		 * If it has already been disconnected (unlikely)

		 * we lose the device name in error messages.

		/*

		 * As we are blocking raid10, it is a little safer to

		 * use __GFP_HIGH.

 This never gets dereferenced */

 Replacement just got moved to main 'rdev' */

 flush_pending_writes() needs access to the rdev so...*/

			/*

			 * Discard request doesn't care the write result

			 * so it doesn't need to wait blocked disk here.

				/*

				 * Mustn't write here until the bad block

				 * is acknowledged

 Have to wait for this device to get unblocked, then retry */

 Need to update reshape_position in metadata */

	/* first select target devices under rcu_lock and

	 * inc refcount on their rdev.  Record them by setting

	 * bios[x] to bio

	 * If there are known/acknowledged bad blocks on any device

	 * on which we have seen a write error, we want to avoid

	 * writing to those blocks.  This potentially requires several

	 * writes to write around the bad blocks.  Each set of writes

	 * gets its own r10_bio with a set of bios attached.

 make sure repl_bio gets freed */

 Cannot write here at all */

					/* Mustn't write more than bad_sectors

					 * to other devices yet

				/* We don't set R10BIO_Degraded as that

				 * only applies if the disk is missing,

				 * so it might be re-added, and we want to

				 * know to recover this chunk.

				 * In this case the device is here, and the

				 * fact that this chunk is not in-sync is

				 * recorded in the bad block log.

	/*

	 * We don't care the return value of discard bio

		/*

		 * raid10_remove_disk uses smp_mb to make sure rdev is set to

		 * replacement before setting replacement to NULL. It can read

		 * rdev first without barrier protect even replacment is NULL

/*

 * There are some limitations to handle discard bio

 * 1st, the discard size is bigger than stripe_size*2.

 * 2st, if the discard bio spans reshape progress, we use the old way to

 * handle discard bio

	/*

	 * Check reshape again to avoid reshape happens after checking

	 * MD_RECOVERY_RESHAPE and before wait_barrier

	/*

	 * Maybe one discard bio is smaller than strip size or across one

	 * stripe and discard region is larger than one stripe size. For far

	 * offset layout, if the discard region is not aligned with stripe

	 * size, there is hole when we submit discard bio to member disk.

	 * For simplicity, we only handle discard bio which discard region

	 * is bigger than stripe_size * 2

	/*

	 * Keep bio aligned with strip size.

 Resend the fist split part */

 Resend the second split part */

	/*

	 * Raid10 uses chunk as the unit to store data. It's similar like raid0.

	 * One stripe contains the chunks from all member disk (one chunk from

	 * one disk at the same HBA address). For layout detail, see 'man md 4'

	/*

	 * For far layout it needs more than one r10bio to cover all regions.

	 * Inspired by raid10_sync_request, we can use the first r10bio->master_bio

	 * to record the discard bio. Other r10bio->master_bio record the first

	 * r10bio. The first r10bio only release after all other r10bios finish.

	 * The discard bio returns only first r10bio finishes

	/*

	 * first select target devices under rcu_lock and

	 * inc refcount on their rdev.  Record them by setting

	 * bios[x] to bio

		/*

		 * Now start to calculate the start and end address for each disk.

		 * The space between dev_start and dev_end is the discard region.

		 *

		 * For dev_start, it needs to consider three conditions:

		 * 1st, the disk is before start_disk, you can imagine the disk in

		 * the next stripe. So the dev_start is the start address of next

		 * stripe.

		 * 2st, the disk is after start_disk, it means the disk is at the

		 * same stripe of first disk

		 * 3st, the first disk itself, we can use start_disk_offset directly

		/*

		 * It only handles discard bio which size is >= stripe size, so

		 * dev_end > dev_start all the time.

		 * It doesn't need to use rcu lock to get rdev here. We already

		 * add rdev->nr_pending in the first loop.

	/*

	 * If this request crosses a chunk boundary, we need to split

	 * it.

 In case raid10d snuck in to freeze_array */

/* check if there are enough drives for

 * every block to appear on atleast one.

 * Don't consider the device numbered 'ignore'

 * as we might be about to remove it.

	/* when calling 'enough', both 'prev' and 'geo' must

	 * be stable.

	 * This is ensured if ->reconfig_mutex or ->device_lock

	 * is held.

	/*

	 * If it is not operational, then we have already marked it as dead

	 * else if it is the last working disks with "fail_last_dev == false",

	 * ignore the error, let the next level up know.

	 * else mark the drive as failed

		/*

		 * Don't fail the drive, just return an IO error.

	/*

	 * If recovery is running, make sure it aborts.

	/* This is only called with ->reconfix_mutex held, so

	/*

	 * Find all non-in_sync disks within the RAID10 configuration

	 * and mark them in_sync

 Replacement has just become active */

				/* Replaced device not technically faulty,

				 * but we need to be sure it gets removed

				 * and never re-added.

		/* only hot-add to in-sync arrays, as recovery is

		 * very different from resync

	/* Only remove non-faulty devices if recovery

	 * is not possible.

 lost the race, try later */

 We must have just cleared 'rdev' */

		smp_mb(); /* Make sure other CPUs may see both as identical

			   * but will never see neither -- if they are careful.

		/* The write handler will notice the lack of

		 * R10BIO_Uptodate and record any errors etc

	/* for reconstruct, we always reschedule after a read.

	 * for resync, only after all reads

		/* we have read all the blocks,

		 * do the comparison in process context in raid10d

 reshape read bio isn't allocated from r10buf_pool */

 the primary of several recovery bios */

/*

 * Note: sync and recover and handled very differently for raid10

 * This code is for resync.

 * For resync, we read through virtual addresses and read all blocks.

 * If there is any error, we schedule a write.  The lowest numbered

 * drive is authoritative.

 * However requests come for physical address, so we need to map.

 * For every physical address there are raid_disks/copies virtual addresses,

 * which is always are least one, but is not necessarly an integer.

 * This means that a physical address can span multiple chunks, so we may

 * have to submit multiple io requests for a single sync request.

/*

 * We check if all blocks are in-sync and only write to blocks that

 * aren't in sync

 find the first device with a block */

 now find blocks with errors */

			/* We know that the bi_io_vec layout is the same for

			 * both 'first' and 'i', so we just compare them.

			 * All vec entries are PAGE_SIZE;

 Don't fix anything. */

 Just give up on this device */

		/* Ok, we need to write this bio, either to correct an

		 * inconsistency or to correct an unreadable block.

		 * First we need to fixup bv_offset, bv_len and

		 * bi_vecs, as the read request might have corrupted these

	/* Now write out to any replacement devices

	 * that are active

/*

 * Now for the recovery code.

 * Recovery happens across physical sectors.

 * We recover all non-is_sync drives by finding the virtual address of

 * each, and then choose a working drive that also has that virt address.

 * There is a separate r10_bio for each non-in_sync drive.

 * Only the first two slots are in use. The first for reading,

 * The second for writing.

 *

	/* We got a read error during recovery.

	 * We repeat the read in smaller page-sized sections.

	 * If a read succeeds, write it to the new device or record

	 * a bad block if we cannot.

	 * If a read fails, record a bad block on both old and

	 * new devices.

			/* We don't worry if we cannot set a bad block -

			 * it really is bad so there is no loss in not

			 * recording it yet

 need bad block on destination too */

 just abort the recovery */

	/*

	 * share the pages with the first bio

	 * and submit the write request

	/* Need to test wbio2->bi_end_io before we call

	 * submit_bio_noacct as if the former is NULL,

	 * the latter is free to free wbio2.

/*

 * Used by fix_read_error() to decay the per rdev read_errors.

 * We halve the read error count for every hour that has elapsed

 * since the last recorded read error.

 *

 first time we've seen a read error */

	/*

	 * if hours_since_last is > the number of bits in read_errors

	 * just set read errors to 0. We do this to avoid

	 * overflowing the shift of read_errors by hours_since_last.

 success */

 need to record an error - either for the block or the device */

/*

 * This is a kernel thread which:

 *

 *	1.	Retries failed read operations on working mirrors.

 *	2.	Updates the raid superblock when problems encounter.

 *	3.	Performs writes following reads for array synchronising.

 Offset from r10_bio->sector */

	/* still own a reference to this rdev, so it cannot

	 * have been cleared recently.

		/* drive has already been failed, just ignore any

			/* Cannot read from anywhere, just mark the block

			 * as bad on the first device to discourage future

			 * reads.

 write it back and re-read */

 Well, this device is dead */

 Well, this device is dead */

	/* bio has the data to be written to slot 'i' where

	 * we just recently had a write error.

	 * We repeatedly clone the bio and trim down to one block,

	 * then try the write.  Where the write fails we record

	 * a bad block.

	 * It is conceivable that the bio doesn't exactly align with

	 * blocks.  We must handle this.

	 *

	 * We currently own a reference to the rdev.

 Write at 'sector' for 'sectors' */

 Failure! */

	/* we got a read error. Maybe the drive is bad.  Maybe just

	 * the block and we can fix it.

	 * We freeze all other IO, and try reading the block from

	 * other devices.  When we find one, we re-write

	 * and check it that fixes the read error.

	 * This is all done synchronously while the array is

	 * frozen.

	/* Some sort of write request has finished and it

	 * succeeded in writing where we thought there was a

	 * bad block.  So forget the bad block.

	 * Or possibly if failed and we need to record

	 * a bad block.

			/*

			 * In case freeze_array() is waiting for condition

			 * nr_pending == nr_queued + extra to be true.

 resync */

 recovery */

/*

 * Set cluster_sync_high since we need other nodes to add the

 * range [cluster_sync_low, cluster_sync_high] to suspend list.

	/*

	 * First, here we define "stripe" as a unit which across

	 * all member devices one time, so we get chunks by use

	 * raid_disks / near_copies. Otherwise, if near_copies is

	 * close to raid_disks, then resync window could increases

	 * linearly with the increase of raid_disks, which means

	 * we will suspend a really large IO window while it is not

	 * necessary. If raid_disks is not divisible by near_copies,

	 * an extra chunk is needed to ensure the whole "stripe" is

	 * covered.

	/*

	 * At least use a 32M window to align with raid1's resync window

/*

 * perform a "sync" on one "block"

 *

 * We need to make sure that no normal I/O request - particularly write

 * requests - conflict with active sync requests.

 *

 * This is achieved by tracking pending requests and a 'barrier' concept

 * that can be installed to exclude normal IO requests.

 *

 * Resync and recovery are handled very differently.

 * We differentiate by looking at MD_RECOVERY_SYNC in mddev->recovery.

 *

 * For resync, we iterate over virtual addresses, read all copies,

 * and update if there are differences.  If only one copy is live,

 * skip it.

 * For recovery, we iterate over physical addresses, read a good

 * value for each non-in_sync drive, and over-write.

 *

 * So, for recovery we may have several outstanding complex requests for a

 * given address, one for each out-of-sync device.  We model this by allocating

 * a number of r10_bio structures, one for each out-of-sync device.

 * As we setup these structures, we collect all bio's together into a list

 * which we then process collectively to add pages, and then process again

 * to pass to submit_bio_noacct.

 *

 * The r10_bio structures are linked using a borrowed master_bio pointer.

 * This link is counted in ->remaining.  When the r10_bio that points to NULL

 * has its remaining count decremented to 0, the whole complex operation

 * is complete.

 *

	/*

	 * Allow skipping a full rebuild for incremental assembly

	 * of a clean array, like RAID1 does.

		/* If we aborted, we need to abort the

		 * sync on the 'current' bitmap chucks (there can

		 * be several when recovering multiple devices).

		 * as we may have started syncing it but not finished.

		 * We can find the current address in

		 * mddev->curr_resync, but for recovery,

		 * we need to convert that to several

		 * virtual addresses.

 aborted */

 completed sync */

				/* Completed a full sync so the replacements

				 * are now fully recovered.

		/* if there has been nothing to do on any drive,

		 * then there is nothing to do at all..

 Don't do IO beyond here */

	/* make sure whole request will fit in a chunk - if chunks

	 * are meaningful

	/*

	 * If there is non-resync activity waiting for a turn, then let it

	 * though before starting on this new sync request.

	/* Again, very different code for resync and recovery.

	 * Both must result in an r10bio with a list of bios that

	 * have bi_end_io, bi_sector, bi_bdev set,

	 * and bi_private set to the r10bio.

	 * For recovery, we may actually create several r10bios

	 * with 2 bios in each, that correspond to the bios in the main one.

	 * In this case, the subordinate r10bios link back through a

	 * borrowed master_bio pointer, and the counter in the master

	 * includes a ref from each subordinate.

	/* First, we decide what to do and set ->bi_end_io

	 * To end_sync_read if we want to read, and

	 * end_sync_write if we will want to write.

 recovery... the complicated one */

 want to reconstruct this device */

				/* last stripe is not complete - don't

				 * try to recover this sector.

			/* Unless we are doing a full sync, or a replacement

			 * we only need to recover the block if it is set in

			 * the bitmap

				/* yep, skip the sync_blocks here, but don't assume

				 * that there will never be anything to do here

			/* Need to check if the array will still be

			 * degraded

 This is where we read from */

 and we write to 'i' (if not in_sync) */

 and maybe write to replacement */

				/* Note: if need_replace, then bio

				 * cannot be NULL as r10buf_pool_alloc will

				 * have allocated it.

				/* Cannot recover, so abort the recovery or

					/* problem is that there are bad blocks

					 * on other device(s)

				/* Only want this if there is elsewhere to

				 * read from. 'j' is currently the first

				 * readable copy.

 resync. Schedule a read for every block at this virt offset */

		/*

		 * Since curr_resync_completed could probably not update in

		 * time, and we will set cluster_sync_low based on it.

		 * Let's check against "sector_nr + 2 * RESYNC_SECTORS" for

		 * safety reason, which ensures curr_resync_completed is

		 * updated in bitmap_cond_end_sync.

 We can skip this block */

 Need to set up for writing to the replacement */

			/*

			 * won't fail because the vec table is big enough

			 * to hold all these pages

 It is resync not recovery */

 Send resync message */

 This is recovery not resync */

			/*

			 * sector_nr is a device address for recovery, so we

			 * need translate it to array address before compare

			 * with cluster_sync_high.

				/*

				 * curr_resync_completed is similar as

				 * sector_nr, so make the translation too.

		/* pretend they weren't skipped, it makes

		 * no important difference in this case

	/* There is nowhere to write, so all non-sync

	 * drives must be failed or in resync, all drives

	 * have a bad block, so try the next chunk...

	/* Calculate the number of sectors-per-device that will

	 * actually be used, and set conf->dev_sectors and

	 * conf->stride

 'size' is now the number of chunks in the array */

 calculate "used chunks per device" */

	/* We need to round up when dividing by raid_disks to

	 * get the stride size.

 avoid 'may be unused' warnings */

	case geo_start: /* new when starting reshape - raid_disks not

 original layout.  simple but not always optimal */

	case 1: /* "improved" layout which was buggy.  Hopefully no-one is

 "improved" layout fixed to match documentation */

 Not a valid layout */

 FIXME calc properly */

 far_copies must be 1 */

 need to check that every block has at least one working mirror */

 must ensure that shape change is supported */

 The replacement is all we have - use it */

	/*

	 * Ok, everything is just fine now

 This cannot work */

	/* Resize of 'far' arrays is not supported.

	 * For 'near' and 'offset' arrays we can set the

	 * number of sectors used to be an appropriate multiple

	 * of the chunk size.

	 * For 'offset', this is far_copies*chunksize.

	 * For 'near' the multiplier is the LCM of

	 * near_copies and raid_disks.

	 * So if far_copies > 1 && !far_offset, fail.

	 * Else find LCM(raid_disks, near_copy)*far_copies and

	 * multiply by chunk_size.  Then round to this number.

	 * This is mostly done by raid10_size()

 Set new parameters */

 new layout: far_copies = 1, near_copies = 2 */

 make sure it will be not marked as dirty */

	/* raid10 can take over:

	 *  raid0 - providing it has only two drives

 for raid0 takeover only one zone is supported */

	/* Called when there is a request to change

	 * - layout (to ->new_layout)

	 * - chunk size (to ->new_chunk_sectors)

	 * - raid_disks (by delta_disks)

	 * or when trying to restart a reshape that was ongoing.

	 *

	 * We need to validate the request and possibly allocate

	 * space if that might be an issue later.

	 *

	 * Currently we reject any reshape of a 'far' mode array,

	 * allow chunk size to change if new is generally acceptable,

	 * allow raid_disks to increase, and allow

	 * a switch between 'near' mode and 'offset' mode.

 mustn't change number of copies */

 Cannot switch to 'far' mode */

 not factor of array size */

 allocate new 'mirrors' list */

/*

 * Need to check if array has failed when deciding whether to:

 *  - start an array

 *  - remove non-faulty devices

 *  - add a spare

 *  - allow a reshape

 * This determination is simple when no reshape is happening.

 * However if there is a reshape, we need to carefully check

 * both the before and after sections.

 * This is because some failed devices may only affect one

 * of the two sections, and some non-in_sync devices may

 * be insync in the section most affected by failed devices.

 'prev' section first */

			/* When we can reduce the number of devices in

			 * an array, this might not contribute to

			 * 'degraded'.  It does now.

			/* If reshape is increasing the number of devices,

			 * this section has already been recovered, so

			 * it doesn't contribute to degraded.

			 * else it does.

	/* A 'reshape' has been requested. This commits

	 * the various 'new' fields and sets MD_RECOVER_RESHAPE

	 * This also checks if there are enough spares and adds them

	 * to the array.

	 * We currently require enough spares to make the final

	 * array non-degraded.  We also require that the difference

	 * between old and new data_offset - on each device - is

	 * enough that we never risk over-writing.

		/*

		 * some node is already performing reshape, and no need to

		 * call md_bitmap_resize again since it should be called when

		 * receiving BITMAP_RESIZE msg

 Failure here is OK */

 This is a spare that was manually added */

	/* When a reshape changes the number of devices,

	 * ->degraded is measured against the larger of the

	 * pre and  post numbers.

/* Calculate the last device-address that could contain

 * any block from the chunk that includes the array-address 's'

 * and report the next address.

 * i.e. the address returned will be chunk-aligned and after

 * any data that is in the chunk containing 's'.

/* Calculate the first device-address that could contain

 * any block from the chunk that includes the array-address 's'.

 * This too will be the start of a chunk

	/* We simply copy at most one chunk (smallest of old and new)

	 * at a time, possibly less if that exceeds RESYNC_PAGES,

	 * or we hit a bad block or something.

	 * This might mean we pause for normal IO in the middle of

	 * a chunk, but that is not a problem as mddev->reshape_position

	 * can record any location.

	 *

	 * If we will want to write to a location that isn't

	 * yet recorded as 'safe' (i.e. in metadata on disk) then

	 * we need to flush all reshape requests and update the metadata.

	 *

	 * When reshaping forwards (e.g. to more devices), we interpret

	 * 'safe' as the earliest block which might not have been copied

	 * down yet.  We divide this by previous stripe size and multiply

	 * by previous stripe length to get lowest device offset that we

	 * cannot write to yet.

	 * We interpret 'sector_nr' as an address that we want to write to.

	 * From this we use last_device_address() to find where we might

	 * write to, and first_device_address on the  'safe' position.

	 * If this 'next' write position is after the 'safe' position,

	 * we must update the metadata to increase the 'safe' position.

	 *

	 * When reshaping backwards, we round in the opposite direction

	 * and perform the reverse test:  next write position must not be

	 * less than current safe position.

	 *

	 * In all this the minimum difference in data offsets

	 * (conf->offset_diff - always positive) allows a bit of slack,

	 * so next can be after 'safe', but not by more than offset_diff

	 *

	 * We need to prepare all the bios here before we start any IO

	 * to ensure the size we choose is acceptable to all devices.

	 * The means one for each copy for write-out and an extra one for

	 * read-in.

	 * We store the read-in bio in ->master_bio and the others in

	 * ->devs[x].bio and ->devs[x].repl_bio.

 If restarting in the middle, skip the initial sectors */

	/* We don't use sector_nr to track where we are up to

	 * as that doesn't work well for ->reshape_backwards.

	 * So just use ->reshape_progress.

		/* 'next' is the earliest device address that we might

		 * write to for this chunk in the new layout

		/* 'safe' is the last device address that we might read from

		 * in the old layout after a restart

		/* 'next' is after the last device address that we

		 * might write to for this chunk in the new layout

		/* 'safe' is the earliest device address that we might

		 * read from in the old layout after a restart

		/* Need to update metadata if 'next' might be beyond 'safe'

		 * as that would possibly corrupt data

 Need to update reshape_position in metadata */

 Now schedule reads for blocks from sector_nr to last */

		/* Cannot read from here, so need to record bad blocks

		 * on all the target devices.

 FIXME

	/*

	 * Broadcast RESYNC message to other nodes, so all nodes would not

	 * write to the region to avoid conflict.

			/*

			 * Set cluster_sync_low again if next address for array

			 * reshape is less than cluster_sync_low. Since we can't

			 * update cluster_sync_low until it has finished reshape.

 Now find the locations in the new layout */

 Now add as many pages as possible to all of these bios. */

			/*

			 * won't fail because the vec table is big enough

			 * to hold all these pages

 Now submit the read */

	/* Now that we have done the whole section we can

	 * update reshape_progress

	/* Reshape read completed.  Hopefully we have a block

	 * to write out.

	 * If we got a read error then we do sync 1-page reads from

	 * elsewhere until we find the data - or give up.

 Reshape has been aborted */

	/* We definitely have the data in the pages, schedule the

	 * writes.

 Use sync reads to get the blocks from somewhere else */

 reshape IOs share pages from .devs[0].bio */

 couldn't read this block, must give up */

 FIXME should record badblock */

 RAID10 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Shaohua Li <shli@fb.com>

 * Copyright (C) 2016 Song Liu <songliubraving@fb.com>

/*

 * metadata/data stored in disk with 4k size unit (a block) regardless

 * underneath hardware sector size. only works with PAGE_SIZE == 4096

/*

 * log->max_free_space is min(1/4 disk size, 10G reclaimable space).

 *

 * In write through mode, the reclaim runs every log->max_free_space.

 * This can prevent the recovery scans for too long

 sector */

 wake up reclaim thread periodically */

 start flush with these full stripes */

 reclaim stripes in groups */

/*

 * We only need 2 bios per I/O unit to make progress, but ensure we

 * have a few more available to not get too tight.

/*

 * raid5 cache state machine

 *

 * With the RAID cache, each stripe works in two phases:

 *	- caching phase

 *	- writing-out phase

 *

 * These two phases are controlled by bit STRIPE_R5C_CACHING:

 *   if STRIPE_R5C_CACHING == 0, the stripe is in writing-out phase

 *   if STRIPE_R5C_CACHING == 1, the stripe is in caching phase

 *

 * When there is no journal, or the journal is in write-through mode,

 * the stripe is always in writing-out phase.

 *

 * For write-back journal, the stripe is sent to caching phase on write

 * (r5c_try_caching_write). r5c_make_stripe_write_out() kicks off

 * the write-out phase by clearing STRIPE_R5C_CACHING.

 *

 * Stripes in caching phase do not write the raid disks. Instead, all

 * writes are committed from the log device. Therefore, a stripe in

 * caching phase handles writes as:

 *	- write to log device

 *	- return IO

 *

 * Stripes in writing-out phase handle writes as:

 *	- calculate parity

 *	- write pending data and parity to journal

 *	- write data and parity to raid disks

 *	- return IO for pending writes

	sector_t device_size;		/* log device size, round to

	sector_t max_free_space;	/* reclaim run if free space is at

	sector_t last_checkpoint;	/* log tail. where recovery scan

 log tail sequence */

 log head. where new data appends */

 log head sequence */

 current io_unit accepting new data */

	struct list_head running_ios;	/* io_units which are still running,

					 * and have not yet been completely

	struct list_head io_end_ios;	/* io_units which have been completely

					 * written to the log but not yet written

	struct list_head flushing_ios;	/* io_units which are waiting for log

 io_units which settle down in log disk */

 pending stripes, -ENOMEM */

	unsigned long reclaim_target;	/* number of space that need to be

					 * reclaimed.  if it's 0, reclaim spaces

					 * used by io_units which are in

					 * IO_UNIT_STRIPE_END state (eg, reclaim

					 * dones't wait for specific io_unit

					 * switching to IO_UNIT_STRIPE_END

 pending stripes, log has no space */

 for r5c_cache */

 all stripes in r5cache, in the order of seq at sh->log_start */

 to submit async io_units, to fulfill ordering of flush */

 to disable write back during in degraded mode */

 to for chunk_aligned_read in writeback mode, details below */

/*

 * Enable chunk_aligned_read() with write back cache.

 *

 * Each chunk may contain more than one stripe (for example, a 256kB

 * chunk contains 64 4kB-page, so this chunk contain 64 stripes). For

 * chunk_aligned_read, these stripes are grouped into one "big_stripe".

 * For each big_stripe, we count how many stripes of this big_stripe

 * are in the write back cache. These data are tracked in a radix tree

 * (big_stripe_tree). We use radix_tree item pointer as the counter.

 * r5c_tree_index() is used to calculate keys for the radix tree.

 *

 * chunk_aligned_read() calls r5c_big_stripe_cached() to look up

 * big_stripe of each chunk in the tree. If this big_stripe is in the

 * tree, chunk_aligned_read() aborts. This look up is protected by

 * rcu_read_lock().

 *

 * It is necessary to remember whether a stripe is counted in

 * big_stripe_tree. Instead of adding new flag, we reuses existing flags:

 * STRIPE_R5C_PARTIAL_STRIPE and STRIPE_R5C_FULL_STRIPE. If either of these

 * two flags are set, the stripe is counted in big_stripe_tree. This

 * requires moving set_bit(STRIPE_R5C_PARTIAL_STRIPE) to

 * r5c_try_caching_write(); and moving clear_bit of

 * STRIPE_R5C_PARTIAL_STRIPE and STRIPE_R5C_FULL_STRIPE to

 * r5c_finish_stripe_write_out().

/*

 * radix tree requests lowest 2 bits of data pointer to be 2b'00.

 * So it is necessary to left shift the counter by 2 bits before using it

 * as data pointer of the tree.

/*

 * calculate key for big_stripe_tree

 *

 * sect: align_bi->bi_iter.bi_sector or sh->sector

/*

 * an IO range starts from a meta data block and end at the next meta data

 * block. The io unit's the meta data block tracks data/parity followed it. io

 * unit is written to log disk with normal write, as we always flush log disk

 * first and then start move data to raid disks, there is no requirement to

 * write io unit with FLUSH/FUA

 store meta block */

 current offset in meta_page */

 current_bio accepting new data */

 how many stripes not flushed to raid */

 seq number of the metablock */

 where the io_unit starts */

 where the io_unit ends */

 log->running_ios */

 stripes added to the io_unit */

 include flush request */

 include fua request */

 include null flush request */

 include flush payload  */

	/*

	 * io isn't sent yet, flush/fua request can only be submitted till it's

	 * the first IO in running_ios list

 size == 0 flush bios */

 r5l_io_unit state */

 accepting new IO */

	IO_UNIT_IO_START = 1,	/* io_unit bio start writing to log,

 io_unit bio finish writing to log */

 stripes data finished writing to raid */

 Check whether we should flush some stripes to free up stripe cache */

	/*

	 * The following condition is true for either of the following:

	 *   - stripe cache pressure high:

	 *          total_cached > 3/4 min_nr_stripes ||

	 *          empty_inactive_list_nr > 0

	 *   - stripe cache pressure moderate:

	 *          total_cached > 1/2 min_nr_stripes

/*

 * flush cache when there are R5C_FULL_STRIPE_FLUSH_BATCH or more full

 * stripes in the cache

	/*

	 * wake up reclaim for R5C_FULL_STRIPE_FLUSH_BATCH cached stripes

	 * or a full stripe (chunk size / 4k stripes).

/*

 * Total log space (in sectors) needed to flush all data in cache

 *

 * To avoid deadlock due to log space, it is necessary to reserve log

 * space to flush critical stripes (stripes that occupying log space near

 * last_checkpoint). This function helps check how much log space is

 * required to flush all cached stripes.

 *

 * To reduce log space requirements, two mechanisms are used to give cache

 * flush higher priorities:

 *    1. In handle_stripe_dirtying() and schedule_reconstruction(),

 *       stripes ALREADY in journal can be flushed w/o pending writes;

 *    2. In r5l_write_stripe() and r5c_cache_data(), stripes NOT in journal

 *       can be delayed (r5l_add_no_space_stripe).

 *

 * In cache flush, the stripe goes through 1 and then 2. For a stripe that

 * already passed 1, flushing it requires at most (conf->max_degraded + 1)

 * pages of journal space. For stripes that has not passed 1, flushing it

 * requires (conf->raid_disks + 1) pages of journal space. There are at

 * most (conf->group_cnt + 1) stripe that passed 1. So total journal space

 * required to flush all cached stripes (in pages) is:

 *

 *     (stripe_in_journal_count - group_cnt - 1) * (max_degraded + 1) +

 *     (group_cnt + 1) * (raid_disks + 1)

 * or

 *     (stripe_in_journal_count) * (max_degraded + 1) +

 *     (group_cnt + 1) * (raid_disks - max_degraded)

/*

 * evaluate log space usage and update R5C_LOG_TIGHT and R5C_LOG_CRITICAL

 *

 * R5C_LOG_TIGHT is set when free space on the log device is less than 3x of

 * reclaim_required_space. R5C_LOG_CRITICAL is set when free space on the log

 * device is less than 2x of reclaim_required_space.

/*

 * Put the stripe into writing-out phase by clearing STRIPE_R5C_CACHING.

 * This function should only be called in write-back mode.

/*

 * this journal write must contain full parity,

 * it may also contain some data pages

/*

 * Setting proper flags after writing (or flushing) data and/or parity to the

 * log device. This is called from r5l_log_endio() or r5l_log_flush_endio().

		/*

		 * Set R5_InJournal for parity dev[pd_idx]. This means

		 * all data AND parity in the journal. For RAID 6, it is

		 * NOT necessary to set the flag for dev[qd_idx], as the

		 * two parities are written out together.

 don't change list order */

 don't change list order */

	/*

	 * if the io doesn't not have null_flush or flush payload,

	 * it is not safe to access it after releasing io_list_lock.

	 * Therefore, it is necessary to check the condition with

	 * the lock held.

		/*

		 * FLUSH/FUA io_unit is deferred because of ordering, now we

		 * can dispatch it

 finish flush only io_unit and PAYLOAD_FLUSH only io_unit */

 decrease pending_stripe for flush payload */

	/*

	 * In case of journal device failures, submit_bio will get error

	 * and calls endio, then active stripes will continue write

	 * process. Therefore, it is not necessary to check Faulty bit

	 * of journal device here.

	 *

	 * We can't check split_bio after current_bio is submitted. If

	 * io->split_bio is null, after current_bio is submitted, current_bio

	 * might already be completed and the io_unit is freed. We submit

	 * split_bio first to avoid the issue.

 deferred io_unit will be dispatched here */

 wait superblock change before suspend */

	/*

	 * If we filled up the log device start from the beginning again,

	 * which will require a new bio.

	 *

	 * Note: for this to work properly the log size needs to me a multiple

	 * of BLOCK_SECTORS.

	/*

	 * payload_flush requires extra writes to the journal.

	 * To avoid handling the extra IO in quiesce, just skip

	 * flush_payload

 current implementation is one stripe per flush payload */

 multiple flush payloads count as one pending_stripe */

			/*

			 * we need to flush journal to make sure recovery can

			 * reach the data with fua flag

 Just writing data, not parity, in caching phase */

 add stripe to no_space_stripes, and then wake up reclaim */

/*

 * running in raid5d, where reclaim could wait for raid5d too (when it flushes

 * data from log to raid disks), so we shouldn't wait for reclaim here

 Don't support stripe batch */

 the stripe is written to log, we start writing it to raid */

 checksum is already calculated in last run */

	/*

	 * The stripe must enter state machine again to finish the write, so

	 * don't delay.

 meta + data */

 R5C_JOURNAL_MODE_WRITE_BACK */

		/*

		 * log space critical, do not process stripes that are

		 * not in cache yet (sh->log_start == MaxSector).

		/*

		 * in write through (journal only)

		 * we flush log disk cache first, then write stripe data to

		 * raid disks. So if bio is finished, the log disk cache is

		 * flushed already. The recovery guarantees we can recovery

		 * the bio from log disk, so we don't need to flush again

 write back (with cache) */

 This will run after log space is reclaimed */

/*

 * calculate new last_checkpoint

 * for write through mode, returns log->next_checkpoint

 * for write back, returns log_start of first sh in stripe_in_journal_list

 all stripes flushed */

 don't change list order */

/*

 * Starting dispatch IO to raid.

 * io_unit(meta) consists of a log. There is one situation we want to avoid. A

 * broken meta in the middle of a log causes recovery can't find meta at the

 * head of log. If operations require meta at the head persistent in log, we

 * must make sure meta before it persistent in log too. A case is:

 *

 * stripe data/parity is in log, we start write stripe to raid disks. stripe

 * data/parity must be persistent in log before we do the write to raid disks.

 *

 * The solution is we restrictly maintain io_unit list order. In this case, we

 * only write stripes of an io_unit to raid disks till the io_unit is the first

 * one whose data/parity is in log.

 flush bio is running */

	/*

	 * Discard could zero data, so before discard we must make sure

	 * superblock is updated to new log tail. Updating superblock (either

	 * directly call md_update_sb() or depend on md thread) must hold

	 * reconfig mutex. On the other hand, raid5_quiesce is called with

	 * reconfig_mutex hold. The first step of raid5_quiesce() is waitting

	 * for all IO finish, hence waitting for reclaim thread, while reclaim

	 * thread is calling this function and waitting for reconfig mutex. So

	 * there is a deadlock. We workaround this issue with a trylock.

	 * FIXME: we could miss discard if we can't take reconfig mutex

 discard IO error really doesn't matter, ignore it */

/*

 * r5c_flush_stripe moves stripe from cached list to handle_list. When called,

 * the stripe must be on r5c_cached_full_stripes or r5c_cached_partial_stripes.

 *

 * must hold conf->device_lock

	/*

	 * The stripe is not ON_RELEASE_LIST, so it is safe to call

	 * raid5_release_stripe() while holding conf->device_lock

/*

 * if num == 0, flush all full stripes

 * if num > 0, flush all full stripes. If less than num full stripes are

 *             flushed, flush some partial stripes until totally num stripes are

 *             flushed or there is no more cached stripes.

		/*

		 * if stripe cache pressure high, flush all full stripes and

		 * some partial stripes

		/*

		 * if stripe cache pressure moderate, or if there is many full

		 * stripes,flush all full stripes

 no need to flush */

 if log space is tight, flush stripes on stripe_in_journal_list */

			/*

			 * stripes on stripe_in_journal_list could be in any

			 * state of the stripe_cache state machine. In this

			 * case, we only want to flush stripe on

			 * r5c_cached_full/partial_stripes. The following

			 * condition makes sure the stripe is on one of the

			 * two lists.

	/*

	 * move proper io_unit to reclaim list. We should not change the order.

	 * reclaimable/unreclaimable io_unit can be mixed in the list, we

	 * shouldn't reuse space of an unreclaimable io_unit

	/*

	 * write_super will flush cache of each raid disk. We must write super

	 * here, because the log area might be reused soon and we don't want to

	 * confuse recovery

 overflow in theory */

 make sure r5l_write_super_and_discard_space exits */

 don't allow write if journal disk is missing */

 current meta */

 total size of current meta and data */

 recovery position */

 recovery position seq */

 number of data_parity stripes */

 number of data_only stripes */

	/*

	 * read ahead page pool (ra_pool)

	 * in recovery, log is read sequentially. It is not efficient to

	 * read every page with sync_page_io(). The read ahead page pool

	 * reads multiple pages with one IO, so further log read can

	 * just copy data from the pool.

 offset of first page in the pool */

 total allocated pages */

 pages with valid data */

 bio to do the read ahead */

/*

 * fetch ctx->valid_pages pages from offset

 * In normal cases, ctx->valid_pages == ctx->total_pages after the call.

 * However, if the offset is close to the end of the journal device,

 * ctx->valid_pages could be smaller than ctx->total_pages

 reached end of the device */

/*

 * try read a page from the read ahead page pool, if the page is not in the

 * pool, call r5l_recovery_fetch_ra_pool

/*

 * r5l_recovery_load_data and r5l_recovery_load_parity uses flag R5_Wantwrite

 * to mark valid (potentially not flushed) data in the journal.

 *

 * We already verified checksum in r5l_recovery_verify_data_checksum_for_mb,

 * so there should not be any mismatch here.

	/*

	 * stripes that only have parity must have been flushed

	 * before the crash that we are now recovering from, so

	 * there is nothing more to recovery.

 in case device is broken */

 no more stripe available */

 if matches return 0; otherwise return -EINVAL */

/*

 * before loading data to stripe cache, we need verify checksum for all data,

 * if there is mismatch for any data page, we drop all data in the mata block

 q for RAID 6 */

 nothing to do for R5LOG_PAYLOAD_FLUSH here */

 not R5LOG_PAYLOAD_DATA/PARITY/FLUSH */

 DATA or PARITY payload */

/*

 * Analyze all data/parity pages in one meta block

 * Returns:

 * 0 for success

 * -EINVAL for unknown playload type

 * -EAGAIN for checksum mismatch of data page

 * -ENOMEM for run out of memory (alloc_page failed or run out of stripes)

	/*

	 * for mismatch in data blocks, we will drop all data in this mb, but

	 * we will still read next mb for other data with FLUSH flag, as

	 * io_unit could finish out of order.

 -ENOMEM duo to alloc_page() failed */

 DATA or PARITY payload */

			/*

			 * cannot get stripe from raid5_get_active_stripe

			 * try replay some stripes

/*

 * Load the stripe into cache. The stripe will be written out later by

 * the stripe cache state machine.

/*

 * Scan through the log for all to-be-flushed data

 *

 * For stripes with data and parity, namely Data-Parity stripe

 * (STRIPE_R5C_CACHING == 0), we simply replay all the writes.

 *

 * For stripes with only data, namely Data-Only stripe

 * (STRIPE_R5C_CACHING == 1), we load them to stripe cache state machine.

 *

 * For a stripe, if we see data after parity, we should discard all previous

 * data and parity for this stripe, as these data are already flushed to

 * the array.

 *

 * At the end of the scan, we return the new journal_tail, which points to

 * first data-only stripe on the journal device, or next invalid meta block.

 scan through the log */

		/*

		 * -EAGAIN means mismatch in data block, in this case, we still

		 * try scan the next metablock

 ret == -EINVAL or -ENOMEM */

 replay data-parity stripes */

 load data-only stripes to stripe cache */

/*

 * we did a recovery. Now ctx.pos points to an invalid meta block. New

 * log will start here. but we can't let superblock point to last valid

 * meta block. The log might looks like:

 * | meta 1| meta 2| meta 3|

 * meta 1 is valid, meta 2 is invalid. meta 3 could be valid. If

 * superblock points to meta 1, we write a new valid meta 2n.  if crash

 * happens again, new recovery will start from meta 1. Since meta 2n is

 * valid now, recovery will think meta 3 is valid, which is wrong.

 * The solution is we create a new meta in meta2 with its seq == meta

 * 1's seq + 10000 and let superblock points to meta2. The same recovery

 * will not think meta 3 is a valid meta, because its seq doesn't match

/*

 * Before recovery, the log looks like the following

 *

 *   ---------------------------------------------

 *   |           valid log        | invalid log  |

 *   ---------------------------------------------

 *   ^

 *   |- log->last_checkpoint

 *   |- log->last_cp_seq

 *

 * Now we scan through the log until we see invalid entry

 *

 *   ---------------------------------------------

 *   |           valid log        | invalid log  |

 *   ---------------------------------------------

 *   ^                            ^

 *   |- log->last_checkpoint      |- ctx->pos

 *   |- log->last_cp_seq          |- ctx->seq

 *

 * From this point, we need to increase seq number by 10 to avoid

 * confusing next recovery.

 *

 *   ---------------------------------------------

 *   |           valid log        | invalid log  |

 *   ---------------------------------------------

 *   ^                              ^

 *   |- log->last_checkpoint        |- ctx->pos+1

 *   |- log->last_cp_seq            |- ctx->seq+10001

 *

 * However, it is not safe to start the state machine yet, because data only

 * parities are not yet secured in RAID. To save these data only parities, we

 * rewrite them from seq+11.

 *

 *   -----------------------------------------------------------------

 *   |           valid log        | data only stripes | invalid log  |

 *   -----------------------------------------------------------------

 *   ^                                                ^

 *   |- log->last_checkpoint                          |- ctx->pos+n

 *   |- log->last_cp_seq                              |- ctx->seq+10000+n

 *

 * If failure happens again during this process, the recovery can safe start

 * again from log->last_checkpoint.

 *

 * Once data only stripes are rewritten to journal, we move log_tail

 *

 *   -----------------------------------------------------------------

 *   |     old log        |    data only stripes    | invalid log  |

 *   -----------------------------------------------------------------

 *                        ^                         ^

 *                        |- log->last_checkpoint   |- ctx->pos+n

 *                        |- log->last_cp_seq       |- ctx->seq+10000+n

 *

 * Then we can safely start the state machine. If failure happens from this

 * point on, the recovery will start from new log->last_checkpoint.

 reuse conf->wait_for_quiescent in recovery */

/*

 * Set journal cache mode on @mddev (external API initially needed by dm-raid).

 *

 * @mode as defined in 'enum r5c_journal_mode'.

 *

/*

 * Try handle write operation in caching phase. This function should only

 * be called in write-back mode.

 *

 * If all outstanding writes can be handled in caching phase, returns 0

 * If writes requires write-out phase, call r5c_make_stripe_write_out()

 * and returns -EAGAIN

		/*

		 * There are two different scenarios here:

		 *  1. The stripe has some data cached, and it is sent to

		 *     write-out phase for reclaim

		 *  2. The stripe is clean, and this is the first write

		 *

		 * For 1, return -EAGAIN, so we continue with

		 * handle_stripe_dirtying().

		 *

		 * For 2, set STRIPE_R5C_CACHING and continue with caching

		 * write.

 case 1: anything injournal or anything in written */

 case 2 */

	/*

	 * When run in degraded mode, array is set to write-through mode.

	 * This check helps drain pending write safely in the transition to

	 * write-through mode.

	 *

	 * When a stripe is syncing, the write is also handled in write

	 * through mode.

 if non-overwrite, use writing-out phase */

 if the stripe is not counted in big_stripe_tree, add it now */

			/*

			 * this radix_tree_insert can fail safely, so no

			 * need to call radix_tree_preload()

		/*

		 * set STRIPE_R5C_PARTIAL_STRIPE, this shows the stripe is

		 * counted in the radix tree

		/*

		 * set STRIPE_LOG_TRAPPED, which triggers r5c_cache_data()

		 * in ops_run_io(). STRIPE_LOG_TRAPPED will be cleared in

		 * r5c_handle_data_cached()

/*

 * free extra pages (orig_page) we allocated for prexor

/*

 * clean up the stripe (clear R5_InJournal for dev[pd_idx] etc.) after the

 * stripe is committed to RAID disks.

	/*

	 * analyse_stripe() runs before r5c_finish_stripe_write_out(),

	 * We updated R5_InJournal, so we also update s->injournal.

 stop counting this stripe in big_stripe_tree */

 stripe is flused to raid disks, we can do resync now */

	/*

	 * The stripe must enter state machine again to call endio, so

	 * don't delay.

 meta + data */

 check whether this big stripe is in write back cache. */

 Make sure it's valid */

		/*

		 * Make sure super points to correct address. Log might have

		 * data very soon. If super hasn't correct log tail address,

		 * recovery can't find the log

	/*

	 * The PAGE_SIZE must be big enough to hold 1 r5l_meta_block and

	 * raid_disks r5l_payload_data_parity.

	 *

	 * Write journal and cache does not work for very big array

	 * (raid_disks > 203)

 Ensure disable_writeback_work wakes up and exits */

/*

 * Copyright (C) 2012 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * defines a range of metadata versions that this module can handle.

/*

 *  3 for btree insert +

 *  2 for btree lookup used within space map

 for spotting crashes that would invalidate the dirty bitset */

 metadata must be checked using the tools */

/*

 * Each mapping from cache block -> origin block carries a set of flags.

	/*

	 * A valid mapping.  Because we're using an array we clear this

	 * flag for an non existant mapping.

	/*

	 * The data on the cache is different from that on the origin.

	 * This flag is only used by metadata format 1.

	/*

	 * Metadata format 2 fields.

	/*

	 * Reading the space map root can fail, so we read it into this

	 * buffer before the superblock is locked and updated.

	/*

	 * Set if a transaction has to be aborted but the attempt to roll

	 * back to the previous (good) transaction failed.  The only

	 * metadata operation permissible in this state is the closing of

	 * the device.

	/*

	 * Metadata format 2 fields.

	/*

	 * These structures are used when loading metadata.  They're too

	 * big to put on the stack.

/*-------------------------------------------------------------------

 * superblock validator

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * We can't use a validator here - it may be all zeroes.

 FIXME: see if we can lose the max sectors limit */

	/*

	 * dm_sm_copy_root() can fail.  So we need to do it before we start

	 * updating the superblock.

	/*

	 * Check for read-only metadata to skip the following RDWR checks.

 Verify the data block size hasn't changed */

/*

 * The mutator updates the superblock flags.

	/*

	 * We re-read the superblock every time.  Shouldn't need to do this

	 * really.

	/*

	 * We need to know if the cache_disk_superblock exceeds a 512-byte sector.

----------------------------------------------------------------*/

/*

 * The mappings are held in a dm-array that has 64-bit values stored in

 * little-endian format.  The index is the cblock, the high 48bits of the

 * value are the oblock and the low 16 bit the flags.

----------------------------------------------------------------*/

/*

 * We keep a little list of ref counted metadata objects to prevent two

 * different target instances creating separate bufio instances.  This is

 * an issue if a table is reloaded before the suspend.

/*

 * Checks that the given cache block is either unmapped or clean.

 Nothing to do */

		/*

		 * We assume that unmapped blocks have their dirty bit

		 * cleared.

 nothing to do */

	/*

	 * Ensure policy names match.

	/*

	 * Ensure policy major versions match.

	/*

	 * Ensure policy hint sizes match.

 Nothing to do */

		/*

		 * We need to break out before we move the cursors.

 nothing to be done */

 nr_bits is really just a sanity check */

----------------------------------------------------------------*/

/*

 * It's quicker to always delete the hint array, and recreate with

 * dm_array_new().

 short-circuit hints initialization */

 SPDX-License-Identifier: GPL-2.0

/*

 * Asynchronous refcounty things

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 For clearing flags with the same atomic op as a put */

/*

 * closure_put - decrement a closure's refcount

/*

 * closure_wake_up - wake up all closures on a wait list, without memory barrier

 We first reverse the list to preserve FIFO ordering and fairness */

 Then do the wakeups */

/**

 * closure_wait - add a closure to a waitlist

 * @waitlist: will own a ref on @cl, which will be released when

 * closure_wake_up() is called on @waitlist.

 * @cl: closure pointer.

 *

		/*

		 * it is unnecessary to check return value of

		 * debugfs_create_file(), we should not care

		 * about this.

 SPDX-License-Identifier: GPL-2.0

/*

 * Some low level IO code, and hacks for various block layer limitations

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 Bios with headers */

 IO errors */

	/*

	 * Read-ahead requests on a degrading and recovering md raid

	 * (e.g. raid6) device might be failured immediately by md

	 * raid code, which is not a real hardware media failure. So

	 * we shouldn't count failed REQ_RAHEAD bio to dc->io_errors.

	/*

	 * The halflife of an error is:

	 * log2(1/2)/log2(127/128) * refresh ~= 88 * refresh

			/*

			 * First we subtract refresh from count; each time we

			 * successfully do so, we rescale the errors once:

 SPDX-License-Identifier: GPL-2.0

/*

 * bcache setup/teardown code, and some metadata io - read a superblock and

 * figure out what to do with it.

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 limitation of partitions number on single bcache device */

 limitation of bcache devices number on single system */

 Superblock */

			/*

			 * bcache tool will make sure the overflow won't

			 * happen, an error message here is enough.

		/*

		 * Feature bits are needed in read_super_common(),

		 * convert them firstly.

 Check incompatible features */

 I/O request sent to backing device */

 is_read = 0 */

 UUID io */

		/*

		 * Since the new uuid entry is bigger than the old, we have to

		 * convert starting at the highest memory address and work down

		 * in order to do it in place

 Only one bucket used for uuid write */

/*

 * Bucket priorities/gens:

 *

 * For each bucket, we store on disk its

 *   8 bit gen

 *  16 bit priority

 *

 * See alloc.c for an explanation of the gen. The priority is used to implement

 * lru (and in the future other) cache replacement policies; for most purposes

 * it's just an opaque integer.

 *

 * The gens and the priorities don't have a whole lot to do with each other, and

 * it's actually the gens that must be written out at specific times - it's no

 * big deal if the priorities don't get written, if we lose them we just reuse

 * buckets in suboptimal order.

 *

 * On disk they're stored in a packed array, and in as many buckets are required

 * to fit them all. The buckets we use to store them form a list; the journal

 * header points to the first bucket, the first bucket points to the second

 * bucket, et cetera.

 *

 * This code is used by the allocation code; periodically (whenever it runs out

 * of buckets to allocate from) the allocation code will invalidate some

 * buckets, but it can't use those buckets until their new gens are safely on

 * disk.

	/*

	 * Pre-check if there are enough free buckets. In the non-blocking

	 * scenario it's better to fail early rather than starting to allocate

	 * buckets and do a cleanup later in case of failure.

	/*

	 * Don't want the old priorities to get garbage collected until after we

	 * finish writing the new ones, and they're journalled

 Bcache device */

		/*

		 * closure_fn set to

		 * - cached device: cached_dev_flush()

		 * - flash dev: flash_dev_flush()

		/*

		 * This should only happen with BCACHE_SB_VERSION_BDEV.

		 * Block/page size is checked for BCACHE_SB_VERSION_CDEV.

 This also adjusts physical block size/min io size if needed */

 Cached device */

	/*

	 * If this delayed worker is stopping outside, directly quit here.

	 * dc->io_disable might be set via sysfs interface, so check it

	 * here too.

 let others know earlier that io_disable is true */

	/*

	 * won't show up in the uevent file, use udevadm monitor -e instead

	 * only class / kset properties are persistent

/*

 * If BCACHE_DEV_RATE_DW_RUNNING is set, it means routine of the delayed

 * work dc->writeback_rate_update is running. Wait until the routine

 * quits (BCACHE_DEV_RATE_DW_RUNNING is clear), then continue to

 * cancel it. If BCACHE_DEV_RATE_DW_RUNNING is not clear after time_out

 * seconds, give up waiting here and continue to cancel it too.

 Drop ref we took in cached_dev_detach() */

	/*

	 * Block the device from being closed and freed until we're finished

	 * detaching

 Will die */

 Check whether already attached */

	/*

	 * Deadlocks since we're called via sysfs...

	 * sysfs_remove_file(&dc->kobj, &sysfs_attach);

	/*

	 * dc->c must be set before dc->count != 0 - paired with the mb in

	 * cached_dev_get()

 Block writeback thread, but spawn it */

		/*

		 * bch_register_lock is held, bcache_device_stop() is not

		 * able to be directly called. The kthread and kworker

		 * created previously in bch_cached_dev_writeback_start()

		 * have to be stopped manually here.

 Allow the writeback thread to proceed */

 when dc->disk.kobj released */

 default to auto */

 Cached device - bcache superblock */

 attach to a matched cache set if it exists */

 Flash only volumes */

 When d->kobj released */

 make others know io_disable is true earlier */

 Cache set */

	/*

	 * XXX: we can be called from atomic context

	 * acquire_console_sem();

 When c->kobj released */

	/*

	 * Avoid flushing cached nodes if cache set is retiring

	 * due to too many I/O errors detected.

 flush last journal entry if needed */

/*

 * This function is only called when CACHE_SET_IO_DISABLE is set, which means

 * cache set is unregistering due to too many I/O errors. In this condition,

 * the bcache device might be stopped, it depends on stop_when_cache_set_failed

 * value and whether the broken cache has dirty data:

 *

 * dc->stop_when_cache_set_failed    dc->has_dirty   stop bcache device

 *  BCH_CACHED_STOP_AUTO               0               NO

 *  BCH_CACHED_STOP_AUTO               1               YES

 *  BCH_CACHED_DEV_STOP_ALWAYS         0               YES

 *  BCH_CACHED_DEV_STOP_ALWAYS         1               YES

 *

 * The expected behavior is, if stop_when_cache_set_failed is configured to

 * "auto" via sysfs interface, the bcache device will not be stopped if the

 * backing device is clean on the broken cache device.

		/*

		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO

		 * and dc->has_dirty == 1

		/*

		 * There might be a small time gap that cache set is

		 * released but bcache device is not. Inside this time

		 * gap, regular I/O requests will directly go into

		 * backing device as no cache set attached to. This

		 * behavior may also introduce potential inconsistence

		 * data in writeback mode while cache is dirty.

		 * Therefore before calling bcache_device_stop() due

		 * to a broken cache device, dc->io_disable should be

		 * explicitly set to true.

 make others know io_disable is true earlier */

		/*

		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO

		 * and dc->has_dirty == 0

 closure_fn set to __cache_set_unregister() */

 Maybe create continue_at_noreturn() and use it here? */

		/*

		 * If prio_read() fails it'll call cache_set_error and we'll

		 * tear everything down right away, but if we perhaps checked

		 * sooner we could avoid journal replay.

		/*

		 * bcache_journal_next() can't happen sooner, or

		 * btree_gc_finish() will give spurious errors about last_gc >

		 * gc_gen - this is a hack but oh well.

		/*

		 * First place it's safe to allocate: btree_check() and

		 * btree_gc_finish() have to run before we have buckets to

		 * allocate, and bch_bucket_alloc_set() might cause a journal

		 * entry to be written so bcache_journal_next() has to be called

		 * first.

		 *

		 * If the uuids were in the old format we have to rewrite them

		 * before the next journal entry is written:

		/*

		 * We don't want to write the first journal entry until

		 * everything is set up - fortunately journal entries won't be

		 * written until the SET_CACHE_SYNC() here:

 Cache device */

 When ca->kobj released */

	/*

	 * when ca->sb.njournal_buckets is not zero, journal exists,

	 * and in bch_journal_replay(), tree node may split,

	 * so bucket of RESERVE_BTREE type is needed,

	 * the worst situation is all journal buckets are valid journal,

	 * and all the keys need to replay,

	 * so the number of  RESERVE_BTREE type buckets should be as much

	 * as journal buckets

 must be set for any error case */

		/*

		 * If we failed here, it means ca->kobj is not initialized yet,

		 * kobject_put() won't be called and there is no chance to

		 * call blkdev_put() to bdev in bch_cache_release(). So we

		 * explicitly call blkdev_put() here.

 Global interfaces/init */

 blkdev_put() will be called in bch_cache_release() */

 10 jiffies is enough for a delay */

 For latest state of bcache_is_reboot */

 register in asynchronous way */

 No wait and returns to user space */

 blkdev_put() will be called in cached_dev_free() */

 blkdev_put() will be called in bch_cache_release() */

 New registration is rejected since now */

		/*

		 * Make registering caller (if there is) on other CPU

		 * core know bcache_is_reboot set to true earlier

		/*

		 * The reason bch_register_lock is not held to call

		 * bch_cache_set_stop() and bcache_device_stop() is to

		 * avoid potential deadlock during reboot, because cache

		 * set or bcache device stopping process will acquire

		 * bch_register_lock too.

		 *

		 * We are safe here because bcache_is_reboot sets to

		 * true already, register_bcache() will reject new

		 * registration now. bcache_is_reboot also makes sure

		 * bcache_reboot() won't be re-entered on by other thread,

		 * so there is no race in following list iteration by

		 * list_for_each_entry_safe().

		/*

		 * Give an early chance for other kthreads and

		 * kworkers to stop themselves

 What's a condition variable? */

 before any real devices */

 Check and fixup module parameters */

	/*

	 * Let's not make this `WQ_MEM_RECLAIM` for the following reasons:

	 *

	 * 1. It used `system_wq` before which also does no memory reclaim.

	 * 2. With `WQ_MEM_RECLAIM` desktop stalls, increased boot times, and

	 *    reduced throughput can be observed.

	 *

	 * We still want to user our own queue to not congest the `system_wq`.

/*

 * Module hooks

 SPDX-License-Identifier: GPL-2.0

/*

 * Feature set bits and string conversion.

 * Inspired by ext4's features compat/incompat/ro_compat related code.

 *

 * Copyright 2020 Coly Li <colyli@suse.de>

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * bcache journalling code, for btree insertions

 *

 * Copyright 2012 Google, Inc.

/*

 * Journal replay/recovery:

 *

 * This code is all driven from run_cache_set(); we first read the journal

 * entries, do some other stuff, then we mark all the keys in the journal

 * entries (same as garbage collection would), then we replay them - reinserting

 * them into the cache in precisely the same order as they appear in the

 * journal.

 *

 * We only journal keys that go in leaf nodes, which simplifies things quite a

 * bit.

		/* This function could be simpler now since we no longer write

		 * journal entries that overlap bucket boundaries; this means

		 * the start of a bucket will always have a valid journal entry

		 * if it has any journal entries at all.

			/*

			 * Nodes in 'list' are in linear increasing order of

			 * i->j.seq, the node on head has the smallest (oldest)

			 * journal seq, the node on tail has the biggest

			 * (latest) journal seq.

			/*

			 * Check from the oldest jset for last_seq. If

			 * i->j.seq < j->last_seq, it means the oldest jset

			 * in list is expired and useless, remove it from

			 * this list. Otherwise, j is a candidate jset for

			 * further following checks.

 iterate list in reverse order (from latest jset) */

				/*

				 * if j->seq is less than any i->j.last_seq

				 * in list, j is an expired and useless jset.

				/*

				 * 'where' points to first jset in list which

				 * is elder then j.

 Add to the location after 'where' points to */

	/*

	 * Read journal buckets ordered by golden ratio hash to quickly

	 * find a sequence of buckets with valid journal entries

		/*

		 * We must try the index l with ZERO first for

		 * correctness due to the scenario that the journal

		 * bucket is circular buffer which might have wrapped

	/*

	 * If that fails, check all the buckets we haven't checked

	 * already

 no journal entries on this device? */

 Binary search */

	/*

	 * Read buckets in reverse order until we stop finding more

	 * journal entries

			/*

			 * When journal_reclaim() goes to allocate for

			 * the first time, it'll use the bucket after

			 * ja->cur_idx

	/*

	 * journal.pin should never fill up - we never write a journal

	 * entry when it would fill up. But if for some reason it does, we

	 * iterate over the list in reverse order so that we can just skip that

	 * refcount instead of bugging.

 Journalling */

 get the oldest journal entry and check its refcount */

		/*

		 * do nothing if no btree node references

		 * the oldest journal entry

		/*

		 * It is safe to get now_fifo_front_p without holding

		 * c->journal.lock here, because we don't need to know

		 * the exactly accurate value, just check whether the

		 * front pointer of c->journal.pin is changed.

		/*

		 * If the oldest journal entry is reclaimed and front

		 * pointer of c->journal.pin changes, it is unnecessary

		 * to scan c->btree_cache anymore, just quit the loop and

		 * flush out what we have already.

		/*

		 * quit this loop if all matching btree nodes are

		 * scanned and record in btree_nodes[] already.

		/*

		 * Only select the btree node which exactly references

		 * the oldest journal entry.

		 *

		 * If the journal entry pointed by fifo_front_p is

		 * reclaimed in parallel, don't worry:

		 * - the list_for_each_xxx loop will quit when checking

		 *   next now_fifo_front_p.

		 * - If there are matched nodes recorded in btree_nodes[],

		 *   they are clean now (this is why and how the oldest

		 *   journal entry can be reclaimed). These selected nodes

		 *   will be ignored and skipped in the following for-loop.

		/*

		 * To avoid holding c->bucket_lock too long time,

		 * only scan for BTREE_FLUSH_NR matched btree nodes

		 * at most. If there are more btree nodes reference

		 * the oldest journal entry, try to flush them next

		 * time when btree_flush_write() is called.

 safe to check without holding b->write_lock */

 Update last_idx */

 No space available on this device */

	/*

	 * The fifo_push() needs to happen at the same time as j->seq is

	 * incremented for last_seq() to be calculated correctly

 If KEY_PTRS(k) == 0, this jset gets lost in air */

			/*

			 * XXX: If we were inserting so many keys that they

			 * won't fit in an _empty_ journal write, we'll

			 * deadlock. For now, handle this in

			 * bch_keylist_realloc() - but something to think about.

 unlocks */

/*

 * Entry point to the journalling code - bio_insert() and btree_invalidate()

 * pass bch_journal() a list of keys to be journalled, and then

 * bch_journal() hands those same keys off to btree_insert_async()

 No journaling if CACHE_SET_IO_DISABLE set already */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2010 Kent Overstreet <kent.overstreet@gmail.com>

 *

 * Uses a block device as cache for other block devices; optimized for SSDs.

 * All allocation is done in buckets, which should match the erase block size

 * of the device.

 *

 * Buckets containing cached data are kept on a heap sorted by priority;

 * bucket priority is increased on cache hit, and periodically all the buckets

 * on the heap have their priority scaled down. This currently is just used as

 * an LRU but in the future should allow for more intelligent heuristics.

 *

 * Buckets have an 8 bit counter; freeing is accomplished by incrementing the

 * counter. Garbage collection is used to remove stale pointers.

 *

 * Indexing is done via a btree; nodes are not necessarily fully sorted, rather

 * as keys are inserted we only sort the pages that have not yet been written.

 * When garbage collection is run, we resort the entire node.

 *

 * All configuration is done via sysfs; see Documentation/admin-guide/bcache.rst.

 Common among btree and extent ptrs */

 Btree ptrs */

 Extents */

/*

 * Returns true if l > r - unless l == r, in which case returns true if l is

 * older than r.

 *

 * Necessary for btree_sort_fixup() - if there are multiple keys that compare

 * equal in different sets, we have to process them newest to oldest.

 can't happen because of comparison func */

		/*

		 * We might overlap with 0 size extents; we can't skip these

		 * because if they're in the set we're inserting to we have to

		 * adjust them so they don't overlap with the key we're

		 * inserting. But we don't want to check them for replace

		 * operations.

			/*

			 * k might have been split since we inserted/found the

			 * key we're replacing

 But it must be a subset of the replace key */

 We didn't find a key that we were supposed to */

 skip past gen */

			/*

			 * We overlapped in the middle of an existing key: that

			 * means we have to split the old key. But we have to do

			 * slightly different things depending on whether the

			 * old key has been written out yet.

				/*

				 * We insert a new key to cover the top of the

				 * old key, and the old key is modified in place

				 * to represent the bottom split.

				 *

				 * It's completely arbitrary whether the new key

				 * is the top or the bottom, but it has to match

				 * up with what btree_sort_fixup() does - it

				 * doesn't check for this kind of overlap, it

				 * depends on us inserting a new key for the top

				 * here.

				/*

				 * Completely overwrote, so we don't have to

				 * invalidate the binary search tree

	/* Keys with no pointers aren't restricted to one bucket and could

	 * overflow KEY_SIZE

 SPDX-License-Identifier: GPL-2.0

/*

 * background writeback - scan btree for dirty data and write it to the backing

 * device

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 Rate limiting */

	/*

	 * This is the size of the cache, minus the amount used for

	 * flash-only devices

	/*

	 * Unfortunately there is no control of global dirty data.  If the

	 * user states that they want 10% dirty data in the cache, and has,

	 * e.g., 5 backing volumes of equal size, we try and ensure each

	 * backing volume uses about 2% of the cache for dirty data.

 Ensure each backing dev gets at least one dirty share */

	/*

	 * PI controller:

	 * Figures out the amount that should be written per second.

	 *

	 * First, the error (number of sectors that are dirty beyond our

	 * target) is calculated.  The error is accumulated (numerically

	 * integrated).

	 *

	 * Then, the proportional value and integral value are scaled

	 * based on configured values.  These are stored as inverses to

	 * avoid fixed point math and to make configuration easy-- e.g.

	 * the default value of 40 for writeback_rate_p_term_inverse

	 * attempts to write at a rate that would retire all the dirty

	 * blocks in 40 seconds.

	 *

	 * The writeback_rate_i_inverse value of 10000 means that 1/10000th

	 * of the error is accumulated in the integral term per second.

	 * This acts as a slow, long-term average that is not subject to

	 * variations in usage like the p term.

	/*

	 * We need to consider the number of dirty buckets as well

	 * when calculating the proportional_scaled, Otherwise we might

	 * have an unreasonable small writeback rate at a highly fragmented situation

	 * when very few dirty sectors consumed a lot dirty buckets, the

	 * worst case is when dirty buckets reached cutoff_writeback_sync and

	 * dirty data is still not even reached to writeback percent, so the rate

	 * still will be at the minimum value, which will cause the write

	 * stuck at a non-writeback mode.

 Only overrite the p when fragment > 3 */

		/*

		 * Only decrease the integral term if it's more than

		 * zero.  Only increase the integral term if the device

		 * is keeping up.  (Don't wind up the integral

		 * ineffectively in either case).

		 *

		 * It's necessary to scale this by

		 * writeback_rate_update_seconds to keep the integral

		 * term dimensioned properly.

 Don't sst max writeback rate if it is disabled */

 Don't set max writeback rate if gc is running */

	/*

	 * Idle_counter is increased everytime when update_writeback_rate() is

	 * called. If all backing devices attached to the same cache set have

	 * identical dc->writeback_rate_update_seconds values, it is about 6

	 * rounds of update_writeback_rate() on each backing device before

	 * c->at_max_writeback_rate is set to 1, and then max wrteback rate set

	 * to each dc->writeback_rate.rate.

	 * In order to avoid extra locking cost for counting exact dirty cached

	 * devices number, c->attached_dev_nr is used to calculate the idle

	 * throushold. It might be bigger if not all cached device are in write-

	 * back mode, but it still works well with limited extra rounds of

	 * update_writeback_rate().

 keep writeback_rate_target as existing value */

	/*

	 * Check c->idle_counter and c->at_max_writeback_rate agagain in case

	 * new I/O arrives during before set_at_max_writeback_rate() returns.

	 * Then the writeback rate is set to 1, and its new value should be

	 * decided via __update_writeback_rate().

	/*

	 * should check BCACHE_DEV_RATE_DW_RUNNING before calling

	 * cancel_delayed_work_sync().

 paired with where BCACHE_DEV_RATE_DW_RUNNING is tested */

	/*

	 * CACHE_SET_IO_DISABLE might be set via sysfs interface,

	 * check it here too.

 paired with where BCACHE_DEV_RATE_DW_RUNNING is tested */

		/*

		 * If the whole cache set is idle, set_at_max_writeback_rate()

		 * will set writeback rate to a max number. Then it is

		 * unncessary to update writeback rate for an idle cache set

		 * in maximum writeback rate number(s).

	/*

	 * CACHE_SET_IO_DISABLE might be set via sysfs interface,

	 * check it here too.

	/*

	 * should check BCACHE_DEV_RATE_DW_RUNNING before calling

	 * cancel_delayed_work_sync().

 paired with where BCACHE_DEV_RATE_DW_RUNNING is tested */

 This is kind of a dumb way of signalling errors. */

 Not our turn to write; wait for a write to complete */

			/*

			 * Edge case-- it happened in indeterminate order

			 * relative to when we were added to wait list..

	/*

	 * IO errors are signalled using the dirty bit on the key.

	 * If we failed to read, we should not attempt to write to the

	 * backing device.  Instead, immediately go to write_dirty_finish

	 * to clean up.

 I/O request sent to backing device */

 is_read = 1 */

	/*

	 * XXX: if we error, background writeback just spins. Should use some

	 * mempools.

			/*

			 * Don't combine too many operations, even if they

			 * are all small.

			/*

			 * If the current operation is very large, don't

			 * further combine operations.

			/*

			 * Operations are only eligible to be combined

			 * if they are contiguous.

			 *

			 * TODO: add a heuristic willing to fire a

			 * certain amount of non-contiguous IO per pass,

			 * so that we can benefit from backing device

			 * command queueing.

 Now we have gathered a set of 1..5 keys to write back. */

			/*

			 * We've acquired a semaphore for the maximum

			 * simultaneous number of writebacks; from here

			 * everything happens asynchronously.

	/*

	 * Wait for outstanding writeback IOs to finish (and keybuf slots to be

	 * freed) before refilling again

 Scan for dirty data */

/*

 * Returns true if we scanned the entire disk

	/*

	 * make sure keybuf pos is inside the range for this disk - at bringup

	 * we might not be attached yet so this disk's inode nr isn't

	 * initialized then

	/*

	 * If we get to the end start scanning again from the beginning, and

	 * only scan up to where we initially started scanning from:

		/*

		 * If the bache device is detaching, skip here and continue

		 * to perform writeback. Otherwise, if no dirty data on cache,

		 * or there is dirty data on cache but writeback is disabled,

		 * the writeback thread should sleep here and wait for others

		 * to wake up it.

			/*

			 * If bcache device is detaching via sysfs interface,

			 * writeback thread should stop after there is no dirty

			 * data on cache. BCACHE_DEV_DETACHING flag is set in

			 * bch_cached_dev_detach().

			/*

			 * When dirty data rate is high (e.g. 50%+), there might

			 * be heavy buckets fragmentation after writeback

			 * finished, which hurts following write performance.

			 * If users really care about write performance they

			 * may set BCH_ENABLE_AUTO_GC via sysfs, then when

			 * BCH_DO_AUTO_GC is set, garbage collection thread

			 * will be wake up here. After moving gc, the shrunk

			 * btree and discarded free buckets SSD space may be

			 * helpful for following write requests.

 Init */

 Update state->enough earlier */

 In order to wake up state->wait in time */

 Just count root keys if no leaf node */

 Fetch latest state->enough earlier */

 SPDX-License-Identifier: GPL-2.0

/*

 * Code for working with individual keys, and sorted sets of keys with in a

 * btree node

 *

 * Copyright 2012 Google, Inc.

 Keylists */

 Pop the top key of keylist by pointing l->top to its previous key */

 Pop the bottom key of keylist and update l->top_p */

 Key/pointer manipulation */

 Only copy the header, key, and one pointer. */

 We didn't copy the checksum so clear that bit. */

 Auxiliary search trees */

 32 bits total: */

/*

 * BSET_CACHELINE was originally intended to match the hardware cacheline size -

 * it used to be 64, but I realized the lookup code would touch slightly less

 * memory if it was 128.

 *

 * It definites the number of bytes (in struct bset) per struct bkey_float in

 * the auxiliar search tree - when we're done searching the bset_float tree we

 * have this many bytes left that we do a linear search over.

 *

 * Since (after level 5) every level of the bset_tree is on a new cacheline,

 * we're touching one fewer cacheline in the bset tree in exchange for one more

 * cacheline in the linear search - but the linear search might stop before it

 * gets to the second cacheline.

 Space required for the btree node keys */

 Space required for the auxiliary search trees */

 Space required for the prev pointers */

 Memory allocation */

	/*

	 * struct btree_keys in embedded in struct btree, and struct

	 * bset_tree is embedded into struct btree_keys. They are all

	 * initialized as 0 by kzalloc() in mca_bucket_alloc(), and

	 * b->set[0].data is allocated in bch_btree_keys_alloc(), so we

	 * don't have to initiate b->set[].size and b->set[].data here

	 * any more.

 Binary tree stuff for auxiliary search trees */

/*

 * return array index next to j when does in-order traverse

 * of a binary tree which is stored in a linear array

/*

 * return array index previous to j when does in-order traverse

 * of a binary tree which is stored in a linear array

/*

 * I have no idea why this code works... and I'm the one who wrote it

 *

 * However, I do know what it does:

 * Given a binary tree constructed in an array (i.e. how you normally implement

 * a heap), it converts a node in the tree - referenced by array index - to the

 * index it would have if you did an inorder traversal.

 *

 * Also tested for every j, size up to size somewhere around 6 million.

 *

 * The binary tree starts at array index 1, not 0

 * extra is a function of size:

 *   extra = (size - rounddown_pow_of_two(size - 1)) << 1;

/*

 * Return the cacheline index in bset_tree->data, where j is index

 * from a linear array which stores the auxiliar binary tree

/*

 * Return an index from a linear array which stores the auxiliar binary

 * tree, j is the cacheline index of t->data.

/*

 * Cacheline/offset <-> bkey pointer arithmetic:

 *

 * t->tree is a binary search tree in an array; each node corresponds to a key

 * in one cacheline in t->set (BSET_CACHELINE bytes).

 *

 * This means we don't have to store the full index of the key that a node in

 * the binary tree points to; to_inorder() gives us the cacheline, and then

 * bkey_float->m gives us the offset within that cacheline, in units of 8 bytes.

 *

 * cacheline_to_bkey() and friends abstract out all the pointer arithmetic to

 * make this work.

 *

 * To construct the bfloat for an arbitrary key we need to know what the key

 * immediately preceding it is: we have to check if the two keys differ in the

 * bits we're going to store in bkey_float->mantissa. t->prev[j] stores the size

 * of the previous key so we can walk backwards to it from t->tree[j]'s key.

/*

 * For the write set - the one we're currently inserting keys into - we don't

 * maintain a full search tree, we just keep a simple lookup table in t->prev.

/*

 * Calculate mantissa value for struct bkey_float.

 * If most significant bit of f->exponent is not set, then

 *  - f->exponent >> 6 is 0

 *  - p[0] points to bkey->low

 *  - p[-1] borrows bits from KEY_INODE() of bkey->high

 * if most isgnificant bits of f->exponent is set, then

 *  - f->exponent >> 6 is 1

 *  - p[0] points to bits from KEY_INODE() of bkey->high

 *  - p[-1] points to other bits from KEY_INODE() of

 *    bkey->high too.

 * See make_bfloat() to check when most significant bit of f->exponent

 * is set or not.

	/*

	 * If l and r have different KEY_INODE values (different backing

	 * device), f->exponent records how many least significant bits

	 * are different in KEY_INODE values and sets most significant

	 * bits to 1 (by +64).

	 * If l and r have same KEY_INODE value, f->exponent records

	 * how many different bits in least significant bits of bkey->low.

	 * See bfloat_mantiss() how the most significant bit of

	 * f->exponent is used to calculate bfloat mantissa value.

	/*

	 * Setting f->exponent = 127 flags this node as failed, and causes the

	 * lookup code to fall back to comparing against the original key.

/*

 * Build auxiliary binary tree 'struct bset_tree *t', this tree is used to

 * accelerate bkey search in a btree node (pointed by bset_tree->data in

 * memory). After search in the auxiliar tree by calling bset_search_tree(),

 * a struct bset_search_iter is returned which indicates range [l, r] from

 * bset_tree->data where the searching bkey might be inside. Then a followed

 * linear comparison does the exact search, see __bch_bset_search() for how

 * the auxiliary tree is used.

 First we figure out where the first key in each cacheline is */

 Then we build the tree */

 Insert */

 We're getting called from btree_split() or btree_gc, just bail out */

	/*

	 * k is the key we just inserted; we need to find the entry in the

	 * lookup table for the first key that is strictly greater than k:

	 * it's either k's cacheline or the next one

	/*

	 * Adjust all the lookup table entries, and find a new key for any that

	 * have gotten too big

 Possibly add a new entry to the end of the lookup table */

/*

 * Tries to merge l and r: l should be lower than r

 * Returns true if we were able to merge. If we did merge, l will be the merged

 * key, r will be untouched.

	/*

	 * Generic header checks

	 * Assumes left and right are in order

	 * Left and right must be exactly aligned

	/*

	 * If k has preceding key, preceding_key_p will be set to address

	 *  of k's preceding key; otherwise preceding_key_p will be set

	 * to NULL inside preceding_key().

 prev is in the tree, if we merge we're done */

 Lookup */

	/*

	 * n would have been the node we recursed to - the low bit tells us if

	 * we recursed left or recursed right.

	/*

	 * First, we search for a cacheline, then lastly we do a linear search

	 * within that cacheline.

	 *

	 * To search for the cacheline, there's three different possibilities:

	 *  * The set is too small to have a search tree, so we just do a linear

	 *    search over the whole set.

	 *  * The set is the one we're currently inserting into; keeping a full

	 *    auxiliary search tree up to date would be too expensive, so we

	 *    use a much simpler lookup table to do a binary search -

	 *    bset_search_write_set().

	 *  * Or we use the auxiliary search tree we constructed earlier -

	 *    bset_search_tree()

		/*

		 * Each node in the auxiliary search tree covers a certain range

		 * of bits, and keys above and below the set it covers might

		 * differ outside those bits - so we have to special case the

		 * start and end - handle that here:

 Btree iterator */

 Mergesort */

 Heapify the iterator, using our comparison function */

		/*

		 * Our temporary buffer is the same size as the btree node's

		 * buffer, we can just swap buffers instead of doing a big

		 * memcpy()

		 *

		 * Don't worry event 'out' is allocated from mempool, it can

		 * still be swapped here. Because state->pool is a page mempool

		 * creaated by by mempool_init_page_pool(), which allocates

		 * pages by alloc_pages() indeed.

 XXX: why?

 Don't sort if nothing to do */

 Sort if we'd overflow */

 SPDX-License-Identifier: GPL-2.0

/*

 * random utiility code, for bcache but in theory not specific to bcache

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

/**

 * bch_hprint - formats @v to human readable string for sysfs.

 * @buf: the (at least 8 byte) buffer to format the result into.

 * @v: signed 64 bit integer

 *

 * Returns the number of bytes used by format.

	/* For as long as the number is more than 3 digits, but at least

	 * once, shift right / divide by 1024.  Keep the remainder for

	 * a digit after the decimal point.

		/* '-', up to 3 digits, '.', 1 digit, 1 character, null;

		 * yields 8 bytes.

/**

 * bch_next_delay() - update ratelimiting statistics and calculate next delay

 * @d: the struct bch_ratelimit to update

 * @done: the amount of work done, in arbitrary units

 *

 * Increment @d by the amount of work done, and return how long to delay in

 * jiffies until the next time to do some work.

	/* Bound the time.  Don't let us fall further than 2 seconds behind

	 * (this prevents unnecessary backlog that would make it impossible

	 * to catch up).  If we're ahead of the desired writeback rate,

	 * don't let us sleep more than 2.5 seconds (so we can notice/respond

	 * if the control system tells us to speed up!).

/*

 * Generally it isn't good to access .bi_io_vec and .bi_vcnt directly,

 * the preferred way is bio_add_page, but in this case, bch_bio_map()

 * supposes that the bvec table is empty, so it is safe to access

 * .bi_vcnt & .bi_io_vec in this way even after multipage bvec is

 * supported.

/**

 * bch_bio_alloc_pages - allocates a single page for each bvec in a bio

 * @bio: bio to allocate pages for

 * @gfp_mask: flags for allocation

 *

 * Allocates pages up to @bio->bi_vcnt.

 *

 * Returns 0 on success, -ENOMEM on failure. On failure, any allocated pages are

 * freed.

	/*

	 * This is called on freshly new bio, so it is safe to access the

	 * bvec table directly.

 SPDX-License-Identifier: GPL-2.0

/*

 * bcache stats code

 *

 * Copyright 2012 Google, Inc.

/*

 * We keep absolute totals of various statistics, and addionally a set of three

 * rolling averages.

 *

 * Every so often, a timer goes off and rescales the rolling averages.

 * accounting_rescale[] is how many times the timer has to go off before we

 * rescale each set of numbers; that gets us half lives of 5 minutes, one hour,

 * and one day.

 *

 * accounting_delay is how often the timer goes off - 22 times in 5 minutes,

 * and accounting_weight is what we use to rescale:

 *

 * pow(31 / 32, 22) ~= 1/2

 *

 * So that we don't have to increment each set of numbers every time we (say)

 * get a cache hit, we increment a single atomic_t in acc->collector, and when

 * the rescale function runs it resets the atomic counter to 0 and adds its

 * old value to each of the exported numbers.

 *

 * To reduce rounding error, the numbers in struct cache_stats are all

 * stored left shifted by 16, and scaled back in the sysfs show() function.

 sysfs reading/writing */

 EWMA scaling */

 SPDX-License-Identifier: GPL-2.0

/*

 * bcache sysfs interfaces

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 Default is 0 ("writethrough") */

 Default is 0 ("auto") */

		/*

		 * Except for dirty and target, other values should

		 * be 0 if writeback is not running.

 convert binary uuid into 36-byte string plus '\0' */

 no user space access if system is rebooting */

 no user space access if system is rebooting */

 dc->writeback_running changed in __cached_dev_store() */

			/*

			 * reject setting it to 1 via sysfs if writeback

			 * kthread is not created yet.

			/*

			 * writeback kthread will check if dc->writeback_running

			 * is true or false.

	/*

	 * Only set BCACHE_DEV_WB_RUNNING when cached device attached to

	 * a cache set, otherwise it doesn't make sense.

 no user space access if system is rebooting */

 See count_io_errors for why 88 */

 no user space access if system is rebooting */

 See count_io_errors() for why 88 */

	/*

	 * write gc_after_writeback here may overwrite an already set

	 * BCH_DO_AUTO_GC, it doesn't matter because this flag will be

	 * set in next chance.

 no user space access if system is rebooting */

 Compute 31 quantiles */

 no user space access if system is rebooting */

 SPDX-License-Identifier: GPL-2.0

/*

 * Assorted bcache debug code

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 XXX: cache set refcounting */

	/*

	 * it is unnecessary to check return value of

	 * debugfs_create_file(), we should not care

	 * about this.

 SPDX-License-Identifier: GPL-2.0

/*

 * Main bcache entry point - handle a read or a write request and decide what to

 * do with it; the make_request functions are called by the block layer.

 *

 * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>

 * Copyright 2012 Google, Inc.

 Insert data into cache */

	/*

	 * The journalling code doesn't handle the case where the keys to insert

	 * is bigger than an empty write: If we just return -ENOMEM here,

	 * bch_data_insert_keys() will insert the keys created so far

	 * and finish the rest when the keylist is empty.

 get in bch_data_insert() */

	/*

	 * Our data write just errored, which means we've got a bunch of keys to

	 * insert that point to data that wasn't successfully written.

	 *

	 * We don't have to insert those keys but we still have to invalidate

	 * that region of the cache - so, if we just strip off all the pointers

	 * from the keys we'll accomplish just that.

 TODO: We could try to recover from this. */

	/*

	 * Journal writes are marked REQ_PREFLUSH; if the original write was a

	 * flush, it'll wait on the journal write.

 1 for the device pointer and 1 for the chksum */

 bch_alloc_sectors() blocks if s->writeback = true */

	/*

	 * But if it's not a writeback write we'd rather just bail out if

	 * there aren't any buckets ready to write to - it might take awhile and

	 * we might be starving btree writes for gc or something.

		/*

		 * Writethrough write: We can't complete the write until we've

		 * updated the index. But we don't want to delay the write while

		 * we wait for buckets to be freed up, so just invalidate the

		 * rest of the write.

		/*

		 * From a cache miss, we can just insert the keys for the data

		 * we have written or bail out if we didn't do anything.

/**

 * bch_data_insert - stick some data in the cache

 * @cl: closure pointer.

 *

 * This is the starting point for any data to end up in a cache device; it could

 * be from a normal write, or a writeback write, or a write to a flash only

 * volume - it's also used by the moving garbage collector to compact data in

 * mostly empty buckets.

 *

 * It first writes the data to the cache, creating a list of keys to be inserted

 * (if the data had to be fragmented there will be multiple keys); after the

 * data is written it calls bch_journal, and after the keys have been added to

 * the next journal write they're inserted into the btree.

 *

 * It inserts the data in op->bio; bi_sector is used for the key offset,

 * and op->inode is used for the key inode.

 *

 * If op->bypass is true, instead of inserting the data it invalidates the

 * region of the cache represented by op->bio and op->inode.

/*

 * Congested?  Return 0 (not congested) or the limit (in sectors)

 * beyond which we should bypass the cache due to congestion.

	/*

	 * If the bio is for read-ahead or background IO, bypass it or

	 * not depends on the following situations,

	 * - If the IO is for meta data, always cache it and no bypass

	 * - If the IO is not meta data, check dc->cache_reada_policy,

	 *      BCH_CACHE_READA_ALL: cache it and not bypass

	 *      BCH_CACHE_READA_META_ONLY: not cache it and bypass

	 * That is, read-ahead request for metadata always get cached

	 * (eg, for gfs2 or xfs).

 Cache lookup */

 Stack frame for bio_complete */

	/*

	 * If the bucket was reused while our bio was in flight, we might have

	 * read the wrong data. Set s->error but not error so it doesn't get

	 * counted against the cache device, but we'll still reread the data

	 * from the backing device.

/*

 * Read from a single key, handling the initial cache miss if the key starts in

 * the middle of the bio

 if this was a complete miss we shouldn't get here */

 XXX: figure out best pointer - for multiple cache devices */

	/*

	 * The bucket we're reading from might be reused while our bio

	 * is in flight, and we could then end up reading the wrong

	 * data.

	 *

	 * We guard against this by checking (in cache_read_endio()) if

	 * the pointer is stale again; if so, we treat it as an error

	 * and reread from the backing device (but we don't pass that

	 * error up anywhere).

	/*

	 * We might meet err when searching the btree, If that happens, we will

	 * get negative ret, in this scenario we should not recover data from

	 * backing device (when cache device is dirty) because we don't know

	 * whether bkeys the read request covered are all clean.

	 *

	 * And after that happened, s->iop.status is still its initial value

	 * before we submit s->bio.bio

 Common code for the make_request functions */

 Only cache read errors are recoverable */

		/*

		 * If a bio has REQ_PREFLUSH for writeback mode, it is

		 * speically assembled in cached_dev_write() for a non-zero

		 * write request which has REQ_PREFLUSH. we don't set

		 * s->iop.status by this failure, the status will be decided

		 * by result of bch_data_insert() operation.

 set to orig_bio->bi_status in bio_complete() */

 should count I/O error for backing device here */

 Count on bcache device */

	/*

	 * bi_end_io can be set separately somewhere else, e.g. the

	 * variants in,

	 * - cache_bio->bi_end_io from cached_dev_cache_miss()

	 * - n->bi_end_io from cache_lookup_fn()

 Count on the bcache device */

 Cached devices */

 Process reads */

	/*

	 * If read request hit dirty data (s->read_dirty_data is true),

	 * then recovery a failed read request from cached device may

	 * get a stale data back. So read failure recovery is only

	 * permitted when read request hit clean data in cache device,

	 * or when cache read race happened.

 Retry from the backing device: */

 XXX: invalidate cache */

 I/O request sent to backing device */

	/*

	 * We had a cache miss; cache_bio now contains data ready to be inserted

	 * into the cache.

	 *

	 * First, we copy the data we just read from cache_bio's bounce buffers

	 * to the buffers the original bio pointed to:

 Limitation for valid replace key size and cache_bio bvecs number */

 btree_search_recurse()'s btree iterator is no good anymore */

 I/O request sent to backing device */

 I/O request sent to backing device */

 Process writes */

		/*

		 * We overlap with some dirty data undergoing background

		 * writeback, force this write to writeback

	/*

	 * Discards aren't _required_ to do anything, so skipping if

	 * check_overlapping returned true is ok

	 *

	 * But check_overlapping drops dirty keys for which io hasn't started,

	 * so we still want to call it.

 I/O request sent to backing device */

			/*

			 * Also need to send a flush to the backing

			 * device.

 I/O request sent to backing device */

 I/O request sent to backing device */

 If it's a flush, we send the flush to the backing device too */

 Count on the bcache device */

 should count I/O error for backing device here */

	/*

	 * no need to call closure_get(&dc->disk.cl),

	 * because upper layer had already opened bcache device,

	 * which would call closure_get(&dc->disk.cl)

 Count on the bcache device */

	/*

	 * mutex bch_register_lock may compete with other parallel requesters,

	 * or attach/detach operations on other backing device. Waiting to

	 * the mutex lock may increase I/O request latency for seconds or more.

	 * To avoid such situation, if mutext_trylock() failed, only writeback

	 * rate of current cached device is set to 1, and __update_write_back()

	 * will decide writeback rate of other cached devices (remember now

	 * c->idle_counter is 0 already).

			/*

			 * set writeback rate to default minimum value,

			 * then let update_writeback_rate() to decide the

			 * upcoming rate.

 Cached devices - read & write stuff */

		/*

		 * If at_max_writeback_rate of cache set is true and new I/O

		 * comes, quit max writeback rate of all cached devices

		 * attached to this cache set, and set at_max_writeback_rate

		 * to false.

			/*

			 * can't call bch_journal_meta from under

			 * submit_bio_noacct

 I/O request sent to backing device */

 Flash backed devices */

		/*

		 * can't call bch_journal_meta from under submit_bio_noacct

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2010 Kent Overstreet <kent.overstreet@gmail.com>

 *

 * Uses a block device as cache for other block devices; optimized for SSDs.

 * All allocation is done in buckets, which should match the erase block size

 * of the device.

 *

 * Buckets containing cached data are kept on a heap sorted by priority;

 * bucket priority is increased on cache hit, and periodically all the buckets

 * on the heap have their priority scaled down. This currently is just used as

 * an LRU but in the future should allow for more intelligent heuristics.

 *

 * Buckets have an 8 bit counter; freeing is accomplished by incrementing the

 * counter. Garbage collection is used to remove stale pointers.

 *

 * Indexing is done via a btree; nodes are not necessarily fully sorted, rather

 * as keys are inserted we only sort the pages that have not yet been written.

 * When garbage collection is run, we resort the entire node.

 *

 * All configuration is done via sysfs; see Documentation/admin-guide/bcache.rst.

/*

 * Todo:

 * register_bcache: Return errors out to userspace correctly

 *

 * Writeback: don't undirty key until after a cache flush

 *

 * Create an iterator for key pointers

 *

 * On btree write error, mark bucket such that it won't be freed from the cache

 *

 * Journalling:

 *   Check for bad keys in replay

 *   Propagate barriers

 *   Refcount journal entries in journal_replay

 *

 * Garbage collection:

 *   Finish incremental gc

 *   Gc should free old UUIDs, data for invalid UUIDs

 *

 * Provide a way to list backing device UUIDs we have data cached for, and

 * probably how long it's been since we've seen them, and a way to invalidate

 * dirty data for devices that will never be attached again

 *

 * Keep 1 min/5 min/15 min statistics of how busy a block device has been, so

 * that based on that and how much dirty data we have we can keep writeback

 * from being starved

 *

 * Add a tracepoint or somesuch to watch for writeback starvation

 *

 * When btree depth > 1 and splitting an interior node, we have to make sure

 * alloc_bucket() cannot fail. This should be true but is not completely

 * obvious.

 *

 * Plugging?

 *

 * If data write is less than hard sector size of ssd, round up offset in open

 * bucket to the next whole sector

 *

 * Superblock needs to be fleshed out for multiple cache devices

 *

 * Add a sysfs tunable for the number of writeback IOs in flight

 *

 * Add a sysfs tunable for the number of open data buckets

 *

 * IO tracking: Can we track when one process is doing io on behalf of another?

 * IO tracking: Don't use just an average, weigh more recent stuff higher

 *

 * Test module load/unload

 If not a leaf node, always sort */

 Btree key manipulation */

 Btree IO */

	/*

	 * c->fill_iter can allocate an iterator with more memory space

	 * than static MAX_BSETS.

	 * See the comment arount cache_set->fill_iter.

	/*

	 * If we're appending to a leaf node, we don't technically need FUA -

	 * this write just needs to be persisted before the next journal write,

	 * which will be marked FLUSH|FUA.

	 *

	 * Similarly if we're writing a new btree root - the pointer is going to

	 * be in the next journal entry.

	 *

	 * But if we're writing a new btree node (that isn't a root) or

	 * appending to a non leaf btree node, we need either FUA or a flush

	 * when we write the parent with the new pointer. FUA is cheaper than a

	 * flush, and writes appending to leaf nodes aren't blocking anything so

	 * just make all btree node writes FUA to keep things sane.

		/*

		 * No problem for multipage bvec since the bio is

		 * just allocated

 If caller isn't waiting for write, parent refcount is cache set */

	/*

	 * do verify if there was more than one set initially (i.e. we did a

	 * sort) and we sorted down to a single set:

	/*

	 * w->journal is always the oldest journal pin of all bkeys

	 * in the leaf node, to make sure the oldest jset seq won't

	 * be increased before this btree node is flushed.

 Force write if set is too big */

/*

 * Btree in memory cache - allocation/freeing

 * mca -> memory cache

	/*

	 * kzalloc() is necessary here for initialization,

	 * see code comments in bch_btree_keys_init().

	/*

	 * BTREE_NODE_dirty might be cleared in btree_flush_btree() by

	 * __bch_btree_node_write(). To avoid an extra flush, acquire

	 * b->write_lock before checking BTREE_NODE_dirty bit.

	/*

	 * If this btree node is selected in btree_flush_write() by journal

	 * code, delay and retry until the node is flushed by journal code

	 * and BTREE_NODE_journal_flush bit cleared by btree_flush_write().

 wait for any in flight btree write */

 Return -1 if we can't do anything right now */

	/*

	 * It's _really_ critical that we don't free too many btree nodes - we

	 * have to always leave ourselves a reserve. The reserve is how we

	 * guarantee that allocating memory for a new btree node can always

	 * succeed, so that inserting keys into the btree can always succeed and

	 * IO can always make forward progress:

		/*

		 * This function is called by cache_set_free(), no I/O

		 * request on cache now, it is unnecessary to acquire

		 * b->write_lock before clearing BTREE_NODE_dirty anymore.

		/*

		 * Don't worry about the mca_rereserve buckets

		 * allocated in previous for-loop, they will be

		 * handled properly in bch_cache_set_unregister().

 Btree in memory cache - hash table */

/*

 * We can only have one thread cannibalizing other cached btree nodes at a time,

 * or we'll deadlock. We use an open coded mutex to ensure that, which a

 * cannibalize_bucket() will take. This means every time we unlock the root of

 * the btree, we need to release this lock if we have it held.

	/* btree_free() doesn't free memory; it sticks the node on the end of

	 * the list. Check if there's any freed nodes there:

	/* We never free struct btree itself, just the memory that holds the on

	 * disk node. Check the freed list before allocating a new one:

/*

 * bch_btree_node_get - find a btree node in the cache and lock it, reading it

 * in from disk if necessary.

 *

 * If IO is necessary and running under submit_bio_noacct, returns -EAGAIN.

 *

 * The btree node will have either a read or a write lock held, depending on

 * level and op->lock.

 Btree alloc */

	/*

	 * If the btree node is selected and flushing in btree_flush_write(),

	 * delay and retry until the BTREE_NODE_journal_flush bit cleared,

	 * then it is safe to free the btree node here. Otherwise this btree

	 * node will be in race condition.

 Garbage collection */

	/*

	 * ptr_invalid() can't return true for the keys that mark btree nodes as

	 * freed, but since ptr_bad() returns true we'll never actually use them

	 * for anything and thus we don't want mark their pointers here

 guard against overflow */

	/*

	 * We have to check the reserve here, after we've allocated our new

	 * nodes, to make sure the insert below will succeed - we also check

	 * before as an optimization to potentially avoid a bunch of expensive

	 * allocs/sorts

			/*

			 * Last node we're not getting rid of - we're getting

			 * rid of the node at r[0]. Have to try and fit all of

			 * the remaining keys into this node; we can't ensure

			 * they will always fit due to rounding and variable

			 * length keys (shouldn't be possible in practice,

			 * though)

 Take the key of the node we're getting rid of */

 We emptied out this node */

 Invalidated our iterator */

 recheck reserve after allocating replacement node */

 Invalidated our iterator */

	/*

	 * Since incremental GC would stop 100ms when front

	 * side I/O comes, so when there are many btree nodes,

	 * if GC only processes constant (100) nodes each time,

	 * GC would last a long time, and the front side I/Os

	 * would run out of the buckets (since no new bucket

	 * can be allocated during GC), and be blocked again.

	 * So GC should not process constant nodes, but varied

	 * nodes according to the number of btree nodes, which

	 * realized by dividing GC into constant(100) times,

	 * so when there are many btree nodes, GC can process

	 * more nodes each time, otherwise, GC will process less

	 * nodes each time (but no less than MIN_GC_NODES)

			/*

			 * Must flush leaf nodes before gc ends, since replace

			 * operations aren't journalled

 don't reclaim buckets to which writeback keys point */

 if CACHE_SET_IO_DISABLE set, gc thread should stop too */

 Initial partial gc */

				/*

				 * initiallize c->gc_stats.nodes

				 * for incremental GC

 root node keys are checked before thread created */

		/*

		 * Fetch a root node key index, skip the keys which

		 * should be fetched by other threads, then check the

		 * sub-tree indexed by the fetched key.

				/*

				 * No more keys to check in root node,

				 * current checking threads are enough,

				 * stop creating more.

 Update check_state->enough earlier */

 update check_state->started among all CPUs */

 check and mark root node keys */

	/*

	 * Run multiple threads to check btree nodes in parallel,

	 * if check_state->enough is non-zero, it means current

	 * running check threads are enough, unncessary to create

	 * more.

 fetch latest check_state->enough earlier */

	/*

	 * We need to put some unused buckets directly on the prio freelist in

	 * order to get the allocator thread started - it needs freed buckets in

	 * order to rewrite the prios and gens, and it needs to rewrite prios

	 * and gens in order to free buckets.

	 *

	 * This is only safe for buckets that have no live data in them, which

	 * there should always be some of.

 Btree insertion */

	/*

	 * Might land in the middle of an existing extent and have to split it

		/*

		 * Has to be a linear search because we don't have an auxiliary

		 * search tree yet

 Depth increases, make a new root */

 Root filled up but didn't need to be split */

 Split a non root node */

 just wrote a set */

 wait for btree node write if necessary, after unlock */

 Invalidated all iterators */

 Map across nodes or keys */

 Keybuf code */

 Overlapping keys compare equal */

 end key */

 SPDX-License-Identifier: GPL-2.0

/*

 * Moving/copying garbage collector

 *

 * Copyright 2012 Google, Inc.

 Moving GC - IO loop */

 XXX: if we error, background writeback could stall indefinitely */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Primary bucket allocation code

 *

 * Copyright 2012 Google, Inc.

 *

 * Allocation in bcache is done in terms of buckets:

 *

 * Each bucket has associated an 8 bit gen; this gen corresponds to the gen in

 * btree pointers - they must match for the pointer to be considered valid.

 *

 * Thus (assuming a bucket has no dirty data or metadata in it) we can reuse a

 * bucket simply by incrementing its gen.

 *

 * The gens (along with the priorities; it's really the gens are important but

 * the code is named as if it's the priorities) are written in an arbitrary list

 * of buckets on disk, with a pointer to them in the journal header.

 *

 * When we invalidate a bucket, we have to write its new gen to disk and wait

 * for that write to complete before we use it - otherwise after a crash we

 * could have pointers that appeared to be good but pointed to data that had

 * been overwritten.

 *

 * Since the gens and priorities are all stored contiguously on disk, we can

 * batch this up: We fill up the free_inc list with freshly invalidated buckets,

 * call prio_write(), and when prio_write() finishes we pull buckets off the

 * free_inc list and optionally discard them.

 *

 * free_inc isn't the only freelist - if it was, we'd often to sleep while

 * priorities and gens were being written before we could allocate. c->free is a

 * smaller freelist, and buckets on that list are always ready to be used.

 *

 * If we've got discards enabled, that happens when a bucket moves from the

 * free_inc list to the free list.

 *

 * There is another freelist, because sometimes we have buckets that we know

 * have nothing pointing into them - these we can reuse without waiting for

 * priorities to be rewritten. These come from freed btree nodes and buckets

 * that garbage collection discovered no longer had valid keys pointing into

 * them (because they were overwritten). That's the unused list - buckets on the

 * unused list move to the free list, optionally being discarded in the process.

 *

 * It's also important to ensure that gens don't wrap around - with respect to

 * either the oldest gen in the btree or the gen on disk. This is quite

 * difficult to do in practice, but we explicitly guard against it anyways - if

 * a bucket is in danger of wrapping around we simply skip invalidating it that

 * time around, and we garbage collect or rewrite the priorities sooner than we

 * would have otherwise.

 *

 * bch_bucket_alloc() allocates a single bucket from a specific cache.

 *

 * bch_bucket_alloc_set() allocates one  bucket from different caches

 * out of a cache set.

 *

 * free_some_buckets() drives all the processes described above. It's called

 * from bch_bucket_alloc() and a few other places that need to make sure free

 * buckets are ready.

 *

 * invalidate_buckets_(lru|fifo)() find buckets that are available to be

 * invalidated, and then invalidate them and stick them on the free_inc list -

 * in either lru or fifo order.

 Bucket heap / gen */

/*

 * Background allocation thread: scans for buckets to be invalidated,

 * invalidates them, rewrites prios/gens (marking them as invalidated on disk),

 * then optionally issues discard commands to the newly free buckets, then puts

 * them on the various freelists.

/*

 * Determines what order we're going to reuse buckets, smallest bucket_prio()

 * first: we also take into account the number of sectors of live data in that

 * bucket, and in order for that multiply to make sense we have to scale bucket

 *

 * Thus, we scale the bucket priorities so that the bucket with the smallest

 * prio is worth 1/8th of what INITIAL_PRIO is worth.

			/*

			 * We don't want to be calling invalidate_buckets()

			 * multiple times when it can't do anything

 Prios/gens are actually the most important reserve */

		/*

		 * First, we pull buckets off of the unused and free_inc lists,

		 * possibly issue discards to them, then we add the bucket to

		 * the free list:

		/*

		 * We've run out of free buckets, we need to find some buckets

		 * we can invalidate. First, invalidate them in memory and add

		 * them to the free_inc list:

		/*

		 * Now, we write their new gens to disk so we can start writing

		 * new stuff to them:

			/*

			 * This could deadlock if an allocation with a btree

			 * node locked ever blocked - having the btree node

			 * locked would block garbage collection, but here we're

			 * waiting on garbage collection before we invalidate

			 * and free anything.

			 *

			 * But this should be safe since the btree code always

			 * uses btree_check_reserve() before allocating now, and

			 * if it fails it blocks without btree nodes locked.

 Allocation */

 No allocation if CACHE_SET_IO_DISABLE bit is set */

 fastpath */

 No allocation if CACHE_SET_IO_DISABLE bit is set */

 Sector allocator */

/*

 * We keep multiple buckets open for writes, and try to segregate different

 * write streams for better cache utilization: first we try to segregate flash

 * only volume write streams from cached devices, secondly we look for a bucket

 * where the last write to it was sequential with the current write, and

 * failing that we look for a bucket that was last used by the same task.

 *

 * The ideas is if you've got multiple tasks pulling data into the cache at the

 * same time, you'll get better cache utilization if you try to segregate their

 * data and preserve locality.

 *

 * For example, dirty sectors of flash only volume is not reclaimable, if their

 * dirty sectors mixed with dirty sectors of cached device, such buckets will

 * be marked as dirty and won't be reclaimed, though the dirty data of cached

 * device have been written back to backend device.

 *

 * And say you've starting Firefox at the same time you're copying a

 * bunch of files. Firefox will likely end up being fairly hot and stay in the

 * cache awhile, but the data you copied might not be; if you wrote all that

 * data to the same buckets it'd get invalidated at the same time.

 *

 * Both of those tasks will be doing fairly random IO so we can't rely on

 * detecting sequential IO to segregate their data, but going off of the task

 * should be a sane heuristic.

/*

 * Allocates some space in the cache to write to, and k to point to the newly

 * allocated space, and updates KEY_SIZE(k) and KEY_OFFSET(k) (to point to the

 * end of the newly allocated space).

 *

 * May allocate fewer sectors than @sectors, KEY_SIZE(k) indicates how many

 * sectors were actually allocated.

 *

 * If s->writeback is true, will not fail.

	/*

	 * We might have to allocate a new bucket, which we can't do with a

	 * spinlock held. So if we have to allocate, we drop the lock, allocate

	 * and then retry. KEY_PTRS() indicates whether alloc points to

	 * allocated bucket(s).

	/*

	 * If we had to allocate, we might race and not need to allocate the

	 * second time we call pick_data_bucket(). If we allocated a bucket but

	 * didn't use it, drop the refcount bch_bucket_alloc_set() took:

 Set up the pointer to the space we're allocating: */

	/*

	 * Move b to the end of the lru, and keep track of what this bucket was

	 * last used for:

	/*

	 * k takes refcounts on the buckets it points to until it's inserted

	 * into the btree, but if we're done with this bucket we just transfer

	 * get_data_bucket()'s refcount.

 Init */

/*

 * Copyright (C) 2012 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * The array is implemented as a fully populated btree, which points to

 * blocks that contain the packed values.  This is more space efficient

 * than just using a btree since we don't store 1 key per value.

 Block this node is supposed to live in. */

----------------------------------------------------------------*/

/*

 * Validator methods.  As usual we calculate a checksum, and also write the

 * block location into the header (paranoia about ssds remapping areas by

 * mistake).

----------------------------------------------------------------*/

/*

 * Functions for manipulating the array blocks.

/*

 * Returns a pointer to a value within an array block.

 *

 * index - The index into _this_ specific block.

/*

 * Utility function that calls one of the value_type methods on every value

 * in an array block.

/*

 * Increment every value in an array block.

/*

 * Decrement every value in an array block.

/*

 * Each array block can hold this many values.

/*

 * Allocate a new array block.  The caller will need to unlock block.

/*

 * Pad an array block out with a particular value.  Every instance will

 * cause an increment of the value_type.  new_nr must always be more than

 * the current number of entries.

/*

 * Remove some entries from the back of an array block.  Every value

 * removed will be decremented.  new_nr must be <= the current number of

 * entries.

/*

 * Read locks a block, and coerces it to an array block.  The caller must

 * unlock 'block' when finished.

/*

 * Unlocks an array block.

----------------------------------------------------------------*/

/*

 * Btree manipulation.

/*

 * Looks up an array block in the btree, and then read locks it.

 *

 * index is the index of the index of the array_block, (ie. the array index

 * / max_entries).

/*

 * Insert an array block into the btree.  The block is _not_ unlocked.

----------------------------------------------------------------*/

/*

 * The shadow op will often be a noop.  Only insert if it really

 * copied data.

		/*

		 * dm_tm_shadow_block will have already decremented the old

		 * block, but it is still referenced by the btree.  We

		 * increment to stop the insert decrementing it below zero

		 * when overwriting the old value.

/*

 * Looks up an array block in the btree.  Then shadows it, and updates the

 * btree to point to this new shadow.  'root' is an input/output parameter

 * for both the current root block, and the new one.

/*

 * Allocate an new array block, and fill it with some values.

/*

 * There are a bunch of functions involved with resizing an array.  This

 * structure holds information that commonly needed by them.  Purely here

 * to reduce parameter count.

	/*

	 * Describes the array.

	/*

	 * The current root of the array.  This gets updated.

	/*

	 * Metadata block size.  Used to calculate the nr entries in an

	 * array block.

	/*

	 * Maximum nr entries in an array block.

	/*

	 * nr of completely full blocks in the array.

	 *

	 * 'old' refers to before the resize, 'new' after.

	/*

	 * Number of entries in the final block.  0 iff only full blocks in

	 * the array.

	/*

	 * The default value used when growing the array.

/*

 * Removes a consecutive set of array blocks from the btree.  The values

 * in block are decremented as a side effect of the btree remove.

 *

 * begin_index - the index of the first array block to remove.

 * end_index - the one-past-the-end value.  ie. this block is not removed.

/*

 * Calculates how many blocks are needed for the array.

/*

 * Shrink an array.

	/*

	 * Lose some blocks from the back?

	/*

	 * Trim the new tail block

/*

 * Grow an array.

----------------------------------------------------------------*/

/*

 * These are the value_type functions for the btree elements, which point

 * to array blocks.

		/*

		 * We're about to drop the last reference to this ablock.

		 * So we need to decrement the ref count of the contents.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * This is a read/write semaphore with a couple of differences.

 *

 * i) There is a restriction on the number of concurrent read locks that

 * may be held at once.  This is just an implementation detail.

 *

 * ii) Recursive locking attempts are detected and return EINVAL.  A stack

 * trace is also emitted for the previous lock acquisition.

 *

 * iii) Priority is given to write locks.

 call this *after* you increment lock->count */

 call this *before* you decrement lock->count */

/*

 * We either wake a few readers or a single writer.

 still read locked */

	/*

	 * Writers given priority. We know there's only one mutator in the

	 * system, so ignoring the ordering reversal.

 !CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING */

 CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING */

----------------------------------------------------------------*/

/*

 * Block manager is currently implemented using dm-bufio.  struct

 * dm_block_manager and struct dm_block map directly onto a couple of

 * structs in the bufio interface.  I want to retain the freedom to move

 * away from bufio in the future.  So these structs are just cast within

 * this .c file, rather than making it through to the public interface.

/*----------------------------------------------------------------

 * Public interface

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * It would be nice if we scaled with the size of transaction.

----------------------------------------------------------------*/

/*

 * This can silently fail if there's no memory.  We're ok with this since

 * creating redundant shadows causes no harm.

----------------------------------------------------------------*/

	/*

	 * New blocks count as shadows in that they don't need to be

	 * shadowed again.

	/*

	 * It would be tempting to use dm_bm_unlock_move here, but some

	 * code, such as the space maps, keeps using the old data structures

	 * secure in the knowledge they won't be changed until the next

	 * transaction.  Using unlock_move would force a synchronous read

	 * since the old block would no longer be in the cache.

	/*

	 * The non-blocking clone doesn't support this.

	/*

	 * The non-blocking clone doesn't support this.

	/*

	 * The non-blocking clone doesn't support this.

	/*

	 * The non-blocking clone doesn't support this.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

/*

 * Removing an entry from a btree

 * ==============================

 *

 * A very important constraint for our btree is that no node, except the

 * root, may have fewer than a certain number of entries.

 * (MIN_ENTRIES <= nr_entries <= MAX_ENTRIES).

 *

 * Ensuring this is complicated by the way we want to only ever hold the

 * locks on 2 nodes concurrently, and only change nodes in a top to bottom

 * fashion.

 *

 * Each node may have a left or right sibling.  When decending the spine,

 * if a node contains only MIN_ENTRIES then we try and increase this to at

 * least MIN_ENTRIES + 1.  We do this in the following ways:

 *

 * [A] No siblings => this can only happen if the node is the root, in which

 *     case we copy the childs contents over the root.

 *

 * [B] No left sibling

 *     ==> rebalance(node, right sibling)

 *

 * [C] No right sibling

 *     ==> rebalance(left sibling, node)

 *

 * [D] Both siblings, total_entries(left, node, right) <= DEL_THRESHOLD

 *     ==> delete node adding it's contents to left and right

 *

 * [E] Both siblings, total_entries(left, node, right) > DEL_THRESHOLD

 *     ==> rebalance(left, node, right)

 *

 * After these operations it's possible that the our original node no

 * longer contains the desired sub tree.  For this reason this rebalancing

 * is performed on the children of the current node.  This also avoids

 * having a special case for the root.

 *

 * Once this rebalancing has occurred we can then step into the child node

 * for internal nodes.  Or delete the entry for leaf nodes.

/*

 * Some little utilities for moving node data around.

/*

 * Delete a specific entry from a leaf node.

	/*

	 * Ensure the number of entries in each child will be greater

	 * than or equal to (max_entries / 3 + 1), so no matter which

	 * child is used for removal, the number will still be not

	 * less than (max_entries / 3).

		/*

		 * Merge

		/*

		 * We need to decrement the right block, but not it's

		 * children, since they're still referenced by left.

		/*

		 * Rebalance.

/*

 * We dump as many entries from center as possible into left, then the rest

 * in right, then rebalance2.  This wastes some cpu, but I want something

 * simple atm.

/*

 * Redistributes entries among 3 sibling nodes.

 not enough in central node */

 not enough in central node */

	/*

	 * FIXME: fill out an array?

/*

 * Prepares for removal from one level of the hierarchy.  The caller must

 * call delete_at() to remove the entry at index.

		/*

		 * We have to patch up the parent node, ugly, but I don't

		 * see a way to do this automatically as part of the spine

		 * op.

		/*

		 * We know the key is present, or else

		 * rebalance_children would have returned

		 * -ENODATA

----------------------------------------------------------------*/

		/*

		 * We have to patch up the parent node, ugly, but I don't

		 * see a way to do this automatically as part of the spine

		 * op.

		/*

		 * We know the key is present, or else

		 * rebalance_children would have returned

		 * -ENODATA

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * Index validator.

----------------------------------------------------------------*/

/*

 * Bitmap validator

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * Because the new bitmap blocks are created via a shadow

	 * operation, the old entry has already had its reference count

	 * decremented and we don't need the btree to do any bookkeeping.

	/*

	 * We need to set this before the dm_tm_new_block() call below.

	/*

	 * FIXME: Use shifts

			/*

			 * This might happen because we started searching

			 * part way through the bitmap.

 double check this block wasn't used in the old transaction */

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Holds useful intermediate results for the range based inc and dec

 * operations.

/*

 * Confirms a btree node contains a particular key at an index.

	/*

	 * bitmap_block needs to be unlocked because getting the

	 * overflow_leaf may need to allocate, and thus use the space map.

	/*

	 * Do we already have the correct overflow leaf?

/*

 * Once shadow_bitmap has been called, which always happens at the start of inc/dec,

 * we can reopen the bitmap with a simple write lock, rather than re calling

 * dm_tm_shadow_block().

/*

 * Loops round incrementing entries in a single bitmap.

		/*

		 * We only need to drop the bitmap if we need to find a new btree

		 * leaf for the overflow.  So if it was dropped last iteration,

		 * we now re-get it.

 inc bitmap, adjust nr_allocated */

 inc bitmap */

 inc bitmap and insert into overflow */

			/*

			 * inc within the overflow tree only.

/*

 * Finds a bitmap that contains entries in the block range, and increments

 * them.

----------------------------------------------------------------*/

	/*

	 * Do we already have the correct overflow leaf?

/*

 * Loops round incrementing entries in a single bitmap.

		/*

		 * We only need to drop the bitmap if we need to find a new btree

		 * leaf for the overflow.  So if it was dropped last iteration,

		 * we now re-get it.

 dec bitmap */

 dec bitmap and insert into overflow */

----------------------------------------------------------------*/

----------------------------------------------------------------*/

	/*

	 * We don't know the alignment of the root_le buffer, so need to

	 * copy into a new structure.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2012 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

/*----------------------------------------------------------------

 * Array manipulation

----------------------------------------------------------------*/

 makes the assumption that no two keys are the same. */

----------------------------------------------------------------*/

/*

 * We want 3n entries (for some n).  This works more nicely for repeated

 * insert remove loops than (2n + 1).

 key + value */

 rounds down */

----------------------------------------------------------------*/

/*

 * Deletion uses a recursive algorithm, since we have limited stack space

 * we explicitly manage our own stack on the heap.

		/*

		 * This is a shared node, so we can just decrement it's

		 * reference counter and leave the children.

	/*

	 * dm_btree_del() is called via an ioctl, as such should be

	 * considered an FS op.  We can't recurse back into the FS, so we

	 * allocate GFP_NOFS.

 cleanup all frames of del_stack */

----------------------------------------------------------------*/

			/*

			 * avoid early -ENODATA return when all entries are

			 * higher than the search @key.

----------------------------------------------------------------*/

/*

 * Copies entries from one region of a btree node to another.  The regions

 * must not overlap.

/*

 * Moves entries from one region fo a btree node to another.  The regions

 * may overlap.

/*

 * Erases the first 'count' entries of a btree node, shifting following

 * entries down into their place.

/*

 * Moves entries in a btree node up 'count' places, making space for

 * new entries at the start of the node.

/*

 * Redistributes entries between two btree nodes to make them

 * have similar numbers of entries.

/*

 * Redistribute entries between three nodes.  Assumes the central

 * node is empty.

/*

 * Splits a node by creating a sibling node and shifting half the nodes

 * contents across.  Assumes there is a parent node, and it has room for

 * another child.

 *

 * Before:

 *	  +--------+

 *	  | Parent |

 *	  +--------+

 *	     |

 *	     v

 *	+----------+

 *	| A ++++++ |

 *	+----------+

 *

 *

 * After:

 *		+--------+

 *		| Parent |

 *		+--------+

 *		  |	|

 *		  v	+------+

 *	    +---------+	       |

 *	    | A* +++  |	       v

 *	    +---------+	  +-------+

 *			  | B +++ |

 *			  +-------+

 *

 * Where A* is a shadow of A.

 patch up the parent */

 patch up the spine */

/*

 * We often need to modify a sibling node.  This function shadows a particular

 * child of the given parent node.  Making sure to update the parent to point

 * to the new shadow.

/*

 * Splits two nodes into three.  This is more work, but results in fuller

 * nodes, so saves metadata space.

 patch up the parent */

 patch up the spine */

----------------------------------------------------------------*/

/*

 * Splits a node by creating two new children beneath the given node.

 *

 * Before:

 *	  +----------+

 *	  | A ++++++ |

 *	  +----------+

 *

 *

 * After:

 *	+------------+

 *	| A (shadow) |

 *	+------------+

 *	    |	|

 *   +------+	+----+

 *   |		     |

 *   v		     v

 * +-------+	 +-------+

 * | B +++ |	 | C +++ |

 * +-------+	 +-------+

 create & init the left block */

 create & init the right block */

 new_parent should just point to l and r now */

----------------------------------------------------------------*/

/*

 * Redistributes a node's entries with its left sibling.

/*

 * Redistributes a nodes entries with its right sibling.

/*

 * Returns the number of spare entries in a node.

/*

 * Make space in a node, either by moving some entries to a sibling,

 * or creating a new sibling node.  SPACE_THRESHOLD defines the minimum

 * number of free entries that must be in the sibling to make the move

 * worth while.  If the siblings are shared (eg, part of a snapshot),

 * then they are not touched, since this break sharing and so consume

 * more space than we save.

 Should we move entries to the left sibling? */

 Should we move entries to the right sibling? */

	/*

	 * We need to split the node, normally we split two nodes

	 * into three.	But when inserting a sequence that is either

	 * monotonically increasing or decreasing it's better to split

	 * a single node into two.

/*

 * Does the node contain a particular key?

/*

 * In general we preemptively make sure there's a free entry in every

 * node on the spine when doing an insert.  But we can avoid that with

 * leaf nodes if we know it's an overwrite.

 we don't need space if it's an overwrite */

		/*

		 * We have to patch up the parent node, ugly, but I don't

		 * see a way to do this automatically as part of the spine

		 * op.

 FIXME: second clause unness. */

 making space can cause the current node to change */

 change the bounds on the lowest key */

		/*

		 * We have to patch up the parent node, ugly, but I don't

		 * see a way to do this automatically as part of the spine

		 * op.

		/*

		 * Decrement the count so exit_shadow_spine() doesn't

		 * unlock the leaf.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * FIXME: We shouldn't use a recursive algorithm when we have limited stack

 * space.  Also this only works for single level trees.

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * Space map interface.

	/*

	 * Any block we allocate has to be free in both the old and current ll.

		/*

		 * There's no free block between smd->begin and the end of the metadata device.

		 * We search before smd->begin in case something has been freed.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

/*

 * An edge triggered threshold.

----------------------------------------------------------------*/

/*

 * Space map interface.

 *

 * The low level disk format is written using the standard btree and

 * transaction manager.  This means that performing disk operations may

 * cause us to recurse into the space map in order to allocate new blocks.

 * For this reason we have a pool of pre-allocated blocks large enough to

 * service any metadata_ll_disk operation.

/*

 * FIXME: we should calculate this based on the size of the device.

 * Only the metadata space map needs this functionality.

	/*

	 * We don't allow the last bop to be filled, this way we can

	 * differentiate between full and empty.

----------------------------------------------------------------*/

	/*

	 * If we're not recursing then very bad things are happening.

/*

 * When using the out() function above, we often want to combine an error

 * code for the operation run in the recursive context with that from

 * out().

	/*

	 * We may have some uncommitted adjustments to add.  This list

	 * should always be really short.

	/*

	 * We may have some uncommitted adjustments to add.  This list

	 * should always be really short.

		/*

		 * We err on the side of caution, and always return true.

	/*

	 * Any block we allocate has to be free in both the old and current ll.

		/*

		 * There's no free block between smm->begin and the end of the metadata device.

		 * We search before smm->begin in case something has been freed.

----------------------------------------------------------------*/

/*

 * When a new space map is created that manages its own space.  We use

 * this tiny bootstrap allocator.

	/*

	 * We know the entire device is unused.

----------------------------------------------------------------*/

	/*

	 * Flick into a mode where all blocks get allocated in the new area.

	/*

	 * Extend.

	/*

	 * We repeatedly increment then commit until the commit doesn't

	 * allocate any new blocks.

	/*

	 * Switch back to normal behaviour.

----------------------------------------------------------------*/

	/*

	 * Now we need to update the newly created data structures with the

	 * allocated blocks that they were built from.

/*

 * Copyright (C) 2011 Red Hat, Inc.

 *

 * This file is released under the GPL.

----------------------------------------------------------------*/

	/*

	 * The node must be either INTERNAL or LEAF.

----------------------------------------------------------------*/

----------------------------------------------------------------*/

----------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * extcon_gpio.c - Single-state GPIO extcon driver based on extcon class

 *

 * Copyright (C) 2008 Google, Inc.

 * Author: Mike Lockwood <lockwood@android.com>

 *

 * Modified by MyungJoo Ham <myungjoo.ham@samsung.com> to support extcon

 * (originally switch class is supported)

/**

 * struct gpio_extcon_data - A simple GPIO-controlled extcon device state container.

 * @edev:		Extcon device.

 * @work:		Work fired by the interrupt.

 * @debounce_jiffies:	Number of jiffies to wait for the GPIO to stabilize, from the debounce

 *			value.

 * @gpiod:		GPIO descriptor for this external connector.

 * @extcon_id:		The unique id of specific external connector.

 * @debounce:		Debounce time for GPIO IRQ in ms.

 * @check_on_resume:	Boolean describing whether to check the state of gpio

 *			while resuming from sleep.

	/*

	 * FIXME: extcon_id represents the unique identifier of external

	 * connectors such as EXTCON_USB, EXTCON_DISP_HDMI and so on. extcon_id

	 * is necessary to register the extcon device. But, it's not yet

	 * developed to get the extcon id from device-tree or others.

	 * On later, it have to be solved.

	/*

	 * It is unlikely that this is an acknowledged interrupt that goes

	 * away after handling, what we are looking for are falling edges

	 * if the signal is active low, and rising edges if the signal is

	 * active high.

 Allocate the memory of extcon devie and register extcon device */

	/*

	 * Request the interrupt of gpio to detect whether external connector

	 * is attached or detached.

 Perform initial detection */

 SPDX-License-Identifier: GPL-2.0

 ChromeOS Embedded Controller extcon



 Copyright (C) 2017 Google, Inc.

 Author: Benson Leung <bleung@chromium.org>

 data role */

 power role (true if VBUS enabled) */

 DisplayPort enabled */

 SuperSpeed (usb3) enabled */

/**

 * cros_ec_pd_command() - Send a command to the EC.

 * @info: pointer to struct cros_ec_extcon_info

 * @command: EC command

 * @version: EC command version

 * @outdata: EC command output data

 * @outsize: Size of outdata

 * @indata: EC command input data

 * @insize: Size of indata

 *

 * Return: 0 on success, <0 on failure.

/**

 * cros_ec_usb_get_power_type() - Get power type info about PD device attached

 * to given port.

 * @info: pointer to struct cros_ec_extcon_info

 *

 * Return: power type on success, <0 on failure.

/**

 * cros_ec_usb_get_pd_mux_state() - Get PD mux state for given port.

 * @info: pointer to struct cros_ec_extcon_info

 *

 * Return: PD mux state on success, <0 on failure.

/**

 * cros_ec_usb_get_role() - Get role info about possible PD device attached to a

 * given port.

 * @info: pointer to struct cros_ec_extcon_info

 * @polarity: pointer to cable polarity (return value)

 *

 * Return: role info on success, -ENOTCONN if no cable is connected, <0 on

 * failure.

/**

 * cros_ec_pd_get_num_ports() - Get number of EC charge ports.

 * @info: pointer to struct cros_ec_extcon_info

 *

 * Return: number of ports on success, <0 on failure.

	/* FIXME : Guppy, Donnettes, and other chargers will be miscategorized

	 * because they identify with USB_CHG_TYPE_C, but we can't return true

	 * here from that code because that breaks Suzy-Q and other kinds of

	 * USB Type-C cables and peripherals.

	/*

	 * When there is no USB host (e.g. USB PD charger),

	 * we are not really a UFP for the AP.

 Get PD events from the EC */

 Perform initial detection */

 CONFIG_PM_SLEEP */

 sentinel */ }

 CONFIG_OF */

 SPDX-License-Identifier: GPL-2.0-only

/**

 * drivers/extcon/extcon-usb-gpio.c - USB GPIO extcon driver

 *

 * Copyright (C) 2015 Texas Instruments Incorporated - https://www.ti.com

 * Author: Roger Quadros <rogerq@ti.com>

 ms */

/*

 * "USB" = VBUS and "USB-HOST" = !ID, so we have:

 * Both "USB" and "USB-HOST" can't be set as active at the

 * same time so if "USB-HOST" is active (i.e. ID is 0)  we keep "USB" inactive

 * even if VBUS is on.

 *

 *  State              |    ID   |   VBUS

 * ----------------------------------------

 *  [1] USB            |    H    |    H

 *  [2] none           |    H    |    L

 *  [3] USB-HOST       |    L    |    H

 *  [4] USB-HOST       |    L    |    L

 *

 * In case we have only one of these signals:

 * - VBUS only - we want to distinguish between [1] and [2], so ID is always 1.

 * - ID only - we want to distinguish between [1] and [4], so VBUS = ID.

 check ID and VBUS and update cable state */

 at first we clean states which are no longer active */

 Perform initial detection */

	/*

	 * We don't want to process any IRQs after this point

	 * as GPIOs used behind I2C subsystem might not be

	 * accessible until resume completes. So disable IRQ.

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Palmas USB transceiver driver

 *

 * Copyright (C) 2013 Texas Instruments Incorporated - https://www.ti.com

 * Author: Graeme Gregory <gg@slimlogic.co.uk>

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

 * Based on twl6030_usb.c

 * Author: Hema HK <hemahk@ti.com>

 ms */

 cold plug for host mode needs this delay */

 remux GPIO_1 as VBUSDET */

 perform initial detection */

 check if GPIO states changed while suspend/resume */

 end */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * extcon-sm5502.c - Silicon Mitus SM5502 extcon drvier to support USB switches

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd

 * Author: Chanwoo Choi <cw00.choi@samsung.com>

 unit: millisecond */

	/*

	 * Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 Default value of SM5502 register to bring up MUIC device. */

 Default value of SM5504 register to bring up MUIC device. */

 List of detectable cables */

 Define supported accessory type */

	/*

	 * The below accessories have same ADC value (0x1f or 0x1e).

	 * So, Device type1 is used to separate specific accessory.

 |---------|--ADC| */

 |    [7:5]|[4:0]| */

 |      001|11110| */

 |      010|11110| */

 |Dev Type1|--ADC| */

 |      100|00000| */

 |      010|11111| */

 |      110|11111| */

 |      111|11111| */

 List of supported interrupt for SM5502 */

 Define interrupt list of SM5502 to register regmap_irq */

 INT1 interrupts */

 INT2 interrupts */

 List of supported interrupt for SM5504 */

 Define interrupt list of SM5504 to register regmap_irq */

 INT1 interrupts */

 INT2 interrupts */

 Define regmap configuration of SM5502 for I2C communication  */

 Change DM_CON/DP_CON/VBUSIN switch according to cable type */

 Return cable type of attached or detached accessories */

 Read ADC value according to external cable or button */

	/*

	 * If ADC is SM5502_MUIC_ADC_GROUND(0x0), external cable hasn't

	 * connected with to MUIC device.

		/*

		 * Check whether cable type is

		 * SM5502_MUIC_ADC_AUDIO_TYPE1_FULL_REMOTE

		 * or SM5502_MUIC_ADC_AUDIO_TYPE1_SEND_END

		 * by using Button event.

 Get the type of attached or detached cable */

 Change internal hardware path(DM_CON/DP_CON, VBUSIN) */

 Change the state of external accessory */

 Detect attached or detached cables */

/*

 * Sets irq_attach or irq_detach in sm5502_muic_info and returns 0.

 * Returns -ESRCH if irq_type does not match registered IRQ for this dev type.

 Notify the state of connector cable or not  */

 To test I2C, Print version_id and vendor_id of SM5502 */

 Initiazle the register of SM5502 device to bring-up */

 Support irq domain for SM5502 MUIC device */

 Allocate extcon device */

 Register extcon device */

	/*

	 * Detect accessory after completing the initialization of platform

	 *

	 * - Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 Initialize SM5502 device and print vendor id and version id */

 SPDX-License-Identifier: GPL-2.0+

/*

 * extcon-fsa9480.c - Fairchild Semiconductor FSA9480 extcon driver

 *

 * Copyright (c) 2019 Tomasz Figa <tomasz.figa@gmail.com>

 *

 * Loosely based on old fsa9480 misc-device driver.

 FSA9480 I2C registers */

 Control */

 Device Type 1 */

 Device Type 2 */

/*

 * Manual Switch

 * D- [7:5] / D+ [4:2]

 * 000: Open all / 001: USB / 010: AUDIO / 011: UART / 100: V_AUDIO

 Interrupt 1 */

 Interrupt 2 mask */

 Timing Set 1 */

 Define regmap configuration of FSA9480 for I2C communication  */

 handle detached cables first */

 then handle attached ones */

 clear interrupt */

 device detection */

 External connector */

 ADC Detect Time: 500ms */

 configure automatic switching */

 unmask interrupt (attach/detach only) */

 SPDX-License-Identifier: GPL-2.0

/*

 * Extcon charger detection driver for Intel Cherrytrail Whiskey Cove PMIC

 * Copyright (C) 2017 Hans de Goede <hdegoede@redhat.com>

 *

 * Based on various non upstream patches to support the CHT Whiskey Cove PMIC:

 * Copyright (C) 2013-2015 Intel Corporation. All rights reserved.

 0 - open drain, 1 - regular push-pull output */

 0 - pin is controlled by SW, 1 - by HW */

		/*

		 * Once we have IIO support for the GPADC we should read

		 * the USBID GPADC channel here and determine ACA role

		 * based on that.

 Charger detection can take upto 600ms, wait 800ms max. */

 Wait a bit before retrying */

 Save fallback */

 Save fallback */

 MHL2+ delivers upto 2A, treat as DCP */

	/*

	 * The 5V boost converter is enabled through a gpio on the PMIC, since

	 * there currently is no gpio driver we access the gpio reg directly.

 Small helper to sync EXTCON_CHG_USB_SDP and EXTCON_USB state */

 Ignore errors in host mode, as the 5v boost converter is on then */

 The 5v boost causes a false VBUS / SDP detect, skip */

 Plugged into a host/charger or not connected? */

 Route D+ and D- to PMIC for future charger detection */

 Route D+ and D- to SoC for the host or gadget controller */

 Initialize extcon device */

	/*

	 * When a host-cable is detected the BIOS enables an external 5v boost

	 * converter to power connected devices there are 2 problems with this:

	 * 1) This gets seen by the external battery charger as a valid Vbus

	 *    supply and it then tries to feed Vsys from this creating a

	 *    feedback loop which causes aprox. 300 mA extra battery drain

	 *    (and unless we drive the external-charger-disable pin high it

	 *    also tries to charge the battery causing even more feedback).

	 * 2) This gets seen by the pwrsrc block as a SDP USB Vbus supply

	 * Since the external battery charger has its own 5v boost converter

	 * which does not have these issues, we simply turn the separate

	 * external 5v boost converter off and leave it off entirely.

 Enable sw control */

 Disable charging by external battery charger */

 Register extcon device */

	/*

	 * If no USB host or device connected, route D+ and D- to PMIC for

	 * initial charger detection

 Get initial state */

 Unmask irqs */

 SPDX-License-Identifier: GPL-2.0

/**

 * drivers/extcon/extcon-tusb320.c - TUSB320 extcon driver

 *

 * Copyright (C) 2020 National Instruments Corporation

 * Author: Michael Auchter <michael.auchter@ni.com>

 Mode cannot be changed while cable is attached */

 Write mode */

 Disable CC state machine */

 Write mode */

 Re-enable CC state machine */

 Set mode to default (follow PORT pin) */

 Perform soft reset */

 Wait for chip to go through reset */

 update initial state */

 Reset chip to its default state */

		/*

		 * State and polarity might change after a reset, so update

		 * them again and make sure the interrupt status bit is cleared.

 SPDX-License-Identifier: GPL-2.0+



 extcon-ptn5150.c - PTN5150 CC logic extcon driver to support USB detection



 Based on extcon-sm5502.c driver

 Copyright (c) 2018-2019 by Vijai Kumar K

 Author: Vijai Kumar K <vijaikumar.kanagarajan@gmail.com>

 Copyright (c) 2020 Krzysztof Kozlowski <krzk@kernel.org>

 PTN5150 registers */

 Define PTN5150 MASK/SHIFT constant */

 List of detectable cables */

 Clear interrupt. Read would clear the register */

 Clear interrupt. Read would clear the register */

 Clear any existing interrupts */

 Allocate extcon device */

 Register extcon device */

 Initialize PTN5150 device and print vendor id and version id */

	/*

	 * Update current extcon state if for example OTG connection was there

	 * before the probe

 SPDX-License-Identifier: GPL-2.0+



 extcon-max14577.c - MAX14577/77836 extcon driver to support MUIC



 Copyright (C) 2013,2014 Samsung Electronics

 Chanwoo Choi <cw00.choi@samsung.com>

 Krzysztof Kozlowski <krzk@kernel.org>

 unit: millisecond */

/**

 * struct max14577_muic_irq

 * @irq: the index of irq list of MUIC device.

 * @name: the name of irq.

 * @virq: the virtual irq to use irq domain

	/*

	 * Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

	/*

	 * Default usb/uart path whether UART/USB or AUX_UART/AUX_USB

	 * h/w path of COMP2/COMN1 on CONTROL1 register.

 Define supported accessory type */

 with Remote and Simple Ctrl */

/*

 * max14577_muic_set_debounce_time - Set the debounce time of ADC

 * @info: the instance including private data of max14577 MUIC

 * @time: the debounce time of ADC

/*

 * max14577_muic_set_path - Set hardware line according to attached cable

 * @info: the instance including private data of max14577 MUIC

 * @value: the path according to attached cable

 * @attached: the state of cable (true:attached, false:detached)

 *

 * The max14577 MUIC device share outside H/W line among a varity of cables

 * so, this function set internal path of H/W line according to the type of

 * attached cable.

 Set open state to path before changing hw path */

 LowPwr=0, CPEn=1 */

 LowPwr=1, CPEn=0 */

/*

 * max14577_muic_get_cable_type - Return cable type and check cable state

 * @info: the instance including private data of max14577 MUIC

 * @group: the path according to attached cable

 * @attached: store cable state and return

 *

 * This function check the cable state either attached or detached,

 * and then divide precise type of cable according to cable group.

 *	- max14577_CABLE_GROUP_ADC

 *	- max14577_CABLE_GROUP_CHG

		/*

		 * Read ADC value to check cable type and decide cable state

		 * according to cable type

		/*

		 * Check current cable state/cable type and store cable type

		 * (info->prev_cable_type) for handling cable when cable is

		 * detached.

		/*

		 * Read charger type to check cable type and decide cable state

		 * according to type of charger cable.

			/*

			 * Check current cable state/cable type and store cable

			 * type(info->prev_chg_type) for handling cable when

			 * charger cable is detached.

 ADC_JIG_USB_OFF */

 ADC_JIG_USB_ON */

 PATH:AP_USB */

 ADC_JIG_UART_OFF */

 PATH:AP_UART */

 Check accessory state which is either detached or attached */

 JIG */

		/*

		 * This accessory isn't used in general case if it is specially

		 * needed to detect additional accessory, should implement

		 * proper operation when this accessory is attached/detached.

 PATH:AP_USB */

/*

 * Sets irq_adc or irq_chg in max14577_muic_info and returns 1.

 * Returns 0 if irq_type does not match registered IRQ for this device type.

		/*

		 * Handle all of accessory except for

		 * type of charger accessory.

 Handle charger accessory */

/*

 * Sets irq_adc or irq_chg in max14577_muic_info and returns 1.

 * Returns 0 if irq_type does not match registered IRQ for this device type.

 First check common max14577 interrupts */

 Handle charger accessory */

	/*

	 * We may be called multiple times for different nested IRQ-s.

	 * Including changes in INT1_ADC and INT2_CGHTYP at once.

	 * However we only need to know whether it was ADC, charger

	 * or both interrupts so decode IRQ and turn on proper flags.

 Read STATUSx register to detect accessory */

 Support irq domain for max14577 MUIC device */

 Initialize extcon device */

 Default h/w line path */

 Set initial path for UART when JIG is connected to get serial logs */

 Check revision number of MUIC device*/

 Set ADC debounce time */

	/*

	 * Detect accessory after completing the initialization of platform

	 *

	 * - Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 SPDX-License-Identifier: GPL-2.0+



 extcon-max77693.c - MAX77693 extcon driver to support MAX77693 MUIC



 Copyright (C) 2012 Samsung Electrnoics

 Chanwoo Choi <cw00.choi@samsung.com>

 unit: millisecond */

/*

 * Default value of MAX77693 register to bring up MUIC device.

 * If user don't set some initial value for MUIC device through platform data,

 * extcon-max77693 driver use 'default_init_data' to bring up base operation

 * of MAX77693 MUIC device.

 STATUS2 - [3]ChgDetRun */

 INTMASK1 - Unmask [3]ADC1KM,[0]ADCM */

 INTMASK2 - Unmask [0]ChgTypM */

 INTMASK3 - Mask all of interrupts */

 CDETCTRL2 */

	/*

	 * Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 Button of dock device */

	/*

	 * Default usb/uart path whether UART/USB or AUX_UART/AUX_USB

	 * h/w path of COMP2/COMN1 on CONTROL1 register.

/**

 * struct max77693_muic_irq

 * @irq: the index of irq list of MUIC device.

 * @name: the name of irq.

 * @virq: the virtual irq to use irq domain

 Define supported accessory type */

	/*

	 * The below accessories have same ADC value so ADCLow and

	 * ADC1K bit is used to separate specific accessory.

 ADC|VBVolot|ADCLow|ADC1K| */

 0x0|      0|     0|    0| */

 0x0|      1|     0|    0| */

 0x0|      0|     1|    0| */

 0x0|      0|     1|    1| */

 0x0|      1|     1|    1| */

/*

 * MAX77693 MUIC device support below list of accessories(external connector)

/*

 * max77693_muic_set_debounce_time - Set the debounce time of ADC

 * @info: the instance including private data of max77693 MUIC

 * @time: the debounce time of ADC

		/*

		 * Don't touch BTLDset, JIGset when you want to change adc

		 * debounce time. If it writes other than 0 to BTLDset, JIGset

		 * muic device will be reset and loose current state.

/*

 * max77693_muic_set_path - Set hardware line according to attached cable

 * @info: the instance including private data of max77693 MUIC

 * @value: the path according to attached cable

 * @attached: the state of cable (true:attached, false:detached)

 *

 * The max77693 MUIC device share outside H/W line among a varity of cables

 * so, this function set internal path of H/W line according to the type of

 * attached cable.

 LowPwr=0, CPEn=1 */

 LowPwr=1, CPEn=0 */

/*

 * max77693_muic_get_cable_type - Return cable type and check cable state

 * @info: the instance including private data of max77693 MUIC

 * @group: the path according to attached cable

 * @attached: store cable state and return

 *

 * This function check the cable state either attached or detached,

 * and then divide precise type of cable according to cable group.

 *	- MAX77693_CABLE_GROUP_ADC

 *	- MAX77693_CABLE_GROUP_ADC_GND

 *	- MAX77693_CABLE_GROUP_CHG

 *	- MAX77693_CABLE_GROUP_VBVOLT

		/*

		 * Read ADC value to check cable type and decide cable state

		 * according to cable type

		/*

		 * Check current cable state/cable type and store cable type

		 * (info->prev_cable_type) for handling cable when cable is

		 * detached.

		/*

		 * Read ADC value to check cable type and decide cable state

		 * according to cable type

		/*

		 * Check current cable state/cable type and store cable type

		 * (info->prev_cable_type/_gnd) for handling cable when cable

		 * is detached.

			/**

			 * [0x1|VBVolt|ADCLow|ADC1K]

			 * [0x1|     0|     0|    0] USB_HOST

			 * [0x1|     1|     0|    0] USB_HSOT_VB

			 * [0x1|     0|     1|    0] Audio Video cable with load

			 * [0x1|     0|     1|    1] MHL without charging cable

			 * [0x1|     1|     1|    1] MHL with charging cable

		/*

		 * Read charger type to check cable type and decide cable state

		 * according to type of charger cable.

			/*

			 * Check current cable state/cable type and store cable

			 * type(info->prev_chg_type) for handling cable when

			 * charger cable is detached.

		/*

		 * Read ADC value to check cable type and decide cable state

		 * according to cable type

		/*

		 * Read vbvolt field, if vbvolt is 1,

		 * this cable is used for charging.

 Dock-Smart */

		/*

		 * Check power cable whether attached or detached state.

		 * The Dock-Smart device need surely external power supply.

		 * If power cable(USB/TA) isn't connected to Dock device,

		 * user can't use Dock-Smart for desktop mode.

		/*

		 * Notify Dock/MHL state.

		 * - Dock device include three type of cable which

		 * are HDMI, USB for mouse/keyboard and micro-usb port

		 * for USB/TA cable. Dock device need always exteranl

		 * power supply(USB/TA cable through micro-usb cable). Dock

		 * device support screen output of target to separate

		 * monitor and mouse/keyboard for desktop mode.

		 *

		 * Features of 'USB/TA cable with Dock device'

		 * - Support MHL

		 * - Support external output feature of audio

		 * - Support charging through micro-usb port without data

		 *	     connection if TA cable is connected to target.

		 * - Support charging and data connection through micro-usb port

		 *           if USB cable is connected between target and host

		 *	     device.

		 * - Support OTG(On-The-Go) device (Ex: Mouse/Keyboard)

 Dock-Desk */

 Dock-Audio */

 Dock-Car/Desk/Audio, PATH:AUDIO */

 DOCK_KEY_PREV */

 DOCK_KEY_NEXT */

 DOCK_VOL_DOWN */

 DOCK_VOL_UP */

 DOCK_KEY_PLAY_PAUSE */

 USB_HOST, PATH: AP_USB */

 Audio Video Cable with load, PATH:AUDIO */

 MHL or MHL with USB/TA cable */

 ADC_JIG_USB_OFF */

 ADC_JIG_USB_ON */

 PATH:AP_USB */

 ADC_JIG_UART_OFF */

 ADC_JIG_UART_ON */

 PATH:AP_UART */

 Check accessory state which is either detached or attached */

 USB_HOST/MHL/Audio */

 JIG */

 Dock-Smart */

 Dock-Desk */

 Dock-Audio */

		/*

		 * DOCK device

		 *

		 * The MAX77693 MUIC device can detect total 34 cable type

		 * except of charger cable and MUIC device didn't define

		 * specfic role of cable in the range of from 0x01 to 0x12

		 * of ADC value. So, can use/define cable with no role according

		 * to schema of hardware board.

 DOCK_KEY_PREV */

 DOCK_KEY_NEXT */

 DOCK_VOL_DOWN */

 DOCK_VOL_UP */

 DOCK_KEY_PLAY_PAUSE */

		/*

		 * Button of DOCK device

		 * - the Prev/Next/Volume Up/Volume Down/Play-Pause button

		 *

		 * The MAX77693 MUIC device can detect total 34 cable type

		 * except of charger cable and MUIC device didn't define

		 * specfic role of cable in the range of from 0x01 to 0x12

		 * of ADC value. So, can use/define cable with no role according

		 * to schema of hardware board.

		/*

		 * This accessory isn't used in general case if it is specially

		 * needed to detect additional accessory, should implement

		 * proper operation when this accessory is attached/detached.

 Check MAX77693_CABLE_GROUP_ADC_GND type */

			/*

			 * MHL cable with USB/TA cable

			 * - MHL cable include two port(HDMI line and separate

			 * micro-usb port. When the target connect MHL cable,

			 * extcon driver check whether USB/TA cable is

			 * connected. If USB/TA cable is connected, extcon

			 * driver notify state to notifiee for charging battery.

			 *

			 * Features of 'USB/TA with MHL cable'

			 * - Support MHL

			 * - Support charging through micro-usb port without

			 *   data connection

 Check MAX77693_CABLE_GROUP_ADC type */

 Dock-Audio */

			/*

			 * Dock-Audio device with USB/TA cable

			 * - Dock device include two port(Dock-Audio and micro-

			 * usb port). When the target connect Dock-Audio device,

			 * extcon driver check whether USB/TA cable is connected

			 * or not. If USB/TA cable is connected, extcon driver

			 * notify state to notifiee for charging battery.

			 *

			 * Features of 'USB/TA cable with Dock-Audio device'

			 * - Support external output feature of audio.

			 * - Support charging through micro-usb port without

			 *   data connection.

 Dock-Smart */

			/*

			 * Dock-Smart device with USB/TA cable

			 * - Dock-Desk device include three type of cable which

			 * are HDMI, USB for mouse/keyboard and micro-usb port

			 * for USB/TA cable. Dock-Smart device need always

			 * exteranl power supply(USB/TA cable through micro-usb

			 * cable). Dock-Smart device support screen output of

			 * target to separate monitor and mouse/keyboard for

			 * desktop mode.

			 *

			 * Features of 'USB/TA cable with Dock-Smart device'

			 * - Support MHL

			 * - Support external output feature of audio

			 * - Support charging through micro-usb port without

			 *   data connection if TA cable is connected to target.

			 * - Support charging and data connection through micro-

			 *   usb port if USB cable is connected between target

			 *   and host device

			 * - Support OTG(On-The-Go) device (Ex: Mouse/Keyboard)

 Check MAX77693_CABLE_GROUP_CHG type */

			/*

			 * When MHL(with USB/TA cable) or Dock-Audio with USB/TA

			 * cable is attached, muic device happen below two irq.

			 * - 'MAX77693_MUIC_IRQ_INT1_ADC' for detecting

			 *    MHL/Dock-Audio.

			 * - 'MAX77693_MUIC_IRQ_INT2_CHGTYP' for detecting

			 *    USB/TA cable connected to MHL or Dock-Audio.

			 * Always, happen eariler MAX77693_MUIC_IRQ_INT1_ADC

			 * irq than MAX77693_MUIC_IRQ_INT2_CHGTYP irq.

			 *

			 * If user attach MHL (with USB/TA cable and immediately

			 * detach MHL with USB/TA cable before MAX77693_MUIC_IRQ

			 * _INT2_CHGTYP irq is happened, USB/TA cable remain

			 * connected state to target. But USB/TA cable isn't

			 * connected to target. The user be face with unusual

			 * action. So, driver should check this situation in

			 * spite of, that previous charger type is N/A.

 Only USB cable, PATH:AP_USB */

 Only TA cable */

		/*

		 * Handle all of accessory except for

		 * type of charger accessory.

 Handle charger accessory */

 Read STATUSx register to detect accessory */

 Register input device for button of dock device */

 Support irq domain for MAX77693 MUIC device */

 Initialize extcon device */

 Initialize MUIC register by using platform data or default data */

		/*

		 * Default usb/uart path whether UART/USB or AUX_UART/AUX_USB

		 * h/w path of COMP2/COMN1 on CONTROL1 register.

		/*

		 * Default delay time for detecting cable state

		 * after certain time.

 Set initial path for UART when JIG is connected to get serial logs */

 Check revision number of MUIC device*/

 Set ADC debounce time */

	/*

	 * Detect accessory after completing the initialization of platform

	 *

	 * - Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Maxim Integrated MAX3355 USB OTG chip extcon driver

 *

 * Copyright (C)  2014-2015 Cogent Embedded, Inc.

 * Author: Sergei Shtylyov <sergei.shtylyov@cogentembedded.com>

		/*

		 * ID = 1 means USB HOST cable detached.

		 * As we don't have event for USB peripheral cable attached,

		 * we simulate USB peripheral attach here.

		/*

		 * ID = 0 means USB HOST cable attached.

		 * As we don't have event for USB peripheral cable detached,

		 * we simulate USB peripheral detach here.

 Perform initial detection */

 SPDX-License-Identifier: GPL-2.0+



 extcon-max77843.c - Maxim MAX77843 extcon driver to support

			MUIC(Micro USB Interface Controller)



 Copyright (C) 2015 Samsung Electronics

 Author: Jaewon Kim <jaewon02.kim@samsung.com>

 unit: millisecond */

 Define accessory cable type */

 SmartDock */

	/*

	 * The below accessories should check

	 * not only ADC value but also ADC1K and VBVolt value.

 Offset|ADC1K|VBVolt| */

    0x1|    0|     0| */

    0x1|    0|     1| */

    0x1|    1|     0| */

    0x1|    1|     1| */

 Define charger cable type */

 INT1 interrupt */

 INT2 interrupt */

 INT3 interrupt */

 Disable BC1.2 protocol and force manual switch control */

 Check GROUND accessory with charger cable */

				/*

				 * The following state when charger cable is

				 * disconnected but the GROUND accessory still

				 * connected.

				/*

				 * The following state when charger cable is

				 * connected on the GROUND accessory.

 SmartDock */

			/*

			 * Offset|ADC1K|VBVolt|

			 *    0x1|    0|     0| USB-HOST

			 *    0x1|    0|     1| USB-HOST with VB

			 *    0x1|    1|     0| MHL

			 *    0x1|    1|     1| MHL with VB

 Get ADC1K register bit */

 Get VBVolt register bit */

 Offset of GND cable */

 SmartDock */

 Charger cable on MHL accessory is attach or detach */

 Initialize i2c and regmap */

 Turn off auto detection configuration */

 Initialize extcon device */

 Set ADC debounce time */

 Set initial path for UART when JIG is connected to get serial logs */

 Check revision number of MUIC device */

 Support virtual irq domain for max77843 MUIC device */

 Clear IRQ bits before request IRQs */

 Detect accessory after completing the initialization of platform */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0+



 extcon-max8997.c - MAX8997 extcon driver to support MAX8997 MUIC



  Copyright (C) 2012 Samsung Electronics

  Donggeun Kim <dg77.kim@samsung.com>

 unit: millisecond */

 0.5ms */

 10ms */

 25ms */

 38.62ms */

 Define supported cable type */

 MHL*/

 JIG-USB-OFF */

 JIG-USB-ON */

 DESKDOCK */

 JIG-UART */

 CARDOCK */

 OPEN */

	/*

	 * Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

	/*

	 * Default usb/uart path whether UART/USB or AUX_UART/AUX_USB

	 * h/w path of COMP2/COMN1 on CONTROL1 register.

/*

 * max8997_muic_set_debounce_time - Set the debounce time of ADC

 * @info: the instance including private data of max8997 MUIC

 * @time: the debounce time of ADC

/*

 * max8997_muic_set_path - Set hardware line according to attached cable

 * @info: the instance including private data of max8997 MUIC

 * @value: the path according to attached cable

 * @attached: the state of cable (true:attached, false:detached)

 *

 * The max8997 MUIC device share outside H/W line among a varity of cables,

 * so this function set internal path of H/W line according to the type of

 * attached cable.

 LowPwr=0, CPEn=1 */

 LowPwr=1, CPEn=0 */

/*

 * max8997_muic_get_cable_type - Return cable type and check cable state

 * @info: the instance including private data of max8997 MUIC

 * @group: the path according to attached cable

 * @attached: store cable state and return

 *

 * This function check the cable state either attached or detached,

 * and then divide precise type of cable according to cable group.

 *	- MAX8997_CABLE_GROUP_ADC

 *	- MAX8997_CABLE_GROUP_CHG

		/*

		 * Read ADC value to check cable type and decide cable state

		 * according to cable type

		/*

		 * Check current cable state/cable type and store cable type

		 * (info->prev_cable_type) for handling cable when cable is

		 * detached.

		/*

		 * Read charger type to check cable type and decide cable state

		 * according to type of charger cable.

			/*

			 * Check current cable state/cable type and store cable

			 * type(info->prev_chg_type) for handling cable when

			 * charger cable is detached.

 switch to UART */

 Check cable state which is either detached or attached */

		/*

		 * This cable isn't used in general case if it is specially

		 * needed to detect additional cable, should implement

		 * proper operation when this cable is attached/detached.

 Handle all of cable except for charger cable */

 Handle charger cable */

 Read STATUSx register to detect accessory */

 External connector */

 Initialize registers according to platform data */

		/*

		 * Default usb/uart path whether UART/USB or AUX_UART/AUX_USB

		 * h/w path of COMP2/COMN1 on CONTROL1 register.

		/*

		 * Default delay time for detecting cable state

		 * after certain time.

 Set initial path for UART when JIG is connected to get serial logs */

 Set ADC debounce time */

	/*

	 * Detect accessory after completing the initialization of platform

	 *

	 * - Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 SPDX-License-Identifier: GPL-2.0-only

/**

 * extcon-qcom-spmi-misc.c - Qualcomm USB extcon driver to support USB ID

 *			and VBUS detection based on extcon-usb-gpio.c.

 *

 * Copyright (C) 2016 Linaro, Ltd.

 * Stephen Boyd <stephen.boyd@linaro.org>

 ms */

 check ID and update cable state */

 check VBUS and update cable state */

 Perform initial detection */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel INT3496 ACPI device extcon driver

 *

 * Copyright (c) 2016 Hans de Goede <hdegoede@redhat.com>

 *

 * Based on android x86 kernel code which is:

 *

 * Copyright (c) 2014, Intel Corporation.

 * Author: David Cohen <david.a.cohen@linux.intel.com>

	/*

	 * Some platforms have a bug in ACPI GPIO description making IRQ

	 * GPIO to be output only. Ask the GPIO core to ignore this limit.

 id == 1: PERIPHERAL, id == 0: HOST */

	/*

	 * Peripheral: set USB mux to peripheral and disable VBUS

	 * Host: set USB mux to host and enable VBUS

 Let the pin settle before processing it */

 register extcon device */

 process id-pin so that we start with the right status */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * extcon-rt8973a.c - Richtek RT8973A extcon driver to support USB switches

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd

 * Author: Chanwoo Choi <cw00.choi@samsung.com>

 unit: millisecond */

	/*

	 * Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 Default value of RT8973A register to bring up MUIC device. */

 sentinel */ }

 List of detectable cables */

 Define OVP (Over Voltage Protection), OTP (Over Temperature Protection) */

 Define supported accessory type */

	/*

	 * The below accessories has same ADC value (0x1f).

	 * So, Device type1 is used to separate specific accessory.

 |---------|--ADC| */

 |    [7:5]|[4:0]| */

 |      001|11111| */

 List of supported interrupt for RT8973A */

 Define interrupt list of RT8973A to register regmap_irq */

 INT1 interrupts */

 INT2 interrupts */

 Define regmap configuration of RT8973A for I2C communication  */

 Change DM_CON/DP_CON/VBUSIN switch according to cable type */

	/*

	 * Don't need to set h/w path according to cable type

	 * if Auto-configuration mode of CONTROL1 register is true.

 Read ADC value according to external cable or button */

 Read Device 1 reigster to identify correct cable type */

 Change internal hardware path(DM_CON/DP_CON) */

 Change the state of external accessory */

 Detect attached or detached cables */

 Notify the state of connector cable or not  */

 To test I2C, Print version_id and vendor_id of RT8973A */

 Initiazle the register of RT8973A device to bring-up */

 Check whether RT8973A is auto switching mode or not */

 Support irq domain for RT8973A MUIC device */

 Allocate extcon device */

 Register extcon device */

	/*

	 * Detect accessory after completing the initialization of platform

	 *

	 * - Use delayed workqueue to detect cable state and then

	 * notify cable state to notifiee/platform through uevent.

	 * After completing the booting of platform, the extcon provider

	 * driver should notify cable state to upper layer.

 Initialize RT8973A device and print vendor id and version id */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * extcon-axp288.c - X-Power AXP288 PMIC extcon cable detection driver

 *

 * Copyright (c) 2017-2018 Hans de Goede <hdegoede@redhat.com>

 * Copyright (C) 2015 Intel Corporation

 * Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>

 Power source status register */

 BC module global register */

 BC module vbus control and status register */

 BC USB status register */

 BC detect status register */

 Power up/down reason string array */

/*

 * Decode and log the given "reset source indicator" (rsi)

 * register and then clear it.

 Clear the register value for next reboot (write 1 to clear bit) */

/*

 * The below code to control the USB role-switch on devices with an AXP288

 * may seem out of place, but there are 2 reasons why this is the best place

 * to control the USB role-switch on such devices:

 * 1) On many devices the USB role is controlled by AML code, but the AML code

 *    only switches between the host and none roles, because of Windows not

 *    really using device mode. To make device mode work we need to toggle

 *    between the none/device roles based on Vbus presence, and this driver

 *    gets interrupts on Vbus insertion / removal.

 * 2) In order for our BC1.2 charger detection to work properly the role

 *    mux must be properly set to device mode before we do the detection.

 Returns the id-pin value, note pulled low / false == host-mode */

 We cannot access the id-pin, see what mode the AML code has set */

 Check charger detection completion status */

 Setting the role can take a while */

 We may not sleep and setting the role can take a while */

 Enable the charger detection logic */

 Initialize extcon device */

 Register extcon device */

 Make sure the role-sw is set correctly before doing BC detection */

 Start charger cable type detection */

	/*

	 * Wakeup when a charger is connected to do charger-type

	 * connection and generate an extcon event which makes the

	 * axp288 charger driver set the input current limit.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/extcon/devres.c - EXTCON device's resource management

 *

 * Copyright (C) 2016 Samsung Electronics

 * Author: Chanwoo Choi <cw00.choi@samsung.com>

/**

 * devm_extcon_dev_allocate - Allocate managed extcon device

 * @dev:		the device owning the extcon device being created

 * @supported_cable:	the array of the supported external connectors

 *			ending with EXTCON_NONE.

 *

 * This function manages automatically the memory of extcon device using device

 * resource management and simplify the control of freeing the memory of extcon

 * device.

 *

 * Returns the pointer memory of allocated extcon_dev if success

 * or ERR_PTR(err) if fail

/**

 * devm_extcon_dev_free() - Resource-managed extcon_dev_unregister()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device to be freed

 *

 * Free the memory that is allocated with devm_extcon_dev_allocate()

 * function.

/**

 * devm_extcon_dev_register() - Resource-managed extcon_dev_register()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device to be registered

 *

 * this function, that extcon device is automatically unregistered on driver

 * detach. Internally this function calls extcon_dev_register() function.

 * To get more information, refer that function.

 *

 * If extcon device is registered with this function and the device needs to be

 * unregistered separately, devm_extcon_dev_unregister() should be used.

 *

 * Returns 0 if success or negaive error number if failure.

/**

 * devm_extcon_dev_unregister() - Resource-managed extcon_dev_unregister()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device to unregistered

 *

 * Unregister extcon device that is registered with devm_extcon_dev_register()

 * function.

/**

 * devm_extcon_register_notifier() - Resource-managed extcon_register_notifier()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device

 * @id:		the unique id among the extcon enumeration

 * @nb:		a notifier block to be registered

 *

 * This function manages automatically the notifier of extcon device using

 * device resource management and simplify the control of unregistering

 * the notifier of extcon device.

 *

 * Note that the second parameter given to the callback of nb (val) is

 * "old_state", not the current state. The current state can be retrieved

 * by looking at the third pameter (edev pointer)'s state value.

 *

 * Returns 0 if success or negaive error number if failure.

/**

 * devm_extcon_unregister_notifier()

 *			- Resource-managed extcon_unregister_notifier()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device

 * @id:		the unique id among the extcon enumeration

 * @nb:		a notifier block to be registered

/**

 * devm_extcon_register_notifier_all()

 *		- Resource-managed extcon_register_notifier_all()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device

 * @nb:		a notifier block to be registered

 *

 * This function manages automatically the notifier of extcon device using

 * device resource management and simplify the control of unregistering

 * the notifier of extcon device. To get more information, refer that function.

 *

 * Returns 0 if success or negaive error number if failure.

/**

 * devm_extcon_unregister_notifier_all()

 *		- Resource-managed extcon_unregister_notifier_all()

 * @dev:	the device owning the extcon device being created

 * @edev:	the extcon device

 * @nb:		a notifier block to be registered

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/extcon/extcon.c - External Connector (extcon) framework.

 *

 * Copyright (C) 2015 Samsung Electronics

 * Author: Chanwoo Choi <cw00.choi@samsung.com>

 *

 * Copyright (C) 2012 Samsung Electronics

 * Author: Donggeun Kim <dg77.kim@samsung.com>

 * Author: MyungJoo Ham <myungjoo.ham@samsung.com>

 *

 * based on android/drivers/switch/switch_class.c

 * Copyright (C) 2008 Google, Inc.

 * Author: Mike Lockwood <lockwood@android.com>

 USB external connector */

 Charging external connector */

 Jack external connector */

 Display external connector */

 Miscellaneous external connector */

 sentinel */ }

/**

 * struct extcon_cable - An internal data for an external connector.

 * @edev:		the extcon device

 * @cable_index:	the index of this cable in the edev

 * @attr_g:		the attribute group for the cable

 * @attr_name:		"name" sysfs entry

 * @attr_state:		"state" sysfs entry

 * @attrs:		the array pointing to attr_name and attr_state for attr_g

 to be fed to attr_g.attrs */

 calculate the total number of bits set */

 Find the the index of extcon cable in edev->supported_cable */

 Check whether the property is supported or not. */

 Check whether a specific extcon id supports the property or not. */

 Check whether the property is supported or not. */

/**

 * extcon_sync() - Synchronize the state for an external connector.

 * @edev:	the extcon device

 *

 * Note that this function send a notification in order to synchronize

 * the state and property of an external connector.

 *

 * Returns 0 if success or error number if fail.

	/*

	 * Call functions in a raw notifier chain for the specific one

	 * external connector.

	/*

	 * Call functions in a raw notifier chain for the all supported

	 * external connectors.

 This could be in interrupt handler */

 Unlock early before uevent */

 Unlock early before uevent */

/**

 * extcon_get_state() - Get the state of an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 *

 * Returns 0 if success or error number if fail.

/**

 * extcon_set_state() - Set the state of an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @state:	the new state of an external connector.

 *		the default semantics is true: attached / false: detached.

 *

 * Note that this function set the state of an external connector without

 * a notification. To synchronize the state of an external connector,

 * have to use extcon_set_state_sync() and extcon_sync().

 *

 * Returns 0 if success or error number if fail.

 Check whether the external connector's state is changed. */

	/*

	 * Initialize the value of extcon property before setting

	 * the detached state for an external connector.

 Update the state for an external connector. */

/**

 * extcon_set_state_sync() - Set the state of an external connector with sync.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @state:	the new state of external connector.

 *		the default semantics is true: attached / false: detached.

 *

 * Note that this function set the state of external connector

 * and synchronize the state by sending a notification.

 *

 * Returns 0 if success or error number if fail.

 Check whether the external connector's state is changed. */

/**

 * extcon_get_property() - Get the property value of an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @prop:	the property id indicating an extcon property

 * @prop_val:	the pointer which store the value of extcon property

 *

 * Note that when getting the property value of external connector,

 * the external connector should be attached. If detached state, function

 * return 0 without property value. Also, the each property should be

 * included in the list of supported properties according to extcon type.

 *

 * Returns 0 if success or error number if fail.

 Check whether the property is supported or not */

 Find the cable index of external connector by using id */

 Check whether the property is available or not. */

	/*

	 * Check whether the external connector is attached.

	 * If external connector is detached, the user can not

	 * get the property value.

 Get the property value according to extcon type */

/**

 * extcon_set_property() - Set the property value of an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @prop:	the property id indicating an extcon property

 * @prop_val:	the pointer including the new value of extcon property

 *

 * Note that each property should be included in the list of supported

 * properties according to the extcon type.

 *

 * Returns 0 if success or error number if fail.

 Check whether the property is supported or not */

 Find the cable index of external connector by using id */

 Check whether the property is available or not. */

 Set the property value according to extcon type */

/**

 * extcon_set_property_sync() - Set property of an external connector with sync.

 * @prop_val:	the pointer including the new value of extcon property

 *

 * Note that when setting the property value of external connector,

 * the external connector should be attached. The each property should

 * be included in the list of supported properties according to extcon type.

 *

 * Returns 0 if success or error number if fail.

/**

 * extcon_get_property_capability() - Get the capability of the property

 *					for an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @prop:	the property id indicating an extcon property

 *

 * Returns 1 if the property is available or 0 if not available.

 Check whether the property is supported or not */

 Find the cable index of external connector by using id */

/**

 * extcon_set_property_capability() - Set the capability of the property

 *					for an external connector.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @prop:	the property id indicating an extcon property

 *

 * Note that this function set the capability of the property

 * for an external connector in order to mark the bit in capability

 * bitmap which mean the available state of the property.

 *

 * Returns 0 if success or error number if fail.

 Check whether the property is supported or not. */

 Find the cable index of external connector by using id. */

/**

 * extcon_get_extcon_dev() - Get the extcon device instance from the name.

 * @extcon_name:	the extcon name provided with extcon_dev_register()

 *

 * Return the pointer of extcon device if success or ERR_PTR(err) if fail.

/**

 * extcon_register_notifier() - Register a notifier block to get notified by

 *				any state changes from the extcon.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @nb:		a notifier block to be registered

 *

 * Note that the second parameter given to the callback of nb (val) is

 * the current state of an external connector and the third pameter

 * is the pointer of extcon device.

 *

 * Returns 0 if success or error number if fail.

/**

 * extcon_unregister_notifier() - Unregister a notifier block from the extcon.

 * @edev:	the extcon device

 * @id:		the unique id indicating an external connector

 * @nb:		a notifier block to be registered

 *

 * Returns 0 if success or error number if fail.

/**

 * extcon_register_notifier_all() - Register a notifier block for all connectors.

 * @edev:	the extcon device

 * @nb:		a notifier block to be registered

 *

 * Note that this function registers a notifier block in order to receive

 * the state change of all supported external connectors from extcon device.

 * And the second parameter given to the callback of nb (val) is

 * the current state and the third pameter is the pointer of extcon device.

 *

 * Returns 0 if success or error number if fail.

/**

 * extcon_unregister_notifier_all() - Unregister a notifier block from extcon.

 * @edev:	the extcon device

 * @nb:		a notifier block to be registered

 *

 * Returns 0 if success or error number if fail.

/*

 * extcon_dev_allocate() - Allocate the memory of extcon device.

 * @supported_cable:	the array of the supported external connectors

 *			ending with EXTCON_NONE.

 *

 * Note that this function allocates the memory for extcon device 

 * and initialize default setting for the extcon device.

 *

 * Returns the pointer memory of allocated extcon_dev if success

 * or ERR_PTR(err) if fail.

/*

 * extcon_dev_free() - Free the memory of extcon device.

 * @edev:	the extcon device

/**

 * extcon_dev_register() - Register an new extcon device

 * @edev:	the extcon device to be registered

 *

 * Among the members of edev struct, please set the "user initializing data"

 * do not set the values of "internal data", which are initialized by

 * this function.

 *

 * Note that before calling this funciton, have to allocate the memory

 * of an extcon device by using the extcon_dev_allocate(). And the extcon

 * dev should include the supported_cable information.

 *

 * Returns 0 if success or error number if fail.

 Count the size of mutually_exclusive array */

/**

 * extcon_dev_unregister() - Unregister the extcon device.

 * @edev:	the extcon device to be unregistered.

 *

 * Note that this does not call kfree(edev) because edev was not allocated

 * by this class.

/*

 * extcon_find_edev_by_node - Find the extcon device from devicetree.

 * @node	: OF node identifying edev

 *

 * Return the pointer of extcon device if success or ERR_PTR(err) if fail.

/*

 * extcon_get_edev_by_phandle - Get the extcon device from devicetree.

 * @dev		: the instance to the given device

 * @index	: the index into list of extcon_dev

 *

 * Return the pointer of extcon device if success or ERR_PTR(err) if fail.

 CONFIG_OF */

/**

 * extcon_get_edev_name() - Get the name of the extcon device.

 * @edev:	the extcon device

 SPDX-License-Identifier: GPL-2.0

/*

 * extcon driver for Basin Cove PMIC

 *

 * Copyright (c) 2019, Intel Corporation.

 * Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>

	/*

	 * PMIC A0 reports USBIDSTS_GND = 1 for ID_GND,

	 * but PMIC B0 reports USBIDSTS_GND = 0 for ID_GND.

	 * Thus we must check this bit at last.

 Unknown or unsupported type */

	/*

	 * It seems SCU firmware clears the content of BCOVE_CHGRIRQ1

	 * and makes it useless for OS. Instead we compare a previously

	 * stored status to the current one, provided by BCOVE_SCHGRIRQ1.

 Get initial state */

	/*

	 * Cached status value is used for cable detection, see comments

	 * in mrfld_extcon_cable_detect(), we need to sync cached value

	 * with a real state of the hardware.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/extcon/extcon-adc-jack.c

 *

 * Analog Jack extcon driver with ADC-based detection capability.

 *

 * Copyright (C) 2016 Samsung Electronics

 * Chanwoo Choi <cw00.choi@samsung.com>

 *

 * Copyright (C) 2012 Samsung Electronics

 * MyungJoo Ham <myungjoo.ham@samsung.com>

 *

 * Modified for calling to IIO to get adc by <anish.singh@samsung.com>

/**

 * struct adc_jack_data - internal data for adc_jack device driver

 * @edev:		extcon device.

 * @cable_names:	list of supported cables.

 * @adc_conditions:	list of adc value conditions.

 * @num_conditions:	size of adc_conditions.

 * @irq:		irq number of attach/detach event (0 if not exist).

 * @handling_delay:	interrupt handler will schedule extcon event

 *			handling at handling_delay jiffies.

 * @handler:		extcon event handler called by interrupt handler.

 * @chan:		iio channel being queried.

 in jiffies */

 Get state from adc value with adc_conditions */

 Set the detached state if adc value is not included in the range */

 Check the length of array and set num_conditions */

 CONFIG_PM_SLEEP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Character line display core support

 *

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 *

 * Copyright (C) 2021 Glider bv

/**

 * linedisp_scroll() - scroll the display by a character

 * @t: really a pointer to the private data structure

 *

 * Scroll the current message along the display by one character, rearming the

 * timer if required.

 update the current message string */

 copy as many characters from the string as possible */

 wrap around to the start of the string */

 update the display */

 move on to the next character */

 rearm the timer */

/**

 * linedisp_display() - set the message to be displayed

 * @linedisp: pointer to the private data structure

 * @msg: the message to display

 * @count: length of msg, or -1

 *

 * Display a new message @msg on the display. @msg can be longer than the

 * number of characters the display can display, in which case it will begin

 * scrolling across the display.

 *

 * Return: 0 on success, -ENOMEM on memory allocation failure

 stop the scroll timer */

 if the string ends with a newline, trim it */

 Clear the display */

 update the display */

/**

 * message_show() - read message via sysfs

 * @dev: the display device

 * @attr: the display message attribute

 * @buf: the buffer to read the message into

 *

 * Read the current message being displayed or scrolled across the display into

 * @buf, for reads from sysfs.

 *

 * Return: the number of characters written to @buf

/**

 * message_store() - write a new message via sysfs

 * @dev: the display device

 * @attr: the display message attribute

 * @buf: the buffer containing the new message

 * @count: the size of the message in @buf

 *

 * Write a new message to display or scroll across the display from sysfs.

 *

 * Return: the size of the message on success, else -ERRNO

/**

 * linedisp_register - register a character line display

 * @linedisp: pointer to character line display structure

 * @parent: parent device

 * @num_chars: the number of characters that can be displayed

 * @buf: pointer to a buffer that can hold @num_chars characters

 * @update: Function called to update the display.  This must not sleep!

 *

 * Return: zero on success, else a negative error code.

 initialise a timer for scrolling the message */

 display a default message */

/**

 * linedisp_unregister - unregister a character line display

 * @linedisp: pointer to character line display structure registered previously

 *	      with linedisp_register()

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for the on-board character LCD found on some ARM reference boards

 * This is basically an Hitachi HD44780 LCD with a custom IP block to drive it

 * https://en.wikipedia.org/wiki/HD44780_Character_LCD

 * Currently it will just display the text "ARM Linux" and the linux version

 *

 * Author: Linus Walleij <triad@df.lth.se>

 Offsets to registers */

 Hitachi HD44780 display commands */

/**

 * struct charlcd - Private data structure

 * @dev: a pointer back to containing device

 * @phybase: the offset to the controller in physical memory

 * @physize: the size of the physical page

 * @virtbase: the offset to the controller in virtual memory

 * @irq: reserved interrupt number

 * @complete: completion structure for the last LCD command

 * @init_work: delayed work structure to initialize the display on boot

 Clear IRQ */

 Disable IRQ after completion */

 If we can, use an IRQ to wait for the data, else poll */

 Read the 4 high bits of the data */

	/*

	 * The second read for the low bits does not trigger an IRQ

	 * so in this case we have to poll for the 4 lower bits

 Read the 4 low bits of the data */

		/*

		 * If we'll use IRQs to wait for the busyflag, clear any

		 * pending flag and enable IRQ

	/*

	 * We support line 0, 1

	 * Line 1 runs from 0x00..0x27

	 * Line 2 runs from 0x28..0x4f

 Set offset */

 Send string */

 These commands cannot be checked with the busy flag */

 Go to 4bit mode */

	/*

	 * 4bit mode, 2 lines, 5x8 font, after this the number of lines

	 * and the font cannot be changed until the next initialization sequence

 Put something useful in the display */

 If no IRQ is supplied, we'll survive without it */

	/*

	 * Initialize the display in a delayed work, because

	 * it is VERY slow and would slow down the boot of the system.

 Power the display off */

 Turn the display back on */

 SPDX-License-Identifier: GPL-2.0

/*

 * HT16K33 driver

 *

 * Author: Robin van der Gracht <robin@protonic.nl>

 *

 * Copyright: (C) 2016 Protonic Holland.

 * Copyright (C) 2021 Glider bv

 Registers */

 Defines */

/*

 * This gets the fb data from cache and copies it to ht16k33 display RAM

 Search for the first byte with changes */

 No changes found */

 Determine i2c transfer length */

 Clear RAM (8 * 16 bits) */

 Turn on internal oscillator */

 Configure INT pin */

/*

 * Blank events will be passed to the actual device handling the backlight when

 * we return zero here.

/*

 * This gets the keys from keypad and reports it to input subsystem.

 * Returns true if a key is pressed.

 The LED is optional */

 backwards compatibility with DT lacking an led subnode */

 Framebuffer (2 bytes per column) */

 not handled here */

 LED */

 Keypad */

 Frame Buffer Display */

 Segment Display */

 0.56" 4-Digit 7-Segment FeatherWing Display (Red) */

 0.54" Quad Alphanumeric FeatherWing Display (Red) */

 Generic, assumed Dot-Matrix Display */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Character LCD driver for Linux

 *

 * Copyright (C) 2000-2008, Willy Tarreau <w@1wt.eu>

 * Copyright (C) 2016-2017 Glider bvba

 Keep the backlight on this many seconds for each flash */

 Max chars for LCD escape command */

 Use char 27 for escape command */

 Protects access to bl_tempo */

 contains the LCD config state */

 Current escape sequence and it's length or -1 if outside */

 Device single-open policy control */

 turn the backlight on or off */

 turn the backlight on for a little while */

 prevents the cursor from wrapping onto the next line */

/*

 * Parses a movement command of the form "(.*);", where the group can be

 * any number of subcommands of the form "(x|y)[0-9]+".

 *

 * Returns whether the command is valid. The position arguments are

 * only written if the parsing was successful.

 *

 * For instance:

 *   - ";"          returns (<original x>, <original y>).

 *   - "x1;"        returns (1, <original y>).

 *   - "y2x1;"      returns (1, 2).

 *   - "x12y34x56;" returns (56, 34).

 *   - ""           fails.

 *   - "x"          fails.

 *   - "x;"         fails.

 *   - "x1"         fails.

 *   - "xy12;"      fails.

 *   - "x12yy12;"   fails.

 *   - "xx"         fails.

/*

 * These are the file operation function for user access to /dev/lcd

 * This function can also be called from inside the kernel, by

 * setting file and ppos to NULL.

 *

 LCD special codes */

 check for display mode flags */

 Display ON */

 Display OFF */

 Cursor ON */

 Cursor OFF */

 Blink ON */

 Blink OFF */

 Back light ON */

 Back light OFF */

 Flash back light */

 Small Font */

 Large Font */

 One Line */

 Two Lines */

 Shift Cursor Left */

 shift cursor right */

 shift display left */

 shift display right */

 kill end of line */

 restore cursor position */

 reinitialize display */

 gotoxy : LxXXX[yYYY]; */

 gotoxy : LyYYY[xXXX]; */

 If the command is valid, move to the new address */

 Regardless of its validity, mark as processed */

 first, we'll test if we're in escape mode */

 yes, let's add this char to the buffer */

 aborts any previous escape sequence */

 start of an escape sequence */

 go back one char and clear it */

 back one char */

 replace with a space */

 back one char again */

 quickly clear the display */

			/*

			 * flush the remainder of the current line and

			 * go to the beginning of the next line

 go to the beginning of the same line */

 print a space instead of the tab */

 simply print this char */

	/*

	 * now we'll see if we're in an escape mode and if the current

	 * escape sequence can be understood.

 clear the display */

 cursor to home */

 codes starting with ^[[L */

 LCD special escape codes */

		/*

		 * flush the escape sequence if it's been processed

		 * or if it is getting too long.

 escape codes */

			/*

			 * charlcd_write() is invoked as a VFS->write() callback

			 * and as such it is always invoked from preemptible

			 * context and may sleep.

 open only once at a time */

 device is write-only */

 initialize the LCD driver */

	/*

	 * before this line, we must NOT send anything to the display.

	 * Since charlcd_init_display() needs to write data, we have to

	 * enable mark the LCD initialized just before.

 display a short message */

 clear the display on the next device opening */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

/**

 * struct img_ascii_lcd_config - Configuration information about an LCD model

 * @num_chars: the number of characters the LCD can display

 * @external_regmap: true if registers are in a system controller, else false

 * @update: function called to update the LCD

/**

 * struct img_ascii_lcd_ctx - Private data structure

 * @base: the base address of the LCD registers

 * @regmap: the regmap through which LCD registers are accessed

 * @offset: the offset within regmap to the start of the LCD registers

 * @cfg: pointer to the LCD model configuration

 * @linedisp: line display structure

 * @curr: the string currently displayed on the LCD

/*

 * MIPS Boston development board

/*

 * MIPS Malta development board

/*

 * MIPS SEAD3 development board

 sentinel */ }

/**

 * img_ascii_lcd_probe() - probe an LCD display device

 * @pdev: the LCD platform device

 *

 * Probe an LCD display device, ensuring that we have the required resources in

 * order to access the LCD & setting up private data as well as sysfs files.

 *

 * Return: 0 on success, else -ERRNO

 for backwards compatibility */

/**

 * img_ascii_lcd_remove() - remove an LCD display device

 * @pdev: the LCD platform device

 *

 * Remove an LCD display device, freeing private resources & ensuring that the

 * driver stops using the LCD display registers.

 *

 * Return: 0

 SPDX-License-Identifier: GPL-2.0

/*

 *  console driver for LCD2S 4x20 character displays connected through i2c.

 *  The display also has a spi interface, but the driver does not support

 *  this yet.

 *

 *  This is a driver allowing you to use a LCD2S 4x20 from modtronix

 *  engineering as auxdisplay character device.

 *

 *  (C) 2019 by Lemonage Software GmbH

 *  Author: Lars Pöschel <poeschel@lemonage.de>

 *  All rights reserved.

 turn everything off, but display on */

	/* Generator : LGcxxxxx...xx; must have <c> between '0'

	 * and '7', representing the numerical ASCII code of the

	 * redefined character, and <xx...xx> a sequence of 16

	 * hex digits representing 8 bytes for each character.

	 * Most LCDs will only use 5 lower bits of the 7 first

	 * bytes.

 This implicitly sets cursor to first row and column */

 Test, if the display is responding */

 Required properties */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Front panel driver for Linux

 * Copyright (C) 2000-2008, Willy Tarreau <w@1wt.eu>

 * Copyright (C) 2016-2017 Glider bvba

 *

 * This code drives an LCD module (/dev/lcd), and a keypad (/dev/keypad)

 * connected to a parallel printer port.

 *

 * The LCD module may either be an HD44780-like 8-bit parallel LCD, or a 1-bit

 * serial module compatible with Samsung's KS0074. The pins may be connected in

 * any combination, everything is programmable.

 *

 * The keypad consists in a matrix of push buttons connecting input pins to

 * data output pins or to the ground. The combinations have to be hard-coded

 * in the driver, though several profiles exist and adding new ones is easy.

 *

 * Several profiles are provided for commonly found LCD+keypad modules on the

 * market, such as those found in Nexcom's appliances.

 *

 * FIXME:

 *      - the initialization/deinitialization process is very dirty and should

 *        be rewritten. It may even be buggy.

 *

 * TODO:

 *	- document 24 keys keyboard (3 rows of 8 cols, 32 diodes + 2 inputs)

 *      - make the LCD a part of a virtual screen of Vx*Vy

 *	- make the inputs list smp-safe

 *      - change the keyboard to a double mapping : signals -> key_id -> values

 *        so that applications can change values without knowing signals

 *

 max burst write */

 poll the keyboard this every second */

 a key starts to repeat after this times INPUT_POLL_TIME */

 a key repeats this times INPUT_POLL_TIME */

 converts an r_str() input to an active high, bits string : 000BAOSE */

 inverted input, active low */

 direct input, active low */

 direct input, active high */

 direct input, active high */

 direct input, active low */

 bi-directional ports */

 high to read data in or-ed with data out */

 inverted output, active low */

 direct output, active low */

 inverted output, active low */

 inverted output */

 macros to simplify use of the parallel port */

 this defines which bits are to be used and which ones to be ignored */

 logical or of the output bits involved in the scan matrix */

 logical or of the input bits involved in the scan matrix */

 valid when type == INPUT_TYPE_STD */

 valid when type == INPUT_TYPE_KBD */

 list of all defined logical inputs */

/* physical contacts history

 * Physical contacts are a 45 bits string of 9 groups of 5 bits each.

 * The 8 lower groups correspond to output bits 0 to 7, and the 9th group

 * corresponds to the ground.

 * Within each group, bits are stored in the same order as read on the port :

 * BAPSE (busy=4, ack=3, paper empty=2, select=1, error=0).

 * So, each __u64 is represented like this :

 * 0000000000000000000BAPSEBAPSEBAPSEBAPSEBAPSEBAPSEBAPSEBAPSEBAPSE

 * <-----unused------><gnd><d07><d06><d05><d04><d03><d02><d01><d00>

 what has just been read from the I/O ports */

 previous phys_read */

 stabilized phys_read (phys_read|phys_read_prev) */

 previous phys_curr */

 0 means that at least one logical signal needs be computed */

 these variables are specific to the keypad */

 lcd-specific variables */

 TODO: use union here? */

 Needed only for init */

/*

 * Bit masks to convert LCD signals to parallel port outputs.

 * _d_ are values for data port, _c_ are for control port.

 * [0] = signal OFF, [1] = signal ON, [2] = mask

/*

 * one entry for each bit on the LCD

/*

 * each bit can be either connected to a DATA or CTRL port

/*

 * LCD protocols

/*

 * LCD character sets

/*

 * LCD types

/*

 * keypad types

/*

 * panel profiles

/*

 * Construct custom config from the kernel's configuration

 custom */

 DEFAULT_PROFILE == 0 */

 global variables */

 Device single-open policy control */

 internal buffer width (usually 40) */

 hardware buffer width (usually 64) */

), 1=serial, 2=TI LCD Interface");

/*

 * These are the parallel port pins the LCD control signals are connected to.

 * Set this to 0 if the signal is not used. Set it to its opposite value

 * (negative) if the signal is negated. -MAXINT is used to indicate that the

 * pin has not been explicitly specified.

 *

 * WARNING! no check will be performed about collisions with keypad !

 port pin connected to LCD 'E' signal, with polarity (-17..17)");

 port pin connected to LCD 'RS' signal, with polarity (-17..17)");

 port pin connected to LCD 'RW' signal, with polarity (-17..17)");

 port pin connected to serial LCD 'SCL' signal, with polarity (-17..17)");

 port pin connected to serial LCD 'SDA' signal, with polarity (-17..17)");

 port pin connected to LCD backlight, with polarity (-17..17)");

 Deprecated module parameters - consider not using them anymore */

 for some LCD drivers (ks0074) we need a charset conversion table. */

          0|8   1|9   2|A   3|B   4|C   5|D   6|E   7|F */

 0x00 */ 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,

 0x08 */ 0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f,

 0x10 */ 0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,

 0x18 */ 0x18, 0x19, 0x1a, 0x1b, 0x1c, 0x1d, 0x1e, 0x1f,

 0x20 */ 0x20, 0x21, 0x22, 0x23, 0xa2, 0x25, 0x26, 0x27,

 0x28 */ 0x28, 0x29, 0x2a, 0x2b, 0x2c, 0x2d, 0x2e, 0x2f,

 0x30 */ 0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,

 0x38 */ 0x38, 0x39, 0x3a, 0x3b, 0x3c, 0x3d, 0x3e, 0x3f,

 0x40 */ 0xa0, 0x41, 0x42, 0x43, 0x44, 0x45, 0x46, 0x47,

 0x48 */ 0x48, 0x49, 0x4a, 0x4b, 0x4c, 0x4d, 0x4e, 0x4f,

 0x50 */ 0x50, 0x51, 0x52, 0x53, 0x54, 0x55, 0x56, 0x57,

 0x58 */ 0x58, 0x59, 0x5a, 0xfa, 0xfb, 0xfc, 0x1d, 0xc4,

 0x60 */ 0x96, 0x61, 0x62, 0x63, 0x64, 0x65, 0x66, 0x67,

 0x68 */ 0x68, 0x69, 0x6a, 0x6b, 0x6c, 0x6d, 0x6e, 0x6f,

 0x70 */ 0x70, 0x71, 0x72, 0x73, 0x74, 0x75, 0x76, 0x77,

 0x78 */ 0x78, 0x79, 0x7a, 0xfd, 0xfe, 0xff, 0xce, 0x20,

 0x80 */ 0x80, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87,

 0x88 */ 0x88, 0x89, 0x8a, 0x8b, 0x8c, 0x8d, 0x8e, 0x8f,

 0x90 */ 0x90, 0x91, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97,

 0x98 */ 0x98, 0x99, 0x9a, 0x9b, 0x9c, 0x9d, 0x9e, 0x9f,

 0xA0 */ 0x20, 0x40, 0xb1, 0xa1, 0x24, 0xa3, 0xfe, 0x5f,

 0xA8 */ 0x22, 0xc8, 0x61, 0x14, 0x97, 0x2d, 0xad, 0x96,

 0xB0 */ 0x80, 0x8c, 0x82, 0x83, 0x27, 0x8f, 0x86, 0xdd,

 0xB8 */ 0x2c, 0x81, 0x6f, 0x15, 0x8b, 0x8a, 0x84, 0x60,

 0xC0 */ 0xe2, 0xe2, 0xe2, 0x5b, 0x5b, 0xae, 0xbc, 0xa9,

 0xC8 */ 0xc5, 0xbf, 0xc6, 0xf1, 0xe3, 0xe3, 0xe3, 0xe3,

 0xD0 */ 0x44, 0x5d, 0xa8, 0xe4, 0xec, 0xec, 0x5c, 0x78,

 0xD8 */ 0xab, 0xa6, 0xe5, 0x5e, 0x5e, 0xe6, 0xaa, 0xbe,

 0xE0 */ 0x7f, 0xe7, 0xaf, 0x7b, 0x7b, 0xaf, 0xbd, 0xc8,

 0xE8 */ 0xa4, 0xa5, 0xc7, 0xf6, 0xa7, 0xe8, 0x69, 0x69,

 0xF0 */ 0xed, 0x7d, 0xa8, 0xe4, 0xec, 0x5c, 0x5c, 0x25,

 0xF8 */ 0xac, 0xa6, 0xea, 0xef, 0x7e, 0xeb, 0xb2, 0x79,

 signals, press, repeat, release */

 add new signals above this line */

 signals, press, repeat, release */

 add new signals above this line */

 sets data port bits according to current signals values */

 sets ctrl port bits according to current signals values */

 sets ctrl & data port bits according to current signals values */

/*

 * Converts a parallel port pin (from -25 to 25) to data and control ports

 * masks, and data and control port bits. The signal will be considered

 * unconnected if it's on pin 0 or an invalid pin (<-25 or >25).

 *

 * Result will be used this way :

 *   out(dport, in(dport) & d_val[2] | d_val[signal_state])

 *   out(cport, in(cport) & c_val[2] | c_val[signal_state])

 strobe, inverted */

 D0 - D7 = 2 - 9 */

 autofeed, inverted */

 init, direct */

 select_in, inverted */

 unknown pin, ignore */

/*

 * send a serial byte to the LCD panel. The caller is responsible for locking

 * if needed.

	/*

	 * the data bit is set on D0, and the clock on STROBE.

	 * LCD reads D0 on STROBE's rising edge.

 CLK low */

 maintain the data during 2 us before CLK up */

 CLK high */

 maintain the strobe during 1 us */

 turn the backlight on or off */

 The backlight is activated by setting the AUTOFEED line to +5V  */

 send a command to the LCD panel in serial mode */

 R/W=W, RS=0 */

 the shortest command takes at least 40 us */

 send data to the LCD panel in serial mode */

 R/W=W, RS=1 */

 the shortest data takes at least 40 us */

 send a command to the LCD panel in 8 bits parallel mode */

 present the data to the data port */

 maintain the data during 20 us before the strobe */

 maintain the strobe during 40 us */

 the shortest command takes at least 120 us */

 send data to the LCD panel in 8 bits parallel mode */

 present the data to the data port */

 maintain the data during 20 us before the strobe */

 maintain the strobe during 40 us */

 the shortest data takes at least 45 us */

 send a command to the TI LCD panel */

 present the data to the control port */

 send data to the TI LCD panel */

 present the data to the data port */

 initialize the LCD driver */

	/*

	 * Init lcd struct with load-time values to preserve exact

	 * current functionality (at least for now).

 parallel mode, 8 bits */

 serial mode, ks0074 */

 parallel mode, 8 bits, generic */

 customer-defined */

 default geometry will be set later */

 parallel mode, 8 bits, hantronix-like */

 Overwrite with module params set on loading */

 this is used to catch wrong and default values */

 SERIAL */

 PARALLEL */

/*

 * These are the file operation function for user access to /dev/keypad

 open only once at a time */

 device is read-only */

 flush the buffer on opening */

 read */

 open */

 close */

 send the key to the device only if a process is attached to it. */

/* this function scans all the bits involving at least one logical signal,

 * and puts the results in the bitfield "phys_read" (one bit per established

 * contact), and sets "phys_read_prev" to "phys_read".

 *

 * Note: to debounce input signals, we will only consider as switched a signal

 * which is stable across 2 measures. Signals which are different between two

 * reads will be kept as they previously were in their logical form (phys_prev).

 * A signal which has just switched will have a 1 in

 * (phys_read ^ phys_read_prev).

 flush all signals */

 keep track of old value, with all outputs disabled */

 activate all keyboard outputs (active low) */

 will have a 1 for each bit set to gnd */

 disable all matrix signals */

	/* now that all outputs are cleared, the only active input bits are

	 * directly connected to the ground

 1 for each grounded input */

 grounded inputs are signals 40-44 */

		/*

		 * since clearing the outputs changed some inputs, we know

		 * that some input signals are currently tied to some outputs.

		 * So we'll scan them.

 skip unused bits */

 enable this output */

 disable all outputs */

	/*

	 * this is easy: use old bits when they are flapping,

	 * use new ones when stable

	/* FIXME:

	 * this is an invalid test. It tries to catch

	 * transitions from single-key to multiple-key, but

	 * doesn't take into account the contacts polarity.

	 * The only solution to the problem is to parse keys

	 * from the most complex to the simplest combinations,

	 * and mark them as 'caught' once a combination

	 * matches, then unmatch it for all other ones.

	/* try to catch dangerous transitions cases :

	 * someone adds a bit, so this signal was a false

	 * positive resulting from a transition. We should

	 * invalidate the signal immediately and not call the

	 * release function.

	 * eg: 0 -(press A)-> A -(press B)-> AB : don't match A's release.

 invalidate */

 will turn on the light */

 we will need to come back here soon */

 else signal falling down. Let's fall through. */

 FIXME !!! same comment as in input_state_high */

 invalidate */

 will turn on the light */

 we will need to come back here soon */

 call release event */

			/* if all needed ones were already set previously,

			 * this means that this logical signal has been

			 * activated by the releasing of another combined

			 * signal, so we don't want to match.

			 * eg: AB -(release B)-> A -(release A)-> 0 :

			 *     don't match A.

 no need for the parport anymore */

 already started */

/* converts a name of the form "({BbAaPpSsEe}{01234567-})*" to a series of bits.

 * if <omask> or <imask> are non-null, they will be or'ed with the bits

 * corresponding to out and in bits respectively.

 * returns 1 if ok, 0 if error (in which case, nothing is written).

 input name not found */

 odd (lower) names are negated */

 unknown bit name */

/* tries to bind a key to the signal name <name>. The key will send the

 * strings <press>, <repeat>, <release> for these respective events.

 * Returns the pointer to the new key if ok, NULL if the key could not be bound.

/* tries to bind a callback function to the signal name <name>. The function

 * <press_fct> will be called with the <press_data> arg when the signal is

 * activated, and so on for <release_fct>/<release_data>

 * Returns the pointer to the new signal if ok, NULL if the signal could not

 * be bound.

 flushes any eventual noisy keystroke */

 Let's create all known keys */

*************************************************/

 device initialization                          */

*************************************************/

 panel_cb.flags = 0 should be PARPORT_DEV_EXCL? */

	/* must init LCD first, just in case an IRQ from the keypad is

	 * generated at keypad init

 TODO: free all input signals */

 init function */

 take care of an eventual profile */

 custom profile */

 8 bits, 2*16, old keypad */

 TODO: This two are a little hacky, sort it out later */

 serial, 2*16, new keypad */

 8 bits, 2*16 hantronix-like, no keypad */

 generic 8 bits, 2*16, nexcom keypad, eg. Nexcom. */

 8 bits, 2*40, old keypad */

	/*

	 * Overwrite selection with module param values (both keypad and lcd),

	 * where the deprecated params have lower prio.

		/*

		 * Init lcd struct with load-time values to preserve exact

		 * current functionality (at least for now).

 no device enabled, let's exit */

 SPDX-License-Identifier: GPL-2.0+

/*

 * HD44780 Character LCD driver for Linux

 *

 * Copyright (C) 2000-2008, Willy Tarreau <w@1wt.eu>

 * Copyright (C) 2016-2017 Glider bvba

 Order does matter due to writing to GPIO array subsets! */

 Optional */

 Optional */

 Optional */

 Optional */

 Optional */

 Optional */

 Maintain the data during 20 us before the strobe */

 Maintain the strobe during 40 us */

 write to an LCD panel register in 8 bit GPIO mode */

 for DATA[0-7], RS, RW */

 Present the data to the port */

 write to an LCD panel register in 4 bit GPIO mode */

 for DATA[4-7], RS, RW */

 High nibble + RS, RW */

 Present the data to the port */

 Low nibble */

 Present the data to the port */

 Send a command to the LCD panel in 8 bit GPIO mode */

 The shortest command takes at least 120 us */

 Send data to the LCD panel in 8 bit GPIO mode */

 The shortest data takes at least 45 us */

 Send a command to the LCD panel in 4 bit GPIO mode */

 The shortest command takes at least 120 us */

 Send 4-bits of a command to the LCD panel in raw 4 bit GPIO mode */

 for DATA[4-7], RS, RW */

 Command nibble + RS, RW */

 Present the data to the port */

 Send data to the LCD panel in 4 bit GPIO mode */

 The shortest data takes at least 45 us */

 Required pins */

 Optional pins */

 Required properties */

	/*

	 * On displays with more than two rows, the internal buffer width is

	 * usually equal to the display width

 Optional properties */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 *    Filename: ks0108.c

 *     Version: 0.1.0

 * Description: ks0108 LCD Controller driver

 *     Depends: parport

 *

 *      Author: Copyright (C) Miguel Ojeda <ojeda@kernel.org>

 *        Date: 2006-10-31

/*

 * Module Parameters

/*

 * Device

/*

 * ks0108 Exported Commands (don't lock)

 *

 *   You _should_ lock in the top driver: This functions _should not_

 *   get race conditions in any way. Locking for each byte here would be

 *   so slow and useless.

 *

 *   There are not bit definitions because they are not flags,

 *   just arbitrary combinations defined by the documentation for each

 *   function in the ks0108 LCD controller. If you want to know what means

 *   a specific combination, look at the function's name.

 *

 *   The ks0108_writecontrol bits need to be reverted ^(0,1,3) because

 *   the parallel port also revert them using a "not" logic gate.

/*

 * Is the module inited?

/*

 * Module Init & Exit

 SPDX-License-Identifier: GPL-2.0-or-later

 LCD commands */

 Clear entire display */

 Set entry mode */

 Increment cursor */

 Display control */

 Set display on */

 Set cursor on */

 Set blink on */

 Shift cursor/display */

 Shift display instead of cursor */

 Shift display/cursor to the right */

 Set function */

 Set data length to 8 bits */

 Set to two display lines */

 Set char font to 5x10 dots */

 Set char generator RAM address */

 Set display data RAM address */

 sleeps that many milliseconds with a reschedule */

	/*

	 * we force the cursor to stay at the end of the

	 * line if it wants to go farther

 clears the display and resets X/Y */

 datasheet says to wait 1,64 milliseconds */

 wait 20 ms after power-up for the paranoid */

	/*

	 * 8-bit mode, 1 line, small fonts; let's do it 3 times, to make sure

	 * the LCD is in 8-bit mode afterwards

 Switch to 4-bit mode, 1 line, small fonts */

 set font height and lines number */

 display off, cursor off, blink off */

 set display mode */

 entry mode set : increment, cursor shifting */

 back one char if not at end of line */

 allow the cursor to pass the end of the line */

	/* Generator : LGcxxxxx...xx; must have <c> between '0'

	 * and '7', representing the numerical ASCII code of the

	 * redefined character, and <xx...xx> a sequence of 16

	 * hex digits representing 8 bytes for each character.

	 * Most LCDs will only use 5 lower bits of the 7 first

	 * bytes.

 ensures that we stop writing to CGRAM */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Filename: cfag12864bfb.c

 *     Version: 0.1.0

 * Description: cfag12864b LCD framebuffer driver

 *     Depends: cfag12864b

 *

 *      Author: Copyright (C) Miguel Ojeda <ojeda@kernel.org>

 *        Date: 2006-10-31

 cfag12864b_init() must be called first */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Filename: cfag12864b.c

 *     Version: 0.1.0

 * Description: cfag12864b LCD driver

 *     Depends: ks0108

 *

 *      Author: Copyright (C) Miguel Ojeda <ojeda@kernel.org>

 *        Date: 2006-10-31

/*

 * Module Parameters

/*

 * cfag12864b Commands

 *

 *	E = Enable signal

 *		Every time E switch from low to high,

 *		cfag12864b/ks0108 reads the command/data.

 *

 *	CS1 = First ks0108controller.

 *		If high, the first ks0108 controller receives commands/data.

 *

 *	CS2 = Second ks0108 controller

 *		If high, the second ks0108 controller receives commands/data.

 *

 *	DI = Data/Instruction

 *		If low, cfag12864b will expect commands.

 *		If high, cfag12864b will expect data.

 *

/*

 * cfag12864b Internal Commands

/*

 * Update work

/*

 * cfag12864b Exported Symbols

/*

 * Is the module inited?

/*

 * Module Init & Exit

 ks0108_init() must be called first */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. FCU fan control

 *

 * Copyright 2012 Benjamin Herrenschmidt, IBM Corp.

/*

 * This option is "weird" :) Basically, if you define this to 1

 * the control loop for the RPMs fans (not PWMs) will apply the

 * correction factor obtained from the PID to the actual RPM

 * speed read from the FCU.

 *

 * If you define the below constant to 0, then it will be

 * applied to the setpoint RPM speed, that is basically the

 * speed we proviously "asked" for.

 *

 * I'm using 0 for now which is what therm_pm72 used to do and

 * what Darwin -apparently- does based on observed behaviour.

 Default min/max for pumps */

 Programmed value or real current speed */

 Try to fetch pumps min/max infos from eeprom */

	/* Double check the values, this _IS_ needed as the EEPROM on

	 * some dual 2.5Ghz G5s seem, at least, to have both min & max

	 * same to the same value ... (grrrr)

 Default */

 CPU fans have min/max in MPU */

 Rackmac variants, we just use mpu0 intake */

	/* min/max is oddball but the code comes from

	 * therm_pm72 which seems to work so ...

	/* Translation of device-tree location properties to

	 * windfarm fan names

 Device-tree name */

 Control name */

 Detect control type */

 Only care about fans for now */

 Lookup for a matching location */

 We only support the default fans for PowerMac7,2 */

	/*

	 * First we must start the FCU which will query the

	 * shift value to apply to RPMs

 First lookup fans in the device-tree */

	/*

	 * Older machines don't have the device-tree entries

	 * we are looking for, just hard code the list

 Still no fans ? FAIL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control.

 * Control loops for RackMack3,1 (Xserve G5)

 *

 * Copyright (C) 2012 Benjamin Herrenschmidt, IBM Corp.

/* define this to force CPU overtemp to 60 degree, useful for testing

 * the overtemp code

 We currently only handle 2 chips */

 Controls and sensors */

 We keep a temperature history for average calculation of 180s */

 PID loop state */

 Overtemp values */

	/* We max all CPU fans in case of a sensor error. We also do the

	 * cpufreq clamping now, even if it's supposedly done later by the

	 * generic code anyway, we do it earlier here to react faster

 First check for immediate overtemps */

	/*

	 * The first time around, initialize the array with the first

	 * temperature reading

	/*

	 * We calculate a history of max temperatures and use that for the

	 * overtemp management

 Now check for average overtemps */

	/* Now handle overtemp conditions. We don't currently use the windfarm

	 * overtemp handling core as it's not fully suited to the needs of those

	 * new machine. This will be fixed later.

 High overtemp -> immediate shutdown */

 Get diode temperature */

 Get voltage */

 Get current */

 Calculate power */

	/* Scale voltage and current raw sensor values according to fixed scales

	 * obtained in Darwin and calculate power from I and V

 Read current speed */

 Keep track of highest temp */

 Handle possible overtemps */

 Run PID */

 Apply DIMMs clamp */

 Apply result to all cpu fans */

 Implementation... */

 Get PID params from the appropriate MPU EEPROM */

 We keep a global tmax for overtemp calculations */

 Set PID min/max by using the rear fan min/max */

 History size */

 Initialize PID loop */

 seconds */

 Backside/U3 fan */

 DIMMs temperature (clamp the backside fan) */

 Update fan speed from actual fans */

 first time initialize things */

 Slots fan */

 first time initialize things */

 Permanent failure, bail out */

	/*

	 * Clear all failure bits except low overtemp which will be eventually

	 * cleared by the control loop itself

	/* We do CPUs last because they can be clamped high by

	 * DIMM temperature

 Check for failures. Any failure causes cpufreq clamping */

	/* That's it for now, we might want to deal with other failures

	 * differently in the future though

 should release all sensors and controls */

 Count the number of CPU cores */

 Get MPU data for each CPU */

 MODULE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control.  MAX6690 sensor.

 *

 * Copyright (C) 2005 Paul Mackerras, IBM Corp. <paulus@samba.org>

/* This currently only exports the external temperature sensor,

 Some MAX6690 register numbers */

 chip gets initialized by firmware */

	/*

	 * We only expose the external temperature register for

	 * now as this is all we need for our control loops

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver for the Cuda and Egret system controllers found on PowerMacs

 * and 68k Macs.

 *

 * The Cuda or Egret is a 6805 microcontroller interfaced to the 6522 VIA.

 * This MCU controls system power, Parameter RAM, Real Time Clock and the

 * Apple Desktop Bus (ADB) that connects to the keyboard and mouse.

 *

 * Copyright (C) 1996 Paul Mackerras.

 VIA registers - spaced 0x200 bytes apart */

 skip between registers */

 B-side data */

 A-side data */

 B-side direction (1=output) */

 A-side direction (1=output) */

 Timer 1 ctr/latch (low 8 bits) */

 Timer 1 counter (high 8 bits) */

 Timer 1 latch (low 8 bits) */

 Timer 1 latch (high 8 bits) */

 Timer 2 ctr/latch (low 8 bits) */

 Timer 2 counter (high 8 bits) */

 Shift register */

 Auxiliary control register */

 Peripheral control register */

 Interrupt flag register */

 Interrupt enable register */

 A-side data, no handshake */

/*

 * When the Cuda design replaced the Egret, some signal names and

 * logic sense changed. They all serve the same purposes, however.

 *

 *   VIA pin       |  Egret pin

 * ----------------+------------------------------------------

 *   PB3 (input)   |  Transceiver session   (active low)

 *   PB4 (output)  |  VIA full              (active high)

 *   PB5 (output)  |  System session        (active high)

 *

 *   VIA pin       |  Cuda pin

 * ----------------+------------------------------------------

 *   PB3 (input)   |  Transfer request      (active low)

 *   PB4 (output)  |  Byte acknowledge      (active low)

 *   PB5 (output)  |  Transfer in progress  (active low)

 Bits in Port B data register */

 Transfer request */

 Transfer acknowledge */

 Transfer in progress */

 Bits in ACR */

 Shift register control bits */

 Shift on external clock */

 Shift out if 1 */

 Bits in IFR and IER */

 set bits in IER */

 clear bits in IER */

 Shift register full/empty */

 Duration of byte acknowledgement pulse (us) */

 Interval from interrupt to start of session (us) */

 CONFIG_ADB */

 CONFIG_ADB */

 enable autopoll */

 Clear and enable interrupts, but only on PPC. On 68K it's done  */

 for us by the main VIA driver in arch/m68k/mac/via.c        */

 clear interrupts by writing 1s */

 enable interrupt from SR */

 enable autopoll */

 !defined CONFIG_MAC */

 CONFIG_ADB */

 Complete the inbound transfer */

 Terminate the outbound transfer */

 Clear shift register interrupt */

 disable interrupts from VIA */

 disable SR interrupt from VIA */

 TACK & TIP out */

 SR data in */

 clear any left-over data */

 delay 4ms and then clear any pending interrupt */

 sync with the CUDA - assert TACK without TIP */

 wait for the CUDA to assert TREQ in response */

 wait for the interrupt and then clear it */

 finish the sync by negating TACK */

 wait for the CUDA to negate TREQ and the corresponding interrupt */

 Send an ADB command */

 Enable/disable autopolling */

 Reset adb bus - how do we do this?? */

 maybe? */

 CONFIG_ADB */

 Construct and send a cuda request */

 assert cuda_state == idle */

 a byte is coming in from the CUDA */

 set the shift register to shift out and send a byte */

    /* On powermacs, this handler is registered for the VIA IRQ. But they use

     * just the shift register IRQ -- other VIA interrupt sources are disabled.

     * On m68k macs, the VIA IRQ sources are dispatched individually. Unless

     * we are polling, the shift register IRQ flag has already been cleared.

 System controller has unsolicited data for us */

 System controller has reply data for us */

 collision */

 Egret does not raise an "aborted" interrupt */

 not sure about this */

 that's all folks */

 Egret does not raise a "read done" interrupt */

 Have to adjust the reply from ADB commands */

 the 0x2 bit indicates no response */

 leave just the command and result bytes in the reply */

	    /* This is tricky. We must break the spinlock to call

	     * cuda_input. However, doing so means we might get

	     * re-entered from another CPU getting an interrupt

	     * or calling cuda_poll(). I ended up using the stack

	     * (it's only for 16 bytes) and moving the actual

	     * call to cuda_input to outside of the lock.

    	/* Here, we assume that if the request has a done member, the

    	 * struct request will survive to setting req->complete to 1

 CONFIG_XMON */

 CONFIG_ADB */

	/* Egret sends these periodically. Might be useful as a 'heartbeat'

	 * to trigger a recovery for the VIA shift register errata.

 Offset between Unix time (1970-based) and Mac time (1904-based) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. iMac G5

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 *

 * The algorithm used is the PID control algorithm, used the same

 * way the published Darwin code does, using the same values that

 * are present in the Darwin 8.2 snapshot property lists (note however

 * that none of the code has been re-used, it's a complete re-implementation

 *

 * The various control loops found in Darwin config file are:

 *

 * PowerMac8,1 and PowerMac8,2

 * ===========================

 *

 * System Fans control loop. Different based on models. In addition to the

 * usual PID algorithm, the control loop gets 2 additional pairs of linear

 * scaling factors (scale/offsets) expressed as 4.12 fixed point values

 * signed offset, unsigned scale)

 *

 * The targets are modified such as:

 *  - the linked control (second control) gets the target value as-is

 *    (typically the drive fan)

 *  - the main control (first control) gets the target value scaled with

 *    the first pair of factors, and is then modified as below

 *  - the value of the target of the CPU Fan control loop is retrieved,

 *    scaled with the second pair of factors, and the max of that and

 *    the scaled target is applied to the main control.

 *

 * # model_id: 2

 *   controls       : system-fan, drive-bay-fan

 *   sensors        : hd-temp

 *   PID params     : G_d = 0x15400000

 *                    G_p = 0x00200000

 *                    G_r = 0x000002fd

 *                    History = 2 entries

 *                    Input target = 0x3a0000

 *                    Interval = 5s

 *   linear-factors : offset = 0xff38 scale  = 0x0ccd

 *                    offset = 0x0208 scale  = 0x07ae

 *

 * # model_id: 3

 *   controls       : system-fan, drive-bay-fan

 *   sensors        : hd-temp

 *   PID params     : G_d = 0x08e00000

 *                    G_p = 0x00566666

 *                    G_r = 0x0000072b

 *                    History = 2 entries

 *                    Input target = 0x350000

 *                    Interval = 5s

 *   linear-factors : offset = 0xff38 scale  = 0x0ccd

 *                    offset = 0x0000 scale  = 0x0000

 *

 * # model_id: 5

 *   controls       : system-fan

 *   sensors        : hd-temp

 *   PID params     : G_d = 0x15400000

 *                    G_p = 0x00233333

 *                    G_r = 0x000002fd

 *                    History = 2 entries

 *                    Input target = 0x3a0000

 *                    Interval = 5s

 *   linear-factors : offset = 0x0000 scale  = 0x1000

 *                    offset = 0x0091 scale  = 0x0bae

 *

 * CPU Fan control loop. The loop is identical for all models. it

 * has an additional pair of scaling factor. This is used to scale the

 * systems fan control loop target result (the one before it gets scaled

 * by the System Fans control loop itself). Then, the max value of the

 * calculated target value and system fan value is sent to the fans

 *

 *   controls       : cpu-fan

 *   sensors        : cpu-temp cpu-power

 *   PID params     : From SMU sdb partition

 *   linear-factors : offset = 0xfb50 scale  = 0x1000

 *

 * CPU Slew control loop. Not implemented. The cpufreq driver in linux is

 * completely separate for now, though we could find a way to link it, either

 * as a client reacting to overtemp notifications, or directling monitoring

 * the CPU temperature

 *

 * WARNING ! The CPU control loop requires the CPU tmax for the current

 * operating point. However, we currently are completely separated from

 * the cpufreq driver and thus do not know what the current operating

 * point is. Fortunately, we also do not have any hardware supporting anything

 * but operating point 0 at the moment, thus we just peek that value directly

 * from the SDB partition. If we ever end up with actually slewing the system

 * clock and thus changing operating points, we'll have to find a way to

 * communicate with the CPU freq driver;

/* define this to force CPU overtemp to 74 degree, useful for testing

 * the overtemp code

 machine model id */

 Controls & sensors */

 Set to kick the control loop into life */

 Failure handling.. could be nicer */

/*

 * ****** System Fans Control Loop ******

 *

/* Parameters for the System Fans control loop. Parameters

 * not in this table such as interval, history size, ...

 * are common to all versions and thus hard coded for now.

/* State data used by the system fans control loop

/*

 * Configs for SMU System Fan control loop

 Model ID 2 */

 Model ID 3 */

 Model ID 5 */

/*

 * ****** CPU Fans Control Loop ******

 *

/* State data used by the cpu fans control loop

/*

 * ***** Implementation *****

 *

 First, locate the params for this model */

 No params found, put fans to max */

 Alloc & initialize state */

 Fill PID params */

 First, locate the PID params in SMU SBD */

	/* Get the FVT params for operating point 0 (the only supported one

	 * for now) in order to get tmax

 94 degree default */

 Alloc & initialize state */

 Fill PID params */

/*

 * ****** Setup / Init / Misc ... ******

 *

 Skipping ticks */

	/* If entering failure mode, clamp cpufreq and ramp all

	 * fans to full speed.

	/* If leaving failure mode, unclamp cpufreq and readjust

	 * all fans on next iteration

	/* Overtemp condition detected, notify and start skipping a couple

	 * ticks to let the temperature go down

	/* We only clear the overtemp condition if overtemp is cleared

	 * _and_ no other failure is present. Since a sensor error will

	 * clear the overtemp condition (can't measure temperature) at

	 * the control loop levels, but we don't want to keep it clear

	 * here in this case

	/* Darwin property list says the HD fan is only for model ID

	 * 0, 1, 2 and 3

	/* XXX We don't have yet a guarantee that our callback isn't

	 * in progress when returning from wf_unregister_client, so

	 * we add an arbitrary delay. I'll have to fix that in the core

 Release all sensors */

	/* One more crappy race: I don't think we have any guarantee here

	 * that the attribute callback won't race with the sensor beeing

	 * disposed of, and I'm not 100% certain what best way to deal

	 * with that except by adding locks all over... I'll do that

	 * eventually but heh, who ever rmmod this module anyway ?

 Release all controls */

 Destroy control loops state structures */

 MODULE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/macintosh/mac_hid.c

 *

 * HID support stuff for Macintosh computers.

 *

 * Copyright (C) 2000 Franz Sirl.

 *

 * This file will soon be removed in favor of an uinput userspace tool.

 right control key */

 right option key */

 Don't bind to ourselves */

 Restore the old value in case of error */

 file(s) in /proc/sys/dev/mac_hid */

 dir in /proc/sys/dev */

 /proc/sys/dev itself, in case that is not there yet */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control.

 * Control loops for machines with SMU and PPC970MP processors.

 *

 * Copyright (C) 2005 Paul Mackerras, IBM Corp. <paulus@samba.org>

 * Copyright (C) 2006 Benjamin Herrenschmidt, IBM Corp.

/* define this to force CPU overtemp to 60 degree, useful for testing

 * the overtemp code

 We currently only handle 2 chips, 4 cores... */

 Controls and sensors */

 Second pump isn't required (and isn't actually present) */

 We keep a temperature history for average calculation of 180s */

 Scale factor for fan speed, *100 */

 inlet fans run at 97% of exhaust fan */

 updated later */

 updated later */

 PID loop state */

 Overtemp values */

 Implementation... */

 Get FVT params to get Tmax; if not found, assume default */

 default to 95 degrees C */

 We keep a global tmax for overtemp calculations */

 Get PID params from the appropriate SAT */

	/*

	 * Darwin has a minimum fan speed of 1000 rpm for the 4-way and

	 * 515 for the 2-way.  That appears to be overkill, so for now,

	 * impose a minimum of 750 or 515.

 Initialize PID loop */

 seconds */

	/* We max all CPU fans in case of a sensor error. We also do the

	 * cpufreq clamping now, even if it's supposedly done later by the

	 * generic code anyway, we do it earlier here to react faster

 First check for immediate overtemps */

	/* We calculate a history of max temperatures and use that for the

	 * overtemp management

 Now check for average overtemps */

	/* Now handle overtemp conditions. We don't currently use the windfarm

	 * overtemp handling core as it's not fully suited to the needs of those

	 * new machine. This will be fixed later.

 High overtemp -> immediate shutdown */

 Get CPU core temperature */

 Keep track of highest temp */

 Get CPU power */

 Run PID */

 Darwin limits decrease to 20 per iteration */

 Handle possible overtemps */

 Set fans */

 Backside/U4 fan */

 first time; initialize things */

 Drive bay fan */

 first time; initialize things */

 PCI slots area fan */

 This makes the fan speed proportional to the power consumed */

 first time; initialize things */

 Permanent failure, bail out */

	/* Clear all failure bits except low overtemp which will be eventually

	 * cleared by the control loop itself

 Check for failures. Any failure causes cpufreq clamping */

	/* That's it for now, we might want to deal with other failures

	 * differently in the future though

 not a CPU fan, try the others */

 work out pump scaling factors */

 check if we have all the sensors we need */

 should release all sensors and controls */

 Count the number of CPU cores */

 MODULE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. Generic PID helpers

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 Calculate error term */

 Get samples into our history buffer */

 Calculate integral term */

 Calculate derivative term */

 Calculate target */

 Calculate error term */

 Get samples into our history buffer */

 Calculate integral term */

 Calculate derivative term */

 Calculate proportional term */

 Calculate target */

/*

 * via-pmu LED class device

 *

 * Copyright 2006 Johannes Berg <johannes@sipsolutions.net>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

 *

 -1: no change, 0: request off, 1: request on */

	/* if someone requested a change in the meantime

	 * (we only see the last one which is fine)

 reset requested change */

 if request isn't done, then don't do anything */

 only do this on keylargo based models */

 ignore */

 no outstanding req */

 SPDX-License-Identifier: GPL-2.0

/*

 * /dev/lcd driver for Apple Network Servers.

 FIXME: This is ugly, but should work, as a \0 byte is not a valid command code */

 Line #1 */

 Line #3 */

 Line #2 */

 Line #4 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. LM75 sensor

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 Init chip if necessary */

		/* clear shutdown bit, keep other settings as left by

		 * the firmware for now

 If we just powered it up, let's wait 200 ms */

 Read temperature register */

	/* Usual rant about sensor names not beeing very consistent in

	 * the device-tree, oh well ...

	 * Add more entries below as you deal with more setups

 Mark client detached */

 release sensor */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Driver for the media bay on the PowerBook 3400 and 2400.

 *

 * Copyright (C) 1998 Paul Mackerras.

 *

 * Various evolutions by Benjamin Herrenschmidt & Henry Worth

/*

 * Wait that number of ms between each step in normal polling mode

/*

 * Consider the media-bay ID value stable if it is the same for

 * this number of milliseconds

/* Wait after powering up the media bay this delay in ms

 * timeout bumped for some powerbooks

/*

 * Hold the media-bay reset signal true for this many ticks

 * after a device is inserted before releasing it.

/*

 * Wait this long after the reset signal is released and before doing

 * further operations. After this delay, the IDE reset signal is released

 * too for an IDE device

/*

 * Wait this many ticks after an IDE device (e.g. CD-ROM) is inserted

 * (or until the device is ready) before calling into the driver

/*

 * States of a media bay

 Idle */

 power bit set, waiting MB_POWER_DELAY */

 enable bits set, waiting MB_RESET_DELAY */

 reset bit unset, waiting MB_SETUP_DELAY */

 IDE reset bit unser, waiting MB_IDE_WAIT */

 Media bay full */

 Powering down (avoid too fast down/up) */

/*

 * Functions for polling content of media bay

/*

 * Functions for powering up/down the bay, puts the bay device

 * into reset state as well

 Power up device, assert it's reset line */

 Disable all devices */

 Cut power from bay, release reset line */

 Power up device, assert it's reset line */

 Disable all devices */

 Cut power from bay, release reset line */

 Power up device, assert it's reset line */

 Disable all devices */

 Cut power from bay, release reset line */

/*

 * Functions for configuring the media bay for a given type of device,

 * enable the related busses

/*

 * Functions for tweaking resets

 Power up up and assert the bay reset line */

 Make sure everything is powered down & disabled */

		/* If the device type changes without going thru

		 * "MB_NO", we force a pass by "MB_NO" to make sure

		 * things are properly reset

	/* This returns an instant snapshot, not locking, sine

	 * we may be called with the bay lock held. The resulting

	 * fuzzyness of the result if called at the wrong time is

	 * not actually a huge deal

 We don't poll when powering down */

 If timer expired run state machine */

/*

 * This procedure runs as a kernel thread to poll the media bay

 * once each tick and register and unregister the IDE interface

 * with the IDE driver.  It needs to be a thread because

 * ide_register can't be called from interrupt context.

	/* Media bay registers are located at the beginning of the

         * mac-io chip, for now, we trick and align down the first

	 * resource passed in

 Init HW probing */

 Force an immediate detect */

 Mark us ready by filling our mdev data */

 Startup kernel thread */

	       	/* We re-enable the bay using it's previous content

	       	   only if it did not change. Note those bozo timings,

	       	   they seem to help the 3400 get it right.

 Force MB power to 0 */

/* Definitions of "ops" structures.

/*

 * It seems that the bit for the media-bay interrupt in the IRQ_LEVEL

 * register is always set when there is something in the media bay.

 * This causes problems for the interrupt code if we attach an interrupt

 * handler to the media-bay interrupt, because it tends to go into

 * an infinite loop calling the media bay interrupt handler.

 * Therefore we do it all by polling the media bay once each tick.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Bus & driver management routines for devices within

 * a MacIO ASIC. Interface to new driver model mostly

 * stolen from the PCI version.

 * 

 *  Copyright (C) 2005 Ben. Herrenschmidt (benh@kernel.crashing.org)

 *

 * TODO:

 * 

 *  - Don't probe below media bay by default, but instead provide

 *    some hooks for media bay to dynamically add/remove it's own

 *    sub-devices.

/**

 * macio_release_dev - free a macio device structure when all users of it are

 * finished.

 * @dev: device that's been disconnected

 *

 * Will be called only by the device core when all users of this macio device

 * are done. This currently means never as we don't hot remove any macio

 * device yet, though that will happen with mediabay based devices in a later

 * implementation.

/**

 * macio_resource_quirks - tweak or skip some resources for a device

 * @np: pointer to the device node

 * @res: resulting resource

 * @index: index of resource in node

 *

 * If this routine returns non-null, then the resource is completely

 * skipped.

 Only quirks for memory resources for now */

 Grand Central has too large resource 0 on some machines */

 Airport has bogus resource 2 */

 DBDMAs may have bogus sizes */

 CONFIG_PPC64 */

	/* ESCC parent eats child resources. We could have added a

	 * level of hierarchy, but I don't really feel the need

	 * for it

 ESCC has bogus resources >= 3 */

 Media bay has too many resources, keep only first one */

 Some older IDE resources have bogus sizes */

 Gatwick has some missing interrupts on child nodes */

	/* irq_base is always 64 on gatwick. I have no cleaner way to get

	 * that value from here at this point

 Fix SCC */

 Fix media-bay */

 Fix left media bay childs */

		/* Currently, we consider failure as harmless, this may

		 * change in the future, once I've found all the device

		 * tree bugs in older machines & worked around them

/**

 * macio_add_one_device - Add one device from OF node to the device tree

 * @chip: pointer to the macio_chip holding the device

 * @np: pointer to the device node in the OF tree

 * @in_bay: set to 1 if device is part of a media-bay

 *

 * When media-bay is changed to hotswap drivers, this function will

 * be exposed to the bay driver some way...

 Standard DMA paremeters */

	/* Set the DMA ops to the ones from the PCI device, this could be

	 * fishy if we didn't know that on PowerMac it's always direct ops

	 * or iommu ops that will work fine

	 *

	 * To get all the fields, copy all archdata

 CONFIG_PCI && CONFIG_DMA_OPS */

 MacIO itself has a different reg, we use it's PCI base */

 NuBus may want to do something better here */

 Setup interrupts & resources */

 Register with core */

/**

 * macio_pci_add_devices - Adds sub-devices of mac-io to the device tree

 * @chip: pointer to the macio_chip holding the devices

 * 

 * This function will do the job of extracting devices from the

 * Open Firmware device tree, build macio_dev structures and add

 * them to the Linux device tree.

 * 

 * For now, childs of media-bay are added now as well. This will

 * change rsn though.

 Add a node for the macio bus itself */

 Add macio itself to hierarchy */

 First scan 1st level */

 Add media bay devices if any */

 Add serial ports if any */

/**

 * macio_register_driver - Registers a new MacIO device driver

 * @drv: pointer to the driver definition structure

 initialize common driver fields */

 register with core */

/**

 * macio_unregister_driver - Unregisters a new MacIO device driver

 * @drv: pointer to the driver definition structure

 Managed MacIO resources */

/**

 *	macio_request_resource - Request an MMIO resource

 * 	@dev: pointer to the device holding the resource

 *	@resource_no: resource number to request

 *	@name: resource name

 *

 *	Mark  memory region number @resource_no associated with MacIO

 *	device @dev as being reserved by owner @name.  Do not access

 *	any address inside the memory regions unless this call returns

 *	successfully.

 *

 *	Returns 0 on success, or %EBUSY on error.  A warning

 *	message is also printed on failure.

/**

 * macio_release_resource - Release an MMIO resource

 * @dev: pointer to the device holding the resource

 * @resource_no: resource number to release

/**

 *	macio_request_resources - Reserve all memory resources

 *	@dev: MacIO device whose resources are to be reserved

 *	@name: Name to be associated with resource.

 *

 *	Mark all memory regions associated with MacIO device @dev as

 *	being reserved by owner @name.  Do not access any address inside

 *	the memory regions unless this call returns successfully.

 *

 *	Returns 0 on success, or %EBUSY on error.  A warning

 *	message is also printed on failure.

/**

 *	macio_release_resources - Release reserved memory resources

 *	@dev: MacIO device whose resources were previously reserved

	/* Note regarding refcounting: We assume pci_device_to_OF_node() is

	 * ported to new OF APIs and returns a node with refcount incremented.

	/* The above assumption is wrong !!!

	 * fix that here for now until I fix the arch code

	/* We also assume that pmac_feature will have done a get() on nodes

	 * stored in the macio chips array

 XXX Need locking ??? */

	/*

	 * HACK ALERT: The WallStreet PowerBook and some OHare based machines

	 * have 2 macio ASICs. I must probe the "main" one first or IDE

	 * ordering will be incorrect. So I put on "hold" the second one since

	 * it seem to appear first on PCI

/*

 * MacIO is matched against any Apple ID, it's probe() function

 * will then decide wether it applies or not

 end: all zeroes */ }

 pci driver glue; this is a "new style" PCI driver module */

 CONFIG_PCI */

 CONFIG_PCI */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. SMU based 1 CPU desktop control loops

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 *

 * The algorithm used is the PID control algorithm, used the same

 * way the published Darwin code does, using the same values that

 * are present in the Darwin 8.2 snapshot property lists (note however

 * that none of the code has been re-used, it's a complete re-implementation

 *

 * The various control loops found in Darwin config file are:

 *

 * PowerMac9,1

 * ===========

 *

 * Has 3 control loops: CPU fans is similar to PowerMac8,1 (though it doesn't

 * try to play with other control loops fans). Drive bay is rather basic PID

 * with one sensor and one fan. Slots area is a bit different as the Darwin

 * driver is supposed to be capable of working in a special "AGP" mode which

 * involves the presence of an AGP sensor and an AGP fan (possibly on the

 * AGP card itself). I can't deal with that special mode as I don't have

 * access to those additional sensor/fans for now (though ultimately, it would

 * be possible to add sensor objects for them) so I'm only implementing the

 * basic PCI slot control loop

/* define this to force CPU overtemp to 74 degree, useful for testing

 * the overtemp code

 Controls & sensors */

 Set to kick the control loop into life */

 Failure handling.. could be nicer */

/*

 * ****** CPU Fans Control Loop ******

 *

/* State data used by the cpu fans control loop

/*

 * ****** Drive Fan Control Loop ******

 *

/*

 * ****** Slots Fan Control Loop ******

 *

/*

 * ***** Implementation *****

 *

 First, locate the PID params in SMU SBD */

	/* Get the FVT params for operating point 0 (the only supported one

	 * for now) in order to get tmax

 94 degree default */

 Alloc & initialize state */

 Fill PID params */

 Alloc & initialize state */

 Fill PID params */

 Alloc & initialize state */

 Fill PID params */

 Check what makes a good overtemp condition */

/*

 * ****** Setup / Init / Misc ... ******

 *

 Skipping ticks */

	/* If entering failure mode, clamp cpufreq and ramp all

	 * fans to full speed.

	/* If leaving failure mode, unclamp cpufreq and readjust

	 * all fans on next iteration

	/* Overtemp condition detected, notify and start skipping a couple

	 * ticks to let the temperature go down

	/* We only clear the overtemp condition if overtemp is cleared

	 * _and_ no other failure is present. Since a sensor error will

	 * clear the overtemp condition (can't measure temperature) at

	 * the control loop levels, but we don't want to keep it clear

	 * here in this case

	/* XXX We don't have yet a guarantee that our callback isn't

	 * in progress when returning from wf_unregister_client, so

	 * we add an arbitrary delay. I'll have to fix that in the core

 Release all sensors */

	/* One more crappy race: I don't think we have any guarantee here

	 * that the attribute callback won't race with the sensor beeing

	 * disposed of, and I'm not 100% certain what best way to deal

	 * with that except by adding locks all over... I'll do that

	 * eventually but heh, who ever rmmod this module anyway ?

 Release all controls */

 Destroy control loops state structures */

 MODULE */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver for the PMU in Apple PowerBooks and PowerMacs.

 *

 * The VIA (versatile interface adapter) interfaces to the PMU,

 * a 6805 microprocessor core whose primary function is to control

 * battery charging and system power on the PowerBook 3400 and 2400.

 * The PMU also controls the ADB (Apple Desktop Bus) which connects

 * to the keyboard and mouse, as well as the non-volatile RAM

 * and the RTC (real time clock) chip.

 *

 * Copyright (C) 1998 Paul Mackerras and Fabio Riccardi.

 * Copyright (C) 2001-2002 Benjamin Herrenschmidt

 * Copyright (C) 2006-2007 Johannes Berg

 *

 * THIS DRIVER IS BECOMING A TOTAL MESS !

 *  - Cleanup atomically disabling reply to PMU events after

 *    a sleep or a freq. switch

 *

 Some compile options */

 How many iterations between battery polls */

 VIA registers - spaced 0x200 bytes apart */

 skip between registers */

 B-side data */

 A-side data */

 B-side direction (1=output) */

 A-side direction (1=output) */

 Timer 1 ctr/latch (low 8 bits) */

 Timer 1 counter (high 8 bits) */

 Timer 1 latch (low 8 bits) */

 Timer 1 latch (high 8 bits) */

 Timer 2 ctr/latch (low 8 bits) */

 Timer 2 counter (high 8 bits) */

 Shift register */

 Auxiliary control register */

 Peripheral control register */

 Interrupt flag register */

 Interrupt enable register */

 A-side data, no handshake */

 Bits in B data register: both active low */

 Transfer acknowledge (input) */

 Transfer request (output) */

 Bits in ACR */

 Shift register control bits */

 Shift on external clock */

 Shift out if 1 */

 Bits in IFR and IER */

 set bits in IER */

 clear bits in IER */

 Shift register full/empty */

 transition on CB1 input */

 CONFIG_SUSPEND && CONFIG_PPC32 */

 CONFIG_ADB */

 CONFIG_ADB */

/*

 * This table indicates for each PMU opcode:

 * - the number of data bytes to be sent with the command, or -1

 *   if a length byte should be sent,

 * - the number of response bytes which the PMU will return, or

 *   -1 if it will send a length byte.

	   0	   1	   2	   3	   4	   5	   6	   7  */

00*/	{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

08*/	{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

10*/	{ 1, 0},{ 1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

18*/	{ 0, 1},{ 0, 1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{ 0, 0},

20*/	{-1, 0},{ 0, 0},{ 2, 0},{ 1, 0},{ 1, 0},{-1, 0},{-1, 0},{-1, 0},

28*/	{ 0,-1},{ 0,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{ 0,-1},

30*/	{ 4, 0},{20, 0},{-1, 0},{ 3, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

38*/	{ 0, 4},{ 0,20},{ 2,-1},{ 2, 1},{ 3,-1},{-1,-1},{-1,-1},{ 4, 0},

40*/	{ 1, 0},{ 1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

48*/	{ 0, 1},{ 0, 1},{-1,-1},{ 1, 0},{ 1, 0},{-1,-1},{-1,-1},{-1,-1},

50*/	{ 1, 0},{ 0, 0},{ 2, 0},{ 2, 0},{-1, 0},{ 1, 0},{ 3, 0},{ 1, 0},

58*/	{ 0, 1},{ 1, 0},{ 0, 2},{ 0, 2},{ 0,-1},{-1,-1},{-1,-1},{-1,-1},

60*/	{ 2, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

68*/	{ 0, 3},{ 0, 3},{ 0, 2},{ 0, 8},{ 0,-1},{ 0,-1},{-1,-1},{-1,-1},

70*/	{ 1, 0},{ 1, 0},{ 1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

78*/	{ 0,-1},{ 0,-1},{-1,-1},{-1,-1},{-1,-1},{ 5, 1},{ 4, 1},{ 4, 1},

80*/	{ 4, 0},{-1, 0},{ 0, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

88*/	{ 0, 5},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

90*/	{ 1, 0},{ 2, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

98*/	{ 0, 1},{ 0, 1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

a0*/	{ 2, 0},{ 2, 0},{ 2, 0},{ 4, 0},{-1, 0},{ 0, 0},{-1, 0},{-1, 0},

a8*/	{ 1, 1},{ 1, 0},{ 3, 0},{ 2, 0},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

b0*/	{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

b8*/	{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

c0*/	{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

c8*/	{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

d0*/	{ 0, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

d8*/	{ 1, 1},{ 1, 1},{-1,-1},{-1,-1},{ 0, 1},{ 0,-1},{-1,-1},{-1,-1},

e0*/	{-1, 0},{ 4, 0},{ 0, 1},{-1, 0},{-1, 0},{ 4, 0},{-1, 0},{-1, 0},

e8*/	{ 3,-1},{-1,-1},{ 0, 1},{-1,-1},{ 0,-1},{-1,-1},{-1,-1},{ 0, 0},

f0*/	{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},{-1, 0},

f8*/	{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},{-1,-1},

 disable all intrs */

 clear IFR */

 !CONFIG_PPC_PMAC */

 CONFIG_ADB */

/*

 * We can't wait until pmu_init gets called, that happens too late.

 * It happens after IDE and SCSI initialization, which can take a few

 * seconds, and by that time the PMU could have given up on us and

 * turned us off.

 * Thus this is called with arch_initcall rather than device_initcall.

	/* We set IRQF_NO_SUSPEND because we don't want the interrupt

	 * to be disabled between the 2 passes of driver suspend, we

	 * control our own disabling for that one

 Enable interrupts */

 !CONFIG_PPC_PMAC */

	/* Make sure PMU settle down before continuing. This is _very_ important

	 * since the IDE probe may shut interrupts down for quite a bit of time. If

	 * a PMU communication is pending while this happens, the PMU may timeout

	 * Not that on Core99 machines, the PMU keeps sending us environement

	 * messages, we should find a way to either fix IDE or make it call

	 * pmu_suspend() before masking interrupts. This can also happens while

	 * scolling with some fbdevs.

/*

 * This has to be done after pci_init, which is a subsys_initcall.

 Initialize backlight */

 Other stuffs here yet unknown */

 CONFIG_PPC32 */

 Create /proc/pmu */

 Negate TREQ. Set TACK to input and TREQ to output. */

 ack all pending interrupts */

 Tell PMU we are ready.  */

 Read PMU version */

 Read server mode setting */

/* This new version of the code for 2400/3400/3500 powerbooks

 * is inspired from the implementation in gkrellm-pmu

	/* format:

	 *  [0]    :  flags

	 *    0x01 :  AC indicator

	 *    0x02 :  charging

	 *    0x04 :  battery exist

	 *    0x08 :  

	 *    0x10 :  

	 *    0x20 :  full charged

	 *    0x40 :  pcharge reset

	 *    0x80 :  battery exist

	 *

	 *  [1][2] :  battery voltage

	 *  [3]    :  CPU temperature

	 *  [4]    :  battery temperature

	 *  [5]    :  current

	 *  [6][7] :  pcharge

	 *              --tkoba

 If battery installed */

 CONFIG_PPC_PMAC */

	/* format:

	 *  [0] : format of this structure (known: 3,4,5)

	 *  [1] : flags

	 *  

	 *  format 3 & 4:

	 *  

	 *  [2] : charge

	 *  [3] : max charge

	 *  [4] : current

	 *  [5] : voltage

	 *  

	 *  format 5:

	 *  

	 *  [2][3] : charge

	 *  [4][5] : max charge

	 *  [6][7] : current

	 *  [8][9] : voltage

 Send an ADB command */

req->data[1] = req->data[1];*/

 Enable/disable autopolling */

 Reset the ADB bus */

 anyone got a better idea?? */

 CONFIG_ADB */

 Construct and send a pmu request */

	/* Sightly increased the delay, I had one occurrence of the message

	 * reported

/* New PMU seems to be very sensitive to those timings, so we make sure

 assert TREQ */

 resets SR */

    	/* Here, we assume that if the request has a done member, the

    	 * struct request will survive to setting req->complete to 1

 assert pmu_state == idle */

 get the packet to send */

req->reply_expected && */req_awaiting_reply))

	/* Sounds safer to make sure ACK is high before writing. This helped

	 * kill a problem with ADB and some iBooks

 set the shift register to shift out and send a byte */

 Kicks ADB read when PMU is suspended */

/* This function loops until the PMU is idle and prevents it from

 * anwsering to ADB interrupts. pmu_request can still be called.

 * This is done to avoid spurrious shutdowns when we know we'll have

 * interrupts switched off for a long time

 Interrupt data could be the result data from an ADB cmd */

 Get PMU interrupt mask */

 Record zero interrupts for stats */

 Hack to deal with ADB autopoll flag */

	/* Note: for some reason, we get an interrupt with len=1,

	 * data[0]==0 after each normal ADB interrupt, at least

	 * on the Pismo. Still investigating...  --BenH

 CONFIG_XMON */

			/*

			 * XXX On the [23]400 the PMU gives us an up

			 * event for keycodes 0x74 or 0x75 when the PC

			 * card eject buttons are released, so we

			 * ignore those events.

 CONFIG_ADB */		

 Sound/brightness button pressed */

 Tick interrupt */

 Environment or tick interrupt, query batteries */

		/* len == 6 is probably a bad check. But how do I

 The ack may not yet be low when we get the interrupt */

 if reading grab the byte, and reset the interrupt */

 reset TREQ and wait for TACK to go high */

			/* 

			 * For PMU sleep and freq change requests, we lock the

			 * PMU until it's explicitly unlocked. This avoids any

			 * spurrious event polling getting in

 This is a bit brutal, we can probably do better */

		/* On 68k Macs, VIA interrupts are dispatched individually.

		 * Unless we are polling, the relevant IRQ flag has already

		 * been cleared.

			/* Sounds safer to make sure ACK is high before writing.

			 * This helped kill a problem with ADB and some iBooks

 Mark the oldest buffer for flushing */

 Deal with completed PMU requests outside of the lock */

 Deal with interrupt datas outside of the lock */

 Offset between Unix time (1970-based) and Mac time (1904-based) */

		/* Disable server mode on shutdown or we'll just

		 * wake up again

/*

 * Put the powerbook to sleep.

 disable all intrs */

 clear IFR */

 Turn off various things. Darwin does some retry tests here... */

 For 750, save backside cache setting and disable it */

 (returns -1 if not available) */

 Ask the PMU to put us to sleep */

 The VIA is supposed not to be restored correctly*/

 We shut down some HW */

 Apparently, MacOS uses NAP mode for Grackle ??? */

 Call low-level ASM sleep handler */

 We're awake again, stop grackle PM */

 Make sure the PMU is idle */

 Restore L2 cache */

 Restore userland MMU context */

 Power things up */

 Stop environment and ADB interrupts */

 Tell PMU what events will wake us up */

 Save the state of the L2 and L3 caches */

 (returns -1 if not available) */

 (returns -1 if not available) */

 Ask the PMU to put us to sleep */

 The VIA is supposed not to be restored correctly*/

	/* Shut down various ASICs. There's a chance that we can no longer

	 * talk to the PMU after this, so I moved it to _after_ sending the

	 * sleep command to it. Still need to be checked.

 Call low-level ASM sleep handler */

 Restore Apple core ASICs state */

 Restore VIA */

 tweak LPJ before cpufreq is there */

 Restore video */

 Restore L2 cache */

 Restore L3 cache */

 Restore userland MMU context */

 Tell PMU we are ready */

 Restore LPJ, cpufreq will adjust the cpu frequency */

 map in the memory controller registers */

	/* Set the memory controller to keep the memory refreshed

 Ask the PMU to put us to sleep */

 Put the CPU into sleep mode */

 OK, we're awake again, start restoring things */

 CONFIG_SUSPEND && CONFIG_PPC32 */

/*

 * Support for /dev/pmu device

 Call platform functions marked "on sleep" */

 Wait for completion of async requests */

	/* Giveup the lazy FPU & vec so we don't have to back them

	 * up from the low level code

 CONFIG_ALTIVEC */

 Force a poll of ADB interrupts */

 Call platform functions marked "on wake" */

 Compatibility ioctl's for backlight */

 CONFIG_INPUT_ADBHID */

 CONFIG_PMAC_BACKLIGHT_LEGACY */

 N.B. This doesn't work on the 3400 */

 DEBUG_SLEEP */

 Suspend PMU event interrupts */

 Tell backlight code not to muck around with the chip anymore */

 Tell PMU we are ready */

 Tell backlight code it can use the chip again */

 Resume PMU event interrupts */

 CONFIG_SUSPEND && CONFIG_PPC32 */

 CONFIG_SUSPEND && CONFIG_PPC32 */

 SPDX-License-Identifier: GPL-2.0

/*

 * I/O Processor (IOP) ADB Driver

 * Written and (C) 1999 by Joshua M. Thompson (funaho@jurai.org)

 * Based on via-cuda.c by Paul Mackerras.

 *

 * 1999-07-01 (jmt) - First implementation for new driver architecture.

 *

 * 1999-07-31 (jmt) - First working version.

 ADB command byte structure */

/*

 * Completion routine for ADB commands sent to the IOP.

 *

 * This will be called when a packet has been successfully sent.

/*

 * Listen for ADB messages from the IOP.

 *

 * This will be called when unsolicited IOP messages are received.

 * These IOP messages can carry ADB autopoll responses and also occur

 * after explicit ADB commands.

	/* Responses to Talk commands may be unsolicited as they are

	 * produced when the IOP polls devices. They are mostly timeouts.

/*

 * Start sending an ADB packet, IOP style

 *

 * There isn't much to do other than hand the packet over to the IOP

 * after encapsulating it in an adb_iopmsg.

 get the packet to send */

	/* The IOP takes MacII-style packets, so strip the initial

	 * ADB_PACKET byte.

	/* amsg.data immediately follows amsg.cmd, effectively making

	 * &amsg.cmd a pointer to the beginning of a full ADB packet.

	/* Now send it. The IOP manager will call adb_iop_complete

	 * when the message has been sent.

 Command = 0, Address = ignored */

 Don't want any more requests during the Global Reset low time. */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

/*

 * RackMac vu-meter driver

 *

 * (c) Copyright 2006 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 *

 * Support the CPU-meter LEDs of the Xserve G5

 *

 * TODO: Implement PWM to do variable intensity and provide userland

 * interface for fun. Also, the CPU-meter could be made nicer by being

 * a bit less "immediate" but giving instead a more average load over

 * time. Patches welcome :-)

 Number of samples in a sample buffer */

 CPU meter sampling rate in ms */

 To be set as a tunable */

 This GPIO is whacked by the OS X driver when initializing */

/* This is copied from cpufreq_ondemand, maybe we should put it in

 * a common header somewhere

 First whack magic GPIO */

	/* Call feature code to enable the sound channel and the proper

	 * clock sources

	/* Power i2s and stop i2s clock. We whack MacIO FCRs directly for now.

	 * This is a bit racy, thus we should add new platform functions to

	 * handle that. snd-aoa needs that too

	/* Then setup i2s. For now, we use the same magic value that

	 * the OS X driver seems to use. We might want to play around

	 * with the clock divisors later

 Fully restart i2s*/

 Make sure dbdma is reset */

 Prepare 4 dbdma commands for the 2 buffers */

	/* We do a very dumb calculation to update the LEDs for now,

	 * we'll do better once we have actual PWM implemented

 Now check if LEDs are all 0, we can stop DMA */

	/* This driver works only with 1 or 2 CPUs numbered 0 and 1,

	 * but that's really all we have on Apple Xserve. It doesn't

	 * play very nice with CPU hotplug neither but we don't do that

	 * on those machines yet

  XXX FIXME: No PWM yet, this is 0/1 */

	/* Flush PCI buffers with an MMIO read. Maybe we could actually

	 * check the status one day ... in case things go wrong, though

	 * this never happened to me

 Make sure the CPU gets us in order */

 Read mark */

 We allow for 3 errors like that (stale DBDMA irqs) */

 Next buffer we need to fill is mark value */

	/* Fill it now. This routine converts the 8 bits depth sample array

	 * into the PWM bitmap for each LED.

 Get i2s-a node */

 Get lightshow or virtual sound */

 Create and initialize our instance data */

 Check resources availability. We need at least resource 0 and 1 */

 Use that when i2s-a is finally an mdev per-se */

 Stop CPU sniffer timer & work queues */

 Clear reference to private data */

 Stop/reset dbdma */

 Release the IRQ */

 Unmap registers */

 Free DMA */

 Free samples */

 Release resources */

 Get rid of me */

 Stop CPU sniffer timer & work queues */

 Stop/reset dbdma */

/*

 * via-pmu event device for reporting some events that come through the PMU

 *

 * Copyright 2006 Johannes Berg <johannes@sipsolutions.net>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or

 * NON INFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

 *

 do other models report button/lid status? */

 no such key handled */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. iMac G5 iSight

 *

 * (c) Copyright 2007 Étienne Bersac <bersace@gmail.com>

 *

 * Bits & pieces from windfarm_pm81.c by (c) Copyright 2005 Benjamin

 * Herrenschmidt, IBM Corp. <benh@kernel.crashing.org>

 *

 * PowerMac12,1

 * ============

 *

 * The algorithm used is the PID control algorithm, used the same way

 * the published Darwin code does, using the same values that are

 * present in the Darwin 8.10 snapshot property lists (note however

 * that none of the code has been re-used, it's a complete

 * re-implementation

 *

 * There is two models using PowerMac12,1. Model 2 is iMac G5 iSight

 * 17" while Model 3 is iMac G5 20". They do have both the same

 * controls with a tiny difference. The control-ids of hard-drive-fan

 * and cpu-fan is swapped.

 *

 * Target Correction :

 *

 * controls have a target correction calculated as :

 *

 * new_min = ((((average_power * slope) >> 16) + offset) >> 16) + min_value

 * new_value = max(new_value, max(new_min, 0))

 *

 * OD Fan control correction.

 *

 * # model_id: 2

 *   offset		: -19563152

 *   slope		:  1956315

 *

 * # model_id: 3

 *   offset		: -15650652

 *   slope		:  1565065

 *

 * HD Fan control correction.

 *

 * # model_id: 2

 *   offset		: -15650652

 *   slope		:  1565065

 *

 * # model_id: 3

 *   offset		: -19563152

 *   slope		:  1956315

 *

 * CPU Fan control correction.

 *

 * # model_id: 2

 *   offset		: -25431900

 *   slope		:  2543190

 *

 * # model_id: 3

 *   offset		: -15650652

 *   slope		:  1565065

 *

 * Target rubber-banding :

 *

 * Some controls have a target correction which depends on another

 * control value. The correction is computed in the following way :

 *

 * new_min = ref_value * slope + offset

 *

 * ref_value is the value of the reference control. If new_min is

 * greater than 0, then we correct the target value using :

 *

 * new_target = max (new_target, new_min >> 16)

 *

 * # model_id : 2

 *   control	: cpu-fan

 *   ref	: optical-drive-fan

 *   offset	: -15650652

 *   slope	: 1565065

 *

 * # model_id : 3

 *   control	: optical-drive-fan

 *   ref	: hard-drive-fan

 *   offset	: -32768000

 *   slope	: 65536

 *

 * In order to have the moste efficient correction with those

 * dependencies, we must trigger HD loop before OD loop before CPU

 * loop.

 *

 * The various control loops found in Darwin config file are:

 *

 * HD Fan control loop.

 *

 * # model_id: 2

 *   control        : hard-drive-fan

 *   sensor         : hard-drive-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x002D70A3

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x370000

 *                    Interval = 5s

 *

 * # model_id: 3

 *   control        : hard-drive-fan

 *   sensor         : hard-drive-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x002170A3

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x370000

 *                    Interval = 5s

 *

 * OD Fan control loop.

 *

 * # model_id: 2

 *   control        : optical-drive-fan

 *   sensor         : optical-drive-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x001FAE14

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x320000

 *                    Interval = 5s

 *

 * # model_id: 3

 *   control        : optical-drive-fan

 *   sensor         : optical-drive-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x001FAE14

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x320000

 *                    Interval = 5s

 *

 * GPU Fan control loop.

 *

 * # model_id: 2

 *   control        : hard-drive-fan

 *   sensor         : gpu-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x002A6666

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x5A0000

 *                    Interval = 5s

 *

 * # model_id: 3

 *   control        : cpu-fan

 *   sensor         : gpu-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x0010CCCC

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x500000

 *                    Interval = 5s

 *

 * KODIAK (aka northbridge) Fan control loop.

 *

 * # model_id: 2

 *   control        : optical-drive-fan

 *   sensor         : north-bridge-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x003BD70A

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x550000

 *                    Interval = 5s

 *

 * # model_id: 3

 *   control        : hard-drive-fan

 *   sensor         : north-bridge-temp

 *   PID params     : G_d = 0x00000000

 *                    G_p = 0x0030F5C2

 *                    G_r = 0x00019999

 *                    History = 2 entries

 *                    Input target = 0x550000

 *                    Interval = 5s

 *

 * CPU Fan control loop.

 *

 *   control        : cpu-fan

 *   sensors        : cpu-temp, cpu-power

 *   PID params     : from SDB partition

 *

 * CPU Slew control loop.

 *

 *   control        : cpufreq-clamp

 *   sensor         : cpu-temp

 machine model id */

 Controls & sensors */

 unused ! */

 Set to kick the control loop into life */

/* All sys loops. Note the HD before the OD loop in order to have it

	LOOP_GPU,		/* control = hd or cpu, but luckily,

 control = hd */

 control = hd or od */

 control = od */

 FAN_OD */

 MODEL 2 */

 MODEL 3 */

 FAN_HD */

 MODEL 2 */

 MODEL 3 */

 FAN_CPU */

 MODEL 2 */

 MODEL 3 */

 CPUFREQ has no correction (and is not implemented at all) */

 MODEL 2 */

 MODEL 3 */

 pointer to the current model connection */

/*

 * ****** System Fans Control Loop ******

 *

/* Since each loop handles only one control and we want to avoid

 * writing virtual control, we store the control correction with the

 * loop params. Some data are not set, there are common to all loop

 * and thus, hardcoded.

 purely informative since we use mach_model-2 as index */

 use sensor_id instead ? */

 GPU Fan control loop */

 HD Fan control loop */

 KODIAK Fan control loop */

 OD Fan control loop */

 the hardcoded values */

/* State data used by the system fans control loop

/*

 * ****** CPU Fans Control Loop ******

 *

/* State data used by the cpu fans control loop

/*

 * ***** Implementation *****

 *

 correction the value using the output-low-bound correction algo */

 no connection */

 FAN LOOPS */

 First, locate the params for this model */

 No params found, put fans to max */

 Alloc & initialize state */

 Fill PID params */

		/*

		 * This is probably not the right!?

		 * Perhaps goto fail  if control == NULL  above?

	/* note that this is not optimal since another loop may still

 correction */

 linked corretion */

 CPU LOOP */

 First, locate the PID params in SMU SBD */

	/* Get the FVT params for operating point 0 (the only supported one

	 * for now) in order to get tmax

 94 degree default */

 Alloc & initialize state */

 Fill PID params */

 correction */

 connected correction */

/*

 * ****** Common ******

 *

 skipping ticks */

 compute average power */

	/* If entering failure mode, clamp cpufreq and ramp all

	 * fans to full speed.

	/* If leaving failure mode, unclamp cpufreq and readjust

	 * all fans on next iteration

	/* Overtemp condition detected, notify and start skipping a couple

	 * ticks to let the temperature go down

	/* We only clear the overtemp condition if overtemp is cleared

	 * _and_ no other failure is present. Since a sensor error will

	 * clear the overtemp condition (can't measure temperature) at

	 * the control loop levels, but we don't want to keep it clear

	 * here in this case

 SPDX-License-Identifier: GPL-2.0

/*

 * Backlight code for via-pmu

 *

 * Copyright (C) 1998 Paul Mackerras and Fabio Riccardi.

 * Copyright (C) 2001-2002 Benjamin Herrenschmidt

 * Copyright (C) 2006      Michael Hanselmann <linux-kernel@hansmi.ch>

 *

 Look for biggest value */

 Look for nearest value */

 Get and convert the value */

 Don't update brightness when sleeping */

 CONFIG_PM */

 Special case for the old PowerBook since I can't test on it */

 read autosaved value if available */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. SMU based sensors

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

/*

 * Various SMU "partitions" calibration objects for which we

 * keep pointers here for use by bits & pieces of the driver

/*

 * SMU basic sensors objects

 index in SMU */

 Ok, we have to scale & adjust, taking units into account */

 Ok, we have to scale & adjust, taking units into account */

 Ok, we have to scale & adjust, taking units into account */

 Ok, we have to scale & adjust, taking units into account */

	/* We currently pick the sensors based on the OF name and location

	 * properties, while Darwin uses the sensor-id's.

	 * The problem with the IDs is that they are model specific while it

	 * looks like apple has been doing a reasonably good job at keeping

	 * the names and locations consistents so I'll stick with the names

	 * and locations for now.

/*

 * SMU Power combo sensor object

 Some early machines need a faked voltage */

	/* Try to use quadratic transforms on PowerMac8,1 and 9,1 for now,

	 * I yet have to figure out what's up with 8,2 and will have to

	 * adjust for later, unless we can 100% trust the SDB partition...

 Get CPU voltage/current/power calibration data */

 Keep version around */

 Get CPU diode calibration data */

 Get slots power calibration data if any */

 Get debug switches if any */

 Get parameters partitions */

 Look for sensors subdir */

 Create basic sensors */

 keep track of cpu voltage & current */

 Create CPU power sensor if possible */

 dispose of power sensor */

 dispose of basic sensors */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. SMU based controls

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

/*

 * SMU fans control object

 0 = rpm, 1 = pwm */

 index in SMU */

 current value */

 min/max values */

 Fill SMU command structure */

	/* The SMU has an "old" and a "new" way of setting the fan speed

	 * Unfortunately, I found no reliable way to know which one works

	 * on a given machine model. After some investigations it appears

	 * that MacOS X just tries the new one, and if it fails fallbacks

	 * to the old ones ... Ugh.

 Fill argument buffer */

 Handle fallback (see comment above) */

 todo: read from SMU */

	/* We use the name & location here the same way we do for SMU sensors,

	 * see the comment in windfarm_smu_sensors.c. The locations are a bit

	 * less consistent here between the iMac and the desktop models, but

	 * that is good enough for our needs for now at least.

	 *

	 * One problem though is that Apple seem to be inconsistent with case

	 * and the kernel doesn't have strcasecmp =P

 Names used on desktop models */

 Names used on iMac models */

 seen on iMac G5 iSight */

 same */

 Unrecognized fan, bail out */

 Get min & max values*/

 Get "reg" value */

 Look for RPM fans */

 Look for PWM fans */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver for the Apple Desktop Bus

 * and the /dev/adb device on macintoshes.

 *

 * Copyright (C) 1996 Paul Mackerras.

 *

 * Modified to declare controllers as structures, added

 * client notification of bus reset and handles PowerBook

 * sleep, by Benjamin Herrenschmidt.

 *

 * To do:

 *

 * - /sys/bus/adb to list the devices and infos

 * - more /dev/adb to allow userland to receive the

 *   flow of auto-polling datas from a given device.

 * - move bus probe to a kernel thread

/*

 * The adb_handler_mutex mutex protects all accesses to the original_address

 * and handler_id fields of adb_handler[i] for all i, and changes to the

 * handler field.

 * Accesses to the handler field are protected by the adb_handler_lock

 * rwlock.  It is held across all calls to any handler, so that by the

 * time adb_unregister returns, we know that the old handler isn't being

 * called.

 assumes adb_handler[] is all zeroes at this point */

 see if there is anything at address i */

 one or more devices at this address */

 Note we reset noMovement to 0 each time we move a device */

			/*

			 * Send a "talk register 3" command to address i

			 * to provoke a collision if there is more than

			 * one device at this address.

			/*

			 * Move the device(s) which didn't detect a

			 * collision to address `highFree'.  Hopefully

			 * this only moves one device.

			/*

			 * See if anybody actually moved. This is suggested

			 * by HW TechNote 01:

			 *

			 * https://developer.apple.com/technotes/hw/hw_01.html

			/*

			 * Test whether there are any device(s) left

			 * at address i.

				/*

				 * There are still one or more devices

				 * left at address i.  Register the one(s)

				 * we moved to `highFree', and find a new

				 * value for highFree.

				/*

				 * No devices left at address i; move the

				 * one(s) we moved to `highFree' back to i.

 Now fill in the handler_id field of the adb_handler entries. */

/*

 * This kernel task handles ADB probing. It dies once probing is

 * completed.

/*

 * notify clients before sleep

 We need to get a lock on the probe thread */

 Stop autopoll */

/*

 * reset bus after sleep

 CONFIG_PM */

 xmon may do early-init */

 CONFIG_PPC */

 Let the trackpad settle down */

 That one is still a bit synchronous, oh well... */

 Let the trackpad settle down */

 Synchronous requests block using an on-stack completion */

 /* Ultimately this should return the number of devices with

    the given default id.

    And it does it now ! Note: changed behaviour: This function

    will now register if default_id _and_ handler_id both match

    but handler_id can be left to 0 to match with default_id only.

    When handler_id is set, this function will try to adjust

	/* We skip keystrokes and mouse moves when the sleep process

	 * has been started. We stop autopoll, but this is another security

 Try to change handler to new_id. Will return 1 if successful. */

/*

 * /dev/adb device driver.

 major number for /dev/adb */

, i*/;

 If a probe is in progress or we are sleeping, wait for it to complete */

 Queries are special requests sent to the ADB driver itself */

	/* Special case for ADB_BUSRESET request, all others are sent to

 Hibernate hooks */

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for the ADB controller in the Mac I/O (Hydra) chip.

 Bits in intr and intr_enb registers */

 data from bus */

 transfer access grant */

 Bits in dcount register */

 how many bytes */

 auto-poll data */

 Bits in error register */

 no response error */

 data lost error */

 Bits in ctrl register */

 transfer access request */

 data to bus */

 command response expected */

 ADB reset */

 Bits in autopoll register */

 autopoll enable */

 for now, set all devices active */

	/* Hrm... we may want to not lock interrupts for so

	 * long ... oh well, who uses that chip anyway ? :)

	 * That function will be seldom used during boot

	 * on rare machines, so...

 Send an ADB command */

 put the current request in */

 this is the response to a command */

 autopoll data */

	    /* Here, we assume that if the request has a done member, the

    	     * struct request will survive to setting req->complete to 1

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control.

 * Control loops for PowerMac7,2 and 7,3

 *

 * Copyright (C) 2012 Benjamin Herrenschmidt, IBM Corp.

/* define this to force CPU overtemp to 60 degree, useful for testing

 * the overtemp code

 We currently only handle 2 chips */

 Controls and sensors */

 We keep a temperature history for average calculation of 180s */

 Fixed speed for slot fan */

 Scale value for CPU intake fans */

 PID loop state */

 Overtemp values */

	/* We max all CPU fans in case of a sensor error. We also do the

	 * cpufreq clamping now, even if it's supposedly done later by the

	 * generic code anyway, we do it earlier here to react faster

 First check for immediate overtemps */

	/*

	 * The first time around, initialize the array with the first

	 * temperature reading

	/*

	 * We calculate a history of max temperatures and use that for the

	 * overtemp management

 Now check for average overtemps */

	/* Now handle overtemp conditions. We don't currently use the windfarm

	 * overtemp handling core as it's not fully suited to the needs of those

	 * new machine. This will be fixed later.

 High overtemp -> immediate shutdown */

 Get diode temperature */

 Get voltage */

 Get current */

 Calculate power */

	/* Scale voltage and current raw sensor values according to fixed scales

	 * obtained in Darwin and calculate power from I and V

 Read current speed */

 Keep track of highest temp */

 Handle possible overtemps */

 Run PID */

 Apply result directly to exhaust fan */

 Scale result for intake fan */

 Read current speed from cpu 0 */

 Read values for both CPUs */

 Keep track of highest temp */

 Handle possible overtemps */

 Use the max temp & power of both */

 Run PID */

 Scale result for intake fan */

 Same deal with pump speed */

 Implementation... */

 Get PID params from the appropriate MPU EEPROM */

 We keep a global tmax for overtemp calculations */

 Set PID min/max by using the rear fan min/max */

 History size */

 Initialize PID loop */

 seconds */

 Backside/U3 fan */

 Update fan speed from actual fans */

 first time initialize things */

 conservative by default */

 Drive bay fan */

 Update fan speed from actual fans */

 first time initialize things */

		/*

		 * We don't have the right stuff to drive the PCI fan

		 * so we fix it to a default value

 Permanent failure, bail out */

	/*

	 * Clear all failure bits except low overtemp which will be eventually

	 * cleared by the control loop itself

 Check for failures. Any failure causes cpufreq clamping */

	/* That's it for now, we might want to deal with other failures

	 * differently in the future though

 should release all sensors and controls */

 Count the number of CPU cores */

 Get MPU data for each CPU */

 MODULE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Device driver for the i2c thermostat found on the iBook G4, Albook G4

 *

 * Copyright (C) 2003, 2004 Colin Leroy, Rasmus Rohde, Benjamin Herrenschmidt

 *

 * Documentation from 115254175ADT7467_pra.pdf and 3686221171167ADT7460_b.pdf

 * https://www.onsemi.com/PowerSolutions/product.do?id=ADT7467

 * https://www.onsemi.com/PowerSolutions/product.do?id=ADT7460

 *

 local, sensor1, sensor2 */

 local, sensor1, sensor2 */

 local, sensor1, sensor2 */

 local, sensor1, sensor2 */

 should start with low byte */

 "a value of 0xffff means that the fan has stopped" */

 back to automatic */

 last variation, for iBook */

 we don't care about local sensor, so we start at sensor 1 */

			/* hysteresis : change fan speed only if variation is

			/* don't stop fan if sensor2 is cold and sensor1 is not

			return; /* we don't want to re-stop the fan

 Set sensor1 limit higher to avoid powerdowns */

 set our limits to normal */

	/* To maintain ABI compatibility with userspace, create

	 * the old style platform driver and attach the attributes

	 * to it here

 force manual control to start the fan quieter */

 The 7460 needs to be started explicitly */

 record invert bit status because fw can corrupt it after suspend */

 be sure to really write fan speed the first time */

 manual mode, stop fans */

 automatic mode */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * APM emulation for PMU-based machines

 *

 * Copyright 2001 Benjamin Herrenschmidt (benh@kernel.crashing.org)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. Core

 *

 * (c) Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 *

 * This core code tracks the list of sensors & controls, register

 * clients, and holds the kernel thread used for control.

 *

 * TODO:

 *

 * Add some information about sensor/control type and data format to

 * sensors/controls, and have the sysfs attribute stuff be moved

 * generically here instead of hard coded in the platform specific

 * driver as it us currently

 *

 * This however requires solving some annoying lifetime issues with

 * sysfs which doesn't seem to have lifetime rules for struct attribute,

 * I may have to create full features kobjects for every sensor/control

 * instead which is a bit of an overkill imho

/*

 * Utilities & tick thread

 10 seconds overtemp, notify userland */

 30 seconds, shutdown */

/*

 * Controls

 This is really only for debugging... */

 the subsystem still does useful work without the file */

/*

 * Sensors

 the subsystem still does useful work without the file */

/*

 * Client & notification

 SPDX-License-Identifier: GPL-2.0

/*

 * drivers/macintosh/adbhid.c

 *

 * ADB HID driver for Power Macintosh computers.

 *

 * Adapted from drivers/macintosh/mac_keyb.c by Franz Sirl.

 * drivers/macintosh/mac_keyb.c was Copyright (C) 1996 Paul Mackerras

 * with considerable contributions from Ben Herrenschmidt and others.

 *

 * Copyright (C) 2000 Franz Sirl.

 *

 * Adapted to ADB changes and support for more devices by

 * Benjamin Herrenschmidt. Adapted from code in MkLinux

 * and reworked.

 * 

 * Supported devices:

 *

 * - Standard 1 button mouse

 * - All standard Apple Extended protocol (handler ID 4)

 * - mouseman and trackman mice & trackballs 

 * - PowerBook Trackpad (default setup: enable tapping)

 * - MicroSpeed mouse & trackball (needs testing)

 * - CH Products Trackball Pro (needs testing)

 * - Contour Design (Contour Mouse)

 * - Hunter digital (NoHandsMouse)

 * - Kensignton TurboMouse 5 (needs testing)

 * - Mouse Systems A3 mice and trackballs <aidan@kublai.com>

 * - MacAlly 2-buttons mouse (needs testing) <pochini@denise.shiny.it>

 *

 * To do:

 *

 * Improve Kensington support.

 * Split mouse/kbd

 * Move to syfs

 register # for key up/down data */

 register # for leds on ADB keyboard */

 reg# for movement/button codes from mouse */

 Some special keys */

 0x00 */ KEY_A, 		
 0x01 */ KEY_S, 		
 0x02 */ KEY_D,		
 0x03 */ KEY_F,		
 0x04 */ KEY_H,		
 0x05 */ KEY_G,		
 0x06 */ KEY_Z,		
 0x07 */ KEY_X,		
 0x08 */ KEY_C,		
 0x09 */ KEY_V,		
 0x0a */ KEY_102ND,		
 0x0b */ KEY_B,		
 0x0c */ KEY_Q,		
 0x0d */ KEY_W,		
 0x0e */ KEY_E,		
 0x0f */ KEY_R,		
 0x10 */ KEY_Y,		
 0x11 */ KEY_T,		
 0x12 */ KEY_1,		
 0x13 */ KEY_2,		
 0x14 */ KEY_3,		
 0x15 */ KEY_4,		
 0x16 */ KEY_6,		
 0x17 */ KEY_5,		
 0x18 */ KEY_EQUAL,		
 0x19 */ KEY_9,		
 0x1a */ KEY_7,		
 0x1b */ KEY_MINUS,		
 0x1c */ KEY_8,		
 0x1d */ KEY_0,		
 0x1e */ KEY_RIGHTBRACE,	
 0x1f */ KEY_O,		
 0x20 */ KEY_U,		
 0x21 */ KEY_LEFTBRACE,	
 0x22 */ KEY_I,		
 0x23 */ KEY_P,		
 0x24 */ KEY_ENTER,		
 0x25 */ KEY_L,		
 0x26 */ KEY_J,		
 0x27 */ KEY_APOSTROPHE,	
 0x28 */ KEY_K,		
 0x29 */ KEY_SEMICOLON,	
 0x2a */ KEY_BACKSLASH,	
 0x2b */ KEY_COMMA,		
 0x2c */ KEY_SLASH,		
 0x2d */ KEY_N,		
 0x2e */ KEY_M,		
 0x2f */ KEY_DOT,		
 0x30 */ KEY_TAB,		
 0x31 */ KEY_SPACE,		
 0x32 */ KEY_GRAVE,		
 0x33 */ KEY_BACKSPACE,	
 0x34 */ KEY_KPENTER,		
 0x35 */ KEY_ESC,		
 0x36 */ KEY_LEFTCTRL,	
 0x37 */ KEY_LEFTMETA,	
 0x38 */ KEY_LEFTSHIFT,	
 0x39 */ KEY_CAPSLOCK,	
 0x3a */ KEY_LEFTALT,		
 0x3b */ KEY_LEFT,		
 0x3c */ KEY_RIGHT,		
 0x3d */ KEY_DOWN,		
 0x3e */ KEY_UP,		
 0x3f */ KEY_FN,		
 0x40 */ 0,

 0x41 */ KEY_KPDOT,		
 0x42 */ 0,

 0x43 */ KEY_KPASTERISK,	
 0x44 */ 0,

 0x45 */ KEY_KPPLUS,		
 0x46 */ 0,

 0x47 */ KEY_NUMLOCK,		
 0x48 */ 0,

 0x49 */ 0,

 0x4a */ 0,

 0x4b */ KEY_KPSLASH,		
 0x4c */ KEY_KPENTER,		
 0x4d */ 0,

 0x4e */ KEY_KPMINUS,		
 0x4f */ 0,

 0x50 */ 0,

 0x51 */ KEY_KPEQUAL,		
 0x52 */ KEY_KP0,		
 0x53 */ KEY_KP1,		
 0x54 */ KEY_KP2,		
 0x55 */ KEY_KP3,		
 0x56 */ KEY_KP4,		
 0x57 */ KEY_KP5,		
 0x58 */ KEY_KP6,		
 0x59 */ KEY_KP7,		
 0x5a */ 0,

 0x5b */ KEY_KP8,		
 0x5c */ KEY_KP9,		
 0x5d */ KEY_YEN,		
 0x5e */ KEY_RO,		
 0x5f */ KEY_KPCOMMA,		
 0x60 */ KEY_F5,		
 0x61 */ KEY_F6,		
 0x62 */ KEY_F7,		
 0x63 */ KEY_F3,		
 0x64 */ KEY_F8,		
 0x65 */ KEY_F9,		
 0x66 */ KEY_HANJA,		
 0x67 */ KEY_F11,		
 0x68 */ KEY_HANGEUL,		
 0x69 */ KEY_SYSRQ,		
 0x6a */ 0,

 0x6b */ KEY_SCROLLLOCK,	
 0x6c */ 0,

 0x6d */ KEY_F10,		
 0x6e */ KEY_COMPOSE,		
 0x6f */ KEY_F12,		
 0x70 */ 0,

 0x71 */ KEY_PAUSE,		
 0x72 */ KEY_INSERT,		
 0x73 */ KEY_HOME,		
 0x74 */ KEY_PAGEUP,		
 0x75 */ KEY_DELETE,		
 0x76 */ KEY_F4,		
 0x77 */ KEY_END,		
 0x78 */ KEY_F2,		
 0x79 */ KEY_PAGEDOWN,	
 0x7a */ KEY_F1,		
 0x7b */ KEY_RIGHTSHIFT,	
 0x7c */ KEY_RIGHTALT,	
 0x7d */ KEY_RIGHTCTRL,	
 0x7e */ KEY_RIGHTMETA,	
 0x7f */ KEY_POWER,		
 Kind of keyboard, see Apple technote 1152  */

 Kind of mouse  */

 Standard 100cpi mouse (handler 1) */

 Standard 200cpi mouse (handler 2) */

 Apple Extended mouse (handler 4) */

 TrackBall (handler 4) */

 Apple's PowerBook trackpad (handler 4) */

 Turbomouse 5 (previously req. mousehack) */

 Microspeed mouse (&trackball ?), MacPoint */

 Trackball Pro (special buttons) */

 Mouse systems A3 trackball (handler 3) */

 MacAlly 2-button mouse */

 first check this is from register 0 */

 ignore it */

			/* Key pressed, turning on the CapsLock LED.

				/* Throw away this key event if it happens

			/* Scancode 0xff usually signifies that the capslock

			 * key was either pressed or released, or that the

 Key released */

 Key pressed */

 Generate down/up events for CapsLock every time. */

 Power key on PBook 3400 needs remapping */

 Keep track of the power key state */

 Fn + Command will produce a bogus "power" keycode */

 Keep track of the Fn key state */

 Emulate Fn+delete = forward delete */

 Emulate Fn+delete = forward delete */

 CONFIG_PPC_PMAC */

  /*

    Handler 1 -- 100cpi original Apple mouse protocol.

    Handler 2 -- 200cpi original Apple mouse protocol.



    For Apple's standard one-button mouse protocol the data array will

    contain the following values:



                BITS    COMMENTS

    data[0] = dddd 1100 ADB command: Talk, register 0, for device dddd.

    data[1] = bxxx xxxx First button and x-axis motion.

    data[2] = byyy yyyy Second button and y-axis motion.



    Handler 4 -- Apple Extended mouse protocol.



    For Apple's 3-button mouse protocol the data array will contain the

    following values:



		BITS    COMMENTS

    data[0] = dddd 1100 ADB command: Talk, register 0, for device dddd.

    data[1] = bxxx xxxx Left button and x-axis motion.

    data[2] = byyy yyyy Second button and y-axis motion.

    data[3] = byyy bxxx Third button and fourth button.  Y is additional

	      high bits of y-axis motion.  XY is additional

	      high bits of x-axis motion.



    MacAlly 2-button mouse protocol.



    For MacAlly 2-button mouse protocol the data array will contain the

    following values:



		BITS    COMMENTS

    data[0] = dddd 1100 ADB command: Talk, register 0, for device dddd.

    data[1] = bxxx xxxx Left button and x-axis motion.

    data[2] = byyy yyyy Right button and y-axis motion.

    data[3] = ???? ???? unknown

    data[4] = ???? ???? unknown



	/* If it's a trackpad, we alias the second button to the first.

	   NOTE: Apple sends an ADB flush command to the trackpad when

	         the first (the real) button is released. We could do

		 this here using async flush requests.

 Right button is mapped as button 3 */

 Adjustable keyboard button device */

 microphone */

 mute */

 volume decrease */

 volume increase */

 Powerbook button device */

		/*

		 * XXX: Where is the contrast control for the passive?

		 *  -- Cort

 mute */

 volume decrease */

 volume increase */

 eject */

 brightness decrease */

 brightness increase */

 videomode switch */

 keyboard illumination toggle */

 keyboard illumination decrease */

 keyboard illumination increase */

 keypad overlay toogle */

/*

 * Event callback from the input module. Events that change the state of

 * the hardware are processed here.

 Stop the repeat timer. Autopoll is already off at this point */

 Stop pending led requests */

		/* After resume, and if the capslock LED is on, the PMU will

		 * send a "capslock down" key event. This confuses the

		 * restore_capslock_events logic. Remember if the capslock

		 * LED was on before suspend so the unwanted key event can

 Adjustable keyboard button device */

 Powerbook button device */

		/* HACK WARNING!! This should go away as soon there is an utility

		 * to control that for event devices.

 input layer default: 250 */

 input layer default: 33 */

 turn off all leds */

		/* Enable full feature set of the keyboard

		   ->get it to send separate codes for left and right shift,

 handler 5 doesn't send separate codes for R modifiers */

	/* Try to switch all mice to handler 4, or 2 for three-button

			/*

			 * Register 1 is usually used for device

			 * identification.  Here, we try to identify

			 * a known device and call the appropriate

			 * init function.

r1_buffer[6],*/

 Without this flush, the trackpad may be locked up */

	/* This will initialize mice using the Microspeed, MacPoint and

	   other compatible firmware. Bit 12 enables extended protocol.

	   

	   Register 1 Listen (4 Bytes)

            0 -  3     Button is mouse (set also for double clicking!!!)

            4 -  7     Button is locking (affects change speed also)

            8 - 11     Button changes speed

           12          1 = Extended mouse mode, 0 = normal mouse mode

           13 - 15     unused 0

           16 - 23     normal speed

           24 - 31     changed speed



       Register 1 talk holds version and product identification information.

       Register 1 Talk (4 Bytes):

            0 -  7     Product code

            8 - 23     undefined, reserved

           24 - 31     Version number

        

       Speed 0 is max. 1 to 255 set speed in increments of 1/256 of max.

 alt speed = 0x20 (rather slow) */

 norm speed = 0x00 (fastest) */

 extended protocol, no speed change */

 all buttons enabled as mouse buttons, no locking */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. AD7417 sensors

 *

 * Copyright 2012 Benjamin Herrenschmidt, IBM Corp.

 Read temp register */

 Read a a 16-bit signed value */

 Convert 8.8-bit to 16.16 fixed point */

/*

 * Scaling factors for the AD7417 ADC converters (except

 * for the CPU diode which is obtained from the EEPROM).

 * Those values are obtained from the property list of

 * the darwin driver

 _AD2 */

 _AD3 */

 _AD4 */

 Diode */

 12v current */

 core voltage */

 core current */

 Set channel */

 Wait for conversion */

 Switch to data register */

 Read result */

 Read a a 16-bit signed value */

	/*

	 * Read ADC the configuration register and cache it. We

	 * also make sure Config2 contains proper values, I've seen

	 * cases where we got stale grabage in there, thus preventing

	 * proper reading of conv. values

 Clear Config2 */

 Read & cache Config1 */

 Disable shutdown mode */

	/*

	 * Identify which CPU we belong to by looking at the first entry

	 * in the hwsensor-location list

 Initialize the chip */

	/*

	 * We cannot rely on Apple device-tree giving us child

	 * node with the names of the individual sensors so we

	 * just hard code what we know about them

 Mark client detached */

 Release sensor */

 This is only supported on these machines */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PowerMac G5 SMU driver

 *

 * Copyright 2004 J. Mayer <l_indien@magic.fr>

 * Copyright 2005 Benjamin Herrenschmidt, IBM Corp.

/*

 * TODO:

 *  - maybe add timeout to commands ?

 *  - blocking version of time functions

 *  - polling version of i2c commands (including timer that works with

 *    interrupts off)

 *  - maybe avoid some data copies with i2c by directly using the smu cmd

 *    buffer and a lower level internal interface

 *  - understand SMU -> CPU events and implement reception of them via

 *    the userland interface

/*

 * This is the command buffer passed to the SMU hardware

 doorbell gpio */

 doorbell buffer */

 command buffer virtual */

 command buffer absolute */

 pending command */

 pending i2c command */

/*

 * I don't think there will ever be more than one SMU, so

 * for now, just hard code that

/*

 * SMU driver low level stuff

 Fetch first command in queue */

 Fill the SMU command buffer */

 Flush command and data to RAM */

	/* We also disable NAP mode for the duration of the command

	 * on U3 based machines.

	 * This is slightly racy as it can be written back to 1 by a sysctl

	 * but that never happens in practice. There seem to be an issue with

	 * U3 based machines such as the iMac G5 where napping for the

	 * whole duration of the command prevents the SMU from fetching it

	 * from memory. This might be related to the strange i2c based

	 * mechanism the SMU uses to access memory.

	/* This isn't exactly a DMA mapping here, I suspect

	 * the SMU is actually communicating with us via i2c to the

	 * northbridge or the CPU to access RAM.

 Ring the SMU doorbell */

	/* SMU completed the command, well, we hope, let's make sure

	 * of it

		/* CPU might have brought back the cache line, so we need

		 * to flush again before peeking at the SMU response. We

		 * flush the entire buffer for now as we haven't read the

		 * reply length (it's only 2 cache lines anyway)

 Now check ack */

	/* Now complete the command. Write status last in order as we lost

	 * ownership of the command structure as soon as it's no longer -1

 Re-enable NAP mode */

 Start next command if any */

 Call command completion handler if any */

 It's an edge interrupt, nothing to do */

	/* I don't quite know what to do with this one, we seem to never

	 * receive it, so I suspect we have to arm it someway in the SMU

	 * to start getting events that way.

 It's an edge interrupt, nothing to do */

/*

 * Queued command management.

 *

 Workaround for early calls when irq isn't available */

 RTC low level commands */

	/*

	 * SMU based G5s need some memory below 2Gb. Thankfully this is

	 * called at a time where memblock is still available.

	/* smu_cmdbuf_abs is in the low 2G of RAM, can be converted to a

	 * 32 bits value safely

	/* Current setup has one doorbell GPIO that does both doorbell

	 * and ack. GPIOs are at 0x50, best would be to find that out

	 * in the device-tree though.

 Now look for the smu-interrupt GPIO */

	/* Doorbell buffer is currently hard-coded, I didn't find a proper

	 * device-tree entry giving the address. Best would probably to use

	 * an offset for K2 base though, but let's do it that way for now.

 U3 has an issue with NAP mode when issuing SMU commands */

	/*

	 * Try to request the interrupts

/* This has to be before arch_initcall as the low i2c stuff relies on the

 * above having been done before we reach arch_initcalls

/*

 * sysfs visibility

	/*

	 * Ok, we are matched, now expose all i2c busses. We have to defer

	 * that unfortunately or it would deadlock inside the device model

	/*

	 * For now, we don't power manage machines with an SMU chip,

	 * I'm a bit too far from figuring out how that works with those

	 * new chipsets, but that will come back and bite us

/*

 * i2c interface

 Check for read case */

	/* Update status and mark no pending i2c command with lock

	 * held so nobody comes in while we dequeue an eventual

	 * pending next i2c command

 Is there another i2c command waiting ? */

 Fetch it, new current, remove from list */

 Queue with low level smu */

 Call command completion handler if any */

 requeue command simply by resetting reply_len */

 Check for possible status */

	/* Handle failures by requeuing command, after 5ms interval

 If failure or stage 1, command is complete */

 Ok, initial command complete, now poll status */

 Fill most fields of scmd */

	/* Check transfer type, sanitize some "info" fields

	 * based on transfer type and do more checking

	/* Finish setting up command based on transfer direction

	/* Enqueue command in i2c list, and if empty, enqueue also in

	 * main command list

/*

 * Handling of "partitions"

	/* We currently use a chunk size of 0xe. We could check the

	 * SMU firmware version and use bigger sizes though

 First query the partition info */

 Partition doesn't exist (or other error) */

 Fetch address and length from reply */

	/* Calucluate total length to allocate, including the 17 bytes

	 * for "sdb-partition-XX" that we append at the end of the buffer

 Read the datablock */

 Got it, check a few things and create the property */

/* Note: Only allowed to return error code in pointers (using ERR_PTR)

 * when interruptible is 1

/*

 * Userland driver interface

 Not implemented */

 Not yet implemented */

 Mark file as closing to avoid races with new request */

 Wait for any pending request to complete */

/* 

 *   Creation Date: <2003/03/14 20:54:13 samuel>

 *   Time-stamp: <2004/03/20 14:20:59 samuel>

 *   

 *	<therm_windtunnel.c>

 *	

 *	The G4 "windtunnel" has a single fan controlled by an

 *	ADM1030 fan controller and a DS1775 thermostat.

 *

 *	The fan controller is equipped with a temperature sensor

 *	which measures the case temperature. The DS1775 sensor

 *	measures the CPU temperature. This driver tunes the

 *	behavior of the fan. It is based upon empirical observations

 *	of the 'AppleFan' driver under Mac OS X.

 *

 *	WARNING: This driver has only been testen on Apple's

 *	1.25 MHz Dual G4 (March 03). It is tuned for a CPU

 *	temperature around 57 C.

 *

 *   Copyright (C) 2003, 2004 Samuel Rydh (samuel@ibrium.se)

 *

 *   Loosely based upon 'thermostat.c' written by Benjamin Herrenschmidt

 *   

 *   This program is free software; you can redistribute it and/or

 *   modify it under the terms of the GNU General Public License

 *   as published by the Free Software Foundation

 *   

 continuously log temperature */

 100% fan at this temp */

 active fan_table setting */

 saved register */

 min fan */

 on fire */

***********************************************************************/

	controller thread						*/

***********************************************************************/

 write_reg( x.fan, 0x24, val, 1 ); */

 this actually occurs when the computer is loaded */

 save registers (if we unload the module) */

 improve measurement resolution (convergence time 1.5s) */

 disable interrupts and TAC input */

 enable filter */

 remote temp. controls fan */

	/* The thermostat (which besides measureing temperature controls

	 * has a THERM output which puts the fan on 100%) is usually

	 * set to kick in at 80 C (chip default). We reduce this a bit

	 * to be on the safe side (OSX doesn't)...

 set an initial fan setting */

 tune_fan( fan_up_table[x.upind].fan_setting ); */

***********************************************************************/

	i2c probing and setup						*/

***********************************************************************/

 scan 0x48-0x4f (DS1775) and 0x2c-2x2f (ADM1030) */

 check that this is an ADM1030 */

 temperature sanity check */

***********************************************************************/

	initialization / cleanup					*/

***********************************************************************/

 We assume Macs have consecutive I2C bus numbers starting at 0 */

 implementation ID */

 number of fans */

 number of thermostats */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control.  SMU "satellite" controller sensors.

 *

 * Copyright (C) 2005 Paul Mackerras, IBM Corp. <paulus@samba.org>

 If the cache is older than 800ms we'll refetch it */

 jiffies when cache last updated */

 used for power sensors */

 TODO: Add the resulting partition to the device-tree */

 refresh the cache */

 4.12 * 8.8 -> 12.20; shift right 4 to get 16.16 */

 the cooked sensors are between 0x30 and 0x37 */

 expect location to be CPU [AB][01] ... */

 hmmm shouldn't happen */

 the +16 is enough for "cpu-voltage-n" */

 make the power sensors */

 release sensors */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver for the via ADB on (many) Mac II-class machines

 *

 * Based on the original ADB keyboard handler Copyright (c) 1997 Alan Cox

 * Also derived from code Copyright (C) 1996 Paul Mackerras.

 *

 * With various updates provided over the years by Michael Schmitz,

 * Guideo Koerber and others.

 *

 * Rewrite for Unified ADB by Joshua M. Thompson (funaho@jurai.org)

 *

 * 1999-08-02 (jmt) - Initial rewrite for Unified ADB.

 * 2000-03-29 Tony Mantler <tonym@mac.linux-m68k.org>

 *            - Big overhaul, should actually work now.

 * 2006-12-31 Finn Thain - Another overhaul.

 *

 * Suggested reading:

 *   Inside Macintosh, ch. 5 ADB Manager

 *   Guide to the Macinstosh Family Hardware, ch. 8 Apple Desktop Bus

 *   Rockwell R6522 VIA datasheet

 *

 * Apple's "ADB Analyzer" bus sniffer is invaluable:

 *   ftp://ftp.apple.com/developer/Tool_Chest/Devices_-_Hardware/Apple_Desktop_Bus/

 VIA registers - spaced 0x200 bytes apart */

 skip between registers */

 B-side data */

 A-side data */

 B-side direction (1=output) */

 A-side direction (1=output) */

 Timer 1 ctr/latch (low 8 bits) */

 Timer 1 counter (high 8 bits) */

 Timer 1 latch (low 8 bits) */

 Timer 1 latch (high 8 bits) */

 Timer 2 ctr/latch (low 8 bits) */

 Timer 2 counter (high 8 bits) */

 Shift register */

 Auxiliary control register */

 Peripheral control register */

 Interrupt flag register */

 Interrupt enable register */

 A-side data, no handshake */

 Bits in B data register: all active low */

 Controller rcv status (input) */

 mask for selecting ADB state bits */

 Bits in ACR */

 Shift register control bits */

 Shift on external clock */

 Shift out if 1 */

 Bits in IFR and IER */

 set bits in IER */

 clear bits in IER */

 Shift register full/empty */

 ADB transaction states according to GMHW */

 ADB state: command byte */

 ADB state: even data byte */

 ADB state: odd data byte */

 ADB state: idle, nothing to send */

 ADB command byte structure */

 first request struct in the queue */

 last request struct in the queue */

 storage for autopolled replies */

 next byte in reply_buf or req->reply */

 store reply in reply_buf else req->reply */

 index of the next byte to send from req->data */

 number of bytes received in reply_buf or req->reply */

 VIA's ADB status bits captured upon interrupt */

 no data was sent by the device */

 have to poll for the device that asserted it */

 the most recent command byte transmitted */

 the most recent Talk command byte transmitted */

 the most recent Talk R0 command byte transmitted */

 bits set are device addresses to poll */

 Check for MacII style ADB */

 Initialize the driver */

 initialize the hardware */

 We want CTLR_IRQ as input and ST_EVEN | ST_ODD as output lines. */

 Set up state: idle */

 Shift register on input */

 Wipe any pending data and int */

 Send an ADB poll (Talk Register 0 command prepended to the request queue) */

	/* This only polls devices in the autopoll list, which assumes that

	 * unprobed devices never assert SRQ. That could happen if a device was

	 * plugged in after the adb bus scan. Unplugging it again will resolve

	 * the problem. This behaviour is similar to MacOS.

	/* The device most recently polled may not be the best device to poll

	 * right now. Some other device(s) may have signalled SRQ (the active

	 * device won't do that). Or the autopoll list may have been changed.

	 * Try polling the next higher address.

 Send a Talk Register 0 command */

	/* No need to repeat this Talk command. The transceiver will do that

	 * as long as it is idle.

 Send an ADB request; if sync, poll out the reply 'till it's done */

 Send an ADB request (append to request queue) */

 Start auto-polling */

 bit 1 == device 1, and so on. */

 Prod the chip without interrupts */

 Reset the bus */

 Command = 0, Address = ignored */

 Don't want any more requests during the Global Reset low time. */

 Start sending ADB packet */

	/* Now send it. Be careful though, that first byte of the request

	 * is actually ADB_PACKET; the real data begins at index 1!

	 * And req->nbytes is the number of bytes of real data plus one.

 Output mode */

 Load data */

 set ADB state to 'command' */

/*

 * The notorious ADB interrupt handler - does all of the protocol handling.

 * Relies on the ADB controller sending and receiving data, thereby

 * generating shift register interrupts (SR_INT) for us. This means there has

 * to be activity on the ADB bus. The chip will poll to achieve this.

 *

 * The VIA Port B output signalling works as follows. After the ADB transceiver

 * sees a transition on the PB4 and PB5 lines it will crank over the VIA shift

 * register which eventually raises the SR_INT interrupt. The PB4/PB5 outputs

 * are toggled with each byte as the ADB transaction progresses.

 *

 * Request with no reply expected (and empty transceiver buffer):

 *     CMD -> IDLE

 * Request with expected reply packet (or with buffered autopoll packet):

 *     CMD -> EVEN -> ODD -> EVEN -> ... -> IDLE

 * Unsolicited packet:

 *     IDLE -> EVEN -> ODD -> EVEN -> ... -> IDLE

 Clear the SR IRQ flag when polling. */

			/* /CTLR_IRQ asserted in idle state means we must

			 * read an autopoll reply from the transceiver buffer.

 bus timeout */

 set ADB state = even for first data byte */

			/* /CTLR_IRQ de-asserted after the command byte means

			 * the host can continue with the transaction.

 Store command byte */

			/* /CTLR_IRQ asserted after the command byte means we

			 * must read an autopoll reply. The first byte was

			 * lost because the shift register was an output.

 reset to shift in */

 just sent the command byte, set to EVEN */

 invert state bits, toggle ODD/EVEN */

 invert state bits, toggle ODD/EVEN */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Windfarm PowerMac thermal control. LM87 sensor

 *

 * Copyright 2012 Benjamin Herrenschmidt, IBM Corp.

 Set address */

 Read temperature register */

	/*

	 * The lm87 contains a whole pile of sensors, additionally,

	 * the Xserve G5 has several lm87's. However, for now we only

	 * care about the internal temperature sensor

 Mark client detached */

 release sensor */

 We only support this on the Xserve */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Apple Motion Sensor driver (PMU variant)

 *

 * Copyright (C) 2006 Michael Hanselmann (linux-kernel@hansmi.ch)

 Attitude */

 Not exactly known, maybe chip vendor */

 Freefall registers */

 Shock registers */

 Global interrupt and power control register */

 Only call this function from task context */

 Only call this function from task context */

 Enables or disables the specified interrupts */

 Disable interrupts */

 Clear interrupts */

 Set implementation stuff */

 Get PMU command, should be 0x4e, but we can never know */

 Disable interrupts */

 Clear interrupts */

 Set default values */

 Clear interrupts */

 Enable interrupts */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Apple Motion Sensor driver (joystick emulation)

 *

 * Copyright (C) 2005 Stelian Pop (stelian@popies.net)

 * Copyright (C) 2006 Michael Hanselmann (linux-kernel@hansmi.ch)

 Call with ams_info.lock held! */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Apple Motion Sensor driver

 *

 * Copyright (C) 2005 Stelian Pop (stelian@popies.net)

 * Copyright (C) 2006 Michael Hanselmann (linux-kernel@hansmi.ch)

 There is only one motion sensor per machine */

 Call with ams_info.lock held! */

 X and Y swapped */

/* Once hard disk parking is implemented in the kernel, this function can

 * trigger it.

 Call with ams_info.lock held! */

 Get orientation */

 Register freefall interrupt handler */

 Reset saved irqs */

 Register shock interrupt handler */

 Create device */

 Create attributes */

 Init input device */

 Found I2C motion sensor */

 Found PMU motion sensor */

 Remove input device */

 Remove attributes */

	/* Flush interrupt worker

	 *

	 * We do this after ams_info.exit(), because an interrupt might

	 * have arrived before disabling them.

 Remove device */

 Remove handler */

 Shut down implementation */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Apple Motion Sensor driver (I2C variant)

 *

 * Copyright (C) 2005 Stelian Pop (stelian@popies.net)

 * Copyright (C) 2006 Michael Hanselmann (linux-kernel@hansmi.ch)

 *

 * Clean room implementation based on the reverse engineered Mac OS X driver by

 * Johannes Berg <johannes@sipsolutions.net>, documentation available at

 * http://johannes.sipsolutions.net/PowerBook/Apple_Motion_Sensor_Specification

 AMS registers */

 command register */

 status register */

 read control 1 (number of values) */

 read control 2 (offset?) */

 read control 3 (size of each value?) */

 read data 1 */

 read data 2 */

 read data 3 */

 read data 4 */

 data X */

 data Y */

 data Z */

 freefall int control */

 shock int control */

 sensitivity low limit */

 sensitivity high limit */

 control X */

 control Y */

 control Z */

 unknown 1 */

 unknown 2 */

 unknown 3 */

 vendor */

 AMS commands - use with the AMS_COMMAND register */

 There can be only one */

 get version/vendor information */

 Disable interrupts */

 Set default values */

 Clear interrupts */

 Enable interrupts */

 Disable interrupts */

 Clear interrupts */

 Set implementation stuff */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * VME Bridge Framework

 *

 * Author: Martyn Welch <martyn.welch@ge.com>

 * Copyright 2008 GE Intelligent Platforms Embedded Systems, Inc.

 *

 * Based on work by Tom Armistead and Ajit Prem

 * Copyright 2004 Motorola Inc.

 Bitmask and list of registered buses both protected by common mutex */

/*

 * Find the bridge that the resource is associated with.

 Get list to search */

/**

 * vme_alloc_consistent - Allocate contiguous memory.

 * @resource: Pointer to VME resource.

 * @size: Size of allocation required.

 * @dma: Pointer to variable to store physical address of allocation.

 *

 * Allocate a contiguous block of memory for use by the driver. This is used to

 * create the buffers for the slave windows.

 *

 * Return: Virtual address of allocation on success, NULL on failure.

/**

 * vme_free_consistent - Free previously allocated memory.

 * @resource: Pointer to VME resource.

 * @size: Size of allocation to free.

 * @vaddr: Virtual address of allocation.

 * @dma: Physical address of allocation.

 *

 * Free previously allocated block of contiguous memory.

/**

 * vme_get_size - Helper function returning size of a VME window

 * @resource: Pointer to VME slave or master resource.

 *

 * Determine the size of the VME window provided. This is a helper

 * function, wrappering the call to vme_master_get or vme_slave_get

 * depending on the type of window resource handed to it.

 *

 * Return: Size of the window on success, zero on failure.

 The VME_A64_MAX limit is actually U64_MAX + 1 */

 User Defined */

/**

 * vme_slave_request - Request a VME slave window resource.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @address: Required VME address space.

 * @cycle: Required VME data transfer cycle type.

 *

 * Request use of a VME window resource capable of being set for the requested

 * address space and data transfer cycle.

 *

 * Return: Pointer to VME resource on success, NULL on failure.

 Loop through slave resources */

 Find an unlocked and compatible image */

 No free image */

 Unlock image */

/**

 * vme_slave_set - Set VME slave window configuration.

 * @resource: Pointer to VME slave resource.

 * @enabled: State to which the window should be configured.

 * @vme_base: Base address for the window.

 * @size: Size of the VME window.

 * @buf_base: Based address of buffer used to provide VME slave window storage.

 * @aspace: VME address space for the VME window.

 * @cycle: VME data transfer cycle type for the VME window.

 *

 * Set configuration for provided VME slave window.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device, if an invalid resource has been provided or invalid

 *         attributes are provided. Hardware specific errors may also be

 *         returned.

/**

 * vme_slave_get - Retrieve VME slave window configuration.

 * @resource: Pointer to VME slave resource.

 * @enabled: Pointer to variable for storing state.

 * @vme_base: Pointer to variable for storing window base address.

 * @size: Pointer to variable for storing window size.

 * @buf_base: Pointer to variable for storing slave buffer base address.

 * @aspace: Pointer to variable for storing VME address space.

 * @cycle: Pointer to variable for storing VME data transfer cycle type.

 *

 * Return configuration for provided VME slave window.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device or if an invalid resource has been provided.

/**

 * vme_slave_free - Free VME slave window

 * @resource: Pointer to VME slave resource.

 *

 * Free the provided slave resource so that it may be reallocated.

 Unlock image */

 Free up resource memory */

/**

 * vme_master_request - Request a VME master window resource.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @address: Required VME address space.

 * @cycle: Required VME data transfer cycle type.

 * @dwidth: Required VME data transfer width.

 *

 * Request use of a VME window resource capable of being set for the requested

 * address space, data transfer cycle and width.

 *

 * Return: Pointer to VME resource on success, NULL on failure.

 Loop through master resources */

 Find an unlocked and compatible image */

 Check to see if we found a resource */

 Unlock image */

/**

 * vme_master_set - Set VME master window configuration.

 * @resource: Pointer to VME master resource.

 * @enabled: State to which the window should be configured.

 * @vme_base: Base address for the window.

 * @size: Size of the VME window.

 * @aspace: VME address space for the VME window.

 * @cycle: VME data transfer cycle type for the VME window.

 * @dwidth: VME data transfer width for the VME window.

 *

 * Set configuration for provided VME master window.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device, if an invalid resource has been provided or invalid

 *         attributes are provided. Hardware specific errors may also be

 *         returned.

/**

 * vme_master_get - Retrieve VME master window configuration.

 * @resource: Pointer to VME master resource.

 * @enabled: Pointer to variable for storing state.

 * @vme_base: Pointer to variable for storing window base address.

 * @size: Pointer to variable for storing window size.

 * @aspace: Pointer to variable for storing VME address space.

 * @cycle: Pointer to variable for storing VME data transfer cycle type.

 * @dwidth: Pointer to variable for storing VME data transfer width.

 *

 * Return configuration for provided VME master window.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device or if an invalid resource has been provided.

/**

 * vme_master_read - Read data from VME space into a buffer.

 * @resource: Pointer to VME master resource.

 * @buf: Pointer to buffer where data should be transferred.

 * @count: Number of bytes to transfer.

 * @offset: Offset into VME master window at which to start transfer.

 *

 * Perform read of count bytes of data from location on VME bus which maps into

 * the VME master window at offset to buf.

 *

 * Return: Number of bytes read, -EINVAL if resource is not a VME master

 *         resource or read operation is not supported. -EFAULT returned if

 *         invalid offset is provided. Hardware specific errors may also be

 *         returned.

/**

 * vme_master_write - Write data out to VME space from a buffer.

 * @resource: Pointer to VME master resource.

 * @buf: Pointer to buffer holding data to transfer.

 * @count: Number of bytes to transfer.

 * @offset: Offset into VME master window at which to start transfer.

 *

 * Perform write of count bytes of data from buf to location on VME bus which

 * maps into the VME master window at offset.

 *

 * Return: Number of bytes written, -EINVAL if resource is not a VME master

 *         resource or write operation is not supported. -EFAULT returned if

 *         invalid offset is provided. Hardware specific errors may also be

 *         returned.

/**

 * vme_master_rmw - Perform read-modify-write cycle.

 * @resource: Pointer to VME master resource.

 * @mask: Bits to be compared and swapped in operation.

 * @compare: Bits to be compared with data read from offset.

 * @swap: Bits to be swapped in data read from offset.

 * @offset: Offset into VME master window at which to perform operation.

 *

 * Perform read-modify-write cycle on provided location:

 * - Location on VME bus is read.

 * - Bits selected by mask are compared with compare.

 * - Where a selected bit matches that in compare and are selected in swap,

 * the bit is swapped.

 * - Result written back to location on VME bus.

 *

 * Return: Bytes written on success, -EINVAL if resource is not a VME master

 *         resource or RMW operation is not supported. Hardware specific

 *         errors may also be returned.

/**

 * vme_master_mmap - Mmap region of VME master window.

 * @resource: Pointer to VME master resource.

 * @vma: Pointer to definition of user mapping.

 *

 * Memory map a region of the VME master window into user space.

 *

 * Return: Zero on success, -EINVAL if resource is not a VME master

 *         resource or -EFAULT if map exceeds window size. Other generic mmap

 *         errors may also be returned.

/**

 * vme_master_free - Free VME master window

 * @resource: Pointer to VME master resource.

 *

 * Free the provided master resource so that it may be reallocated.

 Unlock image */

 Free up resource memory */

/**

 * vme_dma_request - Request a DMA controller.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @route: Required src/destination combination.

 *

 * Request a VME DMA controller with capability to perform transfers bewteen

 * requested source/destination combination.

 *

 * Return: Pointer to VME DMA resource on success, NULL on failure.

 XXX Not checking resource attributes */

 Loop through DMA resources */

 Find an unlocked and compatible controller */

 Check to see if we found a resource */

 Unlock image */

/**

 * vme_new_dma_list - Create new VME DMA list.

 * @resource: Pointer to VME DMA resource.

 *

 * Create a new VME DMA list. It is the responsibility of the user to free

 * the list once it is no longer required with vme_dma_list_free().

 *

 * Return: Pointer to new VME DMA list, NULL on allocation failure or invalid

 *         VME DMA resource.

/**

 * vme_dma_pattern_attribute - Create "Pattern" type VME DMA list attribute.

 * @pattern: Value to use used as pattern

 * @type: Type of pattern to be written.

 *

 * Create VME DMA list attribute for pattern generation. It is the

 * responsibility of the user to free used attributes using

 * vme_dma_free_attribute().

 *

 * Return: Pointer to VME DMA attribute, NULL on failure.

/**

 * vme_dma_pci_attribute - Create "PCI" type VME DMA list attribute.

 * @address: PCI base address for DMA transfer.

 *

 * Create VME DMA list attribute pointing to a location on PCI for DMA

 * transfers. It is the responsibility of the user to free used attributes

 * using vme_dma_free_attribute().

 *

 * Return: Pointer to VME DMA attribute, NULL on failure.

 XXX Run some sanity checks here */

/**

 * vme_dma_vme_attribute - Create "VME" type VME DMA list attribute.

 * @address: VME base address for DMA transfer.

 * @aspace: VME address space to use for DMA transfer.

 * @cycle: VME bus cycle to use for DMA transfer.

 * @dwidth: VME data width to use for DMA transfer.

 *

 * Create VME DMA list attribute pointing to a location on the VME bus for DMA

 * transfers. It is the responsibility of the user to free used attributes

 * using vme_dma_free_attribute().

 *

 * Return: Pointer to VME DMA attribute, NULL on failure.

/**

 * vme_dma_free_attribute - Free DMA list attribute.

 * @attributes: Pointer to DMA list attribute.

 *

 * Free VME DMA list attribute. VME DMA list attributes can be safely freed

 * once vme_dma_list_add() has returned.

/**

 * vme_dma_list_add - Add enty to a VME DMA list.

 * @list: Pointer to VME list.

 * @src: Pointer to DMA list attribute to use as source.

 * @dest: Pointer to DMA list attribute to use as destination.

 * @count: Number of bytes to transfer.

 *

 * Add an entry to the provided VME DMA list. Entry requires pointers to source

 * and destination DMA attributes and a count.

 *

 * Please note, the attributes supported as source and destinations for

 * transfers are hardware dependent.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device or if the link list has already been submitted for execution.

 *         Hardware specific errors also possible.

/**

 * vme_dma_list_exec - Queue a VME DMA list for execution.

 * @list: Pointer to VME list.

 *

 * Queue the provided VME DMA list for execution. The call will return once the

 * list has been executed.

 *

 * Return: Zero on success, -EINVAL if operation is not supported on this

 *         device. Hardware specific errors also possible.

/**

 * vme_dma_list_free - Free a VME DMA list.

 * @list: Pointer to VME list.

 *

 * Free the provided DMA list and all its entries.

 *

 * Return: Zero on success, -EINVAL on invalid VME resource, -EBUSY if resource

 *         is still in use. Hardware specific errors also possible.

	/*

	 * Empty out all of the entries from the DMA list. We need to go to the

	 * low level driver as DMA entries are driver specific.

/**

 * vme_dma_free - Free a VME DMA resource.

 * @resource: Pointer to VME DMA resource.

 *

 * Free the provided DMA resource so that it may be reallocated.

 *

 * Return: Zero on success, -EINVAL on invalid VME resource, -EBUSY if resource

 *         is still active.

/**

 * vme_irq_request - Request a specific VME interrupt.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @level: Interrupt priority being requested.

 * @statid: Interrupt vector being requested.

 * @callback: Pointer to callback function called when VME interrupt/vector

 *            received.

 * @priv_data: Generic pointer that will be passed to the callback function.

 *

 * Request callback to be attached as a handler for VME interrupts with provided

 * level and statid.

 *

 * Return: Zero on success, -EINVAL on invalid vme device, level or if the

 *         function is not supported, -EBUSY if the level/statid combination is

 *         already in use. Hardware specific errors also possible.

 Enable IRQ level */

/**

 * vme_irq_free - Free a VME interrupt.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @level: Interrupt priority of interrupt being freed.

 * @statid: Interrupt vector of interrupt being freed.

 *

 * Remove previously attached callback from VME interrupt priority/vector.

 Disable IRQ level if no more interrupts attached at this level*/

/**

 * vme_irq_generate - Generate VME interrupt.

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 * @level: Interrupt priority at which to assert the interrupt.

 * @statid: Interrupt vector to associate with the interrupt.

 *

 * Generate a VME interrupt of the provided level and with the provided

 * statid.

 *

 * Return: Zero on success, -EINVAL on invalid vme device, level or if the

 *         function is not supported. Hardware specific errors also possible.

/**

 * vme_lm_request - Request a VME location monitor

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 *

 * Allocate a location monitor resource to the driver. A location monitor

 * allows the driver to monitor accesses to a contiguous number of

 * addresses on the VME bus.

 *

 * Return: Pointer to a VME resource on success or NULL on failure.

 Loop through LM resources */

 Find an unlocked controller */

 Check to see if we found a resource */

 Unlock image */

/**

 * vme_lm_count - Determine number of VME Addresses monitored

 * @resource: Pointer to VME location monitor resource.

 *

 * The number of contiguous addresses monitored is hardware dependent.

 * Return the number of contiguous addresses monitored by the

 * location monitor.

 *

 * Return: Count of addresses monitored or -EINVAL when provided with an

 *	   invalid location monitor resource.

/**

 * vme_lm_set - Configure location monitor

 * @resource: Pointer to VME location monitor resource.

 * @lm_base: Base address to monitor.

 * @aspace: VME address space to monitor.

 * @cycle: VME bus cycle type to monitor.

 *

 * Set the base address, address space and cycle type of accesses to be

 * monitored by the location monitor.

 *

 * Return: Zero on success, -EINVAL when provided with an invalid location

 *	   monitor resource or function is not supported. Hardware specific

 *	   errors may also be returned.

/**

 * vme_lm_get - Retrieve location monitor settings

 * @resource: Pointer to VME location monitor resource.

 * @lm_base: Pointer used to output the base address monitored.

 * @aspace: Pointer used to output the address space monitored.

 * @cycle: Pointer used to output the VME bus cycle type monitored.

 *

 * Retrieve the base address, address space and cycle type of accesses to

 * be monitored by the location monitor.

 *

 * Return: Zero on success, -EINVAL when provided with an invalid location

 *	   monitor resource or function is not supported. Hardware specific

 *	   errors may also be returned.

/**

 * vme_lm_attach - Provide callback for location monitor address

 * @resource: Pointer to VME location monitor resource.

 * @monitor: Offset to which callback should be attached.

 * @callback: Pointer to callback function called when triggered.

 * @data: Generic pointer that will be passed to the callback function.

 *

 * Attach a callback to the specificed offset into the location monitors

 * monitored addresses. A generic pointer is provided to allow data to be

 * passed to the callback when called.

 *

 * Return: Zero on success, -EINVAL when provided with an invalid location

 *	   monitor resource or function is not supported. Hardware specific

 *	   errors may also be returned.

/**

 * vme_lm_detach - Remove callback for location monitor address

 * @resource: Pointer to VME location monitor resource.

 * @monitor: Offset to which callback should be removed.

 *

 * Remove the callback associated with the specificed offset into the

 * location monitors monitored addresses.

 *

 * Return: Zero on success, -EINVAL when provided with an invalid location

 *	   monitor resource or function is not supported. Hardware specific

 *	   errors may also be returned.

/**

 * vme_lm_free - Free allocated VME location monitor

 * @resource: Pointer to VME location monitor resource.

 *

 * Free allocation of a VME location monitor.

 *

 * WARNING: This function currently expects that any callbacks that have

 *          been attached to the location monitor have been removed.

 *

 * Return: Zero on success, -EINVAL when provided with an invalid location

 *	   monitor resource.

	/* XXX

	 * Check to see that there aren't any callbacks still attached, if

	 * there are we should probably be detaching them!

/**

 * vme_slot_num - Retrieve slot ID

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 *

 * Retrieve the slot ID associated with the provided VME device.

 *

 * Return: The slot ID on success, -EINVAL if VME bridge cannot be determined

 *         or the function is not supported. Hardware specific errors may also

 *         be returned.

/**

 * vme_bus_num - Retrieve bus number

 * @vdev: Pointer to VME device struct vme_dev assigned to driver instance.

 *

 * Retrieve the bus enumeration associated with the provided VME device.

 *

 * Return: The bus number on success, -EINVAL if VME bridge cannot be

 *         determined.

 - Bridge Registration --------------------------------------------------- */

 Common bridge initialization */

 - Driver Registration --------------------------------------------------- */

		/*

		 * This cannot cause trouble as we already have vme_buses_lock

		 * and if the bridge is removed, it will have to go through

		 * vme_unregister_bridge() to do it (which calls remove() on

		 * the bridge which in turn tries to acquire vme_buses_lock and

		 * will have to wait).

/**

 * vme_register_driver - Register a VME driver

 * @drv: Pointer to VME driver structure to register.

 * @ndevs: Maximum number of devices to allow to be enumerated.

 *

 * Register a VME device driver with the VME subsystem.

 *

 * Return: Zero on success, error value on registration failure.

/**

 * vme_unregister_driver - Unregister a VME driver

 * @drv: Pointer to VME driver structure to unregister.

 *

 * Unregister a VME device driver from the VME subsystem.

 - Bus Registration ------------------------------------------------------ */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for the Tundra Universe I/II VME-PCI Bridge Chips

 *

 * Author: Martyn Welch <martyn.welch@ge.com>

 * Copyright 2008 GE Intelligent Platforms Embedded Systems, Inc.

 *

 * Based on work by Tom Armistead and Ajit Prem

 * Copyright 2004 Motorola Inc.

 *

 * Derived from ca91c042.c by Michael Wyrick

 Module parameters */

 We only enable interrupts if the callback is set */

 XXX This needs to be split into 4 queues */

 Only look at unmasked interrupts */

 Clear serviced interrupts */

 Need pdev */

 Disable interrupts from PCI to VME */

 Disable PCI interrupts */

 Clear Any Pending PCI Interrupts */

 Ensure all interrupts are mapped to PCI Interrupt 0 */

 Enable DMA, mailbox & LM Interrupts */

 Disable interrupts from PCI to VME */

 Disable PCI interrupts */

 Clear Any Pending PCI Interrupts */

/*

 * Set up an VME interrupt

 Enable IRQ level */

 Universe can only generate even vectors */

 Set Status/ID */

 Assert VMEbus IRQ */

 Wait for IACK */

 Return interrupt to low state */

	/*

	 * Bound address is a valid address for the window, adjust

	 * accordingly

 Disable while we are mucking around */

 Setup mapping */

 Setup address space */

 Setup cycle types */

 Write ctl reg without enable */

 Read Registers */

/*

 * Allocate and map PCI Resource

 Find pci_dev container of dev */

 If the existing size is OK, return */

/*

 * Free and unmap PCI Resource

 Verify input data */

	/*

	 * Let's allocate the resource here rather than further up the stack as

	 * it avoids pushing loads of bus dependent stuff up the stack

	/*

	 * Bound address is a valid address for the window, adjust

	 * according to window granularity.

 Disable while we are mucking around */

 Setup cycle types */

 Setup data width */

 Setup address space */

 Setup mapping */

 Write ctl reg without enable */

 Setup address space */

 XXX Not sure howto check for MBLT */

 Setup cycle types */

 Setup data width */

	/* The following code handles VME address alignment. We cannot use

	 * memcpy_xxx here because it may cut data transfers in to 8-bit

	 * cycles when D16 or D32 cycles are required on the VME bus.

	 * On the other hand, the bridge itself assures that the maximum data

	 * cycle configured for the transfer is used and splits it

	 * automatically for non-aligned addresses, so we don't want the

	 * overhead of needlessly forcing small transfers for the entire cycle.

	/* Here we apply for the same strategy we do in master_read

	 * function in order to assure the correct cycles.

 Find the PCI address that maps to the desired VME address */

 Locking as we can only do one of these at a time */

 Lock image */

 Address must be 4-byte aligned */

 Ensure RMW Disabled whilst configuring */

 Configure registers */

 Enable RMW */

 Kick process off with a read to the required address. */

 Disable RMW */

 XXX descriptor must be aligned on 64-bit boundaries */

 Test descriptor alignment */

 Check we can do fulfill required attributes */

 Check to see if we can fulfill source and destination */

 Setup cycle types */

 Setup data width */

 Setup address space */

 Add to list */

 Fill out previous descriptors "Next Address" */

 We need the bus address for the pointer */

		/*

		 * XXX We have an active DMA transfer and currently haven't

		 *     sorted out the mechanism for "pending" DMA transfers.

		 *     Return busy.

 Need to add to pending here */

 Get first bus address and write into registers */

 Start the operation */

 XXX Could set VMEbus On and Off Counters here */

 Wait for the operation to abort */

	/*

	 * Read status register, this register is valid until we kick off a

	 * new transfer.

 Remove list from running list */

 detach and free each entry */

/*

 * All 4 location monitors reside at the same base - this is therefore a

 * system wide configuration.

 *

 * This does not enable the LM monitor - that should be done when the first

 * callback is attached and disabled when the last callback is removed.

 Check the alignment of the location monitor */

 If we already have a callback attached, we can't move it! */

/* Get configuration of the callback monitor and return whether it is enabled

 * or disabled.

/*

 * Attach a callback to a specific location monitor.

 *

 * Callback will be passed the monitor triggered.

 Ensure that the location monitor is configured - need PGM or DATA */

 Check that a callback isn't already attached */

 Attach callback */

 Enable Location Monitor interrupt */

 Ensure that global Location Monitor Enable set */

/*

 * Detach a callback function forn a specific location monitor.

 Disable Location Monitor and ensure previous interrupts are clear */

 Detach callback */

 If all location monitors disabled, disable global Location Monitor */

 Find pci_dev container of dev */

 Find pci_dev container of dev */

/*

 * Configure CR/CSR space

 *

 * Access to the CR/CSR can be configured at power-up. The location of the

 * CR/CSR registers in the CR/CSR address space is determined by the boards

 * Auto-ID or Geographic address. This function ensures that the window is

 * enabled at an offset consistent with the boards geopgraphic address.

 Write CSR Base Address if slot ID is supplied as a module param */

 Allocate mem for CR/CSR image */

 Turn off CR/CSR space */

 Free image */

	/* We want to support more than one of each bridge so we need to

	 * dynamically allocate the bridge structure

 Enable the device */

 Map Registers */

 map registers in BAR 0 */

 Check to see if the mapping worked out */

 Initialize wait queues & mutual exclusion flags */

 Setup IRQ */

 Add master windows to list */

 Add slave windows to list */

 Only windows 0 and 4 support A16 */

 Add dma engines to list */

 Add location monitor to list */

	/* Need to save ca91cx42_bridge pointer locally in link list for use in

	 * ca91cx42_remove()

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 Turn off Ints */

 Turn off the windows */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Fake VME bridge support.

 *

 * This drive provides a fake VME bridge chip, this enables debugging of the

 * VME framework in the absence of a VME system.

 *

 * This driver has to do a number of things in software that would be driven

 * by hardware if it was available, it will also result in extra overhead at

 * times when compared with driving actual hardware.

 *

 * Author: Martyn Welch <martyn@welches.me.uk>

 * Copyright (c) 2014 Martyn Welch

 *

 * Based on vme_tsi148.c:

 *

 * Author: Martyn Welch <martyn.welch@ge.com>

 * Copyright 2008 GE Intelligent Platforms Embedded Systems, Inc.

 *

 * Based on work by Tom Armistead and Ajit Prem

 * Copyright 2004 Motorola Inc.

/*

 *  Define the number of each that the fake driver supports.

 Max Master Windows */

 Max Slave Windows */

 Structures to hold information normally held in device registers */

 Structure used to hold driver specific information */

 Only one VME interrupt can be generated at a time, provide locking */

 Module parameter */

/*

 * Calling VME bus interrupt callback if provided.

/*

 * Configure VME interrupt

 Nothing to do */

/*

 * Generate a VME bus interrupt at the requested level & vector. Wait for

 * interrupt to be acked.

	/*

	 * Schedule tasklet to run VME handler to emulate normal VME interrupt

	 * handler behaviour.

/*

 * Initialize a slave window with the requested attributes.

	/*

	 * Bound address is a valid address for the window, adjust

	 * accordingly

/*

 * Get slave window configuration.

/*

 * Set the attributes of an outbound window.

 Verify input data */

 Setup data width */

 Setup address space */

/*

 * Set the attributes of an outbound window.

 Get vme_bridge */

 Loop through each location monitor resource */

 If disabled, we're done */

 First make sure that the cycle and address space match */

 Each location monitor covers 8 bytes */

	/* The following code handles VME address alignment. We cannot use

	 * memcpy_xxx here because it may cut data transfers in to 8-bit

	 * cycles when D16 or D32 cycles are required on the VME bus.

	 * On the other hand, the bridge itself assures that the maximum data

	 * cycle configured for the transfer is used and splits it

	 * automatically for non-aligned addresses, so we don't want the

	 * overhead of needlessly forcing small transfers for the entire cycle.

	/* Here we apply for the same strategy we do in master_read

	 * function in order to assure the correct cycles.

/*

 * Perform an RMW cycle on the VME bus.

 *

 * Requires a previously configured master window, returns final value.

 Find the PCI address that maps to the desired VME address */

 Lock image */

 Read existing value */

 Perform check */

 Write back */

 Unlock image */

/*

 * All 4 location monitors reside at the same base - this is therefore a

 * system wide configuration.

 *

 * This does not enable the LM monitor - that should be done when the first

 * callback is attached and disabled when the last callback is removed.

 If we already have a callback attached, we can't move it! */

/* Get configuration of the callback monitor and return whether it is enabled

 * or disabled.

/*

 * Attach a callback to a specific location monitor.

 *

 * Callback will be passed the monitor triggered.

 Ensure that the location monitor is configured - need PGM or DATA */

 Check that a callback isn't already attached */

 Attach callback */

 Ensure that global Location Monitor Enable set */

/*

 * Detach a callback function forn a specific location monitor.

 Detach callback */

 If all location monitors disabled, disable global Location Monitor */

/*

 * Determine Geographical Addressing

/*

	dma_free_coherent(parent, size, vaddr, dma);

/*

 * Configure CR/CSR space

 *

 * Access to the CR/CSR can be configured at power-up. The location of the

 * CR/CSR registers in the CR/CSR address space is determined by the boards

 * Geographic address.

 *

 * Each board has a 512kB window, with the highest 4kB being used for the

 * boards registers, this means there is a fix length 508kB window which must

 * be mapped onto PCI memory.

 Allocate mem for CR/CSR image */

 We need a fake parent device */

	/* If we want to support more than one bridge at some point, we need to

	 * dynamically allocate this so we get one per device.

 Initialize wait queues & mutual exclusion flags */

 Add master windows to list */

 Add slave windows to list */

 Add location monitor to list */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

	/*

	 *  Shutdown all inbound and outbound windows.

	/*

	 *  Shutdown Location monitor.

 resources are stored in link list */

 resources are stored in link list */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for the Tundra TSI148 VME-PCI Bridge Chip

 *

 * Author: Martyn Welch <martyn.welch@ge.com>

 * Copyright 2008 GE Intelligent Platforms Embedded Systems, Inc.

 *

 * Based on work by Tom Armistead and Ajit Prem

 * Copyright 2004 Motorola Inc.

 Module parameter */

/*

 * Wakes up DMA queue.

/*

 * Wake up location monitor queue

 We only enable interrupts if the callback is set */

/*

 * Wake up mail box queue.

 *

 * XXX This functionality is not exposed up though API.

/*

 * Display error & status message when PERR (PCI) exception interrupt occurs.

/*

 * Save address and status when VME error interrupt occurs.

 Check for exception register overflow (we have lost error data) */

 Clear Status */

/*

 * Wake up IACK queue.

/*

 * Calling VME bus interrupt callback if provided.

			/*

			 * Note: Even though the registers are defined as

			 * 32-bits in the spec, we only want to issue 8-bit

			 * IACK cycles on the bus, read from offset 3.

/*

 * Top level interrupt handler.  Clears appropriate interrupt status bits and

 * then calls appropriate sub handler(s).

 Determine which interrupts are unmasked and set */

 Only look at unmasked interrupts */

 Call subhandlers as appropriate */

 DMA irqs */

 Location monitor irqs */

 Mail box irqs */

 PCI bus error */

 VME bus error */

 IACK irq */

 VME bus irqs */

 Clear serviced interrupts */

 Enable and unmask interrupts */

	/* This leaves the following interrupts masked.

	 * TSI148_LCSR_INTEO_VIEEO

	 * TSI148_LCSR_INTEO_SYSFLEO

	 * TSI148_LCSR_INTEO_ACFLEO

	/* Don't enable Location Monitor interrupts here - they will be

	 * enabled when the location monitors are properly configured and

	 * a callback has been attached.

	 * TSI148_LCSR_INTEO_LM0EO

	 * TSI148_LCSR_INTEO_LM1EO

	 * TSI148_LCSR_INTEO_LM2EO

	 * TSI148_LCSR_INTEO_LM3EO

	/* Don't enable VME interrupts until we add a handler, else the board

	 * will respond to it and we don't want that unless it knows how to

	 * properly deal with it.

	 * TSI148_LCSR_INTEO_IRQ7EO

	 * TSI148_LCSR_INTEO_IRQ6EO

	 * TSI148_LCSR_INTEO_IRQ5EO

	 * TSI148_LCSR_INTEO_IRQ4EO

	 * TSI148_LCSR_INTEO_IRQ3EO

	 * TSI148_LCSR_INTEO_IRQ2EO

	 * TSI148_LCSR_INTEO_IRQ1EO

 Turn off interrupts */

 Clear all interrupts */

 Detach interrupt handler */

/*

 * Check to see if an IACk has been received, return true (1) or false (0).

/*

 * Configure VME interrupt

 We need to do the ordering differently for enabling and disabling */

/*

 * Generate a VME bus interrupt at the requested level & vector. Wait for

 * interrupt to be acked.

 Read VICR register */

 Set Status/ID */

 Assert VMEbus IRQ */

 XXX Consider implementing a timeout? */

/*

 * Initialize a slave window with the requested attributes.

 Convert 64-bit variables to 2x 32-bit variables */

	/*

	 * Bound address is a valid address for the window, adjust

	 * accordingly

  Disable while we are mucking around */

 Setup mapping */

 Setup 2eSST speeds */

 Setup cycle types */

 Setup address space */

 Write ctl reg without enable */

/*

 * Get slave window configuration.

 Read registers */

 Convert 64-bit variables to 2x 32-bit variables */

 Need granularity before we set the size */

/*

 * Allocate and map PCI Resource

 If the existing size is OK, return */

 Exit here if size is zero */

/*

 * Free and unmap PCI Resource

/*

 * Set the attributes of an outbound window.

 Verify input data */

	/* Let's allocate the resource here rather than further up the stack as

	 * it avoids pushing loads of bus dependent stuff up the stack. If size

	 * is zero, any existing resource will be freed.

		/*

		 * Bound address is a valid address for the window, adjust

		 * according to window granularity.

 Convert 64-bit variables to 2x 32-bit variables */

 Disable while we are mucking around */

 Setup 2eSST speeds */

 Setup cycle types */

 Setup data width */

 Setup address space */

 Setup mapping */

 Write ctl reg without enable */

/*

 * Set the attributes of an outbound window.

 *

 * XXX Not parsing prefetch information.

 Convert 64-bit variables to 2x 32-bit variables */

 Setup address space */

 Setup 2eSST speeds */

 Setup cycle types */

 Setup data width */

	/* The following code handles VME address alignment. We cannot use

	 * memcpy_xxx here because it may cut data transfers in to 8-bit

	 * cycles when D16 or D32 cycles are required on the VME bus.

	 * On the other hand, the bridge itself assures that the maximum data

	 * cycle configured for the transfer is used and splits it

	 * automatically for non-aligned addresses, so we don't want the

	 * overhead of needlessly forcing small transfers for the entire cycle.

	/* Here we apply for the same strategy we do in master_read

	 * function in order to assure the correct cycles.

	/*

	 * Writes are posted. We need to do a read on the VME bus to flush out

	 * all of the writes before we check for errors. We can't guarantee

	 * that reading the data we have just written is safe. It is believed

	 * that there isn't any read, write re-ordering, so we can read any

	 * location in VME space, so lets read the Device ID from the tsi148's

	 * own registers as mapped into CR/CSR space.

	 *

	 * We check for saved errors in the written address range/space.

/*

 * Perform an RMW cycle on the VME bus.

 *

 * Requires a previously configured master window, returns final value.

 Find the PCI address that maps to the desired VME address */

 Locking as we can only do one of these at a time */

 Lock image */

 Configure registers */

 Enable RMW */

 Kick process off with a read to the required address. */

 Disable RMW */

 Setup 2eSST speeds */

 Setup cycle types */

 Setup data width */

 Setup address space */

 Setup 2eSST speeds */

 Setup cycle types */

 Setup data width */

 Setup address space */

/*

 * Add a link list descriptor to the list

 *

 * Note: DMA engine expects the DMA descriptor to be big endian.

 Descriptor must be aligned on 64-bit boundaries */

 Test descriptor alignment */

	/* Given we are going to fill out the structure, we probably don't

	 * need to zero it, but better safe than sorry for now.

 Fill out source part */

 Default behaviour is 32 bit pattern */

 It seems that the default behaviour is to increment */

 Assume last link - this will be over-written by adding another */

 Fill out destination part */

 Fill out count */

 Add to list */

 Fill out previous descriptors "Next Address" */

/*

 * Check to see if the provided DMA channel is busy.

/*

 * Execute a previously generated link list

 *

 * XXX Need to provide control register configuration.

		/*

		 * XXX We have an active DMA transfer and currently haven't

		 *     sorted out the mechanism for "pending" DMA transfers.

		 *     Return busy.

 Need to add to pending here */

 Get first bus address and write into registers */

 Start the operation */

 Wait for the operation to abort */

	/*

	 * Read status register, this register is valid until we kick off a

	 * new transfer.

 Remove list from running list */

/*

 * Clean up a previously generated link list

 *

 * We have a separate function, don't assume that the chain can't be reused.

 detach and free each entry */

/*

 * All 4 location monitors reside at the same base - this is therefore a

 * system wide configuration.

 *

 * This does not enable the LM monitor - that should be done when the first

 * callback is attached and disabled when the last callback is removed.

 If we already have a callback attached, we can't move it! */

/* Get configuration of the callback monitor and return whether it is enabled

 * or disabled.

/*

 * Attach a callback to a specific location monitor.

 *

 * Callback will be passed the monitor triggered.

 Ensure that the location monitor is configured - need PGM or DATA */

 Check that a callback isn't already attached */

 Attach callback */

 Enable Location Monitor interrupt */

 Ensure that global Location Monitor Enable set */

/*

 * Detach a callback function forn a specific location monitor.

 Disable Location Monitor and ensure previous interrupts are clear */

 Detach callback */

 If all location monitors disabled, disable global Location Monitor */

/*

 * Determine Geographical Addressing

 Find pci_dev container of dev */

 Find pci_dev container of dev */

/*

 * Configure CR/CSR space

 *

 * Access to the CR/CSR can be configured at power-up. The location of the

 * CR/CSR registers in the CR/CSR address space is determined by the boards

 * Auto-ID or Geographic address. This function ensures that the window is

 * enabled at an offset consistent with the boards geopgraphic address.

 *

 * Each board has a 512kB window, with the highest 4kB being used for the

 * boards registers, this means there is a fix length 508kB window which must

 * be mapped onto PCI memory.

 Allocate mem for CR/CSR image */

 Ensure that the CR/CSR is configured at the correct offset */

	/* If we want flushed, error-checked writes, set up a window

	 * over the CR/CSR registers. We read from here to safely flush

	 * through VME writes.

 Turn off CR/CSR space */

 Free image */

	/* If we want to support more than one of each bridge, we need to

	 * dynamically generate this so we get one per device

 Enable the device */

 Map Registers */

 map registers in BAR 0 */

 Check to see if the mapping worked out */

 Initialize wait queues & mutual exclusion flags */

 Setup IRQ */

	/* If we are going to flush writes, we need to read from the VME bus.

	 * We need to do this safely, thus we read the devices own CR/CSR

	 * register. To do this we must set up a window in CR/CSR space and

	 * hence have one less master window resource available.

 Add master windows to list */

 Add slave windows to list */

 Add dma engines to list */

 Add location monitor to list */

 Clear VME bus "board fail", and "power-up reset" lines */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

	/*

	 *  Shutdown all inbound and outbound windows.

	/*

	 *  Shutdown Location monitor.

	/*

	 *  Shutdown CRG map.

	/*

	 *  Clear error status.

	/*

	 *  Remove VIRQ interrupt (if any)

	/*

	 *  Map all Interrupts to PCI INTA

 resources are stored in link list */

 resources are stored in link list */

 resources are stored in link list */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for the VMIVME-7805 board access to the Universe II bridge.

 *

 * Author: Arthur Benilov <arthur.benilov@iba-group.com>

 * Copyright 2010 Ion Beam Application, Inc.

* Base address to access FPGA register */

 Enable the device */

 Map Registers */

 Map registers in BAR 0 */

 Clear the FPGA VME IF contents */

 Clear any initial BERR  */

 Enable the vme interface and byte swapping */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2012-2015, 2017, 2021, The Linux Foundation. All rights reserved.

 PMIC Arbiter configuration registers */

 PMIC Arbiter channel registers offsets */

 Mapping Table */

 Maximum of 16-bits */

 PPID is 12bit */

 Ownership Table */

 Channel Status fields */

 Command register fields */

 Command Opcodes */

/*

 * PMIC arbiter version 5 uses different register offsets for read/write vs

 * observer channels.

 Maximum number of support PMIC peripherals */

 interrupt enable bit */

/**

 * spmi_pmic_arb - SPMI PMIC Arbiter object

 *

 * @rd_base:		on v1 "core", on v2 "observer" register base off DT.

 * @wr_base:		on v1 "core", on v2 "chnls"    register base off DT.

 * @intr:		address of the SPMI interrupt control registers.

 * @cnfg:		address of the PMIC Arbiter configuration registers.

 * @lock:		lock to synchronize accesses.

 * @channel:		execution environment channel to use for accesses.

 * @irq:		PMIC ARB interrupt.

 * @ee:			the current Execution Environment

 * @min_apid:		minimum APID (used for bounding IRQ search)

 * @max_apid:		maximum APID

 * @mapping_table:	in-memory copy of PPID -> APID mapping table.

 * @domain:		irq domain object for PMIC IRQ domain

 * @spmic:		SPMI controller object

 * @ver_ops:		version dependent operations.

 * @ppid_to_apid	in-memory copy of PPID -> APID mapping table.

/**

 * pmic_arb_ver: version dependent functionality.

 *

 * @ver_str:		version string.

 * @ppid_to_apid:	finds the apid for a given ppid.

 * @non_data_cmd:	on v1 issues an spmi non-data command.

 *			on v2 no HW support, returns -EOPNOTSUPP.

 * @offset:		on v1 offset of per-ee channel.

 *			on v2 offset of per-ee and per-ppid channel.

 * @fmt_cmd:		formats a GENI/SPMI command.

 * @owner_acc_status:	on v1 address of PMIC_ARB_SPMI_PIC_OWNERm_ACC_STATUSn

 *			on v2 address of SPMI_PIC_OWNERm_ACC_STATUSn.

 * @acc_enable:		on v1 address of PMIC_ARB_SPMI_PIC_ACC_ENABLEn

 *			on v2 address of SPMI_PIC_ACC_ENABLEn.

 * @irq_status:		on v1 address of PMIC_ARB_SPMI_PIC_IRQ_STATUSn

 *			on v2 address of SPMI_PIC_IRQ_STATUSn.

 * @irq_clear:		on v1 address of PMIC_ARB_SPMI_PIC_IRQ_CLEARn

 *			on v2 address of SPMI_PIC_IRQ_CLEARn.

 * @apid_map_offset:	offset of PMIC_ARB_REG_CHNLn

 spmi commands (read_cmd, write_cmd, cmd) functionality */

 Interrupts controller functionality (offset of PIC registers) */

/**

 * pmic_arb_read_data: reads pmic-arb's register and copy 1..4 bytes to buf

 * @bc:		byte count -1. range: 0..3

 * @reg:	register's address

 * @buf:	output parameter, length must be bc + 1

/**

 * pmic_arb_write_data: write 1..4 bytes from buf to pmic-arb's register

 * @bc:		byte-count -1. range: 0..3.

 * @reg:	register's address.

 * @buf:	buffer to write. length must be bc + 1.

 Non-data command */

 Check for valid non-data command */

 Check the opcode */

 Check the opcode */

 Write data to FIFOs */

 Start the transaction */

 1 -> edge */

 Simplified accessor functions for irqchip callbacks */

		/*

		 * Since the interrupt is currently disabled, write to both the

		 * LATCHED_CLR and EN_SET registers so that a spurious interrupt

		 * cannot be triggered when the interrupt is enabled

 level trig */

 Keep track of {max,min}_apid for bounding search during interrupt */

 v1 offset per ee */

	/*

	 * In order to allow multiple EEs to write to a single PPID in arbiter

	 * version 5, there is more than one APID mapped to each PPID.

	 * The owner field for each of these mappings specifies the EE which is

	 * allowed to write to the APID.  The owner of the last (highest) APID

	 * for a given PPID will receive interrupts from the PPID.

			/*

			 * Duplicate PPID mapping after the one for this EE;

			 * override the irq owner

 First PPID mapping or duplicate for another EE */

 Dump the mapping table for debug purposes. */

 v2 offset per ppid and per ee */

/*

 * v5 offset per ee and per apid for observer channels and per apid for

 * read/write channels.

	/* Initialize max_apid/min_apid to the opposite bounds, during

 SPDX-License-Identifier: GPL-2.0

/*

 * SPMI register addr

/*

 * SPMI cmd register

 Command Opcodes */

/*

 * SPMI status register

 Command register fields */

 Maximum number of support PMIC peripherals */

 slvid */

 slave_addr */

 Write data to FIFOs */

 Start the transaction */

 Callbacks */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.

/**

 * spmi_device_add() - add a device previously constructed via spmi_device_alloc()

 * @sdev:	spmi_device to be added

/**

 * spmi_device_remove(): remove an SPMI device

 * @sdev:	spmi_device to be removed

/**

 * spmi_register_read() - register read

 * @sdev:	SPMI device.

 * @addr:	slave register address (5-bit address).

 * @buf:	buffer to be populated with data from the Slave.

 *

 * Reads 1 byte of data from a Slave device register.

 5-bit register address */

/**

 * spmi_ext_register_read() - extended register read

 * @sdev:	SPMI device.

 * @addr:	slave register address (8-bit address).

 * @buf:	buffer to be populated with data from the Slave.

 * @len:	the request number of bytes to read (up to 16 bytes).

 *

 * Reads up to 16 bytes of data from the extended register space on a

 * Slave device.

 8-bit register address, up to 16 bytes */

/**

 * spmi_ext_register_readl() - extended register read long

 * @sdev:	SPMI device.

 * @addr:	slave register address (16-bit address).

 * @buf:	buffer to be populated with data from the Slave.

 * @len:	the request number of bytes to read (up to 8 bytes).

 *

 * Reads up to 8 bytes of data from the extended register space on a

 * Slave device using 16-bit address.

 16-bit register address, up to 8 bytes */

/**

 * spmi_register_write() - register write

 * @sdev:	SPMI device

 * @addr:	slave register address (5-bit address).

 * @data:	buffer containing the data to be transferred to the Slave.

 *

 * Writes 1 byte of data to a Slave device register.

 5-bit register address */

/**

 * spmi_register_zero_write() - register zero write

 * @sdev:	SPMI device.

 * @data:	the data to be written to register 0 (7-bits).

 *

 * Writes data to register 0 of the Slave device.

/**

 * spmi_ext_register_write() - extended register write

 * @sdev:	SPMI device.

 * @addr:	slave register address (8-bit address).

 * @buf:	buffer containing the data to be transferred to the Slave.

 * @len:	the request number of bytes to read (up to 16 bytes).

 *

 * Writes up to 16 bytes of data to the extended register space of a

 * Slave device.

 8-bit register address, up to 16 bytes */

/**

 * spmi_ext_register_writel() - extended register write long

 * @sdev:	SPMI device.

 * @addr:	slave register address (16-bit address).

 * @buf:	buffer containing the data to be transferred to the Slave.

 * @len:	the request number of bytes to read (up to 8 bytes).

 *

 * Writes up to 8 bytes of data to the extended register space of a

 * Slave device using 16-bit address.

 4-bit Slave Identifier, 16-bit register address, up to 8 bytes */

/**

 * spmi_command_reset() - sends RESET command to the specified slave

 * @sdev:	SPMI device.

 *

 * The Reset command initializes the Slave and forces all registers to

 * their reset values. The Slave shall enter the STARTUP state after

 * receiving a Reset command.

/**

 * spmi_command_sleep() - sends SLEEP command to the specified SPMI device

 * @sdev:	SPMI device.

 *

 * The Sleep command causes the Slave to enter the user defined SLEEP state.

/**

 * spmi_command_wakeup() - sends WAKEUP command to the specified SPMI device

 * @sdev:	SPMI device.

 *

 * The Wakeup command causes the Slave to move from the SLEEP state to

 * the ACTIVE state.

/**

 * spmi_command_shutdown() - sends SHUTDOWN command to the specified SPMI device

 * @sdev:	SPMI device.

 *

 * The Shutdown command causes the Slave to enter the SHUTDOWN state.

/**

 * spmi_controller_alloc() - Allocate a new SPMI device

 * @ctrl:	associated controller

 *

 * Caller is responsible for either calling spmi_device_add() to add the

 * newly allocated controller, or calling spmi_device_put() to discard it.

/**

 * spmi_controller_alloc() - Allocate a new SPMI controller

 * @parent:	parent device

 * @size:	size of private data

 *

 * Caller is responsible for either calling spmi_controller_add() to add the

 * newly allocated controller, or calling spmi_controller_put() to discard it.

 * The allocated private data region may be accessed via

 * spmi_controller_get_drvdata()

/**

 * spmi_controller_add() - Add an SPMI controller

 * @ctrl:	controller to be registered.

 *

 * Register a controller previously allocated via spmi_controller_alloc() with

 * the SPMI core.

 Can't register until after driver model init */

 Remove a device associated with a controller */

/**

 * spmi_controller_remove(): remove an SPMI controller

 * @ctrl:	controller to remove

 *

 * Remove a SPMI controller.  Caller is responsible for calling

 * spmi_controller_put() to discard the allocated controller.

/**

 * spmi_driver_register() - Register client driver with SPMI core

 * @sdrv:	client driver to be associated with client-device.

 *

 * This API will register the client driver with the SPMI framework.

 * It is typically called from the driver's module-init function.

 SPDX-License-Identifier: GPL-2.0

/*

 * STM32 Timer Encoder and Counter driver

 *

 * Copyright (C) STMicroelectronics 2018

 *

 * Author: Benjamin Gaignard <benjamin.gaignard@st.com>

 *

 Store enable status */

 Make sure that registers are updated */

 Restore the enable status */

 TIMx_ARR register shouldn't be buffered (ARPE=0) */

 Keep enabled state to properly handle low power states */

 counts on internal clock when CEN=1 */

 counts up/down on TI1FP1 edge depending on TI2FP2 level */

 counts up/down on TI2FP2 edge depending on TI1FP1 level */

 counts up/down on both TI1FP1 and TI2FP2 edges */

 Register Counter device */

 Only take care of enabled counter: don't disturb other MFD child */

 Backup registers that may get lost in low power mode */

 Disable the counter */

 Restore registers that may have been lost */

 Also re-enables the counter */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic Counter character device interface

 * Copyright (C) 2020 William Breathitt Gray

 Free associated component nodes */

 Free event node */

 Search for event in the list */

 If event is not already in the list */

 Allocate new event node */

 Configure event node and add to the list */

 Check if component watch has already been set before */

 Allocate component node */

 Add component node to event node */

 Free event node if no one else is watching */

 Configure parent component info for comp node */

 Configure component info for comp node */

 Free any lingering held memory */

 Initialize Counter events lists */

 Initialize character device */

 Allocate Counter events queue */

/**

 * counter_push_event - queue event for userspace reading

 * @counter:	pointer to Counter structure

 * @event:	triggered event

 * @channel:	event channel

 *

 * Note: If no one is watching for the respective event, it is silently

 * discarded.

 Could be in an interrupt context, so use a spin lock */

 Search for event in the list */

 If event is not in the list */

 Read and queue relevant comp for userspace */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic Counter sysfs interface

 * Copyright (C) 2020 William Breathitt Gray

/**

 * struct counter_attribute - Counter sysfs attribute

 * @dev_attr:	device attribute for sysfs

 * @l:		node to add Counter attribute to attribute group list

 * @comp:	Counter component callbacks and data

 * @scope:	Counter scope of the attribute

 * @parent:	pointer to the parent component

/**

 * struct counter_attribute_group - container for attribute group

 * @name:	name of the attribute group

 * @attr_list:	list to keep track of created attributes

 * @num_attr:	number of attributes

 data should already be boolean but ensure just to be safe */

 Configure Counter attribute */

 Initialize sysfs attribute */

 Configure device attribute */

 Store list node */

 Configure Counter attribute */

 Configure device attribute */

 Store list node */

 Create "*_available" attribute if needed */

 Configure Counter attribute */

 Configure device attribute */

 Store list node */

 Allocate Counter attribute */

 Generate component ID name */

 Configure Counter attribute */

 Configure device attribute */

 Store list node */

 Create main Signal attribute */

 Create Signal name attribute */

 Create an attribute for each extension */

 Add each Signal */

 Generate Signal attribute directory name */

 Create all attributes associated with Signal */

 Add each Synapse */

 Generate Synapse action name */

 Create action attribute */

 Create Synapse component ID attribute */

 Create main Count attribute */

 Create Count name attribute */

 Create Count function attribute */

 Create an attribute for each extension */

 Add each Count */

 Generate Count attribute directory name */

 Add sysfs attributes of the Synapses */

 Create all attributes associated with Count */

 Allocate new events queue */

 Swap in new events queue */

 Add Signals sysfs attributes */

 Add Counts sysfs attributes */

 Create name attribute */

 Create num_signals attribute */

 Create num_counts attribute */

 Create events_queue_size attribute */

 Create an attribute for each extension */

/**

 * counter_sysfs_add - Adds Counter sysfs attributes to the device structure

 * @counter:	Pointer to the Counter device structure

 *

 * Counter sysfs attributes are created and added to the respective device

 * structure for later registration to the system. Resource-managed memory

 * allocation is performed by this function, and this memory should be freed

 * when no longer needed (automatically by a device_unregister call, or

 * manually by a devres_release_all call).

 Allocate space for attribute groups (signals, counts, and ext) */

 Initialize attribute lists */

 Add Counter device sysfs attributes */

 Allocate attribute group pointers for association with device */

 Allocate space for attribute groups */

 Prepare each group of attributes for association */

 Allocate space for attribute pointers */

 Add attribute pointers to attribute group */

 Associate attribute group */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 David Lechner <david@lechnology.com>

 *

 * Counter driver for Texas Instruments Enhanced Quadrature Encoder Pulse (eQEP)

 32-bit registers */

 16-bit registers */

 0x24 */

 0x26 */

 0x28 */

 0x2a */

 0x2c */

 0x2e */

 0x30 */

 0x32 */

 0x34 */

 0x36 */

 0x38 */

 0x3a */

 0x3c */

 0x3e */

 0x40 */

 EQEP Inputs */

 QEPA/XCLK */

 QEPB/XDIR */

 Position Counter Input Modes */

 should never reach this path */

		/* In quadrature mode, the rising and falling edge of both

		 * QEPA and QEPB trigger QCLK.

		/* In direction-count mode only rising edge of QEPA is counted

		 * and QEPB gives direction.

 should never reach this path */

		/* In up/down-count modes only QEPA is counted and QEPB is not

		 * used.

 should never reach this path */

 should never reach this path */

	/*

	 * Need to make sure power is turned on. On AM33xx, this comes from the

	 * parent PWMSS bus driver. On AM17xx, this comes from the PSC power

	 * domain.

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic Counter interface

 * Copyright (C) 2020 William Breathitt Gray

 Provides a unique ID for each counter device */

/**

 * counter_register - register Counter to the system

 * @counter:	pointer to Counter to register

 *

 * This function registers a Counter to the system. A sysfs "counter" directory

 * will be created and populated with sysfs attributes correlating with the

 * Counter Signals, Synapses, and Counts respectively.

 *

 * RETURNS:

 * 0 on success, negative error number on failure.

 Acquire unique ID */

 Configure device structure for Counter */

/**

 * counter_unregister - unregister Counter from the system

 * @counter:	pointer to Counter to unregister

 *

 * The Counter is unregistered from the system.

/**

 * devm_counter_register - Resource-managed counter_register

 * @dev:	device to allocate counter_device for

 * @counter:	pointer to Counter to register

 *

 * Managed counter_register. The Counter registered with this function is

 * automatically unregistered on driver detach. This function calls

 * counter_register internally. Refer to that function for more information.

 *

 * RETURNS:

 * 0 on success, negative error number on failure.

 SPDX-License-Identifier: GPL-2.0

/*

 * Counter driver for the ACCES 104-QUAD-8

 * Copyright (C) 2016 William Breathitt Gray

 *

 * This driver supports the ACCES 104-QUAD-8 and ACCES 104-QUAD-4.

/**

 * struct quad8 - device private data structure

 * @lock:		lock to prevent clobbering device states during R/W ops

 * @counter:		instance of the counter_device

 * @fck_prescaler:	array of filter clock prescaler configurations

 * @preset:		array of preset values

 * @count_mode:		array of count mode configurations

 * @quadrature_mode:	array of quadrature mode configurations

 * @quadrature_scale:	array of quadrature mode scale configurations

 * @ab_enable:		array of A and B inputs enable configurations

 * @preset_enable:	array of set_to_preset_on_index attribute configurations

 * @irq_trigger:	array of current IRQ trigger function configurations

 * @next_irq_trigger:	array of next IRQ trigger function configurations

 * @synchronous_mode:	array of index function synchronous mode configurations

 * @index_polarity:	array of index function polarity configurations

 * @cable_fault_enable:	differential encoder cable status enable configurations

 * @base:		base port address of the device

 Borrow Toggle flip-flop */

 Carry Toggle flip-flop */

 Error flag */

 Up/Down flag */

 Reset and Load Signal Decoders */

 Counter Mode Register */

 Input / Output Control Register */

 Index Control Register */

 Reset Byte Pointer (three byte data pointer) */

 Reset Counter */

 Reset Borrow Toggle, Carry Toggle, Compare Toggle, and Sign flags */

 Reset Error flag */

 Preset Register to Counter */

 Transfer Counter to Output Latch */

 Transfer Preset Register LSB to FCK Prescaler */

 Only Index signal levels can be read */

 Borrow XOR Carry effectively doubles count range */

 Reset Byte Pointer; transfer Counter to Output Latch */

 Only 24-bit values are supported */

 Reset Byte Pointer */

 Counter can only be set via Preset Register */

 Transfer Preset Register to Counter */

 Reset Byte Pointer */

 Set Preset Register back to original value */

 Reset Borrow, Carry, Compare, and Sign flags */

 Reset Error flag */

 Quadrature scaling only available in quadrature mode */

 Synchronous function not supported in non-quadrature mode */

 Disable synchronous function mode */

 should never reach this path */

 Load mode configuration to Counter Mode Register */

 U/D flag: nonzero = up, zero = down */

 Handle Index signals */

 Default action mode */

 Determine action mode based on current count function mode */

 should never reach this path */

 Enable interrupts for the requested channels, disable for the rest */

 Save new IRQ function configuration */

 Load configuration to I/O Control Register */

 Reset next IRQ trigger function configuration */

 Enable IRQ line */

 Load Index Control configuration to Index Control Register */

 Index function must be non-synchronous in non-quadrature mode */

 Load Index Control configuration to Index Control Register */

 Only a floor of 0 is supported */

 Map 104-QUAD-8 count mode to Generic Counter count mode */

 Map Generic Counter count mode to 104-QUAD-8 count mode */

 should never reach this path */

 Set count mode configuration value */

 Add quadrature mode configuration */

 Load mode configuration to Counter Mode Register */

 Load I/O control configuration */

 Reset Byte Pointer */

 Set Preset Register */

 Only 24-bit values are supported */

 Range Limit and Modulo-N count modes use preset value as ceiling */

 By default 0x1FFFFFF (25 bits unsigned) is maximum count */

 Only 24-bit values are supported */

 Range Limit and Modulo-N count modes use preset value as ceiling */

 Preset enable is active low in Input/Output Control register */

 Load I/O control configuration to Input / Output Control Register */

 Logic 0 = cable fault */

 Mask respective channel and invert logic */

 Enable is active low in Differential Encoder Cable Status register */

 Reset Byte Pointer */

 Set filter clock factor */

 should never reach this path */

 Clear pending interrupts on device */

 Initialize Counter device and driver data */

 Reset Index/Interrupt Register */

 Reset all counters and disable interrupt function */

 Set initial configuration for all counters */

 Reset Byte Pointer */

 Reset filter clock factor */

 Reset Byte Pointer */

 Reset Preset Register */

 Reset Borrow, Carry, Compare, and Sign flags */

 Reset Error flag */

 Binary encoding; Normal count; non-quadrature mode */

 Disable A and B inputs; preset on index; FLG1 as Carry */

 Disable index function; negative index polarity */

 Initialize next IRQ trigger function configuration */

 Disable Differential Encoder Cable Status for all channels */

 Enable all counters and enable interrupt function */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel Quadrature Encoder Peripheral driver

 *

 * Copyright (C) 2019-2021 Intel Corporation

 *

 * Author: Felipe Balbi (Intel)

 * Author: Jarkko Nikula <jarkko.nikula@linux.intel.com>

 * Author: Raymond Tan <raymond.tan@intel.com>

 QEPCON */

 QEPFLT */

 QEPINT */

 Context save registers */

	/*

	 * Make sure peripheral is disabled by flushing the write with

	 * a dummy read

 Intel QEP ceiling configuration only supports 32-bit values */

 Enable peripheral and keep runtime PM always on */

 Let runtime PM be idle and disable peripheral */

	/*

	 * Spike filter length is (MAX_COUNT + 2) clock periods.

	 * Disable filter when userspace writes 0, enable for valid

	 * nanoseconds values and error out otherwise.

	/*

	 * Make sure peripheral is disabled when restoring registers and

	 * control register bits that are writable only when the peripheral

	 * is disabled

 Restore all other control register bits except enable status */

 Restore enable status */

 EHL */

 Terminating Entry */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2021 Pengutronix, Oleksij Rempel <kernel@pengutronix.de>

 SPDX-License-Identifier: GPL-2.0

/*

 * STM32 Low-Power Timer Encoder and Counter driver

 *

 * Copyright (C) STMicroelectronics 2017

 *

 * Author: Fabrice Gasnier <fabrice.gasnier@st.com>

 *

 * Inspired by 104-quad-8 and stm32-timer-trigger drivers.

 *

 LP timer must be enabled before writing CMP & ARR */

 ensure CMP & ARR registers are properly written */

 Start LP timer in continuous mode */

 Setup LP timer encoder/counter and polarity, without prescaler */

/*

 * In non-quadrature mode, device counts up on active edge.

 * In quadrature mode, encoder counting scenarios are as follows:

 * +---------+----------+--------------------+--------------------+

 * | Active  | Level on |      IN1 signal    |     IN2 signal     |

 * | edge    | opposite +----------+---------+----------+---------+

 * |         | signal   |  Rising  | Falling |  Rising  | Falling |

 * +---------+----------+----------+---------+----------+---------+

 * | Rising  | High ->  |   Down   |    -    |   Up     |    -    |

 * | edge    | Low  ->  |   Up     |    -    |   Down   |    -    |

 * +---------+----------+----------+---------+----------+---------+

 * | Falling | High ->  |    -     |   Up    |    -     |   Down  |

 * | edge    | Low  ->  |    -     |   Down  |    -     |   Up    |

 * +---------+----------+----------+---------+----------+---------+

 * | Both    | High ->  |   Down   |   Up    |   Up     |   Down  |

 * | edges   | Low  ->  |   Up     |   Down  |   Down   |   Up    |

 * +---------+----------+----------+---------+----------+---------+

 should never reach this path */

 Check nobody uses the timer, or already disabled/enabled */

 LP Timer acts as up-counter on input 1 */

 should never reach this path */

 should never reach this path */

 only set polarity when in counter mode (on input 1) */

 LP timer with encoder */

 LP timer without encoder (counter only) */

 Initialize Counter device */

 Only take care of enabled counter: don't disturb other MFD child */

 Force enable state for later resume */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Microchip

 *

 * Author: Kamel Bouhara <kamel.bouhara@bootlin.com>

 Set capture mode */

 Set highest rate based on whether soc has gclk or not */

 Setup the period capture mode */

 In QDEC mode settings both channels 0 and 1 are required */

 should never reach this path */

 Enable clock and trigger counter */

 QDEC mode is rising edge only */

 should never reach this path */

 sentinel */ }

 max. channels number is 2 when in QDEC mode */

 Register channels and initialize clocks */

 Fallback to t0_clk */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * Flex Timer Module Quadrature decoder

 *

 * This module implements a driver for decoding the FTM quadrature

 * of ex. a LS1021A

 Hold mutex before modifying write protection state */

 First see if it is enabled */

 Reset hardware counter to CNTIN */

	/*

	 * Do not write in the region from the CNTIN register through the

	 * PWMLOAD register when FTMEN = 0.

	 * Also reset other fields to zero

 Set prescaler, reset other fields to zero */

 Select quad mode, reset other fields to zero */

 Unused features and reset to default section */

 Lock the FTM */

	/*

	 * This is enough to disable the counter. No clock has been

	 * selected by writing to FTM_SC in init()

 Also resets the counter as it is undefined anyway now */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * EISA bus support functions for sysfs.

 *

 * (C) 2002, 2003 Marc Zyngier <maz@wild-wind.fr.eu.org>

 No name was found */

		/*

		 * This ugly stuff is used to wake up VL-bus cards

		 * (AHA-284x is the only known example), so we can

		 * read the EISA id.

		 *

		 * Thankfully, this only exists on x86...

 No EISA device here */

 Default DMA mask */

		/* Don't register resource for slot 0, since this is

		 * very likely to fail... :-( Instead, grab the EISA

		 * id, now we can display something in /proc/ioports.

 Only one region for mainboard */

	/* First try to get hold of slot 0. If there is no device

	/* Use our own resources to check if this bus base address has

	 * been already registered. This prevents the virtual root

	 * device from registering after the real one has, for

 for legacy drivers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Minimalist driver for a generic PCI-to-EISA bridge.

 *

 * (C) 2003 Marc Zyngier <maz@wild-wind.fr.eu.org>

 *

 * Ivan Kokshaysky <ink@jurassic.park.msu.ru> :

 * Generalisation from i82375 to PCI_CLASS_BRIDGE_EISA.

 There is only *one* pci_eisa device per machine, right ? */

	/*

	 * The Intel 82375 PCI-EISA bridge is a subtractive-decode PCI

	 * device, so the resources available on EISA are the same as those

	 * available on the 82375 bus.  This works the same as a PCI-PCI

	 * bridge in subtractive-decode mode (see pci_read_bridge_bases()).

	 * We assume other PCI-EISA bridges are similar.

	 *

	 * eisa_root_register() can only deal with a single io port resource,

	*  so we use the first valid io port resource.

/*

 * We have to call pci_eisa_init_early() before pnpacpi_init()/isapnp_init().

 *   Otherwise pnp resource will get enabled early and could prevent eisa

 *   to be initialized.

 * Also need to make sure pci_eisa_init_early() is called after

 * x86/pci_subsys_init().

 * So need to use subsys_initcall_sync with it.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Virtual EISA root driver.

 * Acts as a placeholder if we don't have a proper EISA bridge.

 *

 * (C) 2003 Marc Zyngier <maz@wild-wind.fr.eu.org>

/* The default EISA device parent (virtual root device).

 nothing really to do here */

		/* A real bridge may have been registered before

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio-mem device driver.

 *

 * Copyright Red Hat, Inc. 2020

 *

 * Author(s): David Hildenbrand <david@redhat.com>

/*

 * virtio-mem currently supports the following modes of operation:

 *

 * * Sub Block Mode (SBM): A Linux memory block spans 2..X subblocks (SB). The

 *   size of a Sub Block (SB) is determined based on the device block size, the

 *   pageblock size, and the maximum allocation granularity of the buddy.

 *   Subblocks within a Linux memory block might either be plugged or unplugged.

 *   Memory is added/removed to Linux MM in Linux memory block granularity.

 *

 * * Big Block Mode (BBM): A Big Block (BB) spans 1..X Linux memory blocks.

 *   Memory is added/removed to Linux MM in Big Block granularity.

 *

 * The mode is determined automatically based on the Linux memory block size

 * and the device block size.

 *

 * User space / core MM (auto onlining) is responsible for onlining added

 * Linux memory blocks - and for selecting a zone. Linux Memory Blocks are

 * always onlined separately, and all memory within a Linux memory block is

 * onlined to the same zone - virtio-mem relies on this behavior.

/*

 * State of a Linux memory block in SBM.

 Unplugged, not added to Linux. Can be reused later. */

 (Partially) plugged, not added to Linux. Error on add_memory(). */

 Fully plugged, fully added to Linux, offline. */

 Partially plugged, fully added to Linux, offline. */

 Fully plugged, fully added to Linux, onlined to a kernel zone. */

 Partially plugged, fully added to Linux, online to a kernel zone */

 Fully plugged, fully added to Linux, onlined to ZONE_MOVABLE. */

 Partially plugged, fully added to Linux, onlined to ZONE_MOVABLE. */

/*

 * State of a Big Block (BB) in BBM, covering 1..X Linux memory blocks.

 Unplugged, not added to Linux. Can be reused later. */

 Plugged, not added to Linux. Error on add_memory(). */

 Plugged and added to Linux. */

 All online parts are fake-offline, ready to remove. */

 We might first have to unplug all memory when starting up. */

 Workqueue that processes the plug/unplug requests. */

 Virtqueue for guest->host requests. */

 Wait for a host response to a guest request. */

 Space for one guest request and the host response. */

 The current size of the device. */

 The requested size of the device. */

 The device block size (for communicating with the device). */

 The determined node id for all memory of the device. */

 Physical start address of the memory region. */

 Maximum region size in bytes. */

 The parent resource for all memory added via this device. */

	/*

	 * Copy of "System RAM (virtio_mem)" to be used for

	 * add_memory_driver_managed().

 Memory group identification. */

	/*

	 * We don't want to add too much memory if it's not getting onlined,

	 * to avoid running OOM. Besides this threshold, we allow to have at

	 * least two offline blocks at a time (whatever is bigger).

 If set, the driver is in SBM, otherwise in BBM. */

 Id of the first memory block of this device. */

 Id of the last usable memory block of this device. */

 Id of the next memory bock to prepare when needed. */

 The subblock size. */

 The number of subblocks per Linux memory block. */

 Summary of all memory block states. */

			/*

			 * One byte state per memory block. Allocated via

			 * vmalloc(). Resized (alloc+copy+free) on demand.

			 *

			 * With 128 MiB memory blocks, we have states for 512

			 * GiB of memory in one 4 KiB page.

			/*

			 * Bitmap: one bit per subblock. Allocated similar to

			 * sbm.mb_states.

			 *

			 * A set bit means the corresponding subblock is

			 * plugged, otherwise it's unblocked.

			 *

			 * With 4 MiB subblocks, we manage 128 GiB of memory

			 * in one 4 KiB page.

 Id of the first big block of this device. */

 Id of the last usable big block of this device. */

 Id of the next device bock to prepare when needed. */

 Summary of all big block states. */

 One byte state per big block. See sbm.mb_states. */

 The block size used for plugging/adding/removing. */

	/*

	 * Mutex that protects the sbm.mb_count, sbm.mb_states,

	 * sbm.sb_states, bbm.bb_count, and bbm.bb_states

	 *

	 * When this lock is held the pointers can't change, ONLINE and

	 * OFFLINE blocks can't change the state and no subblocks will get

	 * plugged/unplugged.

	 *

	 * In kdump mode, used to serialize requests, last_block_addr and

	 * last_block_plugged.

 An error occurred we cannot handle - stop processing requests. */

 Cached valued of is_kdump_kernel() when the device was probed. */

 The driver is being removed. */

 Timer for retrying to plug/unplug memory. */

 Memory notifier (online/offline events). */

 vmcore callback for /proc/vmcore handling in kdump mode */

 CONFIG_PROC_VMCORE */

 Next device in the list of virtio-mem devices. */

/*

 * We have to share a single online_page callback among all virtio-mem

 * devices. We use RCU to iterate the list in the callback.

/*

 * Register a virtio-mem device so it will be considered for the online_page

 * callback.

 First device registers the callback. */

/*

 * Unregister a virtio-mem device so it will no longer be considered for the

 * online_page callback.

 Last device unregisters the callback. */

/*

 * Calculate the memory block id of a given address.

/*

 * Calculate the physical start address of a given memory block id.

/*

 * Calculate the big block id of a given address.

/*

 * Calculate the physical start address of a given big block id.

/*

 * Calculate the subblock id of a given address.

/*

 * Set the state of a big block, taking care of the state counter.

/*

 * Get the state of a big block.

/*

 * Prepare the big block state array for the next big block.

/*

 * Set the state of a memory block, taking care of the state counter.

/*

 * Get the state of a memory block.

/*

 * Prepare the state array for the next memory block.

/*

 * Calculate the bit number in the subblock bitmap for the given subblock

 * inside the given memory block.

/*

 * Mark all selected subblocks plugged.

 *

 * Will not modify the state of the memory block.

/*

 * Mark all selected subblocks unplugged.

 *

 * Will not modify the state of the memory block.

/*

 * Test if all selected subblocks are plugged.

 TODO: Helper similar to bitmap_set() */

/*

 * Test if all selected subblocks are unplugged.

 TODO: Helper similar to bitmap_set() */

/*

 * Find the first unplugged subblock. Returns vm->sbm.sbs_per_mb in case there is

 * none.

/*

 * Prepare the subblock bitmap for the next memory block.

/*

 * Test if we could add memory without creating too much offline memory -

 * to avoid running OOM if memory is getting onlined deferred.

/*

 * Try adding memory to Linux. Will usually only fail if out of memory.

 *

 * Must not be called with the vm->hotplug_mutex held (possible deadlock with

 * onlining code).

 *

 * Will not modify the state of memory blocks in virtio-mem.

	/*

	 * When force-unloading the driver and we still have memory added to

	 * Linux, the resource name has to stay.

 Memory might get onlined immediately. */

		/*

		 * TODO: Linux MM does not properly clean up yet in all cases

		 * where adding of memory failed - especially on -ENOMEM.

/*

 * See virtio_mem_add_memory(): Try adding a single Linux memory block.

/*

 * See virtio_mem_add_memory(): Try adding a big block.

/*

 * Try removing memory from Linux. Will only fail if memory blocks aren't

 * offline.

 *

 * Must not be called with the vm->hotplug_mutex held (possible deadlock with

 * onlining code).

 *

 * Will not modify the state of memory blocks in virtio-mem.

		/*

		 * We might have freed up memory we can now unplug, retry

		 * immediately instead of waiting.

/*

 * See virtio_mem_remove_memory(): Try removing a single Linux memory block.

/*

 * Try offlining and removing memory from Linux.

 *

 * Must not be called with the vm->hotplug_mutex held (possible deadlock with

 * onlining code).

 *

 * Will not modify the state of memory blocks in virtio-mem.

		/*

		 * We might have freed up memory we can now unplug, retry

		 * immediately instead of waiting.

/*

 * See virtio_mem_offline_and_remove_memory(): Try offlining and removing

 * a single Linux memory block.

/*

 * See virtio_mem_offline_and_remove_memory(): Try to offline and remove a

 * all Linux memory blocks covered by the big block.

/*

 * Trigger the workqueue so the device can perform its magic.

/*

 * Test if a virtio-mem device overlaps with the given range. Can be called

 * from (notifier) callbacks lockless.

/*

 * Test if a virtio-mem device contains a given range. Can be called from

 * (notifier) callbacks lockless.

	/*

	 * When marked as "fake-offline", all online memory of this device block

	 * is allocated by us. Otherwise, we don't have any memory allocated.

/*

 * This callback will either be called synchronously from add_memory() or

 * asynchronously (e.g., triggered via user space). We have to be careful

 * with locking when calling add_memory().

		/*

		 * In SBM, we add memory in separate memory blocks - we expect

		 * it to be onlined/offlined in the same granularity. Bail out

		 * if this ever changes.

		/*

		 * In BBM, we only care about onlining/offlining happening

		 * within a single big block, we don't care about the

		 * actual granularity as we don't track individual Linux

		 * memory blocks.

	/*

	 * Avoid circular locking lockdep warnings. We lock the mutex

	 * e.g., in MEM_GOING_ONLINE and unlock it in MEM_ONLINE. The

	 * blocking_notifier_call_chain() has it's own lock, which gets unlocked

	 * between both notifier calls and will bail out. False positive.

		/*

		 * Trigger the workqueue. Now that we have some offline memory,

		 * maybe we can handle pending unplug requests.

		/*

		 * Start adding more memory once we onlined half of our

		 * threshold. Don't trigger if it's possibly due to our actipn

		 * (e.g., us adding memory which gets onlined immediately from

		 * the core).

/*

 * Set a range of pages PG_offline. Remember pages that were never onlined

 * (via generic_online_page()) using PageDirty().

 FIXME: remove after cleanups */

/*

 * Clear PG_offline from a range of pages. If the pages were never onlined,

 * (via generic_online_page()), clear PageDirty().

/*

 * Release a range of fake-offline pages to the buddy, effectively

 * fake-onlining them.

	/*

	 * We are always called at least with MAX_ORDER_NR_PAGES

	 * granularity/alignment (e.g., the way subblocks work). All pages

	 * inside such a block are alike.

		/*

		 * If the page is PageDirty(), it was kept fake-offline when

		 * onlining the memory block. Otherwise, it was allocated

		 * using alloc_contig_range(). All pages in a subblock are

		 * alike.

/*

 * Try to allocate a range, marking pages fake-offline, effectively

 * fake-offlining them.

	/*

	 * TODO: We want an alloc_contig_range() mode that tries to allocate

	 * harder (e.g., dealing with temporarily pinned pages, PCP), especially

	 * with ZONE_MOVABLE. So for now, retry a couple of times with

	 * ZONE_MOVABLE before giving up - because that zone is supposed to give

	 * some guarantees.

 whoops, out of memory */

/*

 * Handle fake-offline pages when memory is going offline - such that the

 * pages can be skipped by mm-core when offlining.

	/*

	 * Drop our reference to the pages so the memory can get offlined

	 * and add the unplugged pages to the managed page counters (so

	 * offlining code can correctly subtract them again).

 Drop our reference to the pages so the memory can get offlined. */

/*

 * Handle fake-offline pages when memory offlining is canceled - to undo

 * what we did in virtio_mem_fake_offline_going_offline().

	/*

	 * Get the reference we dropped when going offline and subtract the

	 * unplugged pages from the managed page counters.

			/*

			 * We exploit here that subblocks have at least

			 * MAX_ORDER_NR_PAGES size/alignment - so we cannot

			 * cross subblocks within one call.

			/*

			 * If the whole block is marked fake offline, keep

			 * everything that way.

		/*

		 * virtio_mem_set_fake_offline() might sleep, we don't need

		 * the device anymore. See virtio_mem_remove() how races

		 * between memory onlining and device removal are handled.

 not virtio-mem memory, but e.g., a DIMM. online it */

 don't use the request residing on the stack (vaddr) */

 out: buffer for request */

 in: buffer for response */

 wait for a response */

 usable region might have shrunk */

/*

 * Plug selected subblocks. Updates the plugged state, but not the state

 * of the memory block.

/*

 * Unplug selected subblocks. Updates the plugged state, but not the state

 * of the memory block.

/*

 * Request to unplug a big block.

 *

 * Will not modify the state of the big block.

/*

 * Request to plug a big block.

 *

 * Will not modify the state of the big block.

/*

 * Unplug the desired number of plugged subblocks of a offline or not-added

 * memory block. Will fail if any subblock cannot get unplugged (instead of

 * skipping it).

 *

 * Will not modify the state of the memory block.

 *

 * Note: can fail after some subblocks were unplugged.

 Find the next candidate subblock */

 Try to unplug multiple subblocks at a time */

/*

 * Unplug all plugged subblocks of an offline or not-added memory block.

 *

 * Will not modify the state of the memory block.

 *

 * Note: can fail after some subblocks were unplugged.

/*

 * Prepare tracking data for the next memory block.

 Resize the state array if required. */

 Resize the subblock bitmap if required. */

/*

 * Try to plug the desired number of subblocks and add the memory block

 * to Linux.

 *

 * Will modify the state of the memory block.

	/*

	 * Plug the requested number of subblocks before adding it to linux,

	 * so that onlining will directly online all plugged subblocks.

	/*

	 * Mark the block properly offline before adding it to Linux,

	 * so the memory notifiers will find the block in the right state.

 Add the memory block to linux - if that fails, try to unplug. */

/*

 * Try to plug the desired number of subblocks of a memory block that

 * is already added to Linux.

 *

 * Will modify the state of the memory block.

 *

 * Note: Can fail after some subblocks were successfully plugged.

 fake-online the pages if the memory block is online */

 Don't race with onlining/offlining */

	/*

	 * We won't be working on online/offline memory blocks from this point,

	 * so we can't race with memory onlining/offlining. Drop the mutex.

 Try to plug and add unused blocks */

 Try to prepare, plug and add new blocks */

/*

 * Plug a big block and add it to Linux.

 *

 * Will modify the state of the big block.

 Retry from the main loop. */

/*

 * Prepare tracking data for the next big block.

 Resize the big block state array if required. */

 Try to plug and add unused big blocks */

 Try to prepare, plug and add new big blocks */

/*

 * Try to plug the requested amount of memory.

/*

 * Unplug the desired number of plugged subblocks of an offline memory block.

 * Will fail if any subblock cannot get unplugged (instead of skipping it).

 *

 * Will modify the state of the memory block. Might temporarily drop the

 * hotplug_mutex.

 *

 * Note: Can fail after some subblocks were successfully unplugged.

 some subblocks might have been unplugged even on failure */

		/*

		 * Remove the block from Linux - this should never fail.

		 * Hinder the block from getting onlined by marking it

		 * unplugged. Temporarily drop the mutex, so

		 * any pending GOING_ONLINE requests can be serviced/rejected.

/*

 * Unplug the given plugged subblocks of an online memory block.

 *

 * Will modify the state of the memory block.

 Try to unplug the allocated memory */

 Return the memory to the buddy. */

/*

 * Unplug the desired number of plugged subblocks of an online memory block.

 * Will skip subblock that are busy.

 *

 * Will modify the state of the memory block. Might temporarily drop the

 * hotplug_mutex.

 *

 * Note: Can fail after some subblocks were successfully unplugged. Can

 *       return 0 even if subblocks were busy and could not get unplugged.

 If possible, try to unplug the complete block in one shot. */

 Fallback to single subblocks. */

 Find the next candidate subblock */

	/*

	 * Once all subblocks of a memory block were unplugged, offline and

	 * remove it. This will usually not fail, as no memory is in use

	 * anymore - however some other notifiers might NACK the request.

/*

 * Unplug the desired number of plugged subblocks of a memory block that is

 * already added to Linux. Will skip subblock of online memory blocks that are

 * busy (by the OS). Will fail if any subblock that's not busy cannot get

 * unplugged.

 *

 * Will modify the state of the memory block. Might temporarily drop the

 * hotplug_mutex.

 *

 * Note: Can fail after some subblocks were successfully unplugged. Can

 *       return 0 even if subblocks were busy and could not get unplugged.

	/*

	 * We'll drop the mutex a couple of times when it is safe to do so.

	 * This might result in some blocks switching the state (online/offline)

	 * and we could miss them in this run - we will retry again later.

	/*

	 * We try unplug from partially plugged blocks first, to try removing

	 * whole memory blocks along with metadata. We prioritize ZONE_MOVABLE

	 * as it's more reliable to unplug memory and remove whole memory

	 * blocks, and we don't want to trigger a zone imbalances by

	 * accidentially removing too much kernel memory.

/*

 * Try to offline and remove a big block from Linux and unplug it. Will fail

 * with -EBUSY if some memory is busy and cannot get unplugged.

 *

 * Will modify the state of the memory block. Might temporarily drop the

 * hotplug_mutex.

		/*

		 * Start by fake-offlining all memory. Once we marked the device

		 * block as fake-offline, all newly onlined memory will

		 * automatically be kept fake-offline. Protect from concurrent

		 * onlining/offlining until we have a consistent state.

/*

 * Test if a big block is completely offline.

/*

 * Test if a big block is completely onlined to ZONE_MOVABLE (or offline).

	/*

	 * Try to unplug big blocks. Similar to SBM, start with offline

	 * big blocks.

			/*

			 * As we're holding no locks, these checks are racy,

			 * but we don't care.

/*

 * Try to unplug the requested amount of memory.

/*

 * Try to unplug all blocks that couldn't be unplugged before, for example,

 * because the hypervisor was busy.

/*

 * Update all parts of the config that could have changed.

 the plugged_size is just a reflection of what _we_ did previously */

 calculate the last usable memory block id */

	/*

	 * If we cannot plug any of our device memory (e.g., nothing in the

	 * usable region is addressable), the last usable memory block id will

	 * be smaller than the first usable memory block id. We'll stop

	 * attempting to add memory with -ENOSPC from our main loop.

 see if there is a request to change the size */

/*

 * Workqueue function for handling plug/unplug requests and config updates.

 Make sure we start with a clean state if there are leftovers. */

 Unplug any leftovers from previous runs */

		/*

		 * We cannot add any more memory (alignment, physical limit)

		 * or we have too many offline memory blocks.

		/*

		 * The hypervisor cannot process our request right now

		 * (e.g., out of memory, migrating);

		/*

		 * We cannot free up any memory to unplug it (all plugged memory

		 * is busy).

 Out of memory, try again later. */

 Retry immediately (e.g., the config changed). */

 Unknown error, mark as broken */

 bad device setup - warn only */

 Prepare the offline threshold - make sure we can add two blocks. */

	/*

	 * We want subblocks to span at least MAX_ORDER_NR_PAGES and

	 * pageblock_nr_pages pages. This:

	 * - Simplifies our page onlining code (virtio_mem_online_page_cb)

	 *   and fake page onlining code (virtio_mem_fake_online).

	 * - Is required for now for alloc_contig_range() to work reliably -

	 *   it doesn't properly handle smaller granularity on ZONE_NORMAL.

 SBM: At least two subblocks per Linux memory block. */

 Round up to the next full memory block */

 BBM: At least one Linux memory block. */

 Round up to the next aligned big block */

 Make sure we can add two big blocks. */

 create the parent resource for all memory */

 use a single dynamic memory group to cover the whole memory device */

	/*

	 * If we still have memory plugged, we have to unplug all memory first.

	 * Registering our parent resource makes sure that this memory isn't

	 * actually in use (e.g., trying to reload the driver).

 register callbacks */

	/*

	 * We have to serialize device requests and access to the information

	 * about the block queried last.

 On any kind of error, we're going to signal !ram. */

 CONFIG_PROC_VMCORE */

 CONFIG_PROC_VMCORE */

 CONFIG_PROC_VMCORE */

 Fetch all properties that can't change. */

 Determine the nid for the device based on the lowest address. */

	/*

	 * We don't want to (un)plug or reuse any memory when in kdump. The

	 * memory is still accessible (but not exposed to Linux).

	/*

	 * When force-unloading the driver and removing the device, we

	 * could have a garbage pointer. Duplicate the string.

 Disallow mapping device memory via /dev/mem completely. */

 The memory is not actually busy - make add_memory() work. */

 register the virtqueue */

 initialize the device by querying the config */

 trigger a config update to start processing the requested_size */

	/*

	 * Make sure the workqueue won't be triggered anymore and no memory

	 * blocks can be onlined/offlined until we're finished here.

 wait until the workqueue stopped */

		/*

		 * After we unregistered our callbacks, user space can online

		 * partially plugged offline blocks. Make sure to remove them.

		/*

		 * After we unregistered our callbacks, user space can no longer

		 * offline partially plugged online memory blocks. No need to

		 * worry about them.

 unregister callbacks */

	/*

	 * There is no way we could reliably remove all memory we have added to

	 * the system. And there is no way to stop the driver/device from going

	 * away. Warn at least.

 remove all tracking data - no locking needed */

 CONFIG_PROC_VMCORE */

 reset the device and cleanup the queues */

	/*

	 * When restarting the VM, all memory is usually unplugged. Don't

	 * allow to suspend/hibernate.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio PCI driver - legacy device support

 *

 * This module allows virtio devices to be used over a virtual PCI device.

 * This can be used with QEMU based VMMs like KVM or Xen.

 *

 * Copyright IBM Corp. 2007

 * Copyright Red Hat, Inc. 2014

 *

 * Authors:

 *  Anthony Liguori  <aliguori@us.ibm.com>

 *  Rusty Russell <rusty@rustcorp.com.au>

 *  Michael S. Tsirkin <mst@redhat.com>

 virtio config->get_features() implementation */

	/* When someone needs more than 32 feature bits, we'll need to

 virtio config->finalize_features() implementation */

 Give virtio_ring a chance to accept features. */

 Make sure we don't have any features > 32 bits! */

 We only support 32 feature bits. */

 virtio config->get() implementation */

/* the config->set() implementation.  it's symmetric to the config->get()

 config->{get,set}_status() implementations */

 We should never be setting status to 0. */

 0 status means a reset. */

	/* Flush out the status write, and flush in device writes,

 Disable VQ/configuration callbacks. */

 Check if queue is either not available or already active. */

 create the vring */

 activate the queue */

 Flush the write out to device */

 Select and deactivate the queue */

 the PCI probing function */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio PCI driver - modern (virtio 1.0) device support

 *

 * This module allows virtio devices to be used over a virtual PCI device.

 * This can be used with QEMU based VMMs like KVM or Xen.

 *

 * Copyright IBM Corp. 2007

 * Copyright Red Hat, Inc. 2014

 *

 * Authors:

 *  Anthony Liguori  <aliguori@us.ibm.com>

 *  Rusty Russell <rusty@rustcorp.com.au>

 *  Michael S. Tsirkin <mst@redhat.com>

 virtio config->finalize_features() implementation */

 Give virtio_ring a chance to accept features. */

 Give virtio_pci a chance to accept features. */

 virtio config->get() implementation */

/* the config->set() implementation.  it's symmetric to the config->get()

 config->{get,set}_status() implementations */

 We should never be setting status to 0. */

 0 status means a reset. */

	/* After writing 0 to device_status, the driver MUST wait for a read of

	 * device_status to return 0 before reinitializing the device.

	 * This will flush out the status write, and flush in device writes,

	 * including MSI-X interrupts, if any.

 Disable VQ/configuration callbacks. */

 Check if queue is either not available or already active. */

 create the vring */

 activate the queue */

	/* Select and activate all queues. Has to be done last: once we do

	 * this, there's no way to go back except reset.

 Type, and ID match, looks good */

 Read the lower 32bit of length and offset */

 and now the top half */

 the PCI probing function */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dma-bufs for virtio exported objects

 *

 * Copyright (C) 2020 Google, Inc.

/**

 * virtio_dma_buf_export - Creates a new dma-buf for a virtio exported object

 * @exp_info: [in] see dma_buf_export(). ops MUST refer to a dma_buf_ops

 *	struct embedded in a virtio_dma_buf_ops.

 *

 * This wraps dma_buf_export() to allow virtio drivers to create a dma-buf

 * for an virtio exported object that can be queried by other virtio drivers

 * for the object's UUID.

/**

 * virtio_dma_buf_attach - mandatory attach callback for virtio dma-bufs

/**

 * is_virtio_dma_buf - returns true if the given dma-buf is a virtio dma-buf

 * @dma_buf: buffer to query

/**

 * virtio_dma_buf_get_uuid - gets a virtio dma-buf's exported object's uuid

 * @dma_buf: [in] buffer to query

 * @uuid: [out] the uuid

 *

 * Returns: 0 on success, negative on failure.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VIRTIO based driver for vDPA device

 *

 * Copyright (c) 2020, Red Hat. All rights reserved.

 *     Author: Jason Wang <jasowang@redhat.com>

 *

 The lock to protect virtqueue list */

 List of virtio_vdpa_vq_info */

 the actual virtqueue */

 the list node for the virtqueues list */

 Assume split virtqueue, switch to packed if necessary */

 Queue shouldn't already be set up. */

 Allocate and fill out our active queue description */

 Create the vring */

 Setup virtqueue callback */

 reset virtqueue state index */

 VDPA driver should make sure vq is stopeed here */

 Select and deactivate the queue (best effort) */

 Give virtio_ring a chance to accept features. */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Virtio ring implementation.

 *

 *  Copyright 2007 Rusty Russell IBM Corporation

 For development, we want to crash whenever the ring is screwed. */

 Caller is supposed to guarantee no reentry. */

 No kick or get, with .1 second between?  Warn. */ \

 Data for callback. */

 Indirect descriptor, if any. */

 Data for callback. */

 Indirect descriptor, if any. */

 Descriptor list length. */

 The last desc state in a list. */

 Descriptor DMA addr. */

 Descriptor length. */

 Descriptor flags. */

 The next desc state in a list. */

 Is this a packed ring? */

 Is DMA API used? */

 Can we use weak barriers? */

 Other side has made a mess, don't try any more. */

 Host supports indirect buffers */

 Host publishes avail event idx */

 Head of free buffer list. */

 Number we've added since last sync. */

 Last used index we've seen. */

 Hint for event idx: already triggered no need to disable. */

 Available for split ring */

 Actual memory layout for this queue. */

 Last written value to avail->flags */

			/*

			 * Last written value to avail->idx in

			 * guest byte order.

 Per-descriptor state. */

 DMA address and size information */

 Available for packed ring */

 Actual memory layout for this queue. */

 Driver ring wrap counter. */

 Device ring wrap counter. */

 Avail used flags. */

 Index of the next avail descriptor. */

			/*

			 * Last written value to driver->flags in

			 * guest byte order.

 Per-descriptor state. */

 DMA address and size information */

 Per-descriptor in buffer length */

 How to notify other side. FIXME: commonalize hcalls! */

 DMA, allocation, and size information */

 They're supposed to lock for us. */

 Figure out if their kicks are too delayed. */

/*

 * Helpers.

	/*

	 * If the host supports indirect descriptor tables, and we have multiple

	 * buffers, then go indirect. FIXME: tune this threshold

/*

 * Modern virtio devices have feature bits to specify whether they need a

 * quirk and bypass the IOMMU. If not there, just use the DMA API.

 *

 * If there, the interaction between virtio and DMA API is messy.

 *

 * On most systems with virtio, physical addresses match bus addresses,

 * and it doesn't particularly matter whether we use the DMA API.

 *

 * On some systems, including Xen and any system with a physical device

 * that speaks virtio behind a physical IOMMU, we must use the DMA API

 * for virtio DMA to work at all.

 *

 * On other systems, including SPARC and PPC64, virtio-pci devices are

 * enumerated as though they are behind an IOMMU, but the virtio host

 * ignores the IOMMU, so we must either pretend that the IOMMU isn't

 * there or somehow map everything as the identity.

 *

 * For the time being, we preserve historic behavior and bypass the DMA

 * API.

 *

 * TODO: install a per-device DMA ops structure that does the right thing

 * taking into account all the above quirks, and use the DMA API

 * unconditionally on data path.

 Otherwise, we are left to guess. */

	/*

	 * In theory, it's possible to have a buggy QEMU-supposed

	 * emulated Q35 IOMMU and Xen enabled at the same time.  On

	 * such a configuration, virtio has never worked and will

	 * not work without an even larger kludge.  Instead, enable

	 * the DMA API if we're a Xen guest, which at least allows

	 * all of the sensible Xen configurations to work correctly.

			/*

			 * Sanity check: make sure we dind't truncate

			 * the address.  The only arches I can find that

			 * have 64-bit phys_addr_t but 32-bit dma_addr_t

			 * are certain non-highmem MIPS and x86

			 * configurations, but these configurations

			 * should never allocate physical pages above 32

			 * bits, so this is fine.  Just in case, throw a

			 * warning and abort if we end up with an

			 * unrepresentable address.

/*

 * The DMA ops on various arches are rather gnarly right now, and

 * making all of the arch DMA ops work on the vring device itself

 * is a mess.  For now, we use the parent device for DMA ops.

 Map one sg entry. */

	/*

	 * We can't use dma_map_sg, because we don't use scatterlists in

	 * the way it expects (we don't guarantee that the scatterlist

	 * will exist for the lifetime of the mapping).

/*

 * Split ring specific functions - *_split().

	/*

	 * We require lowmem mappings for the descriptors because

	 * otherwise virt_to_phys will give us bogus addresses in the

	 * virtqueue.

 Use a single buffer which doesn't continue */

 Set up rest to use this indirect table. */

		/* FIXME: for historical reasons, we force a notify here if

		 * there are outgoing parts to the buffer.  Presumably the

			/* Note that we trust indirect descriptor

			 * table since it use stream DMA mapping.

			/* Note that we trust indirect descriptor

			 * table since it use stream DMA mapping.

 Last one doesn't continue. */

 Now that the indirect table is filled in, map it. */

 We're using some buffers from the free list. */

 Update free pointer */

 Store token and indirect buffer state. */

 Store in buffer length if necessary */

	/* Put entry in available array (but don't update avail->idx until they

	/* Descriptors and available array need to be set before we expose the

	/* This is very unlikely, but theoretically possible.  Kick

	/* We need to expose available array entries before checking avail

 Clear data ptr. */

 Put back on free list: unmap first-level descriptors and find end */

 Plus final descriptor */

 Free the indirect table, if any, now that it's unmapped. */

 Only get used array entries after they have been exposed by host. */

 detach_buf_split clears data, so grab it now. */

	/* If we expect an interrupt for the next entry, tell host

	 * by writing event index and flush out the write before

 TODO: this is a hack. Figure out a cleaner value to write. */

	/* We optimistically turn back on interrupts, then check if there was

	/* Depending on the VIRTIO_RING_F_EVENT_IDX feature, we need to

	 * either clear the flags bit or point the event index at the next

	/* We optimistically turn back on interrupts, then check if there was

	/* Depending on the VIRTIO_RING_F_USED_EVENT_IDX feature, we need to

	 * either clear the flags bit or point the event index at the next

 TODO: tune this threshold */

 detach_buf_split clears data, so grab it now. */

 That should have freed everything. */

 We assume num is a power of 2. */

 TODO: allocate each queue chunk individually */

 Try to get a single page. You are my only hope! */

/*

 * Packed ring specific functions - *_packed().

	/*

	 * We require lowmem mappings for the descriptors because

	 * otherwise virt_to_phys will give us bogus addresses in the

	 * virtqueue.

 Now that the indirect table is filled in, map it. */

	/*

	 * A driver MUST NOT make the first descriptor in the list

	 * available before all subsequent descriptors comprising

	 * the list are made available.

 We're using some buffers from the free list. */

 Update free pointer */

 Store token and indirect buffer state. */

 Store in buffer length if necessary */

 fall back on direct */

 We're using some buffers from the free list. */

 Update free pointer */

 Store token. */

 Store in buffer length if necessary */

	/*

	 * A driver MUST NOT make the first descriptor in the list

	 * available before all subsequent descriptors comprising

	 * the list are made available.

	/*

	 * We need to expose the new flags value before checking notification

	 * suppressions.

 Clear data ptr. */

 Free the indirect table, if any, now that it's unmapped. */

 Only get used elements after they have been exposed by host. */

 detach_buf_packed clears data, so grab it now. */

	/*

	 * If we expect an interrupt for the next entry, tell host

	 * by writing event index and flush out the write before

	 * the read in the next get_buf call.

	/*

	 * We optimistically turn back on interrupts, then check if there was

	 * more to do.

		/*

		 * We need to update event offset and event wrap

		 * counter first before updating event flags.

	/*

	 * We optimistically turn back on interrupts, then check if there was

	 * more to do.

 TODO: tune this threshold */

		/*

		 * We need to update event offset and event wrap

		 * counter first before updating event flags.

	/*

	 * We need to update event suppression structure first

	 * before re-checking for more used buffers.

 detach_buf clears data, so grab it now. */

 That should have freed everything. */

 Put everything in free lists. */

 No callback?  Tell other side not to bother us. */

/*

 * Generic functions and exported symbols.

/**

 * virtqueue_add_sgs - expose buffers to other end

 * @_vq: the struct virtqueue we're talking about.

 * @sgs: array of terminated scatterlists.

 * @out_sgs: the number of scatterlists readable by other side

 * @in_sgs: the number of scatterlists which are writable (after readable ones)

 * @data: the token identifying the buffer.

 * @gfp: how to do memory allocations (if necessary).

 *

 * Caller must ensure we don't call this with other virtqueue operations

 * at the same time (except where noted).

 *

 * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).

 Count them first. */

/**

 * virtqueue_add_outbuf - expose output buffers to other end

 * @vq: the struct virtqueue we're talking about.

 * @sg: scatterlist (must be well-formed and terminated!)

 * @num: the number of entries in @sg readable by other side

 * @data: the token identifying the buffer.

 * @gfp: how to do memory allocations (if necessary).

 *

 * Caller must ensure we don't call this with other virtqueue operations

 * at the same time (except where noted).

 *

 * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).

/**

 * virtqueue_add_inbuf - expose input buffers to other end

 * @vq: the struct virtqueue we're talking about.

 * @sg: scatterlist (must be well-formed and terminated!)

 * @num: the number of entries in @sg writable by other side

 * @data: the token identifying the buffer.

 * @gfp: how to do memory allocations (if necessary).

 *

 * Caller must ensure we don't call this with other virtqueue operations

 * at the same time (except where noted).

 *

 * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).

/**

 * virtqueue_add_inbuf_ctx - expose input buffers to other end

 * @vq: the struct virtqueue we're talking about.

 * @sg: scatterlist (must be well-formed and terminated!)

 * @num: the number of entries in @sg writable by other side

 * @data: the token identifying the buffer.

 * @ctx: extra context for the token

 * @gfp: how to do memory allocations (if necessary).

 *

 * Caller must ensure we don't call this with other virtqueue operations

 * at the same time (except where noted).

 *

 * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).

/**

 * virtqueue_kick_prepare - first half of split virtqueue_kick call.

 * @_vq: the struct virtqueue

 *

 * Instead of virtqueue_kick(), you can do:

 *	if (virtqueue_kick_prepare(vq))

 *		virtqueue_notify(vq);

 *

 * This is sometimes useful because the virtqueue_kick_prepare() needs

 * to be serialized, but the actual virtqueue_notify() call does not.

/**

 * virtqueue_notify - second half of split virtqueue_kick call.

 * @_vq: the struct virtqueue

 *

 * This does not need to be serialized.

 *

 * Returns false if host notify failed or queue is broken, otherwise true.

 Prod other side to tell it about changes. */

/**

 * virtqueue_kick - update after add_buf

 * @vq: the struct virtqueue

 *

 * After one or more virtqueue_add_* calls, invoke this to kick

 * the other side.

 *

 * Caller must ensure we don't call this with other virtqueue

 * operations at the same time (except where noted).

 *

 * Returns false if kick failed, otherwise true.

/**

 * virtqueue_get_buf_ctx - get the next used buffer

 * @_vq: the struct virtqueue we're talking about.

 * @len: the length written into the buffer

 * @ctx: extra context for the token

 *

 * If the device wrote data into the buffer, @len will be set to the

 * amount written.  This means you don't need to clear the buffer

 * beforehand to ensure there's no data leakage in the case of short

 * writes.

 *

 * Caller must ensure we don't call this with other virtqueue

 * operations at the same time (except where noted).

 *

 * Returns NULL if there are no used buffers, or the "data" token

 * handed to virtqueue_add_*().

/**

 * virtqueue_disable_cb - disable callbacks

 * @_vq: the struct virtqueue we're talking about.

 *

 * Note that this is not necessarily synchronous, hence unreliable and only

 * useful as an optimization.

 *

 * Unlike other operations, this need not be serialized.

	/* If device triggered an event already it won't trigger one again:

	 * no need to disable.

/**

 * virtqueue_enable_cb_prepare - restart callbacks after disable_cb

 * @_vq: the struct virtqueue we're talking about.

 *

 * This re-enables callbacks; it returns current queue state

 * in an opaque unsigned value. This value should be later tested by

 * virtqueue_poll, to detect a possible race between the driver checking for

 * more work, and enabling callbacks.

 *

 * Caller must ensure we don't call this with other virtqueue

 * operations at the same time (except where noted).

/**

 * virtqueue_poll - query pending used buffers

 * @_vq: the struct virtqueue we're talking about.

 * @last_used_idx: virtqueue state (from call to virtqueue_enable_cb_prepare).

 *

 * Returns "true" if there are pending used buffers in the queue.

 *

 * This does not need to be serialized.

/**

 * virtqueue_enable_cb - restart callbacks after disable_cb.

 * @_vq: the struct virtqueue we're talking about.

 *

 * This re-enables callbacks; it returns "false" if there are pending

 * buffers in the queue, to detect a possible race between the driver

 * checking for more work, and enabling callbacks.

 *

 * Caller must ensure we don't call this with other virtqueue

 * operations at the same time (except where noted).

/**

 * virtqueue_enable_cb_delayed - restart callbacks after disable_cb.

 * @_vq: the struct virtqueue we're talking about.

 *

 * This re-enables callbacks but hints to the other side to delay

 * interrupts until most of the available buffers have been processed;

 * it returns "false" if there are many pending buffers in the queue,

 * to detect a possible race between the driver checking for more work,

 * and enabling callbacks.

 *

 * Caller must ensure we don't call this with other virtqueue

 * operations at the same time (except where noted).

/**

 * virtqueue_detach_unused_buf - detach first unused buffer

 * @_vq: the struct virtqueue we're talking about.

 *

 * Returns NULL or the "data" token handed to virtqueue_add_*().

 * This is not valid on an active queue; it is useful only for device

 * shutdown.

 Just a hint for performance: so it's ok that this can be racy! */

 Only available for split ring */

 No callback?  Tell other side not to bother us. */

 Put everything in free lists. */

 Only available for split ring */

 Manipulates transport-specific feature bits. */

 We don't understand this bit. */

/**

 * virtqueue_get_vring_size - return the size of the virtqueue's vring

 * @_vq: the struct virtqueue containing the vring of interest.

 *

 * Returns the size of the vring.  This is mainly used for boasting to

 * userspace.  Unlike other operations, this need not be serialized.

/*

 * This should prevent the device from being used, allowing drivers to

 * recover.  You may need to grab appropriate locks to flush.

 Pairs with READ_ONCE() in virtqueue_is_broken(). */

 Only available for split ring */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * vp_legacy_probe: probe the legacy virtio pci device, note that the

 * caller is required to enable PCI device before calling this function.

 * @ldev: the legacy virtio-pci device

 *

 * Return 0 on succeed otherwise fail

 We only own devices >= 0x1000 and <= 0x103f: leave the rest. */

		/*

		 * The virtio ring base address is expressed as a 32-bit PFN,

		 * with a page size of 1 << VIRTIO_PCI_QUEUE_ADDR_SHIFT.

/*

 * vp_legacy_probe: remove and cleanup the legacy virtio pci device

 * @ldev: the legacy virtio-pci device

/*

 * vp_legacy_get_features - get features from device

 * @ldev: the legacy virtio-pci device

 *

 * Returns the features read from the device

/*

 * vp_legacy_get_driver_features - get driver features from device

 * @ldev: the legacy virtio-pci device

 *

 * Returns the driver features read from the device

/*

 * vp_legacy_set_features - set features to device

 * @ldev: the legacy virtio-pci device

 * @features: the features set to device

/*

 * vp_legacy_get_status - get the device status

 * @ldev: the legacy virtio-pci device

 *

 * Returns the status read from device

/*

 * vp_legacy_set_status - set status to device

 * @ldev: the legacy virtio-pci device

 * @status: the status set to device

/*

 * vp_legacy_queue_vector - set the MSIX vector for a specific virtqueue

 * @ldev: the legacy virtio-pci device

 * @index: queue index

 * @vector: the config vector

 *

 * Returns the config vector read from the device

 Flush the write out to device */

/*

 * vp_legacy_config_vector - set the vector for config interrupt

 * @ldev: the legacy virtio-pci device

 * @vector: the config vector

 *

 * Returns the config vector read from the device

 Setup the vector used for configuration events */

 Verify we had enough resources to assign the vector */

 Will also flush the write out to device */

/*

 * vp_legacy_set_queue_address - set the virtqueue address

 * @ldev: the legacy virtio-pci device

 * @index: the queue index

 * @queue_pfn: pfn of the virtqueue

/*

 * vp_legacy_get_queue_enable - enable a virtqueue

 * @ldev: the legacy virtio-pci device

 * @index: the queue index

 *

 * Returns whether a virtqueue is enabled or not

/*

 * vp_legacy_get_queue_size - get size for a virtqueue

 * @ldev: the legacy virtio-pci device

 * @index: the queue index

 *

 * Returns the size of the virtqueue

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * vp_modern_map_capability - map a part of virtio pci capability

 * @mdev: the modern virtio-pci device

 * @off: offset of the capability

 * @minlen: minimal length of the capability

 * @align: align requirement

 * @start: start from the capability

 * @size: map size

 * @len: the length that is actually mapped

 * @pa: physical address of the capability

 *

 * Returns the io address of for the part of the capability

/**

 * virtio_pci_find_capability - walk capabilities to find device info.

 * @dev: the pci device

 * @cfg_type: the VIRTIO_PCI_CAP_* value we seek

 * @ioresource_types: IORESOURCE_MEM and/or IORESOURCE_IO.

 * @bars: the bitmask of BARs

 *

 * Returns offset of the capability, or 0.

 Ignore structures with reserved BAR values */

 This is part of the ABI.  Don't screw with it. */

 Note: disk space was harmed in compilation of this function. */

/*

 * vp_modern_probe: probe the modern virtio pci device, note that the

 * caller is required to enable PCI device before calling this function.

 * @mdev: the modern virtio-pci device

 *

 * Return 0 on succeed otherwise fail

 We only own devices >= 0x1000 and <= 0x107f: leave the rest. */

		/* Transitional devices: use the PCI subsystem device id as

		 * virtio device id, same as legacy driver always did.

 Modern devices: simply use PCI device id, but start from 0x1040. */

 check for a common config: if not, use legacy mode (bar 0). */

 If common is there, these should be too... */

	/* Device capability is only mandatory for devices that have

	 * device-specific configuration.

 Read notify_off_multiplier from config space. */

 Read notify length and offset from config space. */

	/* We don't know how many VQs we'll map, ahead of the time.

	 * If notify length is small, map it all now.

	 * Otherwise, map each VQ individually later.

	/* Again, we don't know how much we should map, but PAGE_SIZE

	 * is more than enough for all existing devices.

/*

 * vp_modern_probe: remove and cleanup the modern virtio pci device

 * @mdev: the modern virtio-pci device

/*

 * vp_modern_get_features - get features from device

 * @mdev: the modern virtio-pci device

 *

 * Returns the features read from the device

/*

 * vp_modern_get_driver_features - get driver features from device

 * @mdev: the modern virtio-pci device

 *

 * Returns the driver features read from the device

/*

 * vp_modern_set_features - set features to device

 * @mdev: the modern virtio-pci device

 * @features: the features set to device

/*

 * vp_modern_generation - get the device genreation

 * @mdev: the modern virtio-pci device

 *

 * Returns the genreation read from device

/*

 * vp_modern_get_status - get the device status

 * @mdev: the modern virtio-pci device

 *

 * Returns the status read from device

/*

 * vp_modern_set_status - set status to device

 * @mdev: the modern virtio-pci device

 * @status: the status set to device

/*

 * vp_modern_queue_vector - set the MSIX vector for a specific virtqueue

 * @mdev: the modern virtio-pci device

 * @index: queue index

 * @vector: the config vector

 *

 * Returns the config vector read from the device

 Flush the write out to device */

/*

 * vp_modern_config_vector - set the vector for config interrupt

 * @mdev: the modern virtio-pci device

 * @vector: the config vector

 *

 * Returns the config vector read from the device

 Setup the vector used for configuration events */

 Verify we had enough resources to assign the vector */

 Will also flush the write out to device */

/*

 * vp_modern_queue_address - set the virtqueue address

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 * @desc_addr: address of the descriptor area

 * @driver_addr: address of the driver area

 * @device_addr: address of the device area

/*

 * vp_modern_set_queue_enable - enable a virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 * @enable: whether the virtqueue is enable or not

/*

 * vp_modern_get_queue_enable - enable a virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 *

 * Returns whether a virtqueue is enabled or not

/*

 * vp_modern_set_queue_size - set size for a virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 * @size: the size of the virtqueue

/*

 * vp_modern_get_queue_size - get size for a virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 *

 * Returns the size of the virtqueue

/*

 * vp_modern_get_num_queues - get the number of virtqueues

 * @mdev: the modern virtio-pci device

 *

 * Returns the number of virtqueues

/*

 * vp_modern_get_queue_notify_off - get notification offset for a virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 *

 * Returns the notification offset for a virtqueue

/*

 * vp_modern_map_vq_notify - map notification area for a

 * specific virtqueue

 * @mdev: the modern virtio-pci device

 * @index: the queue index

 * @pa: the pointer to the physical address of the nofity area

 *

 * Returns the address of the notification area

 offset should not wrap */

 SPDX-License-Identifier: GPL-2.0-only

 Unique numbering for virtio devices. */

	/* We actually represent this as a bitstring, as it could be

/* This looks through all the IDs a driver claims to support.  If any of them

 We have a driver! */

 Figure out what features the device supports. */

 Figure out what features the driver supports. */

 Some drivers have a separate feature table for virtio v1.0 */

	/*

	 * Some devices detect legacy solely via F_VERSION_1. Write

	 * F_VERSION_1 to force LE config space accesses before FEATURES_OK for

	 * these when needed.

 Transport features always preserved to pass to finalize_features. */

 If probe didn't do it, mark device DRIVER_OK ourselves. */

 Driver should have reset device. */

 Acknowledge the device's existence again. */

 Catch this early. */

 There can be only 1 child node */

	/*

	 * On powerpc/pseries virtio devices are PCI devices so PCI

	 * vendor/device ids play the role of the "compatible" property.

	 * Simply don't init of_node in this case.

/**

 * register_virtio_device - register virtio device

 * @dev        : virtio device to be registered

 *

 * On error, the caller must call put_device on &@dev->dev (and not kfree),

 * as another code path may have obtained a reference to @dev.

 *

 * Returns: 0 on suceess, -error on failure

 Assign a unique device index and hence name. */

	/* We always start by resetting the device, in case a previous

 Acknowledge that we've seen the device. */

	/*

	 * device_add() causes the bus infrastructure to look for a matching

	 * driver.

 save for after device release */

	/* We always start by resetting the device, in case a previous

 Acknowledge that we've seen the device. */

	/* Maybe driver failed before freeze.

 We have a driver! */

 Finally, tell the device we're all set */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio balloon implementation, inspired by Dor Laor and Marcelo

 * Tosatti's implementations.

 *

 *  Copyright 2008 Rusty Russell IBM Corporation

/*

 * Balloon device works in 4K page units.  So each page is pointed to by

 * multiple balloon pages.  All memory counters in this driver are in balloon

 * page units.

 Maximum number of (4k) pages to deflate on OOM notifications. */

 The order of free page blocks to report to host */

 The size of a free page block in bytes */

 Balloon's own wq for cpu-intensive work items */

 The free page reporting work item submitted to the balloon wq */

 The balloon servicing is delegated to a freezable workqueue. */

 Prevent updating balloon when it is being canceled. */

 Bitmap to indicate if reading the related config fields are needed */

 The list of allocated free pages, waiting to be given back to mm */

 The number of free page blocks on the above list */

	/*

	 * The cmd id received from host.

	 * Read it via virtio_balloon_cmd_id_received to get the latest value

	 * sent from host.

 The cmd id that is actively in use */

 Buffer to store the stop sign */

 Waiting for host to ack the pages we released. */

 Number of balloon pages we've told the Host we're not using. */

	/*

	 * The pages we've told the Host we're not using are enqueued

	 * at vb_dev_info->pages list.

	 * Each page on this list adds VIRTIO_BALLOON_PAGES_PER_PAGE

	 * to num_pages above.

 Synchronize access/update to this struct virtio_balloon elements */

 The array of pfns we tell the Host about. */

 Memory statistics */

 Shrinker to return free pages - VIRTIO_BALLOON_F_FREE_PAGE_HINT */

 OOM notifier to deflate on OOM - VIRTIO_BALLOON_F_DEFLATE_ON_OOM */

 Free page reporting device */

 Convert pfn from Linux page size to balloon page size. */

 We should always be able to add one buffer to an empty queue. */

 When host has read buffer, this completes via balloon_ack */

 We should always be able to add these buffers to an empty queue. */

	/*

	 * In the extremely unlikely case that something has occurred and we

	 * are able to trigger an error we will simply display a warning

	 * and exit without actually processing the pages.

 When host has read buffer, this completes via balloon_ack */

	/*

	 * Set balloon pfns pointing at this page.

	 * Note that the first pfn points at start of the page.

 We can only do one array worth at a time. */

 Sleep for at least 1/5 of a second before retry. */

 Did we get any? */

 balloon reference */

 We can only do one array worth at a time. */

 We can't release more pages than taken */

	/*

	 * Note that if

	 * virtio_has_feature(vdev, VIRTIO_BALLOON_F_MUST_TELL_HOST);

	 * is true, we *have* to do it in this order

/*

 * While most virtqueues communicate guest-initiated requests to the hypervisor,

 * the stats queue operates in reverse.  The driver initializes the virtqueue

 * with a single buffer.  From that point forward, all conversations consist of

 * a hypervisor request (a call to this function) which directs us to refill

 * the virtqueue with a fresh stats buffer.  Since stats collection can sleep,

 * we delegate the job to a freezable workqueue that will do the actual work via

 * stats_handle_request().

 Legacy balloon config space is LE, unlike all other devices. */

 Gives back @num_to_return blocks of free pages to mm. */

 No need to queue the work if the bit was already set. */

 Legacy balloon config space is LE, unlike all other devices. */

	/*

	 * Inflateq and deflateq are used unconditionally. The names[]

	 * will be NULL if the related feature is not enabled, which will

	 * cause no allocation for the corresponding virtqueue in find_vqs.

		/*

		 * Prime this virtqueue with one buffer so the hypervisor can

		 * use it to signal us later (it can't be broken yet!).

 Legacy balloon config space is LE, unlike all other devices. */

 Detach all the used buffers from the vq */

 Detach all the used buffers from the vq */

 Detach all the used buffers from the vq */

	/*

	 * When the allocation returns NULL, it indicates that we have got all

	 * the possible free pages, so return -EINTR to stop.

 There is always 1 entry reserved for the cmd id to use. */

		/*

		 * The vq has no available entry to add this page block, so

		 * just free it.

		/*

		 * If a stop id or a new cmd id was just received from host,

		 * stop the reporting.

		/*

		 * The free page blocks are allocated and sent to host one by

		 * one.

 Start by sending the received cmd id to host with an outbuf. */

 End by sending a stop id to host with an outbuf. */

 Pass ULONG_MAX to give back all the free pages */

/*

 * virtballoon_migratepage - perform the balloon page migration on behalf of

 *			     a compaction thread.     (called under page lock)

 * @vb_dev_info: the balloon device

 * @newpage: page that will replace the isolated page after migration finishes.

 * @page   : the isolated (old) page that is about to be migrated to newpage.

 * @mode   : compaction mode -- not used for balloon page migration.

 *

 * After a ballooned page gets isolated by compaction procedures, this is the

 * function that performs the page migration on behalf of a compaction thread

 * The page migration for virtio balloon is done in a simple swap fashion which

 * follows these two macro steps:

 *  1) insert newpage into vb->pages list and update the host about it;

 *  2) update the host about the old page removed from vb->pages list;

 *

 * This function preforms the balloon page migration task.

 * Called through balloon_mapping->a_ops->migratepage

	/*

	 * In order to avoid lock contention while migrating pages concurrently

	 * to leak_balloon() or fill_balloon() we just give up the balloon_lock

	 * this turn, as it is easier to retry the page migration later.

	 * This also prevents fill_balloon() getting stuck into a mutex

	 * recursion in the case it ends up triggering memory compaction

	 * while it is attempting to inflate the ballon.

 balloon reference */

	/*

	  * When we migrate a page to a different zone and adjusted the

	  * managed page count when inflating, we have to fixup the count of

	  * both involved zones.

 balloon's page migration 1st step  -- inflate "newpage" */

 balloon's page migration 2nd step -- deflate "page" */

 balloon reference */

 CONFIG_BALLOON_COMPACTION */

		/*

		 * There is always one entry reserved for cmd id, so the ring

		 * size needs to be at least two to report free page hints.

		/*

		 * We're allowed to reuse any free pages, even if they are

		 * still to be processed by the host.

 Start with poison val of 0 representing general init */

		/*

		 * Let the hypervisor know that we are expecting a

		 * specific value to be written back in balloon pages.

		 *

		 * If the PAGE_POISON value was larger than a byte we would

		 * need to byte swap poison_val here to guarantee it is

		 * little-endian. However for now it is a single byte so we

		 * can pass it as-is.

		/*

		 * The default page reporting order is @pageblock_order, which

		 * corresponds to 512MB in size on ARM64 when 64KB base page

		 * size is used. The page reporting won't be triggered if the

		 * freeing page can't come up with a free area like that huge.

		 * So we specify the page reporting order to 5, corresponding

		 * to 2MB. It helps to avoid THP splitting if 4KB base page

		 * size is used by host.

		 *

		 * Ideally, the page reporting order is selected based on the

		 * host's base page size. However, it needs more work to report

		 * that value. The hard-coded order would be fine currently.

 There might be pages left in the balloon: free them. */

 There might be free pages that are being reported: release them. */

 Now we reset the device so we can clean up the queues. */

	/*

	 * The workqueue is already frozen by the PM core before this

	 * function is called.

	/*

	 * Inform the hypervisor that our pages are poisoned or

	 * initialized. If we cannot do that then we should disable

	 * page reporting as it could potentially change the contents

	 * of our free pages.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio memory mapped device driver

 *

 * Copyright 2011-2014, ARM Ltd.

 *

 * This module allows virtio devices to be used over a virtual, memory mapped

 * platform device.

 *

 * The guest device(s) may be instantiated in one of three equivalent ways:

 *

 * 1. Static platform device in board's code, eg.:

 *

 *	static struct platform_device v2m_virtio_device = {

 *		.name = "virtio-mmio",

 *		.id = -1,

 *		.num_resources = 2,

 *		.resource = (struct resource []) {

 *			{

 *				.start = 0x1001e000,

 *				.end = 0x1001e0ff,

 *				.flags = IORESOURCE_MEM,

 *			}, {

 *				.start = 42 + 32,

 *				.end = 42 + 32,

 *				.flags = IORESOURCE_IRQ,

 *			},

 *		}

 *	};

 *

 * 2. Device Tree node, eg.:

 *

 *		virtio_block@1e000 {

 *			compatible = "virtio,mmio";

 *			reg = <0x1e000 0x100>;

 *			interrupts = <42>;

 *		}

 *

 * 3. Kernel module (or command line) parameter. Can be used more than once -

 *    one device will be created for each one. Syntax:

 *

 *		[virtio_mmio.]device=<size>@<baseaddr>:<irq>[:<id>]

 *    where:

 *		<size>     := size (can use standard suffixes like K, M or G)

 *		<baseaddr> := physical base address

 *		<irq>      := interrupt number (as passed to request_irq())

 *		<id>       := (optional) platform device id

 *    eg.:

 *		virtio_mmio.device=0x100@0x100b0000:48 \

 *				virtio_mmio.device=1K@0x1001e000:74

 *

 * Based on Virtio PCI driver by Anthony Liguori, copyright IBM Corp. 2007

/* The alignment to use between consumer and producer parts of vring.

 a list of queues so we can dispatch IRQs */

 the actual virtqueue */

 the list node for the virtqueues list */

 Configuration interface */

 Give virtio_ring a chance to accept features. */

 Make sure there are no mixed devices */

 We should never be setting status to 0. */

 0 status means a reset. */

 Transport interface */

 the notify function used when creating a virt queue */

	/* We write the queue's selector into the notification register to

 Notify all virtqueues on an interrupt. */

 Read and acknowledge interrupts */

 Select and deactivate the queue */

 Select the queue we're interested in */

 Queue shouldn't already be set up. */

 Allocate and fill out our active queue description */

 Create the vring */

 Activate the queue */

		/*

		 * virtio-mmio v1 uses a 32bit QUEUE PFN. If we have something

		 * that doesn't fit in 32bit, fail the setup rather than

		 * pretending to be successful.

 Select the region we're interested in */

 Read the region size */

	/* Check if region length is -1. If that's the case, the shared memory

	 * region does not exist and there is no need to proceed further.

 Read the region base address */

 Platform device */

 Check magic value */

 Check device version */

		/*

		 * virtio-mmio device with an ID 0 is a (dummy) placeholder

		 * with no function. End probing now with no error reported.

		/*

		 * In the legacy case, ensure our coherently-allocated virtio

		 * ring will be at an address expressable as a 32-bit PFN.

 Devices list parameter */

 Consume "size" part of the command line parameter */

 Get "@<base>:<irq>[:<id>]" chunks */

	/*

	 * sscanf() must process at least 2 chunks; also there

	 * must be no extra characters after the last chunk, so

	 * str[consumed] must be '\0'

 Platform driver */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio PCI driver - common functionality for all device versions

 *

 * This module allows virtio devices to be used over a virtual PCI device.

 * This can be used with QEMU based VMMs like KVM or Xen.

 *

 * Copyright IBM Corp. 2007

 * Copyright Red Hat, Inc. 2014

 *

 * Authors:

 *  Anthony Liguori  <aliguori@us.ibm.com>

 *  Rusty Russell <rusty@rustcorp.com.au>

 *  Michael S. Tsirkin <mst@redhat.com>

 disable irq handlers */

		/*

		 * The below synchronize() guarantees that any

		 * interrupt for this line arriving after

		 * synchronize_irq() has completed is guaranteed to see

		 * intx_soft_enabled == false.

 enable irq handlers */

		/*

		 * The above disable_irq() provides TSO ordering and

		 * as such promotes the below store to store-release.

 the notify function used when creating a virt queue */

	/* we write the queue's selector into the notification register to

 Handle a configuration change: Tell driver if it wants to know. */

 Notify all virtqueues on an interrupt. */

/* A small wrapper to also acknowledge the interrupt when it's handled.

 * I really need an EIO hook for the vring so I can ack the interrupt once we

 * know that we'll be handling the IRQ but before we invoke the callback since

 * the callback may notify the host which results in the host attempting to

 * raise an interrupt that we would then mask once we acknowledged the

	/* reading the ISR has the effect of also clearing it so it's very

 It's definitely not us if the ISR was not high */

 Configuration change?  Tell driver if it wants to know. */

 virtio config vector */

 Set the vector used for configuration */

 Verify we had enough resources to assign the vector */

 Shared vector for all VQs */

 fill out our structure that represents an active queue */

 the config->del_vqs() implementation */

 Disable the vector used for configuration */

 Best option: one for change interrupt, one per vq. */

 Second best: one for change, shared for all vqs. */

 allocate per-vq irq if available and necessary */

 the config->find_vqs() implementation */

 Try MSI-X with one vector per queue. */

 Fallback: MSI-X with one vector for config, one shared for queues. */

 Finally fall back to regular interrupts. */

/* Setup the affinity for a virtqueue:

 * - force the affinity for per vq vector

 * - OR over all affinities for shared MSI

 * - ignore the affinity request if we're using INTX

 Qumranet donated their vendor ID for devices 0x1000 thru 0x10FF. */

	/* As struct device is a kobject, it's not safe to

	 * free the memory (including the reference counter itself)

 allocate our structure and fill it out */

 enable the device */

 Also try modern mode if we can't map BAR0 (no IO space). */

	/*

	 * Device is marked broken on surprise removal so that virtio upper

	 * layers can abort any ongoing operation.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * On error we are losing the status update, which isn't critical as

 * this is typically used for stuff like keyboard leds.

	/*

	 * Since 29cc309d8bf1 (HID: hid-multitouch: forward MSC_TIMESTAMP),

	 * EV_MSC/MSC_TIMESTAMP is added to each before EV_SYN event.

	 * EV_MSC is configured as INPUT_PASS_TO_ALL.

	 * In case of touch device:

	 *   BE pass EV_MSC/MSC_TIMESTAMP to FE on receiving event from evdev.

	 *   FE pass EV_MSC/MSC_TIMESTAMP back to BE.

	 *   BE writes EV_MSC/MSC_TIMESTAMP to evdev due to INPUT_PASS_TO_ALL.

	 *   BE receives extra EV_MSC/MSC_TIMESTAMP and pass to FE.

	 *   >>> Each new frame becomes larger and larger.

	 * Disable EV_MSC/MSC_TIMESTAMP forwarding for MT.

	/*

	 * Bitmap in virtio config space is a simple stream of bytes,

	 * with the first byte carrying bits 0-7, second bits 8-15 and

	 * so on.

 device -> kernel */

 kernel -> device */

 none */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2020 Linaro Limited. All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

 ETMv4 includes and features */

 preload configurations and features */

 preload in features for ETMv4 */

 strobe feature */

 resource selectors */

 strobe window counter 0 - reload from param 0 */

 strobe period counter 1 - reload from param 1 */

 sequencer */

 view-inst */

 end of regs */

 create an autofdo configuration */

 we will provide 9 sets of preset parameter values */

 the total number of parameters in used features */

/*

 * set of presets leaves strobing window constant while varying period to allow

 * experimentation with mark / space ratios for various workloads

 end of ETM4x configurations */

 IS_ENABLED(CONFIG_CORESIGHT_SOURCE_ETM4X) */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2012, The Linux Foundation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 Linaro Limited, All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

/*

 * Declare the number of static declared attribute groups

 * Value includes groups + NULL value at end of table.

/*

 * List of trigger signal type names. Match the constants declared in

 * include\dt-bindings\arm\coresight-cti-dt.h

 GEN_IO */

 GEN_INTREQ */

 GEN_INTACK */

 GEN_HALTREQ */

 GEN_RESTARTREQ */

 PE_EDBGREQ */

 PE_DBGRESTART */

 PE_CTIIRQ */

 PE_PMUIRQ */

 PE_DBGTRIGGER */

 ETM_EXTOUT */

 ETM_EXTIN */

 SNK_FULL */

 SNK_ACQCOMP */

 SNK_FLUSHCOMP */

 SNK_FLUSHIN */

 SNK_TRIGIN */

 STM_ASYNCOUT */

 STM_TOUT_SPTE */

 STM_TOUT_SW */

 STM_TOUT_HETE */

 STM_HWEVENT */

 ELA_TSTART */

 ELA_TSTOP */

 ELA_DBGREQ */

 Show function pointer used in the connections dynamic declared attributes*/

 Connection attribute types */

 Names for the connection attributes */

 basic attributes */

 attribute and group sysfs tables. */

 register based attributes */

 macro to access RO registers with power check only (no enable check). */

 coresight management registers */

 CTI low level programming registers */

/*

 * Show a simple 32 bit value if enabled and powered.

 * If inaccessible & pcached_val not NULL then show cached value.

/*

 * Store a simple 32 bit value.

 * If pcached_val not NULL, then copy to here too,

 * if reg_offset >= 0 then write through if enabled.

 local store */

 write through if offset and enabled */

 Standard macro for simple rw cti config registers */

 write through if enabled */

 write through if enabled */

 a 1'b1 in appclr clears down the same bit in appset*/

 write through if enabled */

 write through if enabled */

/*

 * Define CONFIG_CORESIGHT_CTI_INTEGRATION_REGS to enable the access to the

 * integration control registers. Normally only used to investigate connection

 * data.

 macro to access RW registers with power check only (no enable check). */

 macro to access WO registers with power check only (no enable check). */

 CORESIGHT_CTI_INTEGRATION_REGS */

 CTI channel x-trigger programming */

 extract chan idx and trigger idx */

 clear all xtrigger / channel programming */

 clear the CTI trigger / channel programming registers */

 clear the other regs */

 if enabled then write through */

/*

 * Write to select a channel to view, read to display the

 * cross triggers for the selected channel.

 scan regs to get bitmap of channels in use. */

 inverse bits if printing free channels */

 list of channels, or 'none' */

 Create the connections trigger groups and attrs dynamically */

/*

 * Each connection has dynamic group triggers<N> + name, trigin/out sigs/types

 * attributes, + each device has static nr_trigger_cons giving the number

 * of groups. e.g. in sysfs:-

 * /cti_<name>/triggers0

 * /cti_<name>/triggers1

 * /cti_<name>/nr_trigger_cons

 * where nr_trigger_cons = 2

 convert a sig type id to a name */

/*

 * Array of show function names declared above to allow selection

 * for the connection attributes

 fill out the underlying attribute struct */

 now the device_attribute struct */

	/*

	 * Initialize the dynamically allocated attribute

	 * to avoid LOCKDEP splat. See include/linux/sysfs.h

	 * for more details.

 create a triggers connection group and the attributes for that group */

 allocate NULL terminated array of attributes */

 create the array of group pointers for the CTI sysfs groups */

 nr groups = dynamic + static + NULL terminator */

 populate first locations with the static set of groups */

 add dynamic set for each connection */

 attribute and group sysfs tables. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019, The Linaro Limited. All rights reserved.

 Number of CTI signals in the v8 architecturally defined connection */

 CTI device tree trigger connection node keyword */

 CTI device tree connection property keywords */

/*

 * CTI can be bound to a CPU, or a system device.

 * CPU can be declared at the device top level or in a connections node

 * so need to check relative to node not device.

 CTI affinity defaults to no cpu */

 No Affinity  if no cpu nodes are found */

/*

 * CTI can be bound to a CPU, or a system device.

 * CPU can be declared at the device top level or in a connections node

 * so need to check relative to node not device.

/*

 * Extract a name from the fwnode.

 * If the device associated with the node is a coresight_device, then return

 * that name and the coresight_device pointer, otherwise return the node name.

 Can optionally have an etm node - return if not  */

 allocate memory */

 build connection data */

 sigs <4,5,6,7> */

 sigs <4,5,6,7> */

	/*

	 * The EXTOUT type signals from the ETM are connected to a set of input

	 * triggers on the CTI, the EXTIN being connected to output triggers.

	/*

	 * We look to see if the ETM coresight device associated with this

	 * handle has been registered with the system - i.e. probed before

	 * this CTI. If so csdev will be non NULL and we can use the device

	 * name and pass the csdev to the connection entry function where

	 * the association will be recorded.

	 * If not, then simply record the name in the connection data, the

	 * probing of the ETM will call into the CTI driver API to update the

	 * association then.

/*

 * Create an architecturally defined v8 connection

 * must have a cpu, can have an ETM.

 Must have a cpu node */

 Allocate the v8 cpu connection memory */

 Set the v8 PE CTI connection data */

 sigs <0 1> */

 sigs <0 1 2 > */

 Create the v8 ETM associated connection */

 filter pe_edbgreq - PE trigout sig <0> */

 set the signal usage mask */

 allocate an array according to number of signals in connection */

 see if any types have been included in the device description */

 need an array to store the values iff there are any */

	/*

	 * Match type id to signal index, 1st type to 1st index etc.

	 * If fewer types than signals default remainder to GEN_IO.

 look to see how many in and out signals we have */

 look for the signals properties. */

 read the connection name if set - may be overridden by later */

 associated cpu ? */

 associated device ? */

 set up a connection */

 get the hardware configuration & connection data. */

 get any CTM ID - defaults to 0 */

 check for a v8 architectural CTI device */

 if no connections, just add a single default based on max IN-OUT */

	/*

	 * Alloc platform data but leave it zero init. CTI does not use the

	 * same connection infrastructuree as trace path components but an

	 * empty struct enables us to use the standard coresight component

	 * registration code.

 get some CTI specifics */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011-2012, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Trace Port Interface Unit driver

* register definition **/

 FFSR - 0x300 */

 FFCR - 0x304 */

/*

 * @base:	memory mapped base address for this component.

 * @atclk:	optional clock for the core parts of the TPIU.

 * @csdev:	component vitals needed by the framework.

 TODO: fill this up */

 Clear formatter and stop on flush */

 Generate manual flush */

 Wait for flush to complete */

 Wait for formatter to stop */

 optional */

 Validity for the resource is already checked by the AMBA core */

 Disable tpiu to support older devices */

 Coresight SoC-600 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 Linaro Limited, All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

/**

 * CTI devices can be associated with a PE, or be connected to CoreSight

 * hardware. We have a list of all CTIs irrespective of CPU bound or

 * otherwise.

 *

 * We assume that the non-CPU CTIs are always powered as we do with sinks etc.

 *

 * We leave the client to figure out if all the CTIs are interconnected with

 * the same CTM, in general this is the case but does not always have to be.

 net of CTI devices connected via CTM */

 protect the list */

 power management handling */

 quick lookup list for CPU bound CTIs when power handling */

/*

 * CTI naming. CTI bound to cores will have the name cti_cpu<N> where

 * N is the CPU ID. System CTIs will have the name cti_sys<I> where I

 * is an index allocated by order of discovery.

 *

 * CTI device name list - for CTI not bound to cores.

 write set of regs to hardware - call with spinlock claimed */

 disable CTI before writing registers */

 write the CTI trigger registers */

 other regs */

 re-enable CTI */

 write regs to hardware and enable */

 no need to do anything if enabled or unpowered*/

 claim the device */

 cannot enable due to error */

 re-enable CTI on CPU when using CPU hotplug */

 no need to do anything if no enable request */

 try to claim the device */

 did not re-enable due to no claim / no request */

 disable hardware */

 check refcount - disable on 0 */

 no need to do anything if disabled or cpu unpowered */

 disable CTI */

 not disabled this call */

 write if enabled */

/*

 * Look at the HW DEVID register for some of the HW settings.

 * DEVID[15:8] - max number of in / out triggers.

 DEVID[19:16] - number of CTM channels */

	/*

	 * no current hardware should exceed this, but protect the driver

	 * in case of fault / out of spec hw

 Most regs default to 0 as zalloc'ed except...*/

/*

 * Add a connection entry to the list of connections for this

 * CTI device.

	/*

	 * Prefer actual associated CS device dev name to supplied value -

	 * which is likely to be node name / other conn name.

 add connection usage bit info to overall info */

 create a trigger connection with appropriately sized signal groups */

/*

 * Add a default connection if nothing else is specified.

 * single connection based on max in/out info, no assoc device

	/*

	 * Assume max trigs for in and out,

	 * all used, default sig types allocated

* cti channel api **/

 attach/detach channel from trigger - write through if enabled. */

 ensure indexes in range */

 ensure registered triggers and not out filtered */

 update the local register values */

 read - modify write - the trigger / channel enable value */

 write local copy */

 write through if enabled */

/*

 * Look for a matching connection device name in the list of connections.

 * If found then swap in the csdev name, set trig con association pointer

 * and return found.

 match: so swap in csdev name & dev */

 try to set sysfs link */

 link failed - remove CTI reference */

/*

 * Search the cti list to add an associated CTI into the supplied CS device

 * This will set the association if CTI declared before the CS device.

 * (called from coresight_register() with coresight_mutex locked).

 protect the list */

 exit if current is an ECT device.*/

 if we didn't find the csdev previously we used the fwnode name */

 for each CTI in list... */

			/*

			 * if we found a matching csdev then update the ECT

			 * association pointer for the device with this CTI.

/*

 * Removing the associated devices is easier.

 * A CTI will not have a value for csdev->ect_dev.

/*

 * Operations to add and remove associated CTI.

 * Register to coresight core driver as call back function.

/*

 * Update the cross references where the associated device was found

 * while we were building the connection info. This will occur if the

 * assoc device was registered before the CTI.

 if we can set the sysfs link */

 set the CTI/csdev association */

 otherwise remove reference from CTI */

* cti PM callbacks **/

 CTI regs all static - we have a copy & nothing to save */

 write hardware registers to re-enable. */

 check enable reference count to enable HW */

 check we can claim the device as we re-power */

 CPU HP handlers */

 release PM registrations */

* cti ect operations **/

/*

 * Free up CTI specific resources

 * called by dev->release, need to call down to underlying csdev release.

 remove from the list */

 driver data*/

 Validity for the resource is already checked by the AMBA core */

 default CTI device info  */

 initialise CTI driver config values */

 default to powered - could change on PM notifications */

 set up device name - will depend if cpu bound or otherwise */

 setup CPU power management handling for CPU bound CTI devices. */

 create dynamic attributes for connections */

 set up coresight component description */

 add to list of CTI devices */

 set any cross references */

 set up release chain */

 all done - dec pm refcount */

  CTI UCI data */

 CTI v2 */

 maj(0x4-debug) min(0x1-ECT) */

 Coresight CTI (SoC 400), C-A72, C-A57 */

 CTI - C-A8 */

 CTI - C-A53 */

 CTI - C-A73 */

 CTI - C-A35 */

 Coresight CTI (SoC 600) */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019, Linaro Limited, All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

/*

 * Connections group - links attribute.

 * Count of created links between coresight components in the group.

/*

 * Create connections group for CoreSight devices.

 * This group will then be used to collate the sysfs links between

 * devices.

 first link orig->target */

 second link target->orig */

 error in second link - remove first - otherwise inc counts */

/*

 * coresight_make_links: Make a link for a connection from a @orig

 * device to @target, represented by @conn.

 *

 *   e.g, for devOrig[output_X] -> devTarget[input_Y] is represented

 *   as two symbolic links :

 *

 *	/sys/.../devOrig/out:X	-> /sys/.../devTarget/

 *	/sys/.../devTarget/in:Y	-> /sys/.../devOrig/

 *

 * The link names are allocated for a device where it appears. i.e, the

 * "out" link on the master and "in" link on the slave device.

 * The link info is stored in the connection record for avoiding

 * the reconstruction of names for removal.

		/*

		 * Install the device connection. This also indicates that

		 * the links are operational on both ends.

/*

 * coresight_remove_links: Remove the sysfs links for a given connection @conn,

 * from @orig device to @target device. See coresight_make_links() for more

 * details.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011-2012, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Funnel driver

/**

 * struct funnel_drvdata - specifics associated to a funnel component

 * @base:	memory mapped base address for this component.

 * @atclk:	optional clock for the core parts of the funnel.

 * @csdev:	component vitals needed by the framework.

 * @priority:	port selection order.

 * @spinlock:	serialize enable/disable operations.

 Claim the device only when we enable the first slave */

 Disclaim the device if none of the slaves are now active */

 optional */

	/*

	 * Map the device base for dynamic-funnel, which has been

	 * validated by AMBA core.

 Static funnel do not have programming base */

 THIS_MODULE is taken care of by platform_driver_register() */

 Coresight SoC-600 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2020 Linaro Limited. All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

 defines to associate register IDs with driver data locations */

/**

 * etm4_cfg_map_reg_offset - validate and map the register offset into a

 *			     location in the driver config struct.

 *

 * Limits the number of registers that can be accessed and programmed in

 * features, to those which are used to control the trace capture parameters.

 *

 * Omits or limits access to those which the driver must use exclusively.

 *

 * Invalid offsets will result in fail code return and feature load failure.

 *

 * @drvdata:	driver data to map into.

 * @reg:	register to map.

 * @offset:	device offset for the register

 sequencer state control registers */

 32 bit, 8 off indexed register sets */

 64 bit, 8 off indexed register sets */

 32 bit resource selection regs, 32 off, skip fixed 0,1 */

 64 bit addr cmp regs, 16 off */

 32 bit counter regs, 4 off (ETMv4_MAX_CNTR - 1) */

/**

 * etm4_cfg_load_feature - load a feature into a device instance.

 *

 * @csdev:	An ETMv4 CoreSight device.

 * @feat:	The feature to be loaded.

 *

 * The function will load a feature instance into the device, checking that

 * the register definitions are valid for the device.

 *

 * Parameter and register definitions will be converted into internal

 * structures that are used to set the values in the driver when the

 * feature is enabled for the device.

 *

 * The feature spinlock pointer is initialised to the same spinlock

 * that the driver uses to protect the internal register values.

	/*

	 * essential we set the device spinlock - this is used in the generic

	 * programming routines when copying values into the drvdata structures

	 * via the pointers setup in etm4_cfg_map_reg_offset().

 process the register descriptions */

 match information when loading configurations */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2012, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Trace Memory Controller driver

 Ensure formatter, unformatter and hardware fifo are empty */

 Ensure flush completes */

	/*

	 * When moving RRP or an offset address forward, the new values must

	 * be byte-address aligned to the width of the trace memory databus

	 * _and_ to a frame boundary (16 byte), whichever is the biggest. For

	 * example, for 32-bit, 64-bit and 128-bit wide trace memory, the four

	 * LSBs must be 0s. For 256-bit wide trace memory, the five LSBs must

	 * be 0s.

	/*

	 * Excerpt from the TRM:

	 *

	 * DEVID::MEMWIDTH[10:8]

	 * 0x2 Memory interface databus is 32 bits wide.

	 * 0x3 Memory interface databus is 64 bits wide.

	 * 0x4 Memory interface databus is 128 bits wide.

	 * 0x5 Memory interface databus is 256 bits wide.

 Only permitted for TMC-ETRs */

 The buffer size should be page aligned */

 Detect and initialise the capabilities of a TMC ETR */

 Set the unadvertised capabilities */

 Check if the AXI address width is available */

	/*

	 * Unless specified in the device configuration, ETR uses a 40-bit

	 * AXI master in place of the embedded SRAM of ETB/ETF.

 Only permissible values are 0 to 15 */

 Validity for the resource is already checked by the AMBA core */

 This device is not associated with a session */

	/*

	 * We do not care about coresight unregister here unlike remove

	 * callback which is required for making coresight modular since

	 * the system is going down after this.

	/*

	 * Since misc_open() holds a refcount on the f_ops, which is

	 * etb fops in this case, device is there until last file

	 * handler to this device is closed.

 Coresight SoC 600 TMC-ETR/ETS */

 Coresight SoC 600 TMC-ETB */

 Coresight SoC 600 TMC-ETF */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2015 Linaro Limited. All rights reserved.

 * Author: Mathieu Poirier <mathieu.poirier@linaro.org>

	/*

	 * TRCACATRn.TYPE bit[1:0]: type of comparison

	 * the trace unit performs

		/*

		 * We are performing instruction address comparison. Set the

		 * relevant bit of ViewInst Include/Exclude Control register

		 * for corresponding address comparator pair.

			/*

			 * Set exclude bit and unset the include bit

			 * corresponding to comparator pair

			/*

			 * Set include bit and unset exclude bit

			 * corresponding to comparator pair

 Disable data tracing: do not trace load and store data transfers */

 Disable data value and data address tracing */

 Disable all events tracing */

 Disable timestamp event */

 Disable stalling */

 Reset trace synchronization period  to 2^8 = 256 bytes*/

	/*

	 * Enable ViewInst to trace everything with start-stop logic in

	 * started state. ARM recommends start-stop logic is set before

	 * each trace run.

 SSSTATUS, bit[9] */

 No address range filtering for ViewInst */

 No start-stop filtering for ViewInst */

 Disable seq events */

 Disable external input events */

 start by clearing instruction P0 field */

 0b01 Trace load instructions as P0 instructions */

 0b10 Trace store instructions as P0 instructions */

			/*

			 * 0b11 Trace load and store instructions

			 * as P0 instructions

 bit[3], Branch broadcast mode */

 bit[4], Cycle counting instruction trace bit */

 bit[6], Context ID tracing bit */

 bits[10:8], Conditional instruction tracing bit */

 bit[11], Global timestamp tracing bit */

 bit[12], Return stack enable bit */

 bits[14:13], Q element enable field */

 start by clearing QE bits */

 if supported, Q elements with instruction counts are enabled */

	/*

	 * if supported, Q elements with and without instruction

	 * counts are enabled

 bit[11], AMBA Trace Bus (ATB) trigger enable bit */

 bit[12], Low-power state behavior override bit */

 bit[8], Instruction stall bit */

 bit[10], Prioritize instruction trace bit */

 bit[13], Trace overflow prevention bit */

 bit[9] Start/stop logic control bit */

 bit[10], Whether a trace unit must trace a Reset exception */

 bit[11], Whether a trace unit must trace a system error exception */

 EVENT0, bits[7:0] */

 EVENT1, bits[15:8] */

 EVENT2, bits[23:16] */

 EVENT3, bits[31:24] */

 start by clearing all instruction event enable bits */

 generate Event element for event 1 */

 generate Event element for event 1 and 2 */

 generate Event element for event 1, 2 and 3 */

 generate Event element for all 4 events */

 mask off max threshold before checking min value */

	/*

	 * Bit[8] controls include(1) / exclude(0), bits[0-7] select

	 * individual range comparators. If include then at least 1

	 * range must be selected.

 clear all EXLEVEL_S bits  */

 enable instruction tracing for corresponding exception level */

 EXLEVEL_NS, bits[23:20] */

 clear EXLEVEL_NS bits  */

 enable instruction tracing for corresponding exception level */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

 TYPE, bits[1:0] */

  exclude is optional, but need at least two parameter */

 lower address comparator cannot have a higher address value */

	/*

	 * Program include or exclude control bits for vinst or vdata

	 * whenever we change addr comparators to ETM_ADDR_TYPE_RANGE

	 * use supplied value, or default to bit set in 'mode'

 CONTEXTTYPE, bits[3:2] */

 start by clearing context type bits */

 0b01 The trace unit performs a Context ID */

 0b10 The trace unit performs a VMID */

		/*

		 * 0b11 The trace unit performs a Context ID

		 * comparison and a VMID

 context ID comparator bits[6:4] */

 clear context ID comparator bits[6:4] */

 clear Exlevel_ns & Exlevel_s bits[14:12, 11:8], bit[15] is res0 */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

 Seq control has two masks B[15:8] F[7:0] */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

	/*

	 * Resource selector pair 0 is always implemented and reserved,

	 * namely an idx with 0 and 1 is illegal.

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

 For odd idx pair inversal bit is RES0 */

 PAIRINV, bit[21] */

 must clear bit 31 in related status register on programming */

 must clear bit 31 in related status register on programming */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

	/*

	 * When contextID tracing is enabled the tracers will insert the

	 * value found in the contextID register in the trace stream.  But if

	 * a process is in a namespace the PID of that process as seen from the

	 * namespace won't be what the kernel sees, something that makes the

	 * feature confusing and can potentially leak kernel only information.

	 * As such refuse to use the feature if @current is not in the initial

	 * PID namespace.

	/*

	 * only implemented when ctxid tracing is enabled, i.e. at least one

	 * ctxid comparator is implemented and ctxid is greater than 0 bits

	 * in length

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

	/*

	 * only implemented when ctxid tracing is enabled, i.e. at least one

	 * ctxid comparator is implemented and ctxid is greater than 0 bits

	 * in length

 one mask if <= 4 comparators, two for up to 8 */

	/*

	 * each byte[0..3] controls mask value applied to ctxid

	 * comparator[0..3]

 COMP0, bits[7:0] */

 COMP1, bits[15:8] */

 COMP2, bits[23:16] */

 COMP3, bits[31:24] */

 COMP4, bits[7:0] */

 COMP5, bits[15:8] */

 COMP6, bits[23:16] */

 COMP7, bits[31:24] */

	/*

	 * If software sets a mask bit to 1, it must program relevant byte

	 * of ctxid comparator value 0x0, otherwise behavior is unpredictable.

	 * For example, if bit[3] of ctxid_mask0 is 1, we must clear bits[31:24]

	 * of ctxid comparator0 value (corresponding to byte 0) register.

 mask value of corresponding ctxid comparator */

		/*

		 * each bit corresponds to a byte of respective ctxid comparator

		 * value register

 Select the next ctxid comparator mask value */

 ctxid comparators[4-7] */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

	/*

	 * only implemented when vmid tracing is enabled, i.e. at least one

	 * vmid comparator is implemented and at least 8 bit vmid size

	/*

	 * only implemented when vmid tracing is enabled, i.e. at least one

	 * vmid comparator is implemented and at least 8 bit vmid size

 one mask if <= 4 comparators, two for up to 8 */

	/*

	 * each byte[0..3] controls mask value applied to vmid

	 * comparator[0..3]

 COMP0, bits[7:0] */

 COMP1, bits[15:8] */

 COMP2, bits[23:16] */

 COMP3, bits[31:24] */

 COMP4, bits[7:0] */

 COMP5, bits[15:8] */

 COMP6, bits[23:16] */

 COMP7, bits[31:24] */

	/*

	 * If software sets a mask bit to 1, it must program relevant byte

	 * of vmid comparator value 0x0, otherwise behavior is unpredictable.

	 * For example, if bit[3] of vmid_mask0 is 1, we must clear bits[31:24]

	 * of vmid comparator0 value (corresponding to byte 0) register.

 mask value of corresponding vmid comparator */

		/*

		 * each bit corresponds to a byte of respective vmid comparator

		 * value register

 Select the next vmid comparator mask value */

 vmid comparators[4-7] */

	/*

	 * smp cross call ensures the CPU will be powered up before

	 * accessing the ETMv4 trace core registers

		/*

		 * Common registers to ETE & ETM4x accessible via system

		 * instructions are always implemented.

		/*

		 * We only support etm4x and ete. So if the device is not

		 * ETE, it must be ETMv4x.

		/*

		 * Registers accessible only via memory-mapped registers

		 * must not be accessed via system instructions.

		 * We cannot access the drvdata->csdev here, as this

		 * function is called during the device creation, via

		 * coresight_register() and the csdev is not initialized

		 * until that is done. So rely on the drvdata->base to

		 * detect if we have a memory mapped access.

		 * Also ETE doesn't implement memory mapped access, thus

		 * it is sufficient to check that we are using mmio.

/*

 * Hide the ETM4x registers that may not be available on the

 * hardware.

 * There are certain management registers unavailable via system

 * instructions. Make those sysfs attributes hidden on such

 * systems.

 trcidr[6,7] are reserved */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2015 Linaro Limited. All rights reserved.

 * Author: Mathieu Poirier <mathieu.poirier@linaro.org>

/*

 * An ETM context for a running event includes the perf aux handle

 * and aux_data. For ETM, the aux_data (etm_event_data), consists of

 * the trace path and the sink configuration. The event data is accessible

 * via perf_get_aux(handle). However, a sink could "end" a perf output

 * handle via the IRQ handler. And if the "sink" encounters a failure

 * to "begin" another session (e.g due to lack of space in the buffer),

 * the handle will be cleared. Thus, the event_data may not be accessible

 * from the handle when we get to the etm_event_stop(), which is required

 * for stopping the trace path. The event_data is guaranteed to stay alive

 * until "free_aux()", which cannot happen as long as the event is active on

 * the ETM. Thus the event_data for the session must be part of the ETM context

 * to make sure we can disable the trace path.

/*

 * The PMU formats were orignally for ETMv3.5/PTM's ETMCR 'config';

 * now take them as general formats and apply on all ETMs.

 contextid1 enables tracing CONTEXTIDR_EL1 for ETMv4 */

 contextid2 enables tracing CONTEXTIDR_EL2 for ETMv4 */

 preset - if sink ID is used as a configuration selector */

 Sink ID - same for all ETMs */

 config ID - set if a system configuration is selected */

/*

 * contextid always traces the "PID".  The PID is in CONTEXTIDR_EL1

 * when the kernel is running at EL1; when the kernel is at EL2,

 * the PID is in CONTEXTIDR_EL2.

 Free the sink buffers, if there are any */

 clear any configuration we were using */

 First get memory for the session's data */

	/*

	 * Each CPU has a single path between source and destination.  As such

	 * allocate an array using CPU numbers as indexes.  That way a path

	 * for any CPU can easily be accessed at any given time.  We proceed

	 * the same way for sessions involving a single CPU.  The cost of

	 * unused memory when dealing with single CPU trace scenarios is small

	 * compared to the cost of searching through an optimized array.

/*

 * Check if two given sinks are compatible with each other,

 * so that they can use the same sink buffers, when an event

 * moves around.

	/*

	 * If the sinks are of the same subtype and driven

	 * by the same driver, we can use the same buffer

	 * on these sinks.

 First get the selected sink from user space. */

 check if user wants a coresight configuration selected */

	/*

	 * Setup the path for each CPU in a trace session. We try to build

	 * trace path for each CPU in the mask. If we don't find an ETM

	 * for the CPU or fail to build a path, we clear the CPU from the

	 * mask and continue with the rest. If ever we try to trace on those

	 * CPUs, we can handle it and fail the session.

		/*

		 * If there is no ETM associated with this CPU clear it from

		 * the mask and continue with the rest. If ever we try to trace

		 * on this CPU, we handle it accordingly.

		/*

		 * No sink provided - look for a default sink for all the ETMs,

		 * where this event can be scheduled.

		 * We allocate the sink specific buffers only once for this

		 * event. If the ETMs have different default sink devices, we

		 * can only use a single "type" of sink as the event can carry

		 * only one sink specific buffer. Thus we have to make sure

		 * that the sinks are of the same type and driven by the same

		 * driver, as the one we allocate the buffer for. As such

		 * we choose the first sink and check if the remaining ETMs

		 * have a compatible default sink. We don't trace on a CPU

		 * if the sink is not compatible.

 Find the default sink for this ETM */

 Check if this sink compatible with the last sink */

		/*

		 * Building a path doesn't enable it, it simply builds a

		 * list of devices from source to sink that can be

		 * referenced later when the path is actually needed.

 no sink found for any CPU - cannot trace */

 If we don't have any CPUs ready for tracing, abort */

	/*

	 * Allocate the sink buffer for this session. All the sinks

	 * where this event can be scheduled are ensured to be of the

	 * same type. Thus the same sink configuration is used by the

	 * sinks.

 Have we messed up our tracking ? */

	/*

	 * Deal with the ring buffer API and get a handle on the

	 * session's information.

	/*

	 * Check if this ETM is allowed to trace, as decided

	 * at etm_setup_aux(). This could be due to an unreachable

	 * sink from this ETM. We can't do much in this case if

	 * the sink was specified or hinted to the driver. For

	 * now, simply don't record anything on this ETM.

	 *

	 * As such we pretend that everything is fine, and let

	 * it continue without actually tracing. The event could

	 * continue tracing when it moves to a CPU where it is

	 * reachable to a sink.

 We need a sink, no need to continue without one */

 Nothing will happen without a path */

 Finally enable the tracer */

 Tell the perf core the event is alive */

 Save the event_data for this ETM */

	/*

	 * Check if the handle is still associated with the event,

	 * to handle cases where if the sink failed to start the

	 * trace and TRUNCATED the handle already.

	/*

	 * If we still have access to the event_data via handle,

	 * confirm that we haven't messed up the tracking.

 Clear the event_data as this ETM is stopping the trace. */

 We must have a valid event_data for a running event */

	/*

	 * Check if this ETM was allowed to trace, as decided at

	 * etm_setup_aux(). If it wasn't allowed to trace, then

	 * nothing needs to be torn down other than outputting a

	 * zero sized record.

 stop tracer */

 tell the core */

	/*

	 * If the handle is not bound to an event anymore

	 * (e.g, the sink driver was unable to restart the

	 * handle due to lack of buffer space), we don't

	 * have to do anything here.

 update trace information */

		/*

		 * Make sure the handle is still valid as the

		 * sink could have closed it from an IRQ.

		 * The sink driver must handle the race with

		 * update_buffer() and IRQ. Thus either we

		 * should get a valid handle and valid size

		 * (which may be 0).

		 *

		 * But we should never get a non-zero size with

		 * an invalid handle.

 Disabling the path make its elements available to other sessions */

		/*

		 * No need to go further if there's no more

		 * room for filters.

 filter::size==0 means single address trigger */

			/*

			 * The existing code relies on START/STOP filters

			 * being address filters.

		/*

		 * At this time we don't allow range and start/stop filtering

		 * to cohabitate, they have to be mutually exclusive.

	/*

	 * If this function is called adding a sink then the hash is used for

	 * sink selection - see function coresight_get_sink_by_id().

	 * If adding a configuration then the hash is used for selection in

	 * cscfg_activate_config()

 set the show function to the custom cscfg event */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2016 Linaro Limited. All rights reserved.

 * Author: Mathieu Poirier <mathieu.poirier@linaro.org>

/*

 * etr_perf_buffer - Perf buffer used for ETR

 * @drvdata		- The ETR drvdaga this buffer has been allocated for.

 * @etr_buf		- Actual buffer used by the ETR

 * @pid			- The PID this etr_perf_buffer belongs to.

 * @snaphost		- Perf session mode

 * @nr_pages		- Number of pages in the ring buffer.

 * @pages		- Array of Pages in the ring buffer.

 Convert the perf index to an offset within the ETR buffer */

 Lower limit for ETR hardware buffer */

/*

 * The TMC ETR SG has a page size of 4K. The SG table contains pointers

 * to 4KB buffers. However, the OS may use a PAGE_SIZE different from

 * 4K (i.e, 16KB or 64KB). This implies that a single OS page could

 * contain more than one SG buffer and tables.

 *

 * A table entry has the following format:

 *

 * ---Bit31------------Bit4-------Bit1-----Bit0--

 * |     Address[39:12]    | SBZ |  Entry Type  |

 * ----------------------------------------------

 *

 * Address: Bits [39:12] of a physical page address. Bits [11:0] are

 *	    always zero.

 *

 * Entry type:

 *	b00 - Reserved.

 *	b01 - Last entry in the tables, points to 4K page buffer.

 *	b10 - Normal entry, points to 4K page buffer.

 *	b11 - Link. The address points to the base of next table.

/*

 * struct etr_sg_table : ETR SG Table

 * @sg_table:		Generic SG Table holding the data/table pages.

 * @hwaddr:		hwaddress used by the TMC, which is the base

 *			address of the table.

/*

 * tmc_etr_sg_table_entries: Total number of table entries required to map

 * @nr_pages system pages.

 *

 * We need to map @nr_pages * ETR_SG_PAGES_PER_SYSPAGE data pages.

 * Each TMC page can map (ETR_SG_PTRS_PER_PAGE - 1) buffer pointers,

 * with the last entry pointing to another page of table entries.

 * If we spill over to a new page for mapping 1 entry, we could as

 * well replace the link entry of the previous page with the last entry.

	/*

	 * If we spill over to a new page for 1 entry, we could as well

	 * make it the LAST entry in the previous page, skipping the Link

	 * address.

/*

 * tmc_pages_get_offset:  Go through all the pages in the tmc_pages

 * and map the device address @addr to an offset within the virtual

 * contiguous buffer.

/*

 * tmc_pages_free : Unmap and free the pages used by tmc_pages.

 * If the pages were not allocated in tmc_pages_alloc(), we would

 * simply drop the refcount.

/*

 * tmc_pages_alloc : Allocate and map pages for a given @tmc_pages.

 * If @pages is not NULL, the list of page virtual addresses are

 * used as the data pages. The pages are then dma_map'ed for @dev

 * with dma_direction @dir.

 *

 * Returns 0 upon success, else the error number.

 Hold a refcount on the page */

/*

 * Alloc pages for the table. Since this will be used by the device,

 * allocate the pages closer to the device (i.e, dev_to_node(dev)

 * rather than the CPU node).

 Allocate data pages on the node requested by the caller */

/*

 * tmc_alloc_sg_table: Allocate and setup dma pages for the TMC SG table

 * and data buffers. TMC writes to the data buffers and reads from the SG

 * Table pages.

 *

 * @dev		- Coresight device to which page should be DMA mapped.

 * @node	- Numa node for mem allocations

 * @nr_tpages	- Number of pages for the table entries.

 * @nr_dpages	- Number of pages for Data buffer.

 * @pages	- Optional list of virtual address of pages.

/*

 * tmc_sg_table_sync_data_range: Sync the data buffer written

 * by the device from @offset upto a @size bytes.

 tmc_sg_sync_table: Sync the page table */

/*

 * tmc_sg_table_get_data: Get the buffer pointer for data @offset

 * in the SG buffer. The @bufpp is updated to point to the buffer.

 * Returns :

 *	the length of linear data available at @offset.

 *	or

 *	<= 0 if no data is available.

 Make sure we don't go beyond the end */

 Respect the page boundaries */

 Map a dma address to virtual address */

 Dump the given sg_table */

/*

 * Populate the SG Table page table entries from table/data

 * pages allocated. Each Data page has ETR_SG_PAGES_PER_SYSPAGE SG pages.

 * So does a Table page. So we keep track of indices of the tables

 * in each system page and move the pointers accordingly.

 index to the current system table_page */

 index to the sg_table within the current syspage */

 the entry within the sg_table */

 index to the current system data_page */

 index to the SG page within the current data page */

 pointer to the table entry to fill */

	/*

	 * Use the contiguous virtual address of the table to update entries.

	/*

	 * Fill all the entries, except the last entry to avoid special

	 * checks within the loop.

			/*

			 * Last entry in a sg_table page is a link address to

			 * the next table page. If this sg_table is the last

			 * one in the system page, it links to the first

			 * sg_table in the next system page. Otherwise, it

			 * links to the next sg_table page within the system

			 * page.

			/*

			 * Update the indices to the data_pages to point to the

			 * next sg_page in the data buffer.

		/*

		 * Move to the next table pointer, moving the table page index

		 * if necessary

 Set up the last entry, which is always a data pointer */

/*

 * tmc_init_etr_sg_table: Allocate a TMC ETR SG table, data buffer of @size and

 * populate the table.

 *

 * @dev		- Device pointer for the TMC

 * @node	- NUMA node where the memory should be allocated

 * @size	- Total size of the data buffer

 * @pages	- Optional list of page virtual address

 TMC should use table base address for DBA */

 Sync the table pages for the HW */

/*

 * tmc_etr_alloc_flat_buf: Allocate a contiguous DMA buffer.

 We cannot reuse existing pages for flat buf */

	/*

	 * Adjust the buffer to point to the beginning of the trace data

	 * and update the available trace data.

	/*

	 * The driver always starts tracing at the beginning of the buffer,

	 * the only reason why we would get a wrap around is when the buffer

	 * is full.  Sync the entire buffer in one go for this case.

	/*

	 * tmc_etr_buf_get_data already adjusts the length to handle

	 * buffer wrapping around.

/*

 * tmc_etr_alloc_sg_buf: Allocate an SG buf @etr_buf. Setup the parameters

 * appropriately.

 Convert hw address to offset in the buffer */

/*

 * TMC ETR could be connected to a CATU device, which can provide address

 * translation service. This is represented by the Output port of the TMC

 * (ETR) connected to the input port of the CATU.

 *

 * Returns	: coresight_device ptr for the CATU device if a CATU is found.

 *		: NULL otherwise.

/*

 * tmc_alloc_etr_buf: Allocate a buffer use by ETR.

 * @drvdata	: ETR device details.

 * @size	: size of the requested buffer.

 * @flags	: Required properties for the buffer.

 * @node	: Node for memory allocations.

 * @pages	: An optional list of pages.

	/*

	 * If we have to use an existing list of pages, we cannot reliably

	 * use a contiguous DMA memory (even if we have an IOMMU). Otherwise,

	 * we use the contiguous DMA memory if at least one of the following

	 * conditions is true:

	 *  a) The ETR cannot use Scatter-Gather.

	 *  b) we have a backing IOMMU

	 *  c) The requested memory size is smaller (< 1M).

	 *

	 * Fallback to available mechanisms.

	 *

/*

 * tmc_etr_buf_get_data: Get the pointer the trace data at @offset

 * with a maximum of @len bytes.

 * Returns: The size of the linear data available @pos, with *bufpp

 * updated to point to the buffer.

 Adjust the length to limit this transaction to end of buffer */

/*

 * tmc_sync_etr_buf: Sync the trace buffer availability with drvdata.

 * Makes sure the trace data is synced to the memory for consumption.

 * @etr_buf->offset will hold the offset to the beginning of the trace data

 * within the buffer, with @etr_buf->len bytes to consume.

	/*

	 * If there were memory errors in the session, truncate the

	 * buffer.

 Wait for TMCSReady bit to be set */

	/*

	 * If the TMC pointers must be programmed before the session,

	 * we have to set it properly (i.e, RRP/RWP to base address and

	 * STS to "not full").

 Callers should provide an appropriate buffer for use */

	/*

	 * If this ETR is connected to a CATU, enable it before we turn

	 * this on.

/*

 * Return the available trace data in the buffer (starts at etr_buf->offset,

 * limited by etr_buf->len) from @pos, with a maximum limit of @len,

 * also updating the @bufpp on where to find it. Since the trace data

 * starts at anywhere in the buffer, depending on the RRP, we adjust the

 * @len returned to handle buffer wrapping around.

 *

 * We are protected here by drvdata->reading != 0, which ensures the

 * sysfs_buf stays alive.

 Compute the offset from which we read the data */

		/*

		 * Insert barrier packets at the beginning, if there was

		 * an overflow.

	/*

	 * When operating in sysFS mode the content of the buffer needs to be

	 * read before the TMC is disabled.

 Disable CATU device if this ETR is connected to one */

 Reset the ETR buf used by hardware */

	/*

	 * If we are enabling the ETR from disabled state, we need to make

	 * sure we have a buffer with the right size. The etr_buf is not reset

	 * immediately after we stop the tracing in SYSFS mode as we wait for

	 * the user to collect the data. We may be able to reuse the existing

	 * buffer, provided the size matches. Any allocation has to be done

	 * with the lock released.

 Allocate memory with the locks released */

 Let's try again */

	/*

	 * In sysFS mode we can have multiple writers per sink.  Since this

	 * sink is already enabled no memory is needed and the HW need not be

	 * touched, even if the buffer size has changed.

	/*

	 * If we don't have a buffer or it doesn't match the requested size,

	 * use the buffer allocated above. Otherwise reuse the existing buffer.

 Free memory outside the spinlock if need be */

/*

 * alloc_etr_buf: Allocate ETR buffer for use by perf.

 * The size of the hardware buffer is dependent on the size configured

 * via sysfs and the perf ring buffer size. We prefer to allocate the

 * largest possible size, scaling down the size by half until it

 * reaches a minimum limit (1M), beyond which we give up.

	/*

	 * Try to match the perf ring buffer size if it is larger

	 * than the size requested via sysfs.

	/*

	 * Else switch to configured size for this ETR

	 * and scale down until we hit the minimum limit.

	/*

	 * An etr_perf_buffer is associated with an event and holds a reference

	 * to the AUX ring buffer that was created for that event.  In CPU-wide

	 * N:1 mode multiple events (one per CPU), each with its own AUX ring

	 * buffer, share a sink.  As such an etr_perf_buffer is created for each

	 * event but a single etr_buf associated with the ETR is shared between

	 * them.  The last event in a trace session will copy the content of the

	 * etr_buf to its AUX ring buffer.  Ring buffer associated to other

	 * events are simply not used an freed as events are destoyed.  We still

	 * need to allocate a ring buffer for each event since we don't know

	 * which event will be last.

	/*

	 * The first thing to do here is check if an etr_buf has already been

	 * allocated for this session.  If so it is shared with this event,

	 * otherwise it is created.

 If we made it here no buffer has been allocated, do so now. */

 Now that we have a buffer, add it to the IDR. */

 Another event with this session ID has allocated this buffer. */

 The IDR can't allocate room for a new session, abandon ship. */

	/*

	 * In per-thread mode the etr_buf isn't shared, so just go ahead

	 * with memory allocation.

	/*

	 * Keep a reference to the ETR this buffer has been allocated for

	 * in order to have access to the IDR in tmc_free_etr_buffer().

 If we are not the last one to use the buffer, don't touch it. */

 We are the last one, remove from the IDR and free the buffer. */

	/*

	 * Something went very wrong if the buffer associated with this ID

	 * is not the same in the IDR.  Leak to avoid use after free.

/*

 * tmc_etr_sync_perf_buffer: Copy the actual trace data from the hardware

 * buffer to the perf ring buffer.

		/*

		 * In one iteration, we can copy minimum of :

		 *  1) what is available in the source buffer,

		 *  2) what is available in the source buffer, before it

		 *     wraps around.

		 *  3) what is available in the destination page.

		 * in one iteration.

 Move destination pointers */

 Move source pointers */

/*

 * tmc_update_etr_buffer : Update the perf ring buffer with the

 * available trace data. We use software double buffering at the moment.

 *

 * TODO: Add support for reusing the perf ring buffer.

 Don't do anything if another tracer is using this sink */

	/*

	 * The ETR buffer may be bigger than the space available in the

	 * perf ring buffer (handle->size).  If so advance the offset so that we

	 * get the latest trace data.  In snapshot mode none of that matters

	 * since we are expected to clobber stale data in favour of the latest

	 * traces.

		/*

		 * Make sure the new size is aligned in accordance with the

		 * requirement explained in function tmc_get_memwidth_mask().

 Insert barrier packets at the beginning, if there was an overflow */

	/*

	 * In snapshot mode we simply increment the head by the number of byte

	 * that were written.  User space will figure out how many bytes to get

	 * from the AUX buffer based on the position of the head.

	/*

	 * Ensure that the AUX trace data is visible before the aux_head

	 * is updated via perf_aux_output_end(), as expected by the

	 * perf ring buffer.

	/*

	 * Don't set the TRUNCATED flag in snapshot mode because 1) the

	 * captured buffer is expected to be truncated and 2) a full buffer

	 * prevents the event from being re-enabled by the perf core,

	 * resulting in stale data being send to user space.

 Don't use this sink if it is already claimed by sysFS */

 Get a handle on the pid of the process to monitor */

 Do not proceed if this device is associated with another session */

	/*

	 * No HW configuration is needed if the sink is already in

	 * use for this session.

 Associate with monitored process. */

 We shouldn't be here */

 Complain if we (somehow) got out of sync */

 Dissociate from monitored process. */

 Reset perf specific data */

 config types are set a boot time and never change */

	/*

	 * We can safely allow reads even if the ETR is operating in PERF mode,

	 * since the sysfs session is captured in mode specific data.

	 * If drvdata::sysfs_data is NULL the trace data has been read already.

 Disable the TMC if we are trying to read from a running session. */

 config types are set a boot time and never change */

 RE-enable the TMC if need be */

		/*

		 * The trace run will continue with the same allocated trace

		 * buffer. Since the tracer is still enabled drvdata::buf can't

		 * be NULL.

		/*

		 * The ETR is not tracing and the buffer was just read.

		 * As such prepare to free the trace buffer.

 Free allocated memory out side of the spinlock */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015-2016, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight System Trace Macrocell driver

 *

 * Initial implementation by Pratik Patel

 * (C) 2014-2015 Pratik Patel <pratikp@codeaurora.org>

 *

 * Serious refactoring, code cleanup and upgrading to the Coresight upstream

 * framework by Mathieu Poirier

 * (C) 2015-2016 Mathieu Poirier <mathieu.poirier@linaro.org>

 *

 * Guaranteed timing and support for various packet type coming from the

 * generic STM API by Chunyan Zhang

 * (C) 2015-2016 Chunyan Zhang <zhang.chunyan@linaro.org>

 Register bit definition */

 Reserve the first 10 channels for kernel usage */

/*

 * Not really modular but using module_param is the easiest way to

 * remain consistent with existing use cases for now.

/*

 * struct channel_space - central management entity for extended ports

 * @base:		memory mapped base address where channels start.

 * @phys:		physical base address of channel region.

 * @guaraneed:		is the channel delivery guaranteed.

/**

 * struct stm_drvdata - specifics associated to an STM component

 * @base:		memory mapped base address for this component.

 * @atclk:		optional clock for the core parts of the STM.

 * @csdev:		component vitals needed by the framework.

 * @spinlock:		only one at a time pls.

 * @chs:		the channels accociated to this STM.

 * @stm:		structure associated to the generic STM interface.

 * @mode:		this tracer's mode, i.e sysFS, or disabled.

 * @traceid:		value of the current ID for this component.

 * @write_bytes:	Maximus bytes this STM can write at a time.

 * @stmsper:		settings for register STMSPER.

 * @stmspscr:		settings for register STMSPSCR.

 * @numsp:		the total number of stimulus port support by this STM.

 * @stmheer:		settings for register STMHEER.

 * @stmheter:		settings for register STMHETER.

 * @stmhebsr:		settings for register STMHEBSR.

 Enable HW event tracing */

 Error detection on event tracing */

 ATB trigger enable on direct writes to TRIG locations */

 4096 byte between synchronisation packets */

 trace id */

 timestamp enable */

 global STM enable */

 Someone is already using the tracer */

 clear global STM enable [0] */

	/*

	 * For as long as the tracer isn't disabled another entity can't

	 * change its status.  As such we can read the status here without

	 * fearing it will change under us.

 Wait until the engine has completely stopped */

 now we are 64bit/32bit aligned */

		/*

		 * The generic STM core sets a size of '0' on flag packets.

		 * As such send a flag packet of size '1' and tell the

		 * core we did so.

 HW event enable and trigger go hand in hand */

 Process as per ARM's TRM recommendation */

 traceid field is 7bit wide on STM32 */

 We have a match and @index is where it's at */

	/*

	 * The stimulus base for STM device must be listed as the second memory

	 * resource, followed by the programming base address as described in

	 * "Section 2.3 Resources" in ACPI for CoreSightTM 1.0 Platform Design

	 * document (DEN0067).

	/*

	 * bit[15:12] represents the fundamental data size

	 * 0 - 32-bit data

	 * 1 - 64-bit data

	/*

	 * NUMPS in STMDEVID is 17 bit long and if equal to 0x0,

	 * 32 stimulus ports are supported.

 Don't use port selection */

	/*

	 * Enable all channel regardless of their number.  When port

	 * selection isn't used (see above) STMSPER applies to all

	 * 32 channel group available, hence setting all 32 bits to 1

	/*

	 * The trace ID value for *ETM* tracers start at CPU_ID * 2 + 0x10 and

	 * anything equal to or higher than 0x70 is reserved.  Since 0x00 is

	 * also reserved the STM trace ID needs to be higher than 0x00 and

	 * lowner than 0x10.

 Set invariant transaction timing on all channels */

	/*

	 * MasterIDs are assigned at HW design phase. As such the core is

	 * using a single master for interaction with this device.

 optional */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2012, The Linux Foundation. All rights reserved.

/*

 * coresight_alloc_conns: Allocate connections record for each output

 * port from the device.

	/*

	 * If we have a non-configurable replicator, it will be found on the

	 * platform bus.

	/*

	 * We have a configurable component - circle through the AMBA bus

	 * looking for the device that matches the endpoint node.

/*

 * Find a registered coresight device from a device fwnode.

 * The node info is associated with the AMBA parent, but the

 * csdev keeps a copy so iterate round the coresight bus to

 * find the device.

	/*

	 * Avoid warnings in of_graph_get_next_endpoint()

	 * if the device doesn't have any graph connections

	/*

	 * Skip one-level up to the real device node, if we

	 * are using the new bindings.

 Defer error handling to parsing */

 Fall back to legacy DT bindings parsing */

/*

 * of_coresight_parse_endpoint : Parse the given output endpoint @ep

 * and fill the connection information in @conn

 *

 * Parses the local port, remote device name and the remote port.

 *

 * Returns :

 *	 0	- If the parsing completed without any fatal errors.

 *	-Errno	- Fatal error, abort the scanning.

 Parse the local port details */

		/*

		 * Get a handle on the remote endpoint and the device it is

		 * attached to.

 If the remote device is not available, defer probing */

		/*

		 * Hold the refcount to the target device. This could be

		 * released via:

		 * 1) coresight_release_platform_data() if the probe fails or

		 *    this device is unregistered.

		 * 2) While removing the target device via

		 *    coresight_remove_match()

 Connection record updated */

 Get the number of input and output port for this component */

 If there are no output connections, we are done */

	/*

	 * If the DT uses obsoleted bindings, the ports are listed

	 * under the device and we need to filter out the input

	 * ports.

 Iterate through each output port to discover topology */

		/*

		 * Legacy binding mixes input/output ports under the

		 * same parent. So, skip the input ports if we are dealing

		 * with legacy binding, as they processed with their

		 * connected output ports.

 ACPI Graph _DSD UUID : "ab02a46b-74c7-45a2-bd68-f7d344ef2153" */

 Coresight ACPI Graph UUID : "3ecbc8b6-1d0e-4fb3-8107-e627f805c6cd" */

/*

 * acpi_guid_matches	- Checks if the given object is a GUID object and

 * that it matches the supplied the GUID.

/*

 * acpi_validate_dsd_graph	- Make sure the given _DSD graph conforms

 * to the ACPI _DSD Graph specification.

 *

 * ACPI Devices Graph property has the following format:

 *  {

 *	Revision	- Integer, must be 0

 *	NumberOfGraphs	- Integer, N indicating the following list.

 *	Graph[1],

 *	 ...

 *	Graph[N]

 *  }

 *

 * And each Graph entry has the following format:

 *  {

 *	GraphID		- Integer, identifying a graph the device belongs to.

 *	UUID		- UUID identifying the specification that governs

 *			  this graph. (e.g, see is_acpi_coresight_graph())

 *	NumberOfLinks	- Number "N" of connections on this node of the graph.

 *	Links[1]

 *	...

 *	Links[N]

 *  }

 *

 * Where each "Links" entry has the following format:

 *

 * {

 *	SourcePortAddress	- Integer

 *	DestinationPortAddress	- Integer

 *	DestinationDeviceName	- Reference to another device

 *	( --- CoreSight specific extensions below ---)

 *	DirectionOfFlow		- Integer 1 for output(master)

 *				  0 for input(slave)

 * }

 *

 * e.g:

 * For a Funnel device

 *

 * Device(MFUN) {

 *   ...

 *

 *   Name (_DSD, Package() {

 *	// DSD Package contains tuples of {  Proeprty_Type_UUID, Package() }

 *	ToUUID("daffd814-6eba-4d8c-8a91-bc9bbf4aa301"), //Std. Property UUID

 *	Package() {

 *		Package(2) { "property-name", <property-value> }

 *	},

 *

 *	ToUUID("ab02a46b-74c7-45a2-bd68-f7d344ef2153"), // ACPI Graph UUID

 *	Package() {

 *	  0,		// Revision

 *	  1,		// NumberOfGraphs.

 *	  Package() {	// Graph[0] Package

 *	     1,		// GraphID

 *	     // Coresight Graph UUID

 *	     ToUUID("3ecbc8b6-1d0e-4fb3-8107-e627f805c6cd"),

 *	     3,		// NumberOfLinks aka ports

 *	     // Link[0]: Output_0 -> Replicator:Input_0

 *	     Package () { 0, 0, \_SB_.RPL0, 1 },

 *	     // Link[1]: Input_0 <- Cluster0_Funnel0:Output_0

 *	     Package () { 0, 0, \_SB_.CLU0.FUN0, 0 },

 *	     // Link[2]: Input_1 <- Cluster1_Funnel0:Output_0

 *	      Package () { 1, 0, \_SB_.CLU1.FUN0, 0 },

 *	  }	// End of Graph[0] Package

 *

 *	}, // End of ACPI Graph Property

 *  })

 The graph must contain at least the Revision and Number of Graphs */

 We only support revision 0 */

 CoreSight devices are only part of a single Graph */

 Make sure the ACPI graph package has right number of elements */

	/*

	 * Each entry must be a graph package with at least 3 members :

	 * { GraphID, UUID, NumberOfLinks(n), Links[.],... }

 acpi_get_dsd_graph	- Find the _DSD Graph property for the given device. */

	/*

	 * _DSD property consists tuples { Prop_UUID, Package() }

	 * Iterate through all the packages and find the Graph.

 All _DSD elements must have a UUID and a Package */

 Skip the non-Graph _DSD packages */

 Invalid graph format, continue */

	/*

	 * Graph must have the following fields :

	 * { GraphID, GraphUUID, NumberOfLinks, Links... }

 The links are validated in acpi_coresight_parse_link() */

/*

 * acpi_get_coresight_graph	- Parse the device _DSD tables and find

 * the Graph property matching the CoreSight Graphs.

 *

 * Returns the pointer to the CoreSight Graph Package when found. Otherwise

 * returns NULL.

 Invalid graph format */

/*

 * acpi_coresight_parse_link	- Parse the given Graph connection

 * of the device and populate the coresight_connection for an output

 * connection.

 *

 * CoreSight Graph specification mandates that the direction of the data

 * flow must be specified in the link. i.e,

 *

 *	SourcePortAddress,	// Integer

 *	DestinationPortAddress,	// Integer

 *	DestinationDeviceName,	// Reference to another device

 *	DirectionOfFlow,	// 1 for output(master), 0 for input(slave)

 *

 * Returns the direction of the data flow [ Input(slave) or Output(master) ]

 * upon success.

 * Returns an negative error number otherwise.

		/*

		 * Hold the refcount to the target device. This could be

		 * released via:

		 * 1) coresight_release_platform_data() if the probe fails or

		 *    this device is unregistered.

		 * 2) While removing the target device via

		 *    coresight_remove_match().

		/*

		 * We are only interested in the port number

		 * for the input ports at this component.

		 * Store the port number in child_port.

 Invalid direction */

/*

 * acpi_coresight_parse_graph	- Parse the _DSD CoreSight graph

 * connection information and populate the supplied coresight_platform_data

 * instance.

	/*

	 * To avoid scanning the table twice (once for finding the number of

	 * output links and then later for parsing the output links),

	 * cache the links information in one go and then later copy

	 * it to the pdata.

			/*

			 * We do not track input port connections for a device.

			 * However we need the highest port number described,

			 * which can be recorded now and reuse this connection

			 * record for an output connection. Hence, do not move

			 * the ptr for input connections

 Copy the connection information to the final location */

 Duplicate output port */

/*

 * acpi_handle_to_logical_cpuid - Map a given acpi_handle to the

 * logical CPU id of the corresponding CPU device.

 *

 * Returns the logical CPU id when found. Otherwise returns >= nr_cpus_id.

/*

 * acpi_coresigh_get_cpu - Find the logical CPU id of the CPU associated

 * with this coresight device. With ACPI bindings, the CoreSight components

 * are listed as child device of the associated CPU.

 *

 * Returns the logical CPU id when found. Otherwise returns 0.

 Cleanup the connection information */

 SPDX-License-Identifier: GPL-2.0

/*

 * This driver enables Trace Buffer Extension (TRBE) as a per-cpu coresight

 * sink device could then pair with an appropriate per-cpu coresight source

 * device (ETE) thus generating required trace data. Trace can be enabled

 * via the perf framework.

 *

 * The AUX buffer handling is inspired from Arm SPE PMU driver.

 *

 * Copyright (C) 2020 ARM Ltd.

 *

 * Author: Anshuman Khandual <anshuman.khandual@arm.com>

/*

 * A padding packet that will help the user space tools

 * in skipping relevant sections in the captured trace

 * data which could not be decoded. TRBE doesn't support

 * formatting the trace data, unlike the legacy CoreSight

 * sinks and thus we use ETE trace packets to pad the

 * sections of the buffer.

/*

 * Minimum amount of meaningful trace will contain:

 * A-Sync, Trace Info, Trace On, Address, Atom.

 * This is about 44bytes of ETE trace. To be on

 * the safer side, we assume 64bytes is the minimum

 * space required for a meaningful session, before

 * we hit a "WRAP" event.

	/*

	 * Even though trbe_base represents vmap()

	 * mapped allocated buffer's start address,

	 * it's being as unsigned long for various

	 * arithmetic and comparision operations &

	 * also to be consistent with trbe_write &

	 * trbe_limit sibling pointers.

 The base programmed into the TRBE */

/*

 * TRBE erratum list

 *

 * The errata are defined in arm64 generic cpu_errata framework.

 * Since the errata work arounds could be applied individually

 * to the affected CPUs inside the TRBE driver, we need to know if

 * a given CPU is affected by the erratum. Unlike the other erratum

 * work arounds, TRBE driver needs to check multiple times during

 * a trace session. Thus we need a quicker access to per-CPU

 * errata and not issue costly this_cpu_has_cap() everytime.

 * We keep a set of the affected errata in trbe_cpudata, per TRBE.

 *

 * We rely on the corresponding cpucaps to be defined for a given

 * TRBE erratum. We map the given cpucap into a TRBE internal number

 * to make the tracking of the errata lean.

 *

 * This helps in :

 *   - Not duplicating the detection logic

 *   - Streamlined detection of erratum across the system

 Sentinel, must be the last entry */

 The total number of listed errata in trbe_errata_cpucaps */

/*

 * Safe limit for the number of bytes that may be overwritten

 * when ARM64_WORKAROUND_TRBE_OVERWRITE_FILL_MODE is triggered.

/*

 * struct trbe_cpudata: TRBE instance specific data

 * @trbe_flag		- TRBE dirty/access flag support

 * @trbe_hw_align	- Actual TRBE alignment required for TRBPTR_EL1.

 * @trbe_align		- Software alignment used for the TRBPTR_EL1.

 * @cpu			- CPU this TRBE belongs to.

 * @mode		- Mode of current operation. (perf/disabled)

 * @drvdata		- TRBE specific drvdata

 * @errata		- Bit map for the errata on this TRBE.

	/*

	 * Disable the TRBE without clearing LIMITPTR which

	 * might be required for fetching the buffer limits.

	/*

	 * Mark the buffer to indicate that there was a WRAP event by

	 * setting the COLLISION flag. This indicates to the user that

	 * the TRBE trace collection was stopped without stopping the

	 * ETE and thus there might be some amount of trace that was

	 * lost between the time the WRAP was detected and the IRQ

	 * was consumed by the CPU.

	 *

	 * Setting the TRUNCATED flag would move the event to STOPPED

	 * state unnecessarily, even when there is space left in the

	 * ring buffer. Using the COLLISION flag doesn't have this side

	 * effect. We only set TRUNCATED flag when there is no space

	 * left in the ring buffer.

	/*

	 * We cannot proceed with the buffer collection and we

	 * do not have any data for the current session. The

	 * etm_perf driver expects to close out the aux_buffer

	 * at event_stop(). So disable the TRBE here and leave

	 * the update_buffer() to return a 0 size.

/*

 * TRBE Buffer Management

 *

 * The TRBE buffer spans from the base pointer till the limit pointer. When enabled,

 * it starts writing trace data from the write pointer onward till the limit pointer.

 * When the write pointer reaches the address just before the limit pointer, it gets

 * wrapped around again to the base pointer. This is called a TRBE wrap event, which

 * generates a maintenance interrupt when operated in WRAP or FILL mode. This driver

 * uses FILL mode, where the TRBE stops the trace collection at wrap event. The IRQ

 * handler updates the AUX buffer and re-enables the TRBE with updated WRITE and

 * LIMIT pointers.

 *

 *	Wrap around with an IRQ

 *	------ < ------ < ------- < ----- < -----

 *	|					|

 *	------ > ------ > ------- > ----- > -----

 *

 *	+---------------+-----------------------+

 *	|		|			|

 *	+---------------+-----------------------+

 *	Base Pointer	Write Pointer		Limit Pointer

 *

 * The base and limit pointers always needs to be PAGE_SIZE aligned. But the write

 * pointer can be aligned to the implementation defined TRBE trace buffer alignment

 * as captured in trbe_cpudata->trbe_align.

 *

 *

 *		head		tail		wakeup

 *	+---------------------------------------+----- ~ ~ ------

 *	|$$$$$$$|################|$$$$$$$$$$$$$$|		|

 *	+---------------------------------------+----- ~ ~ ------

 *	Base Pointer	Write Pointer		Limit Pointer

 *

 * The perf_output_handle indices (head, tail, wakeup) are monotonically increasing

 * values which tracks all the driver writes and user reads from the perf auxiliary

 * buffer. Generally [head..tail] is the area where the driver can write into unless

 * the wakeup is behind the tail. Enabled TRBE buffer span needs to be adjusted and

 * configured depending on the perf_output_handle indices, so that the driver does

 * not override into areas in the perf auxiliary buffer which is being or yet to be

 * consumed from the user space. The enabled TRBE buffer area is a moving subset of

 * the allocated perf auxiliary buffer.

	/*

	 * The ETE trace has alignment synchronization packets allowing

	 * the decoder to reset in case of an overflow or corruption.

	 * So we can use the entire buffer for the snapshot mode.

	/*

	 * When the TRBE is affected by an erratum that could make it

	 * write to the next "virtually addressed" page beyond the LIMIT.

	 * We need to make sure there is always a PAGE after the LIMIT,

	 * within the buffer. Thus we ensure there is at least an extra

	 * page than normal. With this we could then adjust the LIMIT

	 * pointer down by a PAGE later.

/*

 * TRBE Limit Calculation

 *

 * The following markers are used to illustrate various TRBE buffer situations.

 *

 * $$$$ - Data area, unconsumed captured trace data, not to be overridden

 * #### - Free area, enabled, trace will be written

 * %%%% - Free area, disabled, trace will not be written

 * ==== - Free area, padded with ETE_IGNORE_PACKET, trace will be skipped

	/*

	 *		head

	 *	------->|

	 *	|

	 *	head	TRBE align	tail

	 * +----|-------|---------------|-------+

	 * |$$$$|=======|###############|$$$$$$$|

	 * +----|-------|---------------|-------+

	 * trbe_base				trbe_base + nr_pages

	 *

	 * Perf aux buffer output head position can be misaligned depending on

	 * various factors including user space reads. In case misaligned, head

	 * needs to be aligned before TRBE can be configured. Pad the alignment

	 * gap with ETE_IGNORE_PACKET bytes that will be ignored by user tools

	 * and skip this section thus advancing the head.

	/*

	 *	head = tail (size = 0)

	 * +----|-------------------------------+

	 * |$$$$|$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$	|

	 * +----|-------------------------------+

	 * trbe_base				trbe_base + nr_pages

	 *

	 * Perf aux buffer does not have any space for the driver to write into.

 Compute the tail and wakeup indices now that we've aligned head */

	/*

	 * Lets calculate the buffer area which TRBE could write into. There

	 * are three possible scenarios here. Limit needs to be aligned with

	 * PAGE_SIZE per the TRBE requirement. Always avoid clobbering the

	 * unconsumed data.

	 *

	 * 1) head < tail

	 *

	 *	head			tail

	 * +----|-----------------------|-------+

	 * |$$$$|#######################|$$$$$$$|

	 * +----|-----------------------|-------+

	 * trbe_base			limit	trbe_base + nr_pages

	 *

	 * TRBE could write into [head..tail] area. Unless the tail is right at

	 * the end of the buffer, neither an wrap around nor an IRQ is expected

	 * while being enabled.

	 *

	 * 2) head == tail

	 *

	 *	head = tail (size > 0)

	 * +----|-------------------------------+

	 * |%%%%|###############################|

	 * +----|-------------------------------+

	 * trbe_base				limit = trbe_base + nr_pages

	 *

	 * TRBE should just write into [head..base + nr_pages] area even though

	 * the entire buffer is empty. Reason being, when the trace reaches the

	 * end of the buffer, it will just wrap around with an IRQ giving an

	 * opportunity to reconfigure the buffer.

	 *

	 * 3) tail < head

	 *

	 *	tail			head

	 * +----|-----------------------|-------+

	 * |%%%%|$$$$$$$$$$$$$$$$$$$$$$$|#######|

	 * +----|-----------------------|-------+

	 * trbe_base				limit = trbe_base + nr_pages

	 *

	 * TRBE should just write into [head..base + nr_pages] area even though

	 * the [trbe_base..tail] is also empty. Reason being, when the trace

	 * reaches the end of the buffer, it will just wrap around with an IRQ

	 * giving an opportunity to reconfigure the buffer.

	/*

	 * Wakeup may be arbitrarily far into the future. If it's not in the

	 * current generation, either we'll wrap before hitting it, or it's

	 * in the past and has been handled already.

	 *

	 * If there's a wakeup before we wrap, arrange to be woken up by the

	 * page boundary following it. Keep the tail boundary if that's lower.

	 *

	 *	head		wakeup	tail

	 * +----|---------------|-------|-------+

	 * |$$$$|###############|%%%%%%%|$$$$$$$|

	 * +----|---------------|-------|-------+

	 * trbe_base		limit		trbe_base + nr_pages

	/*

	 * There are two situation when this can happen i.e limit is before

	 * the head and hence TRBE cannot be configured.

	 *

	 * 1) head < tail (aligned down with PAGE_SIZE) and also they are both

	 * within the same PAGE size range.

	 *

	 *			PAGE_SIZE

	 *		|----------------------|

	 *

	 *		limit	head	tail

	 * +------------|------|--------|-------+

	 * |$$$$$$$$$$$$$$$$$$$|========|$$$$$$$|

	 * +------------|------|--------|-------+

	 * trbe_base				trbe_base + nr_pages

	 *

	 * 2) head < wakeup (aligned up with PAGE_SIZE) < tail and also both

	 * head and wakeup are within same PAGE size range.

	 *

	 *		PAGE_SIZE

	 *	|----------------------|

	 *

	 *	limit	head	wakeup  tail

	 * +----|------|-------|--------|-------+

	 * |$$$$$$$$$$$|=======|========|$$$$$$$|

	 * +----|------|-------|--------|-------+

	 * trbe_base				trbe_base + nr_pages

	/*

	 * If the head is too close to the limit and we don't

	 * have space for a meaningful run, we rather pad it

	 * and start fresh.

	 *

	 * We might have to do this more than once to make sure

	 * we have enough required space.

	/*

	 * Fill trace buffer mode is used here while configuring the

	 * TRBE for trace capture. In this particular mode, the trace

	 * collection is stopped and a maintenance interrupt is raised

	 * when the current write pointer wraps. This pause in trace

	 * collection gives the software an opportunity to capture the

	 * trace data in the interrupt handler, before reconfiguring

	 * the TRBE.

	/*

	 * Trigger mode is not used here while configuring the TRBE for

	 * the trace capture. Hence just keep this in the ignore mode.

 Synchronize the TRBE enable event */

	/*

	 * Synchronize all the register updates

	 * till now before enabling the TRBE.

	/*

	 * If the trbe is affected by TRBE_WORKAROUND_OVERWRITE_FILL_MODE,

	 * it might write data after a WRAP event in the fill mode.

	 * Thus the check TRBPTR == TRBBASER will not be honored.

	/*

	 * If the TRBE has wrapped around the write pointer has

	 * wrapped and should be treated as limit.

	 *

	 * When the TRBE is affected by TRBE_WORKAROUND_WRITE_OUT_OF_RANGE,

	 * it may write upto 64bytes beyond the "LIMIT". The driver already

	 * keeps a valid page next to the LIMIT and we could potentially

	 * consume the trace data that may have been collected there. But we

	 * cannot be really sure it is available, and the TRBPTR may not

	 * indicate the same. Also, affected cores are also affected by another

	 * erratum which forces the PAGE_SIZE alignment on the TRBPTR, and thus

	 * could potentially pad an entire PAGE_SIZE - 64bytes, to get those

	 * 64bytes. Thus we ignore the potential triggering of the erratum

	 * on WRAP and limit the data to LIMIT.

	/*

	 * TRBE may use a different base address than the base

	 * of the ring buffer. Thus use the beginning of the ring

	 * buffer to compute the offsets.

	/*

	 * If the TRBE is affected by the following erratum, we must fill

	 * the space we skipped with IGNORE packets. And we are always

	 * guaranteed to have at least a PAGE_SIZE space in the buffer.

	/*

	 * TRBE LIMIT and TRBE WRITE pointers must be page aligned. But with

	 * just a single page, there would not be any room left while writing

	 * into a partially filled TRBE buffer after the page size alignment.

	 * Hence restrict the minimum buffer size as two pages.

	/*

	 * We are about to disable the TRBE. And this could in turn

	 * fill up the buffer triggering, an IRQ. This could be consumed

	 * by the PE asynchronously, causing a race here against

	 * the IRQ handler in closing out the handle. So, let us

	 * make sure the IRQ can't trigger while we are collecting

	 * the buffer. We also make sure that a WRAP event is handled

	 * accordingly.

	/*

	 * If the TRBE was disabled due to lack of space in the AUX buffer or a

	 * spurious fault, the driver leaves it disabled, truncating the buffer.

	 * Since the etm_perf driver expects to close out the AUX buffer, the

	 * driver skips it. Thus, just pass in 0 size here to indicate that the

	 * buffer was truncated.

	/*

	 * perf handle structure needs to be shared with the TRBE IRQ handler for

	 * capturing trace data and restarting the handle. There is a probability

	 * of an undefined reference based crash when etm event is being stopped

	 * while a TRBE IRQ also getting processed. This happens due the release

	 * of perf handle via perf_aux_output_end() in etm_event_stop(). Stopping

	 * the TRBE here will ensure that no IRQ could be generated when the perf

	 * handle gets freed in etm_event_stop().

 Check if there is a pending interrupt and handle it here */

		/*

		 * Now that we are handling the IRQ here, clear the IRQ

		 * from the status, to let the irq handler know that it

		 * is taken care of.

		/*

		 * If this was not due to a WRAP event, we have some

		 * errors and as such buffer is empty.

	/*

	 * TRBE_WORKAROUND_OVERWRITE_FILL_MODE causes the TRBE to overwrite a few cache

	 * line size from the "TRBBASER_EL1" in the event of a "FILL".

	 * Thus, we could loose some amount of the trace at the base.

	 *

	 * Before Fix:

	 *

	 *  normal-BASE     head (normal-TRBPTR)         tail (normal-LIMIT)

	 *  |                   \/                       /

	 *   -------------------------------------------------------------

	 *  |   Pg0      |   Pg1       |           |          |  PgN     |

	 *   -------------------------------------------------------------

	 *

	 * In the normal course of action, we would set the TRBBASER to the

	 * beginning of the ring-buffer (normal-BASE). But with the erratum,

	 * the TRBE could overwrite the contents at the "normal-BASE", after

	 * hitting the "normal-LIMIT", since it doesn't stop as expected. And

	 * this is wrong. This could result in overwriting trace collected in

	 * one of the previous runs, being consumed by the user. So we must

	 * always make sure that the TRBBASER is within the region

	 * [head, head+size]. Note that TRBBASER must be PAGE aligned,

	 *

	 *  After moving the BASE:

	 *

	 *  normal-BASE     head (normal-TRBPTR)         tail (normal-LIMIT)

	 *  |                   \/                       /

	 *   -------------------------------------------------------------

	 *  |         |          |xyzdef.     |..   tuvw|                |

	 *   -------------------------------------------------------------

	 *                      /

	 *              New-BASER

	 *

	 * Also, we would set the TRBPTR to head (after adjusting for

	 * alignment) at normal-PTR. This would mean that the last few bytes

	 * of the trace (say, "xyz") might overwrite the first few bytes of

	 * trace written ("abc"). More importantly they will appear in what

	 * userspace sees as the beginning of the trace, which is wrong. We may

	 * not always have space to move the latest trace "xyz" to the correct

	 * order as it must appear beyond the LIMIT. (i.e, [head..head+size]).

	 * Thus it is easier to ignore those bytes than to complicate the

	 * driver to move it, assuming that the erratum was triggered and

	 * doing additional checks to see if there is indeed allowed space at

	 * TRBLIMITR.LIMIT.

	 *

	 *  Thus the full workaround will move the BASE and the PTR and would

	 *  look like (after padding at the skipped bytes at the end of

	 *  session) :

	 *

	 *  normal-BASE     head (normal-TRBPTR)         tail (normal-LIMIT)

	 *  |                   \/                       /

	 *   -------------------------------------------------------------

	 *  |         |          |///abc..     |..  rst|                |

	 *   -------------------------------------------------------------

	 *                      /    |

	 *              New-BASER    New-TRBPTR

	 *

	 * To summarize, with the work around:

	 *

	 *  - We always align the offset for the next session to PAGE_SIZE

	 *    (This is to ensure we can program the TRBBASER to this offset

	 *    within the region [head...head+size]).

	 *

	 *  - At TRBE enable:

	 *     - Set the TRBBASER to the page aligned offset of the current

	 *       proposed write offset. (which is guaranteed to be aligned

	 *       as above)

	 *     - Move the TRBPTR to skip first 256bytes (that might be

	 *       overwritten with the erratum). This ensures that the trace

	 *       generated in the session is not re-written.

	 *

	 *  - At trace collection:

	 *     - Pad the 256bytes skipped above again with IGNORE packets.

	/*

	 * TRBE_WORKAROUND_WRITE_OUT_OF_RANGE could cause the TRBE to write to

	 * the next page after the TRBLIMITR.LIMIT. For perf, the "next page"

	 * may be:

	 *     - The page beyond the ring buffer. This could mean, TRBE could

	 *       corrupt another entity (kernel / user)

	 *     - A portion of the "ring buffer" consumed by the userspace.

	 *       i.e, a page outisde [head, head + size].

	 *

	 * We work around this by:

	 *     - Making sure that we have at least an extra space of PAGE left

	 *       in the ring buffer [head, head + size], than we normally do

	 *       without the erratum. See trbe_min_trace_buf_size().

	 *

	 *     - Adjust the TRBLIMITR.LIMIT to leave the extra PAGE outside

	 *       the TRBE's range (i.e [TRBBASER, TRBLIMITR.LIMI] ).

		/*

		 * We must have more than a PAGE_SIZE worth space in the proposed

		 * range for the TRBE.

 Set the base of the TRBE to the buffer base */

	/*

	 * If the IRQ was spurious, simply re-enable the TRBE

	 * back without modifying the buffer parameters to

	 * retain the trace collected so far.

		/*

		 * We are unable to restart the trace collection,

		 * thus leave the TRBE disabled. The etm-perf driver

		 * is able to detect this with a disconnected handle

		 * (handle->event = NULL).

 Reads to TRBSR_EL1 is fine when TRBE is active */

	/*

	 * If the pending IRQ was handled by update_buffer callback

	 * we have nothing to do here.

 Prohibit the CPU from tracing before we disable the TRBE */

	/*

	 * Ensure the trace is visible to the CPUs and

	 * any external aborts have been resolved.

	/*

	 * If the buffer was truncated, ensure perf callbacks

	 * have completed, which will disable the event.

	 *

	 * Otherwise, restore the trace filter controls to

	 * allow the tracing.

 If the TRBE was not probed on the CPU, we shouldn't be here */

/*

 * Must be called with preemption disabled, for trbe_check_errata().

	/*

	 * Run the TRBE erratum checks, now that we know

	 * this instance is about to be registered.

	/*

	 * If the TRBE is affected by erratum TRBE_WORKAROUND_OVERWRITE_FILL_MODE,

	 * we must always program the TBRPTR_EL1, 256bytes from a page

	 * boundary, with TRBBASER_EL1 set to the page, to prevent

	 * TRBE over-writing 256bytes at TRBBASER_EL1 on FILL event.

	 *

	 * Thus make sure we always align our write pointer to a PAGE_SIZE,

	 * which also guarantees that we have at least a PAGE_SIZE space in

	 * the buffer (TRBLIMITR is PAGE aligned) and thus we can skip

	 * the required bytes at the base.

 If we fail to probe the CPU, let us defer it to hotplug callbacks */

		/*

		 * If this CPU was not probed for TRBE,

		 * initialize it now.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2015 Linaro Limited. All rights reserved.

 * Author: Mathieu Poirier <mathieu.poirier@linaro.org>

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

 Lower address comparator cannot have a higher address value */

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

	/*

	 * Use spinlock to ensure index doesn't change while it gets

	 * dereferenced multiple times within a spinlock block elsewhere.

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

	/*

	 * When contextID tracing is enabled the tracers will insert the

	 * value found in the contextID register in the trace stream.  But if

	 * a process is in a namespace the PID of that process as seen from the

	 * namespace won't be what the kernel sees, something that makes the

	 * feature confusing and can potentially leak kernel only information.

	 * As such refuse to use the feature if @current is not in the initial

	 * PID namespace.

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

	/*

	 * Don't use contextID tracing if coming from a PID namespace.  See

	 * comment in ctxid_pid_store().

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Arm Limited. All rights reserved.

 *

 * Coresight Address Translation Unit support

 *

 * Author: Suzuki K Poulose <suzuki.poulose@arm.com>

 Verbose output for CATU table contents */

/*

 * CATU uses a page size of 4KB for page tables as well as data pages.

 * Each 64bit entry in the table has the following format.

 *

 *	63			12	1  0

 *	------------------------------------

 *	|	 Address [63-12] | SBZ	| V|

 *	------------------------------------

 *

 * Where bit[0] V indicates if the address is valid or not.

 * Each 4K table pages have upto 256 data page pointers, taking upto 2K

 * size. There are two Link pointers, pointing to the previous and next

 * table pages respectively at the end of the 4K page. (i.e, entry 510

 * and 511).

 *  E.g, a table of two pages could look like :

 *

 *                 Table Page 0               Table Page 1

 * SLADDR ===> x------------------x  x--> x-----------------x

 * INADDR    ->|  Page 0      | V |  |    | Page 256    | V | <- INADDR+1M

 *             |------------------|  |    |-----------------|

 * INADDR+4K ->|  Page 1      | V |  |    |                 |

 *             |------------------|  |    |-----------------|

 *             |  Page 2      | V |  |    |                 |

 *             |------------------|  |    |-----------------|

 *             |   ...        | V |  |    |    ...          |

 *             |------------------|  |    |-----------------|

 * INADDR+1020K|  Page 255    | V |  |    |   Page 511  | V |

 * SLADDR+2K==>|------------------|  |    |-----------------|

 *             |  UNUSED      |   |  |    |                 |

 *             |------------------|  |    |                 |

 *             |  UNUSED      |   |  |    |                 |

 *             |------------------|  |    |                 |

 *             |    ...       |   |  |    |                 |

 *             |------------------|  |    |-----------------|

 *             |   IGNORED    | 0 |  |    | Table Page 0| 1 |

 *             |------------------|  |    |-----------------|

 *             |  Table Page 1| 1 |--x    | IGNORED     | 0 |

 *             x------------------x       x-----------------x

 * SLADDR+4K==>

 *

 * The base input address (used by the ETR, programmed in INADDR_{LO,HI})

 * must be aligned to 1MB (the size addressable by a single page table).

 * The CATU maps INADDR{LO:HI} to the first page in the table pointed

 * to by SLADDR{LO:HI} and so on.

 *

 Page pointers are only allocated in the first 2K half */

 CATU expects the INADDR to be aligned to 1M. */

/*

 * catu_get_table : Retrieve the table pointers for the given @offset

 * within the buffer. The buffer is wrapped around to a valid offset.

 *

 * Returns : The CPU virtual address for the beginning of the table

 * containing the data page pointer for @offset. If @daddrp is not NULL,

 * @daddrp points the DMA address of the beginning of the table.

 Make sure offset is within the range */

	/*

	 * Each table can address 1MB and a single kernel page can

	 * contain "CATU_PAGES_PER_SYSPAGE" CATU tables.

 Find the table page where the table_nr lies in */

/*

 * catu_populate_table : Populate the given CATU table.

 * The table is always populated as a circular table.

 * i.e, the "prev" link of the "first" table points to the "last"

 * table and the "next" link of the "last" table points to the

 * "first" table. The buffer should be made linear by calling

 * catu_set_table().

 Index to current system data page */

 Index of CATU page within the system data page */

 Prev link for the first table */

		/*

		 * The @offset is always 1M aligned here and we have an

		 * empty table @table_ptr to fill. Each table can address

		 * upto 1MB data buffer. The last table may have fewer

		 * entries if the buffer size is not aligned.

 Move the pointers for data pages */

		/*

		 * If we have finished all the valid entries, fill the rest of

		 * the table (i.e, last table page) with invalid entries,

		 * to fail the lookups.

 Update the prev/next addresses */

 Sync the table for device */

	/*

	 * Each table can address upto 1MB and we can have

	 * CATU_PAGES_PER_SYSPAGE tables in a system page.

	/*

	 * ETR started off at etr_buf->hwaddr. Convert the RRP/RWP to

	 * offsets within the trace buffer.

 Get the table base address */

 Setup dma mask for the device */

 Default to the 40bits as supported by TMC-ETR */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2020 Linaro Limited. All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

/*

 * This provides a set of generic functions that operate on configurations

 * and features to manage the handling of parameters, the programming and

 * saving of registers used by features on devices.

/*

 * Write the value held in the register structure into the driver internal memory

 * location.

/*

 * Read the driver value into the reg if this is marked as one we want to save.

/*

 * Some register values are set from parameters. Initialise these registers

 * from the current parameter values.

 for param, load routines have validated the index */

 set values into the driver locations referenced in cscfg_reg_csdev */

 copy back values from the driver locations referenced in cscfg_reg_csdev */

 default reset - restore default values */

	/*

	 * set the default values for all parameters and regs from the

	 * relevant static descriptors.

 check if reg set from a parameter otherwise desc default */

			/*

			 * for normal values the union between val64 & val32 + mask32

			 * allows us to init using the 64 bit value

/*

 * For the selected presets, we set the register associated with the parameter, to

 * the value of the preset index associated with the parameter.

 preset in range 1 to nr_presets */

	/*

	 * Go through the array of features, assigning preset values to

	 * feature parameters in the order they appear.

	 * There should be precisely the same number of preset values as the

	 * sum of number of parameters over all the features - but we will

	 * ensure there is no overrun.

 exit early if all params filled */

/*

 * if we are not using a preset, then need to update the feature params

 * with current values. This sets the register associated with the parameter

 * with the current value of that parameter.

/*

 * Configuration values will be programmed into the driver locations if enabling, or read

 * from relevant locations on disable.

/*

 * Enable configuration for the device. Will result in the internal driver data

 * being updated ready for programming into the device.

 *

 * @config_csdev:	config_csdev to set.

 * @preset:		preset values to use - 0 for default.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011-2012, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Program Flow Trace driver

/*

 * Not really modular but using module_param is the easiest way to

 * remain consistent with existing use cases for now.

/*

 * Memory mapped writes to clear os lock are not supported on some processors

 * and OS lock must be unlocked before any memory mapped access on such

 * processors, otherwise memory mapped reads/writes will be invalid.

 Writing any value to ETMOSLAR unlocks the trace registers */

 Ensure pending cp14 accesses complete before setting pwrdwn */

 Ensure pwrup completes before subsequent cp14 accesses */

 Ensure pwrup completes before subsequent cp14 accesses */

 Ensure pending cp14 accesses complete before clearing pwrup */

/**

 * coresight_timeout_etm - loop until a bit has changed to a specific state.

 * @drvdata: etm's private data structure.

 * @offset: address of a register, starting from @addr.

 * @position: the position of the bit of interest.

 * @value: the value the bit should have.

 *

 * Basically the same as @coresight_timeout except for the register access

 * method where we have to account for CP14 configurations.



 * Return: 0 as soon as the bit has taken the desired state or -EAGAIN if

 * TIMEOUT_US has elapsed, which ever happens first.

 Waiting on the bit to go from 0 to 1 */

 Waiting on the bit to go from 1 to 0 */

		/*

		 * Delay is arbitrary - the specification doesn't say how long

		 * we are expected to wait.  Extra check required to make sure

		 * we don't wait needlessly on the last iteration.

	/*

	 * Recommended by spec for cp14 accesses to ensure etmcr write is

	 * complete before polling etmsr

	/*

	 * Recommended by spec for cp14 accesses to ensure etmcr write is

	 * complete before polling etmsr

	/*

	 * Taken verbatim from the TRM:

	 *

	 * To trace all memory:

	 *  set bit [24] in register 0x009, the ETMTECR1, to 1

	 *  set all other bits in register 0x009, the ETMTECR1, to 0

	 *  set all bits in register 0x007, the ETMTECR2, to 0

	 *  set register 0x008, the ETMTEEVR, to 0x6F (TRUE).

 Setting default to 1024 as per TRM recommendation */

 excluding kernel AND user space doesn't make sense */

 nothing to do if neither flags are set */

 instruction execute */

 ARM instruction */

 No data value comparison */

 No exact mach */

 Ignore context ID */

 No need to worry about single address comparators. */

 Bit 0 is address range comparator 1 */

	/*

	 * On ETMv3.5:

	 * ETMACTRn[13,11] == Non-secure state comparison control

	 * ETMACTRn[12,10] == Secure state comparison control

	 *

	 * b00 == Match in all modes in this state

	 * b01 == Do not match in any more in this state

	 * b10 == Match in all modes excepts user mode in this state

	 * b11 == Match only in user mode in this state

 Tracing in secure mode is not supported at this time */

 exclude user, match all modes except user mode */

 exclude kernel, match only in user mode */

	/*

	 * The ETMEEVR register is already set to "hard wire A".  As such

	 * all there is to do is setup an address comparator that spans

	 * the entire address range and configure the state and mode bits.

 Clear configuration from previous run */

 Always start from the default config */

	/*

	 * By default the tracers are configured to trace the whole address

	 * range.  Narrow the field only if requested by user space.

	/*

	 * At this time only cycle accurate, return stack  and timestamp

	 * options are available.

	/*

	 * Possible to have cores with PTM (supports ret stack) and ETM

	 * (never has ret stack) on the same SoC. So if we have a request

	 * for return stack that can't be honoured on this core then

	 * clear the bit - trace will still continue normally

 Turn engine on */

 Apply power to trace registers */

 Make sure all registers are accessible */

 Clear setting from a previous run if need be */

 No external input selected */

 No auxiliary control selected */

 No VMID comparator value selected */

 Configure the tracer based on the session's specifics */

 And enable it */

	/*

	 * Configure the ETM only if the CPU is online.  If it isn't online

	 * hw configuration will take place on the local CPU during bring up.

 Someone is already using the tracer */

 The tracer didn't start */

 Read back sequencer and counters for post trace analysis */

 Setting the prog bit disables tracing immediately */

	/*

	 * There is no way to know when the tracer will be used again so

	 * power down the tracer.

	/*

	 * Taking hotplug lock here protects from clocks getting disabled

	 * with tracing being left on (crash scenario) if user disable occurs

	 * after cpu online mask indicates the cpu is offline but before the

	 * DYING hotplug callback is serviced by the ETM driver.

	/*

	 * Executing etm_disable_hw on the cpu whose ETM is being disabled

	 * ensures that register writes occur when cpu is powered.

	/*

	 * For as long as the tracer isn't disabled another entity can't

	 * change its status.  As such we can read the status here without

	 * fearing it will change under us.

 Make sure all registers are accessible */

 First dummy read */

 Provide power to ETM: ETMPDCR[3] == 1 */

	/*

	 * Clear power down bit since when this bit is set writes to

	 * certain registers might be ignored.

	/*

	 * Set prog bit. It will be set from reset but this is included to

	 * ensure it is set

 Find all capabilities */

 HP dyn state ID returned in ret on success */

 failed dyn state - remove others */

 Validity for the resource is already checked by the AMBA core */

 optional */

	/*

	 * Taking hotplug lock here to avoid racing between etm_remove and

	 * CPU hotplug call backs.

	/*

	 * The readers for etmdrvdata[] are CPU hotplug call backs

	 * and PM notification call backs. Change etmdrvdata[i] on

	 * CPU i ensures these call backs has consistent view

	 * inside one call back function.

 ETM 3.3 */

 ETM 3.5 - Cortex-A5 */

 ETM 3.5 */

 PTM 1.0 */

 PTM 1.1 */

 PTM 1.1 Qualcomm */

 etm_hp_setup() does its own cleanup - exit on error */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2020 Linaro Limited. All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

 Basic features and configurations pre-loaded on initialisation */

 preload called on initialisation */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

 save self-hosted state as per firmware */

 never save any state */

 save self-hosted state only */

/*

 * Check if TRCSSPCICRn(i) is implemented for a given instance.

 *

 * TRCSSPCICRn is implemented only if :

 *	TRCSSPCICR<n> is present only if all of the following are true:

 *		TRCIDR4.NUMSSCC > n.

 *		TRCIDR4.NUMPC > 0b0000 .

 *		TRCSSCSR<n>.PC == 0b1

 Imitate the !relaxed I/O helpers */

 Imitate the !relaxed I/O helpers */

 Imitate the !relaxed I/O helpers */

 Imitate the !relaxed I/O helpers */

 Writing 0 to OS Lock unlocks the trace unit registers */

 Writing 0x1 to OS Lock locks the trace registers */

 Software Lock is only accessible via memory mapped interface */

/*

 * etm4x_prohibit_trace - Prohibit the CPU from tracing at all ELs.

 * When the CPU supports FEAT_TRF, we could move the ETM to a trace

 * prohibited state by filtering the Exception levels via TRFCR_EL1.

 If the CPU doesn't support FEAT_TRF, nothing to do */

/*

 * etm4x_allow_trace - Allow CPU tracing in the respective ELs,

 * as configured by the drvdata->config.mode for the current

 * session. Even though we have TRCVICTLR bits to filter the

 * trace in the ELs, it doesn't prevent the ETM from generating

 * a packet (e.g, TraceInfo) that might contain the addresses from

 * the excluded levels. Thus we use the additional controls provided

 * via the Trace Filtering controls (FEAT_TRF) to make sure no trace

 * is generated for the excluded ELs.

 If the CPU doesn't support FEAT_TRF, nothing to do */

	/*

	 * bit 12 and 13 of HISI_HIP08_CORE_COMMIT_REG are used together

	 * to set core-commit, 2'b00 means cpu is at full speed, 2'b01,

	 * 2'b10, 2'b11 mean reduce pipeline speed, and 2'b01 means level-1

	 * speed(minimun value). So bit 12 and 13 should be cleared together.

 CONFIG_ETM4X_IMPDEF_FEATURE */

 Disable the trace unit before programming trace registers */

	/*

	 * If we use system instructions, we need to synchronize the

	 * write to the TRCPRGCTLR, before accessing the TRCSTATR.

	 * See ARM IHI0064F, section

	 * "4.3.7 Synchronization of register updates"

 wait for TRCSTATR.IDLE to go up */

 nothing specific implemented */

	/*

	 * Resource selector pair 0 is always implemented and reserved.  As

	 * such start at 2.

 always clear status bit on restart if using single-shot */

		/*

		 * Request to keep the trace unit powered and also

		 * emulation of powerdown

	/*

	 * ETE mandates that the TRCRSR is written to before

	 * enabling it.

 Enable the trace unit */

 Synchronize the register updates for sysreg access */

 wait for TRCSTATR.IDLE to go back down to '0' */

	/*

	 * As recommended by section 4.3.7 ("Synchronization when using the

	 * memory-mapped interface") of ARM IHI 0064D

/*

 * The goal of function etm4_config_timestamp_event() is to configure a

 * counter that will tell the tracer to emit a timestamp packet when it

 * reaches zero.  This is done in order to get a more fine grained idea

 * of when instructions are executed so that they can be correlated

 * with execution on other CPUs.

 *

 * To do this the counter itself is configured to self reload and

 * TRCRSCTLR1 (always true) used to get the counter to decrement.  From

 * there a resource selector is configured with the counter and the

 * timestamp control register to use the resource selector to trigger the

 * event that will insert a timestamp packet in the stream.

 No point in trying if we don't have at least one counter */

 Find a counter that hasn't been initialised */

 All the counters have been configured already, bail out */

	/*

	 * Searching for an available resource selector to use, starting at

	 * '2' since every implementation has at least 2 resource selector.

	 * ETMIDR4 gives the number of resource selector _pairs_,

	 * hence multiply by 2.

 Remember what counter we used */

	/*

	 * Initialise original and reload counter value to the smallest

	 * possible value in order to get as much precision as we can.

 Set the trace counter control register */

 Bit 16, reload counter automatically */

 Select single resource selector */

 Resource selector 1, i.e always true */

 Group 0b0010 - Counter and sequencers */

 Counter to use */

 Select single resource selector */

 Resource selector */

 Clear configuration from previous run */

 Always start from the default config */

 Configure filters specified on the perf cmd line, if any. */

 Go from generic option to ETMv4 specifics */

 TRM: Must program this for cycacc to work */

		/*

		 * Configure timestamps to be emitted at regular intervals in

		 * order to correlate instructions executed on different CPUs

		 * (CPU-wide trace scenarios).

		/*

		 * No need to go further if timestamp intervals can't

		 * be configured.

 bit[11], Global timestamp tracing bit */

 bit[6], Context ID tracing bit */

	/*

	 * If set bit ETM_OPT_CTXTID2 in perf config, this asks to trace VMID

	 * for recording CONTEXTIDR_EL2.  Do not enable VMID tracing if the

	 * kernel is not running in EL2.

 return stack - enable if selected and supported */

 bit[12], Return stack enable bit */

	/*

	 * Set any selected configuration and preset.

	 *

	 * This extracts the values of PMU_FORMAT_ATTR(configid) and PMU_FORMAT_ATTR(preset)

	 * in the perf attributes defined in coresight-etm-perf.c.

	 * configid uses bits 63:32 of attr->config2, preset uses bits 3:0 of attr->config.

	 * A zero configid means no configuration active, preset = 0 means no preset selected.

 Configure the tracer based on the session's specifics */

 And enable it */

	/*

	 * Executing etm4_enable_hw on the cpu whose ETM is being enabled

	 * ensures that register writes occur when cpu is powered.

 Someone is already using the tracer */

 The tracer didn't start */

 power can be removed from the trace unit now */

 EN, bit[0] Trace unit enable bit */

	/*

	 * If the CPU supports v8.4 Trace filter Control,

	 * set the ETM to trace prohibited region.

	/*

	 * Make sure everything completes before disabling, as recommended

	 * by section 7.3.77 ("TRCVICTLR, ViewInst Main Control Register,

	 * SSTATUS") of ARM IHI 0064D

 Trace synchronization barrier, is a nop if not supported */

 wait for TRCSTATR.PMSTABLE to go to '1' */

 read the status of the single shot comparators */

 read back the current counter values */

	/*

	 * The config_id occupies bits 63:32 of the config2 perf event attr

	 * field. If this is non-zero then we will have enabled a config.

	/*

	 * Check if the start/stop logic was active when the unit was stopped.

	 * That way we can re-enable the start/stop logic when the process is

	 * scheduled again.  Configuration of the start/stop logic happens in

	 * function etm4_set_event_filters().

 TRCVICTLR::SSSTATUS, bit[9] */

	/*

	 * Taking hotplug lock here protects from clocks getting disabled

	 * with tracing being left on (crash scenario) if user disable occurs

	 * after cpu online mask indicates the cpu is offline but before the

	 * DYING hotplug callback is serviced by the ETM driver.

	/*

	 * Executing etm4_disable_hw on the cpu whose ETM is being disabled

	 * ensures that register writes occur when cpu is powered.

	/*

	 * For as long as the tracer isn't disabled another entity can't

	 * change its status.  As such we can read the status here without

	 * fearing it will change under us.

	/*

	 * ETMs implementing sysreg access must implement TRCDEVARCH.

	/*

	 * All ETMs must implement TRCDEVARCH to indicate that

	 * the component is an ETMv4. To support any broken

	 * implementations we fall back to TRCIDR1 check, which

	 * is not really reliable.

	/*

	 * Always choose the memory mapped io, if there is

	 * a memory map to prevent sysreg access on broken

	 * systems.

	/*

	 * If the CPU supports v8.4 SelfHosted Tracing, enable

	 * tracing at the kernel EL and EL0, forcing to use the

	 * virtual time as the timestamp.

 If we are running at EL2, allow tracing the CONTEXTIDR_EL2. */

	/*

	 * If we are unable to detect the access mechanism,

	 * or unable to detect the trace unit type, fail

	 * early.

 Detect the support for OS Lock before we actually use it */

 Make sure all registers are accessible */

 find all capabilities of the tracing unit */

 INSTP0, bits[2:1] P0 tracing support field */

 TRCBB, bit[5] Branch broadcast tracing support bit */

 TRCCOND, bit[6] Conditional instruction tracing support bit */

 TRCCCI, bit[7] Cycle counting instruction bit */

 RETSTACK, bit[9] Return stack bit */

 NUMEVENT, bits[11:10] Number of events field */

 QSUPP, bits[16:15] Q element support field */

 TSSIZE, bits[28:24] Global timestamp size field */

 maximum size of resources */

 CIDSIZE, bits[9:5] Indicates the Context ID size */

 VMIDSIZE, bits[14:10] Indicates the VMID size */

 CCSIZE, bits[28:25] size of the cycle counter in bits minus 12 */

 CCITMIN, bits[11:0] minimum threshold value that can be programmed */

 EXLEVEL_S, bits[19:16] Secure state instruction tracing */

 EXLEVEL_NS, bits[23:20] Non-secure state instruction tracing */

	/*

	 * TRCERR, bit[24] whether a trace unit can trace a

	 * system error exception.

 SYNCPR, bit[25] implementation has a fixed synchronization period? */

 STALLCTL, bit[26] is stall control implemented? */

 SYSSTALL, bit[27] implementation can support stall control? */

	/*

	 * NUMPROC - the number of PEs available for tracing, 5bits

	 *         = TRCIDR3.bits[13:12]bits[30:28]

	 *  bits[4:3] = TRCIDR3.bits[13:12] (since etm-v4.2, otherwise RES0)

	 *  bits[3:0] = TRCIDR3.bits[30:28]

 NOOVERFLOW, bit[31] is trace overflow prevention supported */

 number of resources trace unit supports */

 NUMACPAIRS, bits[0:3] number of addr comparator pairs for tracing */

 NUMPC, bits[15:12] number of PE comparator inputs for tracing */

	/*

	 * NUMRSPAIR, bits[19:16]

	 * The number of resource pairs conveyed by the HW starts at 0, i.e a

	 * value of 0x0 indicate 1 resource pair, 0x1 indicate two and so on.

	 * As such add 1 to the value of NUMRSPAIR for a better representation.

	 *

	 * For ETM v4.3 and later, 0x0 means 0, and no pairs are available -

	 * the default TRUE and FALSE resource selectors are omitted.

	 * Otherwise for values 0x1 and above the number is N + 1 as per v4.2.

	/*

	 * NUMSSCC, bits[23:20] the number of single-shot

	 * comparator control for tracing. Read any status regs as these

	 * also contain RO capability data.

 NUMCIDC, bits[27:24] number of Context ID comparators for tracing */

 NUMVMIDC, bits[31:28] number of VMID comparators for tracing */

 NUMEXTIN, bits[8:0] number of external inputs implemented */

 TRACEIDSIZE, bits[21:16] indicates the trace ID width */

 ATBTRIG, bit[22] implementation can support ATB triggers? */

	/*

	 * LPOVERRIDE, bit[23] implementation supports

	 * low-power state override

 NUMSEQSTATE, bits[27:25] number of sequencer states implemented */

 NUMCNTR, bits[30:28] number of counters available for tracing */

 Set ELx trace filter access in the TRCVICTLR register */

 disable all events tracing */

 disable stalling */

 enable trace synchronization every 4096 bytes, if available */

 disable timestamp event */

 TRCVICTLR::EVENT = 0x01, select the always on logic */

 TRCVICTLR::EXLEVEL_NS:EXLEVELS: Set kernel / user filtering */

	/*

	 * EXLEVEL_NS, for NonSecure Exception levels.

	 * The mask here is a generic value and must be

	 * shifted to the corresponding field for the registers

 Stay away from hypervisor mode for non-VHE */

/*

 * Construct the exception level masks for a given config.

 * This must be shifted to the corresponding register field

 * for usage.

 All Secure exception levels are excluded from the trace */

 First half of default address comparator */

 Second half of default address comparator */

	/*

	 * Configure the ViewInst function to include this address range

	 * comparator.

	 *

	 * @comparator is divided by two since it is the index in the

	 * etmv4_config::addr_val array but register TRCVIIECTLR deals with

	 * address range comparator _pairs_.

	 *

	 * Therefore:

	 *	index 0 -> compatator pair 0

	 *	index 2 -> comparator pair 1

	 *	index 4 -> comparator pair 2

	 *	...

	 *	index 14 -> comparator pair 7

 Configure the comparator */

	/*

	 * Configure ViewInst Start-Stop control register.

	 * Addresses configured to start tracing go from bit 0 to n-1,

	 * while those configured to stop tracing from 16 to 16 + n-1.

 Trace everything 'default' filter achieved by no filtering */

	/*

	 * TRCVICTLR::SSSTATUS == 1, the start-stop logic is

	 * in the started state

 No start-stop filtering for ViewInst */

	/*

	 * Make default initialisation trace everything

	 *

	 * This is done by a minimum default config sufficient to enable

	 * full instruction trace - with a default filter for trace all

	 * achieved by having no filtering.

	/*

	 * nr_addr_cmp holds the number of comparator _pair_, so time 2

	 * for the total number of comparators.

 Go through the tally of comparators looking for a free one. */

 Address range comparators go in pairs */

 Start/stop address can have odd indexes */

 If we are here all the comparators have been used. */

 Sync events with what Perf got */

	/*

	 * If there are no filters to deal with simply go ahead with

	 * the default filter, i.e the entire address range.

 See if a comparator is free. */

			/*

			 * TRCVICTLR::SSSTATUS == 1, the start-stop logic is

			 * in the started state

 No start-stop filtering for ViewInst */

 Get the right start or stop address */

 Configure comparator */

			/*

			 * If filters::ssstatus == 1, trace acquisition was

			 * started but the process was yanked away before the

			 * the stop address was hit.  As such the start/stop

			 * logic needs to be re-started so that tracing can

			 * resume where it left.

			 *

			 * The start/stop logic status when a process is

			 * scheduled out is checked in function

			 * etm4_disable_perf().

 No include/exclude filtering for ViewInst */

 excluding kernel AND user space doesn't make sense */

 nothing to do if neither flags are set */

	/*

	 * As recommended by 3.4.1 ("The procedure when powering down the PE")

	 * of ARM IHI 0064D

 Lock the OS lock to disable trace and external debugger access */

 wait for TRCSTATR.PMSTABLE to go up */

	/*

	 * Data trace stream is architecturally prohibited for A profile cores

	 * so we don't save (or later restore) trcdvcvr and trcdvcmr - As per

	 * section 1.3.4 ("Possible functional configurations of an ETMv4 trace

	 * unit") of ARM IHI 0064D.

 wait for TRCSTATR.IDLE to go up */

	/*

	 * Power can be removed from the trace unit now. We do this to

	 * potentially save power on systems that respect the TRCPDCR_PU

	 * despite requesting software to save/restore state.

 Save the TRFCR irrespective of whether the ETM is ON */

	/*

	 * Save and restore the ETM Trace registers only if

	 * the ETM is active.

	/*

	 * As recommended by section 4.3.7 ("Synchronization when using the

	 * memory-mapped interface") of ARM IHI 0064D

 Unlock the OS lock to re-enable trace and external debug access */

 Setup PM. Deals with error conditions and counts */

 HP dyn state ID returned in ret on success */

 failed dyn state - remove others */

 TRCPDCR is not accessible with system instructions. */

 ETE v1 has major version == 0b101. Adjust this for logging.*/

 register with config infrastructure & load any current features */

 Validity for the resource is already checked by the AMBA core */

	/*

	 * System register based devices could match the

	 * HW by reading appropriate registers on the HW

	 * and thus we could skip the PID.

  ETMv4 UCI data */

	/*

	 * Taking hotplug lock here to avoid racing between etm4_remove_dev()

	 * and CPU hotplug call backs.

	/*

	 * The readers for etmdrvdata[] are CPU hotplug call backs

	 * and PM notification call backs. Change etmdrvdata[i] on

	 * CPU i ensures these call backs has consistent view

	 * inside one call back function.

 Cortex-A53 */

 Cortex-A57 */

 Cortex-A72 */

 Cortex-A73 */

 Cortex-A35 */

 Cortex-A55 */

 Cortex-A75 */

 Neoverse N1 */

 Cortex-A78 */

 Qualcomm Kryo */

 Qualcomm Kryo */

 Qualcomm Kryo 385 Cortex-A55 */

 Qualcomm Kryo 385 Cortex-A75 */

 Qualcomm Kryo 4XX Cortex-A55 */

 Qualcomm Kryo 4XX Cortex-A76 */

 Qualcomm Kryo 5XX Cortex-A77 */

 Marvell ThunderX2 */

 HiSilicon-Hip08 */

 HiSilicon-Hip09 */

 etm4_pm_setup() does its own cleanup - exit on error */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright(C) 2016 Linaro Limited. All rights reserved.

 * Author: Mathieu Poirier <mathieu.poirier@linaro.org>

 Wait for TMCSReady bit to be set */

 Check if the buffer wrapped around. */

	/*

	 * When operating in sysFS mode the content of the buffer needs to be

	 * read before the TMC is disabled.

 Wait for TMCSReady bit to be set */

/*

 * Return the available trace data in the buffer from @pos, with

 * a maximum limit of @len, updating the @bufpp on where to

 * find it.

 Adjust the len to available size @pos */

	/*

	 * If we don't have a buffer release the lock and allocate memory.

	 * Otherwise keep the lock and move along.

 Allocating the memory here while outside of the spinlock */

 Let's try again */

	/*

	 * In sysFS mode we can have multiple writers per sink.  Since this

	 * sink is already enabled no memory is needed and the HW need not be

	 * touched.

	/*

	 * If drvdata::buf isn't NULL, memory was allocated for a previous

	 * trace run but wasn't read.  If so simply zero-out the memory.

	 * Otherwise use the memory allocated above.

	 *

	 * The memory is freed when users read the buffer using the

	 * /dev/xyz.{etf|etb} interface.  See tmc_read_unprepare_etf() for

	 * details.

 Free up the buffer if we failed to enable */

 Free memory outside the spinlock if need be */

		/*

		 * No need to continue if the ETB/ETF is already operated

		 * from sysFS.

 Get a handle on the pid of the process to monitor */

		/*

		 * No HW configuration is needed if the sink is already in

		 * use for this session.

 Associate with monitored process. */

 We shouldn't be here */

 Complain if we (somehow) got out of sync */

 Dissociate from monitored process. */

 Allocate memory structure for interaction with Perf */

 wrap head around to the amount of space we have */

 find the page to write to */

 and offset within that page */

 This shouldn't happen */

 Don't do anything if another tracer is using this sink */

	/*

	 * Get a hold of the status register and see if a wrap around

	 * has occurred.  If so adjust things accordingly.

	/*

	 * The TMC RAM buffer may be bigger than the space available in the

	 * perf ring buffer (handle->size).  If so advance the RRP so that we

	 * get the latest trace data.  In snapshot mode none of that matters

	 * since we are expected to clobber stale data in favour of the latest

	 * traces.

		/*

		 * Make sure the new size is aligned in accordance with the

		 * requirement explained in function tmc_get_memwidth_mask().

 Move the RAM read pointer up */

 Make sure we are still within our limits */

 Tell the HW */

	/*

	 * Don't set the TRUNCATED flag in snapshot mode because 1) the

	 * captured buffer is expected to be truncated and 2) a full buffer

	 * prevents the event from being re-enabled by the perf core,

	 * resulting in stale data being send to user space.

 for every byte to read */

 wrap around at the end of the buffer */

	/*

	 * In snapshot mode we simply increment the head by the number of byte

	 * that were written.  User space will figure out how many bytes to get

	 * from the AUX buffer based on the position of the head.

	/*

	 * CS_LOCK() contains mb() so it can ensure visibility of the AUX trace

	 * data before the aux_head is updated via perf_aux_output_end(), which

	 * is expected by the perf ring buffer.

 config types are set a boot time and never change */

 Don't interfere if operated from Perf */

 If drvdata::buf is NULL the trace data has been read already */

 Disable the TMC if need be */

 There is no point in reading a TMC in HW FIFO mode */

 config types are set a boot time and never change */

 Re-enable the TMC if need be */

 There is no point in reading a TMC in HW FIFO mode */

		/*

		 * The trace run will continue with the same allocated trace

		 * buffer. As such zero-out the buffer so that we don't end

		 * up with stale data.

		 *

		 * Since the tracer is still enabled drvdata::buf

		 * can't be NULL.

		/*

		 * The ETB/ETF is not tracing and the buffer was just read.

		 * As such prepare to free the trace buffer.

	/*

	 * Free allocated memory outside of the spinlock.  There is no need

	 * to assert the validity of 'buf' since calling kfree(NULL) is safe.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011-2015, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Replicator driver

/**

 * struct replicator_drvdata - specifics associated to a replicator component

 * @base:	memory mapped base address for this component. Also indicates

 *		whether this one is programmable or not.

 * @atclk:	optional clock for the core parts of the replicator.

 * @csdev:	component vitals needed by the framework

 * @spinlock:	serialize enable/disable operations.

 * @check_idfilter_val: check if the context is lost upon clock removal.

/*

 * replicator_reset : Reset the replicator configuration to sane values.

	/*

	 * Some replicator designs lose context when AMBA clocks are removed,

	 * so have a check for this.

 Ensure that the outport is enabled. */

 disable the flow of ATB data through port */

 optional */

	/*

	 * Map the device base for dynamic-replicator, which has been

	 * validated by AMBA core

 Static replicators do not have programming base */

 ARM CoreSight Static Replicator */

 THIS_MODULE is taken care of by platform_driver_register() */

 Coresight SoC-600 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2017 Linaro Limited. All rights reserved.

 *

 * Author: Leo Yan <leo.yan@linaro.org>

 bits definition for EDPCSR */

 bits definition for EDPRCR */

 bits definition for EDPRSR */

 bits definition for EDVIDSR */

/*

 * bits definition for EDDEVID1:PSCROffset

 *

 * NOTE: armv8 and armv7 have different definition for the register,

 * so consolidate the bits definition as below:

 *

 * 0b0000 - Sample offset applies based on the instruction state, we

 *          rely on EDDEVID to check if EDPCSR is implemented or not

 * 0b0001 - No offset applies.

 * 0b0010 - No offset applies, but do not use in AArch32 mode

 *

 bits definition for EDDEVID */

 Unlocks the debug registers */

 Make sure the registers are unlocked before accessing */

/*

 * According to ARM DDI 0487A.k, before access external debug

 * registers should firstly check the access permission; if any

 * below condition has been met then cannot access debug

 * registers to avoid lockup issue:

 *

 * - CPU power domain is powered off;

 * - The OS Double Lock is locked;

 *

 * By checking EDPRSR can get to know if meet these conditions.

 CPU is powered off */

 The OS Double Lock is locked */

	/*

	 * Send request to power management controller and assert

	 * DBGPWRUPREQ signal; if power management controller has

	 * sane implementation, it should enable CPU power domain

	 * in case CPU is in low power state.

 Wait for CPU to be powered up (timeout~=32ms) */

		/*

		 * Unfortunately the CPU cannot be powered up, so return

		 * back and later has no permission to access other

		 * registers. For this case, should disable CPU low power

		 * states to ensure CPU power domain is enabled!

	/*

	 * At this point the CPU is powered up, so set the no powerdown

	 * request bit so we don't lose power and emulate power down.

 The core power domain got switched off on use, try again */

 Unlock os lock */

 Save EDPRCR register */

	/*

	 * Ensure CPU power domain is enabled to let registers

	 * are accessiable.

	/*

	 * As described in ARM DDI 0487A.k, if the processing

	 * element (PE) is in debug state, or sample-based

	 * profiling is prohibited, EDPCSR reads as 0xFFFFFFFF;

	 * EDCIDSR, EDVIDSR and EDPCSR_HI registers also become

	 * UNKNOWN state. So directly bail out for this case.

	/*

	 * A read of the EDPCSR normally has the side-effect of

	 * indirectly writing to EDCIDSR, EDVIDSR and EDPCSR_HI;

	 * at this point it's safe to read value from them.

 Restore EDPRCR register */

 Handle thumb instruction */

	/*

	 * Handle arm instruction offset, if the arm instruction

	 * is not 4 byte alignment then it's possible the case

	 * for implementation defined; keep original value for this

	 * case and print info for notice.

 Read device info */

 Parse implementation feature */

		/*

		 * In ARM DDI 0487A.k, the EDDEVID1.PCSROffset is used to

		 * define if has the offset for PC sampling value; if read

		 * back EDDEVID1.PCSROffset == 0x2, then this means the debug

		 * module does not sample the instruction set state when

		 * armv8 CPU in AArch32 state.

/*

 * Dump out information on panic.

 Bail out if the functionality is disabled */

	/*

	 * Use cpumask to track which debug power domains have

	 * been powered on and use it to handle failure case.

	/*

	 * If pm_runtime_get_sync() has failed, need rollback on

	 * all the other CPUs that have been enabled before that.

	/*

	 * Disable debug power domains, records the error and keep

	 * circling through all other CPUs when an error has been

	 * encountered.

 Create debugfs node */

 Register function to be called for panic */

 Validity for the resource is already checked by the AMBA core */

 Turn off debug power domain if debugging is disabled */

 Turn off debug power domain before rmmod the module */

  CPU Debug UCI data */

 Cortex-A53 */

 Cortex-A57 */

 Cortex-A72 */

 Cortex-A73 */

 Qualcomm Kryo */

 Qualcomm Kryo */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2011-2012, The Linux Foundation. All rights reserved.

 *

 * Description: CoreSight Embedded Trace Buffer driver

 register description */

 STS - 0x00C */

 CTL - 0x020 */

 FFCR - 0x304 */

/**

 * struct etb_drvdata - specifics associated to an ETB component

 * @base:	memory mapped base address for this component.

 * @atclk:	optional clock for the core parts of the ETB.

 * @csdev:	component vitals needed by the framework.

 * @miscdev:	specifics to handle "/dev/xyz.etb" entry.

 * @spinlock:	only one at a time pls.

 * @reading:	synchronise user space access to etb buffer.

 * @pid:	Process ID of the process being monitored by the session

 *		that is using this component.

 * @buf:	area of memory where ETB buffer content gets sent.

 * @mode:	this ETB is being used.

 * @buffer_depth: size of @buf.

 * @trigger_cntr: amount of words to store after a trigger.

 reset write RAM pointer address */

 clear entire RAM buffer */

 reset write RAM pointer address */

 reset read RAM pointer address */

 ETB trace capture enable */

 Don't messup with perf sessions. */

 No need to continue if the component is already in used by sysFS. */

 Get a handle on the pid of the process to monitor */

	/*

	 * No HW configuration is needed if the sink is already in

	 * use for this session.

	/*

	 * We don't have an internal state to clean up if we fail to setup

	 * the perf buffer. So we can perform the step before we turn the

	 * ETB on and leave without cleaning up.

 Associate with monitored process. */

 stop formatter when a stop has completed */

 manually generate a flush of the system */

 disable trace capture */

 Complain if we (somehow) got out of sync */

 Dissociate from monitored process. */

 wrap head around to the amount of space we have */

 find the page to write to */

 and offset within that page */

 Don't do anything if another tracer is using this sink */

 unit is in words, not bytes */

	/*

	 * Entries should be aligned to the frame size.  If they are not

	 * go back to the last alignment point to give decoding tools a

	 * chance to fix things.

	/*

	 * Get a hold of the status register and see if a wrap around

	 * has occurred.  If so adjust things accordingly.  Otherwise

	 * start at the beginning and go until the write pointer has

	 * been reached.

	/*

	 * Make sure we don't overwrite data that hasn't been consumed yet.

	 * It is entirely possible that the HW buffer has more data than the

	 * ring buffer can currently handle.  If so adjust the start address

	 * to take only the last traces.

	 *

	 * In snapshot mode we are looking to get the latest traces only and as

	 * such, we don't care about not overwriting data that hasn't been

	 * processed by user space.

 The new read pointer must be frame size aligned */

		/*

		 * Move the RAM read pointer up, keeping in mind that

		 * everything is in frame size units.

 Wrap around if need be*/

 let the decoder know we've skipped ahead */

	/*

	 * Don't set the TRUNCATED flag in snapshot mode because 1) the

	 * captured buffer is expected to be truncated and 2) a full buffer

	 * prevents the event from being re-enabled by the perf core,

	 * resulting in stale data being send to user space.

 finally tell HW where we want to start reading from */

 wrap around at the end of the buffer */

 reset ETB buffer for next run */

	/*

	 * In snapshot mode we simply increment the head by the number of byte

	 * that were written.  User space will figure out how many bytes to get

	 * from the AUX buffer based on the position of the head.

 optional */

 validity for the resource is already checked by the AMBA core */

 This device is not associated with a session */

	/*

	 * Since misc_open() holds a refcount on the f_ops, which is

	 * etb fops in this case, device is there until last file

	 * handler to this device is closed.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2012, The Linux Foundation. All rights reserved.

/**

 * struct coresight_node - elements of a path, from source to sink

 * @csdev:	Address of an element.

 * @link:	hook to the list.

/*

 * When operating Coresight drivers from the sysFS interface, only a single

 * path can exist from a tracer (associated to a CPU) to a sink.

/*

 * As of this writing only a single STM can be found in CS topologies.  Since

 * there is no way to know if we'll ever see more and what kind of

 * configuration they will enact, for the time being only define a single path

 * for STM.

/*

 * When losing synchronisation a new barrier packet needs to be inserted at the

 * beginning of the data collected in a buffer.  That way the decoder knows that

 * it needs to look for another sync sequence.

	/*

	 * No need to care about oneself and components that are not

	 * sources or not enabled

 Get the source ID for both components */

 All you need is one */

 this shouldn't happen */

/*

 * coresight_claim_device_unlocked : Claim the device for self-hosted usage

 * to prevent an external tool from touching this device. As per PSCI

 * standards, section "Preserving the execution context" => "Debug and Trace

 * save and Restore", DBGCLAIM[1] is reserved for Self-hosted debug/trace and

 * DBGCLAIM[0] is reserved for external tools.

 *

 * Called with CS_UNLOCKed for the component.

 * Returns : 0 on success

 There was a race setting the tags, clean up and fail */

/*

 * coresight_disclaim_device_unlocked : Clear the claim tags for the device.

 * Called with CS_UNLOCKed for the component.

		/*

		 * The external agent may have not honoured our claim

		 * and has manipulated it. Or something else has seriously

		 * gone wrong in our driver.

 enable or disable an associated CTI device of the supplied CS device */

 output warning if ECT enable is preventing trace operation */

/*

 * Set the associated ect / cti device while holding the coresight_mutex

 * to avoid a race with coresight_enable that may try to use this value.

	/*

	 * We need to make sure the "new" session is compatible with the

	 * existing "mode" of operation.

/**

 *  coresight_disable_source - Drop the reference count by 1 and disable

 *  the device if there are no users left.

 *

 *  @csdev: The coresight device to disable

 *

 *  Returns true if the device has been disabled.

/*

 * coresight_disable_path_from : Disable components in the given path beyond

 * @nd in the list. If @nd is NULL, all the components, except the SOURCE are

 * disabled.

		/*

		 * ETF devices are tricky... They can be a link or a sink,

		 * depending on how they are configured.  If an ETF has been

		 * "activated" it will be configured as a sink, otherwise

		 * go ahead with the link configuration.

			/*

			 * We skip the first node in the path assuming that it

			 * is the source. So we don't expect a source device in

			 * the middle of a path.

		/*

		 * ETF devices are tricky... They can be a link or a sink,

		 * depending on how they are configured.  If an ETF has been

		 * "activated" it will be configured as a sink, otherwise

		 * go ahead with the link configuration.

			/*

			 * Sink is the first component turned on. If we

			 * failed to enable the sink, there are no components

			 * that need disabling. Disabling the path here

			 * would mean we could disrupt an existing session.

 sources are enabled from either sysFS or Perf */

	/*

	 * Recursively explore each port found on this element.

/**

 * coresight_get_enabled_sink - returns the first enabled sink using

 * connection based search starting from the source reference

 *

 * @source: Coresight source device reference

		/*

		 * See function etm_perf_add_symlink_sink() to know where

		 * this comes from.

/**

 * coresight_get_sink_by_id - returns the sink that matches the id

 * @id: Id of the sink to match

 *

 * The name of a sink is unique, whether it is found on the AMBA bus or

 * otherwise.  As such the hash of that name can easily be used to identify

 * a sink.

/**

 * coresight_get_ref- Helper function to increase reference count to module

 * and device.

 *

 * @csdev: The coresight device to get a reference on.

 *

 * Return true in successful case and power up the device.

 * Return false when failed to get reference of module.

 Make sure the driver can't be removed */

 Make sure the device can't go away */

/**

 * coresight_put_ref- Helper function to decrease reference count to module

 * and device. Power off the device.

 *

 * @csdev: The coresight device to decrement a reference from.

/*

 * coresight_grab_device - Power up this device and any of the helper

 * devices connected to it for trace operation. Since the helper devices

 * don't appear on the trace path, they should be handled along with the

 * the master device.

/*

 * coresight_drop_device - Release this device and any of the helper

 * devices connected to it.

/**

 * _coresight_build_path - recursively build a path from a @csdev to a sink.

 * @csdev:	The device to start from.

 * @sink:	The final sink we want in this path.

 * @path:	The list to add devices to.

 *

 * The tree of Coresight device is traversed until an activated sink is

 * found.  From there the sink is added to the list along with all the

 * devices that led to that point - the end result is a list from source

 * to sink. In that list the source is the first device and the sink the

 * last one.

 An activated sink has been found.  Enqueue the element */

 Not a sink - recursively explore each port found on this element */

	/*

	 * A path from this element to a sink has been found.  The elements

	 * leading to the sink are already enqueued, all that is left to do

	 * is tell the PM runtime core we need this element and add a node

	 * for it.

/**

 * coresight_release_path - release a previously built path.

 * @path:	the path to release.

 *

 * Go through all the elements of a path and 1) removed it from the list and

 * 2) free the memory allocated for each node.

 return true if the device is a suitable type for a default sink */

 sink & correct subtype */

/**

 * coresight_select_best_sink - return the best sink for use as default from

 * the two provided.

 *

 * @sink:	current best sink.

 * @depth:      search depth where current sink was found.

 * @new_sink:	new sink for comparison with current sink.

 * @new_depth:  search depth where new sink was found.

 *

 * Sinks prioritised according to coresight_dev_subtype_sink, with only

 * subtypes CORESIGHT_DEV_SUBTYPE_SINK_BUFFER or higher being used.

 *

 * Where two sinks of equal priority are found, the sink closest to the

 * source is used (smallest search depth).

 *

 * return @new_sink & update @depth if better than @sink, else return @sink.

 first found at this level */

 found better sink */

 found same but closer sink */

/**

 * coresight_find_sink - recursive function to walk trace connections from

 * source to find a suitable default sink.

 *

 * @csdev: source / current device to check.

 * @depth: [in] search depth of calling dev, [out] depth of found sink.

 *

 * This will walk the connection path from a source (ETM) till a suitable

 * sink is encountered and return that sink to the original caller.

 *

 * If current device is a plain sink return that & depth, otherwise recursively

 * call child connections looking for a sink. Select best possible using

 * coresight_select_best_sink.

 *

 * return best sink found, or NULL if not found at this node or child nodes.

 look past LINKSINK for something better */

	/*

	 * Not a sink we want - or possible child sink may be better.

	 * recursively explore each port found on this element.

 return found sink and depth */

/**

 * coresight_find_default_sink: Find a sink suitable for use as a

 * default sink.

 *

 * @csdev: starting source to find a connected sink.

 *

 * Walks connections graph looking for a suitable sink to enable for the

 * supplied source. Uses CoreSight device subtypes and distance from source

 * to select the best sink.

 *

 * If a sink is found, then the default sink for this device is set and

 * will be automatically used in future.

 *

 * Used in cases where the CoreSight user (perf / sysfs) has not selected a

 * sink.

 look for a default sink if we have not found for this device */

/**

 * coresight_clear_default_sink: Remove all default sink references to the

 * supplied sink.

 *

 * If supplied device is a sink, then check all the bus devices and clear

 * out all the references to this sink from the coresight_device def_sink

 * parameter.

 *

 * @csdev: coresight sink - remove references to this from all sources.

/** coresight_validate_source - make sure a source has the right credentials

 *  @csdev:	the device structure for a source.

 *  @function:	the function this was called from.

 *

 * Assumes the coresight_mutex is held.

		/*

		 * There could be multiple applications driving the software

		 * source. So keep the refcount for each such user when the

		 * source is already enabled.

		/*

		 * When working from sysFS it is important to keep track

		 * of the paths that were created so that they can be

		 * undone in 'coresight_disable()'.  Since there can only

		 * be a single session per tracer (when working from sysFS)

		 * a per-cpu variable will do just fine.

 We can't be here */

 We can't be here */

 No need to check oneself */

 Move on to another component if no connection is orphan */

	/*

	 * Circle throuch all the connection of that component.  If we find

	 * an orphan connection whose name matches @csdev, link it.

 Skip the port if FW doesn't describe it */

 We have found at least one orphan connection */

 Does it match this newly added device? */

 This component still has an orphan */

	/*

	 * Returning '0' in case we didn't encounter any error,

	 * ensures that all known component on the bus will be checked.

 No need to check oneself */

	/*

	 * Circle throuch all the connection of that component.  If we find

	 * a connection whose name matches @csdev, remove it.

			/*

			 * Drop the reference to the handle for the remote

			 * device acquired in parsing the connections from

			 * platform data.

 No need to continue */

	/*

	 * Returning '0' ensures that all known component on the

	 * bus will be checked.

/*

 * coresight_remove_conns - Remove references to this given devices

 * from the connections of other devices.

	/*

	 * Another device will point to this device only if there is

	 * an output port connected to this one. i.e, if the device

	 * doesn't have at least one input port, there is no point

	 * in searching all the devices.

/**

 * coresight_timeout - loop until a bit has changed to a specific register

 *			state.

 * @csa: coresight device access for the device

 * @offset: Offset of the register from the base of the device.

 * @position: the position of the bit of interest.

 * @value: the value the bit should have.

 *

 * Return: 0 as soon as the bit has taken the desired state or -EAGAIN if

 * TIMEOUT_US has elapsed, which ever happens first.

 waiting on the bit to go from 0 to 1 */

 waiting on the bit to go from 1 to 0 */

		/*

		 * Delay is arbitrary - the specification doesn't say how long

		 * we are expected to wait.  Extra check required to make sure

		 * we don't wait needlessly on the last iteration.

/*

 * coresight_release_platform_data: Release references to the devices connected

 * to the output port of this device.

 If we have made the links, remove them now */

		/*

		 * Drop the refcount and clear the handle as this device

		 * is going away

	/*

	 * Hold the reference to our parent device. This will be

	 * dropped only in coresight_device_release().

		/*

		 * All resources are free'd explicitly via

		 * coresight_device_release(), triggered from put_device().

			/*

			 * As with the above, all resources are free'd

			 * explicitly via coresight_device_release() triggered

			 * from put_device(), which is in turn called from

			 * function device_unregister().

 Cleanup the connection information */

 Remove references of that device in the topology */

/*

 * coresight_search_device_idx - Search the fwnode handle of a device

 * in the given dev_idx list. Must be called with the coresight_mutex held.

 *

 * Returns the index of the entry, when found. Otherwise, -ENOENT.

/*

 * coresight_alloc_device_name - Get an index for a given device in the

 * device index list specific to a driver. An index is allocated for a

 * device and is tracked with the fwnode_handle to prevent allocating

 * duplicate indices for the same device (e.g, if we defer probing of

 * a device due to dependencies), in case the index is requested again.

 Make space for the new entry */

 initialise the coresight syscfg API */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 Linaro Limited, All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

 create a default ci_type. */

 configurations sub-group */

 attributes for the config view group */

 list preset values in order of features and params */

 start index on the correct array line */

	/*

	 * A set of presets is the sum of all params in used features,

	 * in order of declaration of features and params in the features

 add in a preset<n> dir for each preset */

 attributes for features view */

 base feature desc attrib structures */

/*

 * configfs has far less functionality provided to add attributes dynamically than sysfs,

 * and the show and store fns pass the enclosing config_item so the actual attribute cannot

 * be determined. Therefore we add each item as a group directory, with a value attribute.

 parameter items - as groups with default_value attribute */

 add configuration to configurations group */

 add feature to features group */

 Add default groups to subsystem */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 Linaro Limited, All rights reserved.

 * Author: Mike Leach <mike.leach@linaro.org>

/*

 * cscfg_ API manages configurations and features for the entire coresight

 * infrastructure.

 *

 * It allows the loading of configurations and features, and loads these into

 * coresight devices as appropriate.

 protect the cscsg_data and device */

 only one of these */

 load features and configuations into the lists */

 get name feature instance from a coresight device list of features */

 allocate the device config instance - with max number of used features */

 this is being allocated using the devm for the coresight device */

 Load a config into a device if there are any feature matches between config and device */

 look at each required feature and see if it matches any feature on the device */

 look for a matching name */

			/*

			 * At least one feature on this device matches the config

			 * add a config instance to the device and a reference to the feature.

 if matched features, add config to device.*/

/*

 * Add the config to the set of registered devices - call with mutex locked.

 * Iterates through devices - any device that matches one or more of the

 * configuration features will load it, the others will ignore it.

/*

 * Allocate a feature object for load into a csdev.

 * memory allocated using the csdev->dev object using devm managed allocator.

 parameters are optional - could be 0 */

	/*

	 * if we need parameters, zero alloc the space here, the load routine in

	 * the csdev device driver will fill out some information according to

	 * feature descriptor.

		/*

		 * fill in the feature reference in the param - other fields

		 * handled by loader in csdev.

	/*

	 * Always have registers to program - again the load routine in csdev device

	 * will fill out according to feature descriptor and device requirements.

 load the feature default values */

 load one feature into one coresight device */

 load the feature into the device */

 add to internal csdev feature list & initialise using reset call */

/*

 * Add feature to any matching devices - call with mutex locked.

 * Iterates through devices - any device that matches the feature will be

 * called to load it.

 check feature list for a named feature - call with mutex locked. */

 check all feat needed for cfg are in the list - call with mutex locked. */

/*

 * load feature - add to feature list.

 add feature to any matching registered devices */

/*

 * load config into the system - validate used features exist then add to

 * config list.

 validate features are present */

 add config to any matching registered device */

 add config to perf fs to allow selection */

 get a feature descriptor by name */

 called with cscfg_mutex held */

 check if any config active & return busy */

 set the value */

 update loaded instances.*/

/**

 * cscfg_load_config_sets - API function to load feature and config sets.

 *

 * Take a 0 terminated array of feature descriptors and/or configuration

 * descriptors and load into the system.

 * Features are loaded first to ensure configuration dependencies can be met.

 *

 * @config_descs: 0 terminated array of configuration descriptors.

 * @feat_descs:   0 terminated array of feature descriptors.

 load features first */

 next any configurations to check feature dependencies */

 Handle coresight device registration and add configs and features to devices */

 iterate through config lists and load matching configs to device */

 iterate through feature lists and load matching features to device */

 Add coresight device to list and copy its matching info */

 allocate the list entry structure */

 remove a coresight device from the list and free data */

/**

 * cscfg_register_csdev - register a coresight device with the syscfg manager.

 *

 * Registers the coresight device with the system. @match_flags used to check

 * if the device is a match for registered features. Any currently registered

 * configurations and features that match the device will be loaded onto it.

 *

 * @csdev:		The coresight device to register.

 * @match_flags:	Matching information to load features.

 * @ops:		Standard operations supported by the device.

 add device to list of registered devices  */

 now load any registered features and configs matching the device. */

/**

 * cscfg_unregister_csdev - remove coresight device from syscfg manager.

 *

 * @csdev: Device to remove.

/**

 * cscfg_csdev_reset_feats - reset features for a CoreSight device.

 *

 * Resets all parameters and register values for any features loaded

 * into @csdev to their default values.

 *

 * @csdev: The CoreSight device.

/**

 * cscfg_activate_config -  Mark a configuration descriptor as active.

 *

 * This will be seen when csdev devices are enabled in the system.

 * Only activated configurations can be enabled on individual devices.

 * Activation protects the configuration from alteration or removal while

 * active.

 *

 * Selection by hash value - generated from the configuration name when it

 * was loaded and added to the cs_etm/configurations file system for selection

 * by perf.

 *

 * Increments the configuration descriptor active count and the global active

 * count.

 *

 * @cfg_hash: Hash value of the selected configuration name.

			/*

			 * increment the global active count - control changes to

			 * active configurations

			/*

			 * mark the descriptor as active so enable config on a

			 * device instance will use it

/**

 * cscfg_deactivate_config -  Mark a config descriptor as inactive.

 *

 * Decrement the configuration and global active counts.

 *

 * @cfg_hash: Hash value of the selected configuration name.

/**

 * cscfg_csdev_enable_active_config - Enable matching active configuration for device.

 *

 * Enables the configuration selected by @cfg_hash if the configuration is supported

 * on the device and has been activated.

 *

 * If active and supported the CoreSight device @csdev will be programmed with the

 * configuration, using @preset parameters.

 *

 * Should be called before driver hardware enable for the requested device, prior to

 * programming and enabling the physical hardware.

 *

 * @csdev:	CoreSight device to program.

 * @cfg_hash:	Selector for the configuration.

 * @preset:	Preset parameter values to use, 0 for current / default values.

 quickly check global count */

	/*

	 * Look for matching configuration - set the active configuration

	 * context if found.

	/*

	 * If found, attempt to enable

		/*

		 * Call the generic routine that will program up the internal

		 * driver structures prior to programming up the hardware.

		 * This routine takes the driver spinlock saved in the configs.

			/*

			 * Successful programming. Check the active_cscfg_ctxt

			 * pointer to ensure no pre-emption disabled it via

			 * cscfg_csdev_disable_active_config() before

			 * we could start.

			 *

			 * Set enabled if OK, err if not.

/**

 * cscfg_csdev_disable_active_config - disable an active config on the device.

 *

 * Disables the active configuration on the CoreSight device @csdev.

 * Disable will save the values of any registers marked in the configurations

 * as save on disable.

 *

 * Should be called after driver hardware disable for the requested device,

 * after disabling the physical hardware and reading back registers.

 *

 * @csdev: The CoreSight device.

	/*

	 * Check if we have an active config, and that it was successfully enabled.

	 * If it was not enabled, we have no work to do, otherwise mark as disabled.

	 * Clear the active config pointer.

 true if there was an enabled active config */

 Initialise system configuration management device. */

 Must have a release function or the kernel will complain on module unload */

 a device is needed to "own" some kernel elements such as sysfs entries.  */

 setup the device */

 Initialise system config management API device  */

 preload built-in configurations */

 SPDX-License-Identifier: GPL-2.0

/*

 * Basic framing protocol for STM devices.

 * Copyright (c) 2018, Intel Corporation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Simple kernel console driver for STM devices

 * Copyright (c) 2014, Intel Corporation.

 *

 * STM console will send kernel messages over STM devices to a trace host.

 SPDX-License-Identifier: GPL-2.0

/*

 * Simple kernel driver to link kernel Ftrace and an STM device

 * Copyright (c) 2016, Linaro Ltd.

 *

 * STM Ftrace will be registered as a trace_export.

/**

 * stm_ftrace_write() - write data to STM via 'stm_ftrace' source

 * @buf:	buffer containing the data packet

 * @len:	length of the data packet

 This is called from trace system with preemption disabled */

 SPDX-License-Identifier: GPL-2.0

/*

 * A dummy STM device for stm/stm_source class testing.

 * Copyright (c) 2014, Intel Corporation.

 *

 * STM class implements generic infrastructure for  System Trace Module devices

 * as defined in MIPI STPv2 specification.

 SPDX-License-Identifier: GPL-2.0

/*

 * System Trace Module (STM) master/channel allocation policy management

 * Copyright (c) 2014, Intel Corporation.

 *

 * A master/channel allocation policy allows mapping string identifiers to

 * master and channel ranges, where allocation can be done.

/*

 * STP Master/Channel allocation policy configfs layout.

 this is the one that's exposed to the attributes */

 must be within [sw_start..sw_end], which is an inclusive range */

 default values for the attributes */

/*

 * Root group: policies.

	/*

	 * stp_policy_release() will not call here if the policy is already

	 * unbound; other users should not either, as no link exists between

	 * this policy and anything else in that case

	/*

	 * Drop the reference on the protocol driver and lose the link.

 a policy *can* be unbound and still exist in configfs tree */

	/*

	 * node must look like <device_name>.<policy_name>, where

	 * <device_name> is the name of an existing stm device; may

	 *               contain dots;

	 * <policy_name> is an arbitrary string; may not contain dots

	 * <device_name>:<protocol_name>.<policy_name>

	/*

	 * look for ":<protocol_name>":

	 *  + no protocol suffix: fall back to whatever is available;

	 *  + unknown protocol: fail the whole thing

		/*

		 * pdrv and stm->pdrv at this point can be quite different,

		 * and only one of them needs to be 'put'

/*

 * Lock the policy mutex from the outside

 SPDX-License-Identifier: GPL-2.0

/*

 * System Trace Module (STM) infrastructure

 * Copyright (c) 2014, Intel Corporation.

 *

 * STM class implements generic infrastructure for  System Trace Module devices

 * as defined in MIPI STPv2 specification.

/*

 * The SRCU here makes sure that STM device doesn't disappear from under a

 * stm_source_write() caller, which may want to have as little overhead as

 * possible.

/**

 * stm_find_device() - find stm device by name

 * @buf:	character buffer containing the name

 *

 * This is called when either policy gets assigned to an stm device or an

 * stm_source device gets linked to an stm device.

 *

 * This grabs device's reference (get_device()) and module reference, both

 * of which the calling path needs to make sure to drop with stm_put_device().

 *

 * Return:	stm device pointer or null if lookup failed.

 matches class_find_device() above */

/**

 * stm_put_device() - drop references on the stm device

 * @stm:	stm device, previously acquired by stm_find_device()

 *

 * This drops the module reference and device reference taken by

 * stm_find_device() or stm_char_open().

/*

 * Internally we only care about software-writable masters here, that is the

 * ones in the range [stm_data->sw_start..stm_data..sw_end], however we need

 * original master numbers to be visible externally, since they are the ones

 * that will appear in the STP stream. Thus, the internal bookkeeping uses

 * $master - stm_data->sw_start to reference master descriptors and such.

/*

 * This is like bitmap_find_free_region(), except it can ignore @start bits

 * at the beginning.

 step over [pos..pos+i) to continue search */

 We no longer accept policy_node==NULL here */

	/*

	 * Also, the caller holds reference to policy_node, so it won't

	 * disappear on us.

 output is already assigned -- shouldn't happen */

 configfs subsys mutex is held by the caller */

/*

 * Framing protocol management

 * Modules can implement STM protocol drivers and (un-)register them

 * with the STM class framework.

	/*

	 * If no name is given (NULL or ""), fall back to "p_basic".

 matches class_find_device() above */

	/*

	 * matches the stm_char_open()'s

	 * class_find_device() + try_module_get()

	/*

	 * On success, stp_policy_node_lookup() will return holding the

	 * configfs subsystem mutex, which is then released in

	 * stp_policy_node_put(). This allows the pdrv->output_open() in

	 * stm_output_assign() to serialize against the attribute accessors.

/**

 * stm_data_write() - send the given payload as data packets

 * @data:	stm driver's data

 * @m:		STP master

 * @c:		STP channel

 * @ts_first:	timestamp the first packet

 * @buf:	data payload buffer

 * @count:	data payload size

 stm->pdrv is serialized against policy_mutex */

	/*

	 * If no m/c have been assigned to this writer up to this

	 * point, try to use the task name and "default" policy entries.

		/*

		 * EBUSY means that somebody else just assigned this

		 * output, which is just fine for write()

	/*

	 * size + 1 to make sure the .id string at the bottom is terminated,

	 * which is also why memdup_user() is not useful here

 initialize the object before it is accessible via sysfs */

	/*

	 * Use delayed autosuspend to avoid bouncing back and forth

	 * on recurring character device writes, with the initial

	 * delay time of 2 seconds.

 matches device_initialize() above */

		/*

		 * src <-> stm link must not change under the same

		 * stm::link_mutex, so complain loudly if it has;

		 * also in this situation ret!=0 means this src is

		 * not connected to this stm and it should be otherwise

		 * safe to proceed with the tear-down of stm.

/*

 * stm::link_list access serialization uses a spinlock and a mutex; holding

 * either of them guarantees that the list is stable; modification requires

 * holding both of them.

 *

 * Lock ordering is as follows:

 *   stm::link_mutex

 *     stm::link_lock

 *       src::link_lock

/**

 * stm_source_link_add() - connect an stm_source device to an stm device

 * @src:	stm_source device

 * @stm:	stm device

 *

 * This function establishes a link from stm_source to an stm device so that

 * the former can send out trace data to the latter.

 *

 * Return:	0 on success, -errno otherwise.

 src->link is dereferenced under stm_source_srcu but not the list */

 this is to notify the STM device that a new link has been made */

 this is to let the source carry out all necessary preparations */

/**

 * __stm_source_link_drop() - detach stm_source from an stm device

 * @src:	stm_source device

 * @stm:	stm device

 *

 * If @stm is @src::link, disconnect them from one another and put the

 * reference on the @stm device.

 *

 * Caller must hold stm::link_mutex.

 for stm::link_list modification, we hold both mutex and spinlock */

	/*

	 * The linked device may have changed since we last looked, because

	 * we weren't holding the src::link_lock back then; if this is the

	 * case, tell the caller to retry.

 matches stm_find_device() from stm_source_link_store() */

	/*

	 * Call the unlink callbacks for both source and stm, when we know

	 * that we have actually performed the unlinking.

/**

 * stm_source_link_drop() - detach stm_source from its stm device

 * @src:	stm_source device

 *

 * Unlinking means disconnecting from source's STM device; after this

 * writes will be unsuccessful until it is linked to a new STM device.

 *

 * This will happen on "stm_source_link" sysfs attribute write to undo

 * the existing link (if any), or on linked STM device's de-registration.

	/*

	 * The stm device will be valid for the duration of this

	 * read section, but the link may change before we grab

	 * the src::link_lock in __stm_source_link_drop().

 if it did change, retry */

 matches the stm_find_device() above */

/**

 * stm_source_register_device() - register an stm_source device

 * @parent:	parent device

 * @data:	device description structure

 *

 * This will create a device of stm_source class that can write

 * data to an stm device once linked.

 *

 * Return:	0 on success, -errno otherwise.

/**

 * stm_source_unregister_device() - unregister an stm_source device

 * @data:	device description that was used to register the device

 *

 * This will remove a previously created stm_source device from the system.

	/*

	 * So as to not confuse existing users with a requirement

	 * to load yet another module, do it here.

 SPDX-License-Identifier: GPL-2.0

/*

 * MIPI SyS-T framing protocol for STM devices.

 * Copyright (c) 2018, Intel Corporation.

 Clock value and frequency */

 We require an existing policy node to proceed */

	/*

	 * STP framing rules for SyS-T frames:

	 *   * the first packet of the SyS-T frame is timestamped;

	 *   * the last packet is a FLAG.

 Message layout: HEADER / GUID / [LENGTH /][TIMESTAMP /] DATA */

 HEADER */

 GUID */

 [LENGTH] */

 [TIMESTAMP] */

 DATA */

 SPDX-License-Identifier: GPL-2.0

/*

 * Simple heartbeat STM source driver

 * Copyright (c) 2016, Intel Corporation.

 *

 * Heartbeat STM source will send repetitive messages over STM devices to a

 * trace host.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub PTI output driver

 *

 * Copyright (C) 2014-2016 Intel Corporation.

 map PTI widths to MODE settings of PTI_CTL register */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub Software Trace Hub support

 *

 * Copyright (C) 2014-2015 Intel Corporation.

 Global packets (GERR, XSYNC, TRIG) are sent with register writes */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub Global Trace Hub

 *

 * Copyright (C) 2014-2015 Intel Corporation.

/**

 * struct gth_output - GTH view on an output port

 * @gth:	backlink to the GTH device

 * @output:	link to output device's output descriptor

 * @index:	output port number

 * @port_type:	one of GTH_* port type values

 * @master:	bitmap of masters configured for this output

/**

 * struct gth_device - GTH device

 * @dev:	driver core's device

 * @base:	register window base address

 * @output_group:	attributes describing output ports

 * @master_group:	attributes describing master assignments

 * @output:		output ports

 * @master:		master/output port assignments

 * @gth_lock:		serializes accesses to GTH bits

/*

 * "masters" attribute group

 disconnect from the previous output port, if any */

		/*

		 * if the port is active, program this setting,

		 * implies that runtime PM is on

 connect to the new output port, if any */

 check if there's a driver for this port */

 if the port is active, program this setting, see above */

/*

 * Reset outputs and sources

 Always save/restore STH and TU registers in S0ix entry/exit */

 output ports */

 disable overrides */

 masters swdest_0~31 and gswdest */

 sources */

 setup CTS for single trigger */

/*

 * "outputs" attribute group

/**

 * intel_th_gth_stop() - stop tracing to an output device

 * @gth:		GTH device

 * @output:		output device's descriptor

 * @capture_done:	set when no more traces will be captured

 *

 * This will stop tracing using force storeEn off signal and wait for the

 * pipelines to be empty for the corresponding output port.

 wait on pipeline empty for the given port */

 wait on output piepline empty */

 clear force capture done for next captures */

/**

 * intel_th_gth_start() - start tracing to an output device

 * @gth:	GTH device

 * @output:	output device's descriptor

 *

 * This will start tracing using force storeEn signal.

/**

 * intel_th_gth_disable() - disable tracing to an output device

 * @thdev:	GTH device

 * @output:	output device's descriptor

 *

 * This will deconfigure all masters set to output to this device,

 * disable tracing using force storeEn off signal and wait for the

 * "pipeline empty" bit for corresponding output port.

	/*

	 * Wait until the output port is in reset before we start

	 * programming it.

/**

 * intel_th_gth_enable() - enable tracing to an output device

 * @thdev:	GTH device

 * @output:	output device's descriptor

 *

 * This will configure all masters set to output to this device and

 * enable tracing using force storeEn signal.

/**

 * intel_th_gth_switch() - execute a switch sequence

 * @thdev:	GTH device

 * @output:	output device's descriptor

 *

 * This will execute a switch sequence that will trigger a switch window

 * when tracing to MSC in multi-block mode.

 trigger */

 wait on trigger status */

 De-assert the trigger */

/**

 * intel_th_gth_assign() - assign output device to a GTH output port

 * @thdev:	GTH device

 * @othdev:	output device

 *

 * This will match a given output device parameters against present

 * output ports on the GTH and fill out relevant bits in output device's

 * descriptor.

 *

 * Return:	0 on success, -errno on error.

/**

 * intel_th_gth_unassign() - deassociate an output device from its output port

 * @thdev:	GTH device

 * @othdev:	output device

 FIXME: make default output configurable */

	/*

	 * everything above TH_CONFIGURABLE_MASTERS is controlled by the

	 * same register

	/*

	 * Host mode can be signalled via SW means or via SCRPD_DEBUGGER_IN_USE

	 * bit. Either way, don't reset HW in this case, and don't export any

	 * capture configuration attributes. Also, refuse to assign output

	 * drivers to ports, see intel_th_gth_assign().

 -ENODEV is ok, we just won't have that device enumerated */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub pci driver

 *

 * Copyright (C) 2014-2015 Intel Corporation.

 Apollo Lake */

 Broxton */

 Broxton B-step */

 Kaby Lake PCH-H */

 Denverton */

 Lewisburg PCH */

 Lewisburg PCH */

 Gemini Lake */

 Cannon Lake H */

 Cannon Lake LP */

 Cedar Fork PCH */

 Ice Lake PCH */

 Comet Lake */

 Comet Lake PCH */

 Comet Lake PCH-V */

 Ice Lake NNPI */

 Ice Lake CPU */

 Tiger Lake CPU */

 Tiger Lake PCH */

 Tiger Lake PCH-H */

 Jasper Lake PCH */

 Jasper Lake CPU */

 Elkhart Lake CPU */

 Elkhart Lake */

 Emmitsburg PCH */

 Alder Lake */

 Alder Lake-P */

 Alder Lake-M */

 Alder Lake CPU */

 Rocket Lake CPU */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub driver core

 *

 * Copyright (C) 2014-2015 Intel Corporation.

 does not talk to hardware */

		/*

		 * disconnect outputs

		 *

		 * intel_th_child_remove returns 0 unconditionally, so there is

		 * no need to check the return value of device_for_each_child.

		/*

		 * Remove outputs, that is, hub's children: they are created

		 * at hub's probe time by having the hub call

		 * intel_th_output_enable() for each of them.

			/*

			 * Move the non-output devices from higher up the

			 * th->thdev[] array to lower positions to maintain

			 * a contiguous array.

 does not talk to hardware */

/*

 * Intel(R) Trace Hub subdevices

 Handle TSCU and CTS from GTH driver */

 CONFIG_MODULES */

		/*

		 * Take .end == 0 to mean 'take the whole bar',

		 * .start then tells us which bar it is. Default to

		 * TH_MMIO_CONFIG.

			/*

			 * Only pass on the IRQ if we have useful interrupts:

			 * the ones that can be configured via MINTCTL.

 need switch driver to be loaded to enumerate the rest */

/**

 * intel_th_output_enable() - find and enable a device for a given output type

 * @th:		Intel TH instance

 * @otype:	output type

 *

 * Go through the unallocated output devices, find the first one whos type

 * matches @otype and instantiate it. These devices are removed when the hub

 * device is removed, see intel_th_remove().

 no unallocated matching subdevices */

		/*

		 * intel_th_subdevices[src] matches our requirements and is

		 * not matched in th::thdev[]

 create devices for each intel_th_subdevice */

 only allow SOURCE and SWITCH devices in host mode */

		/*

		 * don't enable port OUTPUTs in this path; SWITCH enables them

		 * via intel_th_output_enable()

 note: caller should free subdevices from th::thdev[] */

 ENODEV for individual subdevices is allowed */

/**

 * intel_th_alloc() - allocate a new Intel TH device and its subdevices

 * @dev:	parent device

 * @devres:	resources indexed by th_mmio_idx

 * @irq:	irq number

 free the subdevices and undo everything */

/**

 * intel_th_trace_enable() - enable tracing for an output device

 * @thdev:	output device that requests tracing be enabled

/**

 * intel_th_trace_switch() - execute a switch sequence

 * @thdev:	output device that requests tracing switch

/**

 * intel_th_trace_disable() - disable tracing for an output device

 * @thdev:	output device that requests tracing be disabled

 In host mode, this is up to the external debugger, do nothing. */

	/*

	 * hub is instantiated together with the source device that

	 * calls here, so guaranteed to be present.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub driver debugging

 *

 * Copyright (C) 2014-2015 Intel Corporation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub ACPI driver

 *

 * Copyright (C) 2017 Intel Corporation.

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel(R) Trace Hub Memory Storage Unit

 *

 * Copyright (C) 2014-2015 Intel Corporation.

/*

 * Lockout state transitions:

 *   READY -> INUSE -+-> LOCKED -+-> READY -> etc.

 *                   \-----------/

 * WIN_READY:	window can be used by HW

 * WIN_INUSE:	window is in use

 * WIN_LOCKED:	window is filled up and is being processed by the buffer

 * handling code

 *

 * All state transitions happen automatically, except for the LOCKED->READY,

 * which needs to be signalled by the buffer code by calling

 * intel_th_msc_window_unlock().

 *

 * When the interrupt handler has to switch to the next window, it checks

 * whether it's READY, and if it is, it performs the switch and tracing

 * continues. If it's LOCKED, it stops the trace.

/**

 * struct msc_window - multiblock mode window descriptor

 * @entry:	window list linkage (msc::win_list)

 * @pgoff:	page offset into the buffer that this window starts at

 * @lockout:	lockout state, see comment below

 * @lo_lock:	lockout state serialization

 * @nr_blocks:	number of blocks (pages) in this window

 * @nr_segs:	number of segments in this window (<= @nr_blocks)

 * @_sgt:	array of block descriptors

 * @sgt:	array of block descriptors

/**

 * struct msc_iter - iterator for msc buffer

 * @entry:		msc::iter_list linkage

 * @msc:		pointer to the MSC device

 * @start_win:		oldest window

 * @win:		current window

 * @offset:		current logical offset into the buffer

 * @start_block:	oldest block in the window

 * @block:		block number in the window

 * @block_off:		offset into current block

 * @wrap_count:		block wrapping handling

 * @eof:		end of buffer reached

/**

 * struct msc - MSC device representation

 * @reg_base:		register window base address

 * @thdev:		intel_th_device pointer

 * @mbuf:		MSU buffer, if assigned

 * @mbuf_priv		MSU buffer's private data, if @mbuf

 * @win_list:		list of windows in multiblock mode

 * @single_sgt:		single mode buffer

 * @cur_win:		current window

 * @nr_pages:		total number of pages allocated for this buffer

 * @single_sz:		amount of data in single mode

 * @single_wrap:	single mode wrap occurred

 * @base:		buffer's base pointer

 * @base_addr:		buffer's base address

 * @user_count:		number of users of the buffer

 * @mmap_count:		number of mappings

 * @buf_mutex:		mutex to serialize access to buffer-related bits



 * @enabled:		MSC is enabled

 * @wrap:		wrapping is enabled

 * @mode:		MSC operating mode

 * @burst_len:		write burst length

 * @index:		number of this MSC in the MSU

 <0: no buffer, 0: no users, >0: active users */

 config */

/**

 * struct msu_buffer_entry - internal MSU buffer bookkeeping

 * @entry:	link to msu_buffer_list

 * @mbuf:	MSU buffer object

 * @owner:	module that provides this MSU buffer

 header hasn't been written */

 valid_dw includes the header */

/**

 * msc_is_last_win() - check if a window is the last one for a given MSC

 * @win:	window

 * Return:	true if @win is the last window in MSC's multiblock buffer

/**

 * msc_next_window() - return next window in the multiblock buffer

 * @win:	current window

 *

 * Return:	window following the current one

/**

 * msc_find_window() - find a window matching a given sg_table

 * @msc:	MSC device

 * @sgt:	SG table of the window

 * @nonempty:	skip over empty windows

 *

 * Return:	MSC window structure pointer or NULL if the window

 *		could not be found.

	/*

	 * we might need a radix tree for this, depending on how

	 * many windows a typical user would allocate; ideally it's

	 * something like 2, in which case we're good

 skip the empty ones */

/**

 * msc_oldest_window() - locate the window with oldest data

 * @msc:	MSC device

 *

 * This should only be used in multiblock mode. Caller should hold the

 * msc::user_count reference.

 *

 * Return:	the oldest window with valid data

/**

 * msc_win_oldest_sg() - locate the oldest block in a given window

 * @win:	window to look at

 *

 * Return:	index of the block with the oldest data

 without wrapping, first block is the oldest */

	/*

	 * with wrapping, last written block contains both the newest and the

	 * oldest data for this window.

	/*

	 * Reading and tracing are mutually exclusive; if msc is

	 * enabled, open() will fail; otherwise existing readers

	 * will prevent enabling the msc and the rest of fops don't

	 * need to worry about it.

	/*

	 * start with the block with oldest data; if data has wrapped

	 * in this window, it should be in this block

 already started, nothing to do */

 wrapping */

 copied newest data from the wrapped block */

 no wrapping, check for last written block */

 copied newest data for the window */

 block advance */

 no wrapping, sanity check in case there is no last written block */

/**

 * msc_buffer_iterate() - go through multiblock buffer's data

 * @iter:	iterator structure

 * @size:	amount of data to scan

 * @data:	callback's private data

 * @fn:		iterator callback

 *

 * This will start at the window which will be written to next (containing

 * the oldest data) and work its way to the current window, calling @fn

 * for each chunk of data as it goes.

 *

 * Caller should have msc::user_count reference to make sure the buffer

 * doesn't disappear from under us.

 *

 * Return:	amount of data actually scanned.

 start with the oldest window */

		/*

		 * If block wrapping happened, we need to visit the last block

		 * twice, because it contains both the oldest and the newest

		 * data in this window.

		 *

		 * First time (wrap_count==2), in the very beginning, to collect

		 * the oldest data, which is in the range

		 * (data_bytes..DATA_IN_PAGE).

		 *

		 * Second time (wrap_count==1), it's just like any other block,

		 * containing data in the range of [MSC_BDESC..data_bytes].

/**

 * msc_buffer_clear_hw_header() - clear hw header for multiblock

 * @msc:	MSC device

 from intel_th_msc_window_unlock(), don't warn if not locked */

/**

 * msc_configure() - set up MSC hardware

 * @msc:	the MSC device to configure

 *

 * Program storage mode, wrapping, burst length and trace buffer address

 * into a given MSC. Then, enable tracing and set msc::enabled.

 * The latter is serialized on msc::buf_mutex, so make sure to hold it.

/**

 * msc_disable() - disable MSC hardware

 * @msc:	MSC device to disable

 *

 * If @msc is enabled, disable tracing on the switch and then disable MSC

 * storage. Caller must hold msc::buf_mutex.

 if there are readers, refuse */

/**

 * msc_buffer_contig_alloc() - allocate a contiguous buffer for SINGLE mode

 * @msc:	MSC device

 * @size:	allocation size in bytes

 *

 * This modifies msc::base, which requires msc::buf_mutex to serialize, so the

 * caller is expected to hold it.

 *

 * Return:	0 on success, -errno otherwise.

/**

 * msc_buffer_contig_free() - free a contiguous buffer

 * @msc:	MSC configured in SINGLE mode

/**

 * msc_buffer_contig_get_page() - find a page at a given offset

 * @msc:	MSC configured in SINGLE mode

 * @pgoff:	page offset

 *

 * Return:	page, if @pgoff is within the range, NULL otherwise.

 Set the page as uncached */

 Reset the page to write-back */

 !X86 */

 CONFIG_X86 */

/**

 * msc_buffer_win_alloc() - alloc a window for a multiblock mode

 * @msc:	MSC device

 * @nr_blocks:	number of pages in this window

 *

 * This modifies msc::win_list and msc::base, which requires msc::buf_mutex

 * to serialize, so the caller is expected to hold it.

 *

 * Return:	0 on success, -errno otherwise.

/**

 * msc_buffer_win_free() - free a window from MSC's window list

 * @msc:	MSC device

 * @win:	window to free

 *

 * This modifies msc::win_list and msc::base, which requires msc::buf_mutex

 * to serialize, so the caller is expected to hold it.

/**

 * msc_buffer_relink() - set up block descriptors for multiblock mode

 * @msc:	MSC device

 *

 * This traverses msc::win_list, which requires msc::buf_mutex to serialize,

 * so the caller is expected to hold it.

 call with msc::mutex locked */

		/*

		 * Last window's next_win should point to the first window

		 * and MSC_SW_TAG_LASTWIN should be set.

			/*

			 * Similarly to last window, last block should point

			 * to the first one.

	/*

	 * Make the above writes globally visible before tracing is

	 * enabled to make sure hardware sees them coherently.

/**

 * msc_buffer_free() - free buffers for MSC

 * @msc:	MSC device

 *

 * Free MSC's storage buffers.

 *

 * This modifies msc::win_list and msc::base, which requires msc::buf_mutex to

 * serialize, so the caller is expected to hold it.

/**

 * msc_buffer_alloc() - allocate a buffer for MSC

 * @msc:	MSC device

 * @size:	allocation size in bytes

 *

 * Allocate a storage buffer for MSC, depending on the msc::mode, it will be

 * either done via msc_buffer_contig_alloc() for SINGLE operation mode or

 * msc_buffer_win_alloc() for multiblock operation. The latter allocates one

 * window per invocation, so in multiblock mode this can be called multiple

 * times for the same MSC to allocate multiple windows.

 *

 * This modifies msc::win_list and msc::base, which requires msc::buf_mutex

 * to serialize, so the caller is expected to hold it.

 *

 * Return:	0 on success, -errno otherwise.

 -1: buffer not allocated */

 allocation should be visible before the counter goes to 0 */

/**

 * msc_buffer_unlocked_free_unless_used() - free a buffer unless it's in use

 * @msc:	MSC device

 *

 * This will free MSC buffer unless it is in use or there is no allocated

 * buffer.

 * Caller needs to hold msc::buf_mutex.

 *

 * Return:	0 on successful deallocation or if there was no buffer to

 *		deallocate, -EBUSY if there are active users.

 > 0: buffer is allocated and has users */

 0: buffer is allocated, no users */

 < 0: no buffer, nothing to do */

/**

 * msc_buffer_free_unless_used() - free a buffer unless it's in use

 * @msc:	MSC device

 *

 * This is a locked version of msc_buffer_unlocked_free_unless_used().

/**

 * msc_buffer_get_page() - get MSC buffer page at a given offset

 * @msc:	MSC device

 * @pgoff:	page offset into the storage buffer

 *

 * This traverses msc::win_list, so holding msc::buf_mutex is expected from

 * the caller.

 *

 * Return:	page if @pgoff corresponds to a valid buffer page or NULL.

/**

 * struct msc_win_to_user_struct - data for copy_to_user() callback

 * @buf:	userspace buffer to copy data to

 * @offset:	running offset

/**

 * msc_win_to_user() - iterator for msc_buffer_iterate() to copy data to user

 * @data:	callback's private data

 * @src:	source buffer

 * @len:	amount of data to copy from the source buffer

/*

 * file operations' callbacks

/*

 * vm operations callbacks (vm_ops)

 drop page _refcounts */

 last mapping -- drop user_count */

 grab user_count once per mmap; drop in msc_mmap_close() */

/**

 * intel_th_msc_window_unlock - put the window back in rotation

 * @dev:	MSC device to which this relates

 * @sgt:	buffer's sg_table for the window, does nothing if NULL

 grab the window before we do the switch */

 next window: if READY, proceed, if LOCKED, stop the trace */

 current window: INUSE -> LOCKED */

 Buffer sinks only work with a usable IRQ */

 Same buffer: do nothing */

 put the extra reference we just got */

 scan the comma-separated list of allocation sizes */

 consume the number and the following comma, hence +1 */

	/*

	 * Window switch can only happen in the "multi" mode.

	 * If a external buffer is engaged, they have the full

	 * control over window switching.

	/*

	 * Buffers should not be used at this point except if the

	 * output character device is still open and the parent

	 * device gets detached from its bus, which is a FIXME.

 SPDX-License-Identifier: GPL-2.0

/*

 * An example software sink buffer for Intel TH MSU.

 *

 * Copyright (C) 2019 Intel Corporation.

 See also: msc.c: __msc_buffer_win_alloc() */

 See also: msc.c: __msc_buffer_win_free() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Framework for userspace DMA-BUF allocations

 *

 * Copyright (C) 2011 Google, Inc.

 * Copyright (C) 2019 Linaro Ltd.

/**

 * struct dma_heap - represents a dmabuf heap in the system

 * @name:		used for debugging/device-node name

 * @ops:		ops struct for this heap

 * @heap_devt		heap device node

 * @list		list head connecting to list of heaps

 * @heap_cdev		heap char device

 *

 * Represents a heap of memory from which buffers can be made.

	/*

	 * Allocations from all heaps have to begin

	 * and end on page boundaries.

 just return, as put will call release and that will free */

 instance data as context */

 Get the kernel ioctl cmd that matches */

 Figure out the delta between user cmd size and kernel cmd size */

 If necessary, allocate buffer for ioctl argument */

 zero out any difference between the kernel/user structure size */

/**

 * dma_heap_get_drvdata() - get per-subdriver data for the heap

 * @heap: DMA-Heap to retrieve private data for

 *

 * Returns:

 * The per-subdriver data for the heap.

/**

 * dma_heap_get_name() - get heap name

 * @heap: DMA-Heap to retrieve private data for

 *

 * Returns:

 * The char* for the heap name.

 check the name is unique */

 Find unused minor number */

 Create device */

 Add heap to the list */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sync File validation framework and debug information

 *

 * Copyright (C) 2012 Google, Inc.

	/*

	 * The debugfs files won't ever get removed and thus, there is

	 * no need to protect it against removal races. The use of

	 * debugfs_create_file_unsafe() is actually safe here.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sync File validation framework

 *

 * Copyright (C) 2012 Google, Inc.

/*

 * SW SYNC validation framework

 *

 * A sync object driver that uses a 32bit counter to coordinate

 * synchronization.  Useful when there is no hardware primitive backing

 * the synchronization.

 *

 * To start the framework just open:

 *

 * <debugfs>/sync/sw_sync

 *

 * That will create a sync timeline, all fences created under this timeline

 * file descriptor will belong to the this timeline.

 *

 * The 'sw_sync' file can be opened many times as to create different

 * timelines.

 *

 * Fences can be created with SW_SYNC_IOC_CREATE_FENCE ioctl with struct

 * sw_sync_create_fence_data as parameter.

 *

 * To increment the timeline counter, SW_SYNC_IOC_INC ioctl should be used

 * with the increment as u32. This will update the last signaled value

 * from the timeline and signal any fence that has a seqno smaller or equal

 * to it.

 *

 * struct sw_sync_create_fence_data

 * @value:	the seqno to initialise the fence with

 * @name:	the name of the new sync point

 * @fence:	return the fd of the new sync_file with the created fence

 fd of new fence */

/**

 * sync_timeline_create() - creates a sync object

 * @name:	sync_timeline name

 *

 * Creates a new sync_timeline. Returns the sync_timeline object or NULL in

 * case of error.

/**

 * sync_timeline_signal() - signal a status change on a sync_timeline

 * @obj:	sync_timeline to signal

 * @inc:	num to increment on timeline->value

 *

 * A sync implementation should call this any time one of it's fences

 * has signaled or has an error condition.

		/*

		 * A signal callback may release the last reference to this

		 * fence, causing it to be freed. That operation has to be

		 * last to avoid a use after free inside this loop, and must

		 * be after we remove the fence from the timeline in order to

		 * prevent deadlocking on timeline->lock inside

		 * timeline_fence_release().

/**

 * sync_pt_create() - creates a sync pt

 * @obj:	parent sync_timeline

 * @value:	value of the fence

 *

 * Creates a new sync_pt (fence) as a child of @parent.  @size bytes will be

 * allocated allowing for implementation specific data to be kept after

 * the generic sync_timeline struct. Returns the sync_pt object or

 * NULL in case of error.

/*

 * *WARNING*

 *

 * improper use of this can result in deadlocking kernel drivers from userspace.

 opening sw_sync create a new sync obj */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Fence mechanism for dma-buf and to allow for asynchronous dma access

 *

 * Copyright (C) 2012 Canonical Ltd

 * Copyright (C) 2012 Texas Instruments

 *

 * Authors:

 * Rob Clark <robdclark@gmail.com>

 * Maarten Lankhorst <maarten.lankhorst@canonical.com>

/*

 * fence context counter: each execution context should have its own

 * fence context, this allows checking if fences belong to the same

 * context or not. One device can have multiple separate contexts,

 * and they're used if some engine can run independently of another.

/**

 * DOC: DMA fences overview

 *

 * DMA fences, represented by &struct dma_fence, are the kernel internal

 * synchronization primitive for DMA operations like GPU rendering, video

 * encoding/decoding, or displaying buffers on a screen.

 *

 * A fence is initialized using dma_fence_init() and completed using

 * dma_fence_signal(). Fences are associated with a context, allocated through

 * dma_fence_context_alloc(), and all fences on the same context are

 * fully ordered.

 *

 * Since the purposes of fences is to facilitate cross-device and

 * cross-application synchronization, there's multiple ways to use one:

 *

 * - Individual fences can be exposed as a &sync_file, accessed as a file

 *   descriptor from userspace, created by calling sync_file_create(). This is

 *   called explicit fencing, since userspace passes around explicit

 *   synchronization points.

 *

 * - Some subsystems also have their own explicit fencing primitives, like

 *   &drm_syncobj. Compared to &sync_file, a &drm_syncobj allows the underlying

 *   fence to be updated.

 *

 * - Then there's also implicit fencing, where the synchronization points are

 *   implicitly passed around as part of shared &dma_buf instances. Such

 *   implicit fences are stored in &struct dma_resv through the

 *   &dma_buf.resv pointer.

/**

 * DOC: fence cross-driver contract

 *

 * Since &dma_fence provide a cross driver contract, all drivers must follow the

 * same rules:

 *

 * * Fences must complete in a reasonable time. Fences which represent kernels

 *   and shaders submitted by userspace, which could run forever, must be backed

 *   up by timeout and gpu hang recovery code. Minimally that code must prevent

 *   further command submission and force complete all in-flight fences, e.g.

 *   when the driver or hardware do not support gpu reset, or if the gpu reset

 *   failed for some reason. Ideally the driver supports gpu recovery which only

 *   affects the offending userspace context, and no other userspace

 *   submissions.

 *

 * * Drivers may have different ideas of what completion within a reasonable

 *   time means. Some hang recovery code uses a fixed timeout, others a mix

 *   between observing forward progress and increasingly strict timeouts.

 *   Drivers should not try to second guess timeout handling of fences from

 *   other drivers.

 *

 * * To ensure there's no deadlocks of dma_fence_wait() against other locks

 *   drivers should annotate all code required to reach dma_fence_signal(),

 *   which completes the fences, with dma_fence_begin_signalling() and

 *   dma_fence_end_signalling().

 *

 * * Drivers are allowed to call dma_fence_wait() while holding dma_resv_lock().

 *   This means any code required for fence completion cannot acquire a

 *   &dma_resv lock. Note that this also pulls in the entire established

 *   locking hierarchy around dma_resv_lock() and dma_resv_unlock().

 *

 * * Drivers are allowed to call dma_fence_wait() from their &shrinker

 *   callbacks. This means any code required for fence completion cannot

 *   allocate memory with GFP_KERNEL.

 *

 * * Drivers are allowed to call dma_fence_wait() from their &mmu_notifier

 *   respectively &mmu_interval_notifier callbacks. This means any code required

 *   for fence completeion cannot allocate memory with GFP_NOFS or GFP_NOIO.

 *   Only GFP_ATOMIC is permissible, which might fail.

 *

 * Note that only GPU drivers have a reasonable excuse for both requiring

 * &mmu_interval_notifier and &shrinker callbacks at the same time as having to

 * track asynchronous compute work using &dma_fence. No driver outside of

 * drivers/gpu should ever call dma_fence_wait() in such contexts.

/**

 * dma_fence_get_stub - return a signaled fence

 *

 * Return a stub fence which is already signaled. The fence's

 * timestamp corresponds to the first time after boot this

 * function is called.

/**

 * dma_fence_allocate_private_stub - return a private, signaled fence

 *

 * Return a newly allocated and signaled stub fence.

/**

 * dma_fence_context_alloc - allocate an array of fence contexts

 * @num: amount of contexts to allocate

 *

 * This function will return the first index of the number of fence contexts

 * allocated.  The fence context is used for setting &dma_fence.context to a

 * unique number by passing the context to dma_fence_init().

/**

 * DOC: fence signalling annotation

 *

 * Proving correctness of all the kernel code around &dma_fence through code

 * review and testing is tricky for a few reasons:

 *

 * * It is a cross-driver contract, and therefore all drivers must follow the

 *   same rules for lock nesting order, calling contexts for various functions

 *   and anything else significant for in-kernel interfaces. But it is also

 *   impossible to test all drivers in a single machine, hence brute-force N vs.

 *   N testing of all combinations is impossible. Even just limiting to the

 *   possible combinations is infeasible.

 *

 * * There is an enormous amount of driver code involved. For render drivers

 *   there's the tail of command submission, after fences are published,

 *   scheduler code, interrupt and workers to process job completion,

 *   and timeout, gpu reset and gpu hang recovery code. Plus for integration

 *   with core mm with have &mmu_notifier, respectively &mmu_interval_notifier,

 *   and &shrinker. For modesetting drivers there's the commit tail functions

 *   between when fences for an atomic modeset are published, and when the

 *   corresponding vblank completes, including any interrupt processing and

 *   related workers. Auditing all that code, across all drivers, is not

 *   feasible.

 *

 * * Due to how many other subsystems are involved and the locking hierarchies

 *   this pulls in there is extremely thin wiggle-room for driver-specific

 *   differences. &dma_fence interacts with almost all of the core memory

 *   handling through page fault handlers via &dma_resv, dma_resv_lock() and

 *   dma_resv_unlock(). On the other side it also interacts through all

 *   allocation sites through &mmu_notifier and &shrinker.

 *

 * Furthermore lockdep does not handle cross-release dependencies, which means

 * any deadlocks between dma_fence_wait() and dma_fence_signal() can't be caught

 * at runtime with some quick testing. The simplest example is one thread

 * waiting on a &dma_fence while holding a lock::

 *

 *     lock(A);

 *     dma_fence_wait(B);

 *     unlock(A);

 *

 * while the other thread is stuck trying to acquire the same lock, which

 * prevents it from signalling the fence the previous thread is stuck waiting

 * on::

 *

 *     lock(A);

 *     unlock(A);

 *     dma_fence_signal(B);

 *

 * By manually annotating all code relevant to signalling a &dma_fence we can

 * teach lockdep about these dependencies, which also helps with the validation

 * headache since now lockdep can check all the rules for us::

 *

 *    cookie = dma_fence_begin_signalling();

 *    lock(A);

 *    unlock(A);

 *    dma_fence_signal(B);

 *    dma_fence_end_signalling(cookie);

 *

 * For using dma_fence_begin_signalling() and dma_fence_end_signalling() to

 * annotate critical sections the following rules need to be observed:

 *

 * * All code necessary to complete a &dma_fence must be annotated, from the

 *   point where a fence is accessible to other threads, to the point where

 *   dma_fence_signal() is called. Un-annotated code can contain deadlock issues,

 *   and due to the very strict rules and many corner cases it is infeasible to

 *   catch these just with review or normal stress testing.

 *

 * * &struct dma_resv deserves a special note, since the readers are only

 *   protected by rcu. This means the signalling critical section starts as soon

 *   as the new fences are installed, even before dma_resv_unlock() is called.

 *

 * * The only exception are fast paths and opportunistic signalling code, which

 *   calls dma_fence_signal() purely as an optimization, but is not required to

 *   guarantee completion of a &dma_fence. The usual example is a wait IOCTL

 *   which calls dma_fence_signal(), while the mandatory completion path goes

 *   through a hardware interrupt and possible job completion worker.

 *

 * * To aid composability of code, the annotations can be freely nested, as long

 *   as the overall locking hierarchy is consistent. The annotations also work

 *   both in interrupt and process context. Due to implementation details this

 *   requires that callers pass an opaque cookie from

 *   dma_fence_begin_signalling() to dma_fence_end_signalling().

 *

 * * Validation against the cross driver contract is implemented by priming

 *   lockdep with the relevant hierarchy at boot-up. This means even just

 *   testing with a single device is enough to validate a driver, at least as

 *   far as deadlocks with dma_fence_wait() against dma_fence_signal() are

 *   concerned.

/**

 * dma_fence_begin_signalling - begin a critical DMA fence signalling section

 *

 * Drivers should use this to annotate the beginning of any code section

 * required to eventually complete &dma_fence by calling dma_fence_signal().

 *

 * The end of these critical sections are annotated with

 * dma_fence_end_signalling().

 *

 * Returns:

 *

 * Opaque cookie needed by the implementation, which needs to be passed to

 * dma_fence_end_signalling().

 explicitly nesting ... */

 rely on might_sleep check for soft/hardirq locks */

 ... and non-recursive readlock */

/**

 * dma_fence_end_signalling - end a critical DMA fence signalling section

 * @cookie: opaque cookie from dma_fence_begin_signalling()

 *

 * Closes a critical section annotation opened by dma_fence_begin_signalling().

/**

 * dma_fence_signal_timestamp_locked - signal completion of a fence

 * @fence: the fence to signal

 * @timestamp: fence signal timestamp in kernel's CLOCK_MONOTONIC time domain

 *

 * Signal completion for software callbacks on a fence, this will unblock

 * dma_fence_wait() calls and run all the callbacks added with

 * dma_fence_add_callback(). Can be called multiple times, but since a fence

 * can only go from the unsignaled to the signaled state and not back, it will

 * only be effective the first time. Set the timestamp provided as the fence

 * signal timestamp.

 *

 * Unlike dma_fence_signal_timestamp(), this function must be called with

 * &dma_fence.lock held.

 *

 * Returns 0 on success and a negative error value when @fence has been

 * signalled already.

 Stash the cb_list before replacing it with the timestamp */

/**

 * dma_fence_signal_timestamp - signal completion of a fence

 * @fence: the fence to signal

 * @timestamp: fence signal timestamp in kernel's CLOCK_MONOTONIC time domain

 *

 * Signal completion for software callbacks on a fence, this will unblock

 * dma_fence_wait() calls and run all the callbacks added with

 * dma_fence_add_callback(). Can be called multiple times, but since a fence

 * can only go from the unsignaled to the signaled state and not back, it will

 * only be effective the first time. Set the timestamp provided as the fence

 * signal timestamp.

 *

 * Returns 0 on success and a negative error value when @fence has been

 * signalled already.

/**

 * dma_fence_signal_locked - signal completion of a fence

 * @fence: the fence to signal

 *

 * Signal completion for software callbacks on a fence, this will unblock

 * dma_fence_wait() calls and run all the callbacks added with

 * dma_fence_add_callback(). Can be called multiple times, but since a fence

 * can only go from the unsignaled to the signaled state and not back, it will

 * only be effective the first time.

 *

 * Unlike dma_fence_signal(), this function must be called with &dma_fence.lock

 * held.

 *

 * Returns 0 on success and a negative error value when @fence has been

 * signalled already.

/**

 * dma_fence_signal - signal completion of a fence

 * @fence: the fence to signal

 *

 * Signal completion for software callbacks on a fence, this will unblock

 * dma_fence_wait() calls and run all the callbacks added with

 * dma_fence_add_callback(). Can be called multiple times, but since a fence

 * can only go from the unsignaled to the signaled state and not back, it will

 * only be effective the first time.

 *

 * Returns 0 on success and a negative error value when @fence has been

 * signalled already.

/**

 * dma_fence_wait_timeout - sleep until the fence gets signaled

 * or until timeout elapses

 * @fence: the fence to wait on

 * @intr: if true, do an interruptible wait

 * @timeout: timeout value in jiffies, or MAX_SCHEDULE_TIMEOUT

 *

 * Returns -ERESTARTSYS if interrupted, 0 if the wait timed out, or the

 * remaining timeout in jiffies on success. Other error values may be

 * returned on custom implementations.

 *

 * Performs a synchronous wait on this fence. It is assumed the caller

 * directly or indirectly (buf-mgr between reservation and committing)

 * holds a reference to the fence, otherwise the fence might be

 * freed before return, resulting in undefined behavior.

 *

 * See also dma_fence_wait() and dma_fence_wait_any_timeout().

/**

 * dma_fence_release - default relese function for fences

 * @kref: &dma_fence.recfount

 *

 * This is the default release functions for &dma_fence. Drivers shouldn't call

 * this directly, but instead call dma_fence_put().

		/*

		 * Failed to signal before release, likely a refcounting issue.

		 *

		 * This should never happen, but if it does make sure that we

		 * don't leave chains dangling. We set the error flag first

		 * so that the callbacks know this signal is due to an error.

/**

 * dma_fence_free - default release function for &dma_fence.

 * @fence: fence to release

 *

 * This is the default implementation for &dma_fence_ops.release. It calls

 * kfree_rcu() on @fence.

/**

 * dma_fence_enable_sw_signaling - enable signaling on fence

 * @fence: the fence to enable

 *

 * This will request for sw signaling to be enabled, to make the fence

 * complete as soon as possible. This calls &dma_fence_ops.enable_signaling

 * internally.

/**

 * dma_fence_add_callback - add a callback to be called when the fence

 * is signaled

 * @fence: the fence to wait on

 * @cb: the callback to register

 * @func: the function to call

 *

 * Add a software callback to the fence. The caller should keep a reference to

 * the fence.

 *

 * @cb will be initialized by dma_fence_add_callback(), no initialization

 * by the caller is required. Any number of callbacks can be registered

 * to a fence, but a callback can only be registered to one fence at a time.

 *

 * If fence is already signaled, this function will return -ENOENT (and

 * *not* call the callback).

 *

 * Note that the callback can be called from an atomic context or irq context.

 *

 * Returns 0 in case of success, -ENOENT if the fence is already signaled

 * and -EINVAL in case of error.

/**

 * dma_fence_get_status - returns the status upon completion

 * @fence: the dma_fence to query

 *

 * This wraps dma_fence_get_status_locked() to return the error status

 * condition on a signaled fence. See dma_fence_get_status_locked() for more

 * details.

 *

 * Returns 0 if the fence has not yet been signaled, 1 if the fence has

 * been signaled without an error condition, or a negative error code

 * if the fence has been completed in err.

/**

 * dma_fence_remove_callback - remove a callback from the signaling list

 * @fence: the fence to wait on

 * @cb: the callback to remove

 *

 * Remove a previously queued callback from the fence. This function returns

 * true if the callback is successfully removed, or false if the fence has

 * already been signaled.

 *

 * *WARNING*:

 * Cancelling a callback should only be done if you really know what you're

 * doing, since deadlocks and race conditions could occur all too easily. For

 * this reason, it should only ever be done on hardware lockup recovery,

 * with a reference held to the fence.

 *

 * Behaviour is undefined if @cb has not been added to @fence using

 * dma_fence_add_callback() beforehand.

/**

 * dma_fence_default_wait - default sleep until the fence gets signaled

 * or until timeout elapses

 * @fence: the fence to wait on

 * @intr: if true, do an interruptible wait

 * @timeout: timeout value in jiffies, or MAX_SCHEDULE_TIMEOUT

 *

 * Returns -ERESTARTSYS if interrupted, 0 if the wait timed out, or the

 * remaining timeout in jiffies on success. If timeout is zero the value one is

 * returned if the fence is already signaled for consistency with other

 * functions taking a jiffies timeout.

/**

 * dma_fence_wait_any_timeout - sleep until any fence gets signaled

 * or until timeout elapses

 * @fences: array of fences to wait on

 * @count: number of fences to wait on

 * @intr: if true, do an interruptible wait

 * @timeout: timeout value in jiffies, or MAX_SCHEDULE_TIMEOUT

 * @idx: used to store the first signaled fence index, meaningful only on

 *	positive return

 *

 * Returns -EINVAL on custom fence wait implementation, -ERESTARTSYS if

 * interrupted, 0 if the wait timed out, or the remaining timeout in jiffies

 * on success.

 *

 * Synchronous waits for the first fence in the array to be signaled. The

 * caller needs to hold a reference to all fences in the array, otherwise a

 * fence might be freed before return, resulting in undefined behavior.

 *

 * See also dma_fence_wait() and dma_fence_wait_timeout().

 This fence is already signaled */

/**

 * dma_fence_init - Initialize a custom fence.

 * @fence: the fence to initialize

 * @ops: the dma_fence_ops for operations on this fence

 * @lock: the irqsafe spinlock to use for locking this fence

 * @context: the execution context this fence is run on

 * @seqno: a linear increasing sequence number for this context

 *

 * Initializes an allocated fence, the caller doesn't have to keep its

 * refcount after committing with this fence, but it will need to hold a

 * refcount again if &dma_fence_ops.enable_signaling gets called.

 *

 * context and seqno are used for easy comparison between fences, allowing

 * to check which fence is later by simply using dma_fence_later().

 SPDX-License-Identifier: MIT

/*

 * Copyright (C) 2012-2014 Canonical Ltd (Maarten Lankhorst)

 *

 * Based on bo.c which bears the following copyright notice,

 * but is dual licensed:

 *

 * Copyright (c) 2006-2009 VMware, Inc., Palo Alto, CA., USA

 * All Rights Reserved.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a

 * copy of this software and associated documentation files (the

 * "Software"), to deal in the Software without restriction, including

 * without limitation the rights to use, copy, modify, merge, publish,

 * distribute, sub license, and/or sell copies of the Software, and to

 * permit persons to whom the Software is furnished to do so, subject to

 * the following conditions:

 *

 * The above copyright notice and this permission notice (including the

 * next paragraph) shall be included in all copies or substantial portions

 * of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL

 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,

 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR

 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE

 * USE OR OTHER DEALINGS IN THE SOFTWARE.

 *

/*

 * Authors: Thomas Hellstrom <thellstrom-at-vmware-dot-com>

/**

 * DOC: Reservation Object Overview

 *

 * The reservation object provides a mechanism to manage shared and

 * exclusive fences associated with a buffer.  A reservation object

 * can have attached one exclusive fence (normally associated with

 * write operations) or N shared fences (read operations).  The RCU

 * mechanism is used to protect read access to fences from locked

 * write-side updates.

 *

 * See struct dma_resv for more details.

/**

 * dma_resv_list_alloc - allocate fence list

 * @shared_max: number of fences we need space for

 *

 * Allocate a new dma_resv_list and make sure to correctly initialize

 * shared_max.

/**

 * dma_resv_list_free - free fence list

 * @list: list to free

 *

 * Free a dma_resv_list and make sure to drop all references.

/**

 * dma_resv_init - initialize a reservation object

 * @obj: the reservation object

/**

 * dma_resv_fini - destroys a reservation object

 * @obj: the reservation object

	/*

	 * This object should be dead and all references must have

	 * been released to it, so no need to be protected with rcu.

/**

 * dma_resv_reserve_shared - Reserve space to add shared fences to

 * a dma_resv.

 * @obj: reservation object

 * @num_fences: number of fences we want to add

 *

 * Should be called before dma_resv_add_shared_fence().  Must

 * be called with @obj locked through dma_resv_lock().

 *

 * Note that the preallocated slots need to be re-reserved if @obj is unlocked

 * at any time before calling dma_resv_add_shared_fence(). This is validated

 * when CONFIG_DEBUG_MUTEXES is enabled.

 *

 * RETURNS

 * Zero for success, or -errno

	/*

	 * no need to bump fence refcounts, rcu_read access

	 * requires the use of kref_get_unless_zero, and the

	 * references from the old struct are carried over to

	 * the new.

	/*

	 * We are not changing the effective set of fences here so can

	 * merely update the pointer to the new array; both existing

	 * readers and new readers will see exactly the same set of

	 * active (unsignaled) shared fences. Individual fences and the

	 * old array are protected by RCU and so will not vanish under

	 * the gaze of the rcu_read_lock() readers.

 Drop the references to the signaled fences */

/**

 * dma_resv_reset_shared_max - reset shared fences for debugging

 * @obj: the dma_resv object to reset

 *

 * Reset the number of pre-reserved shared slots to test that drivers do

 * correct slot allocation using dma_resv_reserve_shared(). See also

 * &dma_resv_list.shared_max.

 Test shared fence slot reservation */

/**

 * dma_resv_add_shared_fence - Add a fence to a shared slot

 * @obj: the reservation object

 * @fence: the shared fence to add

 *

 * Add a fence to a shared slot, @obj must be locked with dma_resv_lock(), and

 * dma_resv_reserve_shared() has been called.

 *

 * See also &dma_resv.fence for a discussion of the semantics.

 pointer update must be visible before we extend the shared_count */

/**

 * dma_resv_add_excl_fence - Add an exclusive fence.

 * @obj: the reservation object

 * @fence: the exclusive fence to add

 *

 * Add a fence to the exclusive slot. @obj must be locked with dma_resv_lock().

 * Note that this function replaces all fences attached to @obj, see also

 * &dma_resv.fence_excl for a discussion of the semantics.

 write_seqcount_begin provides the necessary memory barrier */

 inplace update, no shared fences */

/**

 * dma_resv_iter_restart_unlocked - restart the unlocked iterator

 * @cursor: The dma_resv_iter object to restart

 *

 * Restart the unlocked iteration by initializing the cursor object.

/**

 * dma_resv_iter_walk_unlocked - walk over fences in a dma_resv obj

 * @cursor: cursor to record the current position

 *

 * Return all the fences in the dma_resv object which are not yet signaled.

 * The returned fence has an extra local reference so will stay alive.

 * If a concurrent modify is detected the whole iteration is started over again.

 Drop the reference from the previous round */

/**

 * dma_resv_iter_first_unlocked - first fence in an unlocked dma_resv obj.

 * @cursor: the cursor with the current position

 *

 * Returns the first fence from an unlocked dma_resv obj.

/**

 * dma_resv_iter_next_unlocked - next fence in an unlocked dma_resv obj.

 * @cursor: the cursor with the current position

 *

 * Returns the next fence from an unlocked dma_resv obj.

/**

 * dma_resv_iter_first - first fence from a locked dma_resv object

 * @cursor: cursor to record the current position

 *

 * Return the first fence in the dma_resv object while holding the

 * &dma_resv.lock.

/**

 * dma_resv_iter_next - next fence from a locked dma_resv object

 * @cursor: cursor to record the current position

 *

 * Return the next fences from the dma_resv object while holding the

 * &dma_resv.lock.

/**

 * dma_resv_copy_fences - Copy all fences from src to dst.

 * @dst: the destination reservation object

 * @src: the source reservation object

 *

 * Copy all fences from src to dst. dst-lock must be held.

/**

 * dma_resv_get_fences - Get an object's shared and exclusive

 * fences without update side lock held

 * @obj: the reservation object

 * @fence_excl: the returned exclusive fence (or NULL)

 * @shared_count: the number of shared fences returned

 * @shared: the array of shared fence ptrs returned (array is krealloc'd to

 * the required size, and must be freed by caller)

 *

 * Retrieve all fences from the reservation object. If the pointer for the

 * exclusive fence is not specified the fence is put into the array of the

 * shared fences as well. Returns either zero or -ENOMEM.

 Eventually re-allocate the array */

/**

 * dma_resv_wait_timeout - Wait on reservation's objects

 * shared and/or exclusive fences.

 * @obj: the reservation object

 * @wait_all: if true, wait on all fences, else wait on just exclusive fence

 * @intr: if true, do interruptible wait

 * @timeout: timeout value in jiffies or zero to return immediately

 *

 * Callers are not required to hold specific locks, but maybe hold

 * dma_resv_lock() already

 * RETURNS

 * Returns -ERESTARTSYS if interrupted, 0 if the wait timed out, or

 * greater than zer on success.

/**

 * dma_resv_test_signaled - Test if a reservation object's fences have been

 * signaled.

 * @obj: the reservation object

 * @test_all: if true, test all fences, otherwise only test the exclusive

 * fence

 *

 * Callers are not required to hold specific locks, but maybe hold

 * dma_resv_lock() already.

 *

 * RETURNS

 *

 * True if all fences signaled, else false.

 for unmap_mapping_range on trylocked buffer objects in shrinkers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * dma-fence-array: aggregate fences to be waited together

 *

 * Copyright (C) 2016 Collabora Ltd

 * Copyright (C) 2016 Advanced Micro Devices, Inc.

 * Authors:

 *	Gustavo Padovan <gustavo@padovan.org>

 *	Christian König <christian.koenig@amd.com>

	/*

	 * Propagate the first error reported by any of our fences, but only

	 * before we ourselves are signaled.

 Clear the error flag if not actually set. */

		/*

		 * As we may report that the fence is signaled before all

		 * callbacks are complete, we need to take an additional

		 * reference count on the array so that we do not free it too

		 * early. The core fence handling will only hold the reference

		 * until we signal the array as complete (but that is now

		 * insufficient).

/**

 * dma_fence_array_create - Create a custom fence array

 * @num_fences:		[in]	number of fences to add in the array

 * @fences:		[in]	array containing the fences

 * @context:		[in]	fence context to use

 * @seqno:		[in]	sequence number to use

 * @signal_on_any:	[in]	signal on any fence in the array

 *

 * Allocate a dma_fence_array object and initialize the base fence with

 * dma_fence_init().

 * In case of error it returns NULL.

 *

 * The caller should allocate the fences array with num_fences size

 * and fill it with the fences it wants to add to the object. Ownership of this

 * array is taken and dma_fence_put() is used on each fence on release.

 *

 * If @signal_on_any is true the fence array signals if any fence in the array

 * signals, otherwise it signals when all fences in the array signal.

 Allocate the callback structures behind the array. */

/**

 * dma_fence_match_context - Check if all fences are from the given context

 * @fence:		[in]	fence or fence array

 * @context:		[in]	fence context to check all fences against

 *

 * Checks the provided fence or, for a fence array, all fences in the array

 * against the given context. Returns false if any fence is from a different

 * context.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Framework for buffer objects that can be shared across devices/subsystems.

 *

 * Copyright(C) 2011 Linaro Limited. All rights reserved.

 * Author: Sumit Semwal <sumit.semwal@ti.com>

 *

 * Many thanks to linaro-mm-sig list, and specially

 * Arnd Bergmann <arnd@arndb.de>, Rob Clark <rob@ti.com> and

 * Daniel Vetter <daniel@ffwll.ch> for their support in creation and

 * refining of this idea.

	/*

	 * If you hit this BUG() it could mean:

	 * * There's a file reference imbalance in dma_buf_poll / dma_buf_poll_cb or somewhere else

	 * * dmabuf->cb_in/out.active are non-0 despite no pending fence callback

 check if buffer supports mmap */

 check for overflowing the buffer's size */

	/* only support discovering the end of the buffer,

	   but also allow SEEK_SET to maintain the idiomatic

/**

 * DOC: implicit fence polling

 *

 * To support cross-device and cross-driver synchronization of buffer access

 * implicit fences (represented internally in the kernel with &struct dma_fence)

 * can be attached to a &dma_buf. The glue for that and a few related things are

 * provided in the &dma_resv structure.

 *

 * Userspace can query the state of these implicitly tracked fences using poll()

 * and related system calls:

 *

 * - Checking for EPOLLIN, i.e. read access, can be use to query the state of the

 *   most recent write or exclusive fence.

 *

 * - Checking for EPOLLOUT, i.e. write access, can be used to query the state of

 *   all attached fences, shared and exclusive ones.

 *

 * Note that this only signals the completion of the respective fences, i.e. the

 * DMA transfers are complete. Cache flushing and any other necessary

 * preparations before CPU access can begin still need to happen.

 Paired with get_file in dma_buf_poll */

 Check that callback isn't busy */

 Paired with fput in dma_buf_poll_cb */

 No callback queued, wake up any other waiters */

 Check that callback isn't busy */

 Paired with fput in dma_buf_poll_cb */

 No callback queued, wake up any other waiters */

/**

 * dma_buf_set_name - Set a name to a specific dma_buf to track the usage.

 * The name of the dma-buf buffer can only be set when the dma-buf is not

 * attached to any devices. It could theoritically support changing the

 * name of the dma-buf if the same piece of memory is used for multiple

 * purpose between different devices.

 *

 * @dmabuf: [in]     dmabuf buffer that will be renamed.

 * @buf:    [in]     A piece of userspace memory that contains the name of

 *                   the dma-buf.

 *

 * Returns 0 on success. If the dma-buf buffer is already attached to

 * devices, return -EBUSY.

 *

 Don't count the temporary reference taken inside procfs seq_show */

/*

 * is_dma_buf_file - Check if struct file* is associated with dma_buf

/**

 * DOC: dma buf device access

 *

 * For device DMA access to a shared DMA buffer the usual sequence of operations

 * is fairly simple:

 *

 * 1. The exporter defines his exporter instance using

 *    DEFINE_DMA_BUF_EXPORT_INFO() and calls dma_buf_export() to wrap a private

 *    buffer object into a &dma_buf. It then exports that &dma_buf to userspace

 *    as a file descriptor by calling dma_buf_fd().

 *

 * 2. Userspace passes this file-descriptors to all drivers it wants this buffer

 *    to share with: First the filedescriptor is converted to a &dma_buf using

 *    dma_buf_get(). Then the buffer is attached to the device using

 *    dma_buf_attach().

 *

 *    Up to this stage the exporter is still free to migrate or reallocate the

 *    backing storage.

 *

 * 3. Once the buffer is attached to all devices userspace can initiate DMA

 *    access to the shared buffer. In the kernel this is done by calling

 *    dma_buf_map_attachment() and dma_buf_unmap_attachment().

 *

 * 4. Once a driver is done with a shared buffer it needs to call

 *    dma_buf_detach() (after cleaning up any mappings) and then release the

 *    reference acquired with dma_buf_get() by calling dma_buf_put().

 *

 * For the detailed semantics exporters are expected to implement see

 * &dma_buf_ops.

/**

 * dma_buf_export - Creates a new dma_buf, and associates an anon file

 * with this buffer, so it can be exported.

 * Also connect the allocator specific data and ops to the buffer.

 * Additionally, provide a name string for exporter; useful in debugging.

 *

 * @exp_info:	[in]	holds all the export related information provided

 *			by the exporter. see &struct dma_buf_export_info

 *			for further details.

 *

 * Returns, on success, a newly created struct dma_buf object, which wraps the

 * supplied private data and operations for struct dma_buf_ops. On either

 * missing ops, or error in allocating struct dma_buf, will return negative

 * error.

 *

 * For most cases the easiest way to create @exp_info is through the

 * %DEFINE_DMA_BUF_EXPORT_INFO macro.

 prevent &dma_buf[1] == dma_buf->resv */

	/*

	 * Set file->f_path.dentry->d_fsdata to NULL so that when

	 * dma_buf_release() gets invoked by dentry_ops, it exits

	 * early before calling the release() dma_buf op.

/**

 * dma_buf_fd - returns a file descriptor for the given struct dma_buf

 * @dmabuf:	[in]	pointer to dma_buf for which fd is required.

 * @flags:      [in]    flags to give to fd

 *

 * On success, returns an associated 'fd'. Else, returns error.

/**

 * dma_buf_get - returns the struct dma_buf related to an fd

 * @fd:	[in]	fd associated with the struct dma_buf to be returned

 *

 * On success, returns the struct dma_buf associated with an fd; uses

 * file's refcounting done by fget to increase refcount. returns ERR_PTR

 * otherwise.

/**

 * dma_buf_put - decreases refcount of the buffer

 * @dmabuf:	[in]	buffer to reduce refcount of

 *

 * Uses file's refcounting done implicitly by fput().

 *

 * If, as a result of this call, the refcount becomes 0, the 'release' file

 * operation related to this fd is called. It calls &dma_buf_ops.release vfunc

 * in turn, and frees the memory allocated for dmabuf when exported.

	/* To catch abuse of the underlying struct page by importers mix

	 * up the bits, but take care to preserve the low SG_ bits to

	 * not corrupt the sgt. The mixing is undone in __unmap_dma_buf

/**

 * dma_buf_dynamic_attach - Add the device to dma_buf's attachments list

 * @dmabuf:		[in]	buffer to attach device to.

 * @dev:		[in]	device to be attached.

 * @importer_ops:	[in]	importer operations for the attachment

 * @importer_priv:	[in]	importer private pointer for the attachment

 *

 * Returns struct dma_buf_attachment pointer for this attachment. Attachments

 * must be cleaned up by calling dma_buf_detach().

 *

 * Optionally this calls &dma_buf_ops.attach to allow device-specific attach

 * functionality.

 *

 * Returns:

 *

 * A pointer to newly created &dma_buf_attachment on success, or a negative

 * error code wrapped into a pointer on failure.

 *

 * Note that this can fail if the backing storage of @dmabuf is in a place not

 * accessible to @dev, and cannot be moved to a more suitable place. This is

 * indicated with the error code -EBUSY.

	/* When either the importer or the exporter can't handle dynamic

	 * mappings we cache the mapping here to avoid issues with the

	 * reservation object lock.

/**

 * dma_buf_attach - Wrapper for dma_buf_dynamic_attach

 * @dmabuf:	[in]	buffer to attach device to.

 * @dev:	[in]	device to be attached.

 *

 * Wrapper to call dma_buf_dynamic_attach() for drivers which still use a static

 * mapping.

 uses XOR, hence this unmangles */

/**

 * dma_buf_detach - Remove the given attachment from dmabuf's attachments list

 * @dmabuf:	[in]	buffer to detach from.

 * @attach:	[in]	attachment to be detached; is free'd after this call.

 *

 * Clean up a device attachment obtained by calling dma_buf_attach().

 *

 * Optionally this calls &dma_buf_ops.detach for device-specific detach.

/**

 * dma_buf_pin - Lock down the DMA-buf

 * @attach:	[in]	attachment which should be pinned

 *

 * Only dynamic importers (who set up @attach with dma_buf_dynamic_attach()) may

 * call this, and only for limited use cases like scanout and not for temporary

 * pin operations. It is not permitted to allow userspace to pin arbitrary

 * amounts of buffers through this interface.

 *

 * Buffers must be unpinned by calling dma_buf_unpin().

 *

 * Returns:

 * 0 on success, negative error code on failure.

/**

 * dma_buf_unpin - Unpin a DMA-buf

 * @attach:	[in]	attachment which should be unpinned

 *

 * This unpins a buffer pinned by dma_buf_pin() and allows the exporter to move

 * any mapping of @attach again and inform the importer through

 * &dma_buf_attach_ops.move_notify.

/**

 * dma_buf_map_attachment - Returns the scatterlist table of the attachment;

 * mapped into _device_ address space. Is a wrapper for map_dma_buf() of the

 * dma_buf_ops.

 * @attach:	[in]	attachment whose scatterlist is to be returned

 * @direction:	[in]	direction of DMA transfer

 *

 * Returns sg_table containing the scatterlist to be returned; returns ERR_PTR

 * on error. May return -EINTR if it is interrupted by a signal.

 *

 * On success, the DMA addresses and lengths in the returned scatterlist are

 * PAGE_SIZE aligned.

 *

 * A mapping must be unmapped by using dma_buf_unmap_attachment(). Note that

 * the underlying backing storage is pinned for as long as a mapping exists,

 * therefore users/importers should not hold onto a mapping for undue amounts of

 * time.

 *

 * Important: Dynamic importers must wait for the exclusive fence of the struct

 * dma_resv attached to the DMA-BUF first.

		/*

		 * Two mappings with different directions for the same

		 * attachment are not allowed.

 CONFIG_DMA_API_DEBUG */

/**

 * dma_buf_unmap_attachment - unmaps and decreases usecount of the buffer;might

 * deallocate the scatterlist associated. Is a wrapper for unmap_dma_buf() of

 * dma_buf_ops.

 * @attach:	[in]	attachment to unmap buffer from

 * @sg_table:	[in]	scatterlist info of the buffer to unmap

 * @direction:  [in]    direction of DMA transfer

 *

 * This unmaps a DMA mapping for @attached obtained by dma_buf_map_attachment().

/**

 * dma_buf_move_notify - notify attachments that DMA-buf is moving

 *

 * @dmabuf:	[in]	buffer which is moving

 *

 * Informs all attachmenst that they need to destroy and recreated all their

 * mappings.

/**

 * DOC: cpu access

 *

 * There are mutliple reasons for supporting CPU access to a dma buffer object:

 *

 * - Fallback operations in the kernel, for example when a device is connected

 *   over USB and the kernel needs to shuffle the data around first before

 *   sending it away. Cache coherency is handled by braketing any transactions

 *   with calls to dma_buf_begin_cpu_access() and dma_buf_end_cpu_access()

 *   access.

 *

 *   Since for most kernel internal dma-buf accesses need the entire buffer, a

 *   vmap interface is introduced. Note that on very old 32-bit architectures

 *   vmalloc space might be limited and result in vmap calls failing.

 *

 *   Interfaces::

 *

 *      void \*dma_buf_vmap(struct dma_buf \*dmabuf)

 *      void dma_buf_vunmap(struct dma_buf \*dmabuf, void \*vaddr)

 *

 *   The vmap call can fail if there is no vmap support in the exporter, or if

 *   it runs out of vmalloc space. Note that the dma-buf layer keeps a reference

 *   count for all vmap access and calls down into the exporter's vmap function

 *   only when no vmapping exists, and only unmaps it once. Protection against

 *   concurrent vmap/vunmap calls is provided by taking the &dma_buf.lock mutex.

 *

 * - For full compatibility on the importer side with existing userspace

 *   interfaces, which might already support mmap'ing buffers. This is needed in

 *   many processing pipelines (e.g. feeding a software rendered image into a

 *   hardware pipeline, thumbnail creation, snapshots, ...). Also, Android's ION

 *   framework already supported this and for DMA buffer file descriptors to

 *   replace ION buffers mmap support was needed.

 *

 *   There is no special interfaces, userspace simply calls mmap on the dma-buf

 *   fd. But like for CPU access there's a need to braket the actual access,

 *   which is handled by the ioctl (DMA_BUF_IOCTL_SYNC). Note that

 *   DMA_BUF_IOCTL_SYNC can fail with -EAGAIN or -EINTR, in which case it must

 *   be restarted.

 *

 *   Some systems might need some sort of cache coherency management e.g. when

 *   CPU and GPU domains are being accessed through dma-buf at the same time.

 *   To circumvent this problem there are begin/end coherency markers, that

 *   forward directly to existing dma-buf device drivers vfunc hooks. Userspace

 *   can make use of those markers through the DMA_BUF_IOCTL_SYNC ioctl. The

 *   sequence would be used like following:

 *

 *     - mmap dma-buf fd

 *     - for each drawing/upload cycle in CPU 1. SYNC_START ioctl, 2. read/write

 *       to mmap area 3. SYNC_END ioctl. This can be repeated as often as you

 *       want (with the new data being consumed by say the GPU or the scanout

 *       device)

 *     - munmap once you don't need the buffer any more

 *

 *    For correctness and optimal performance, it is always required to use

 *    SYNC_START and SYNC_END before and after, respectively, when accessing the

 *    mapped address. Userspace cannot rely on coherent access, even when there

 *    are systems where it just works without calling these ioctls.

 *

 * - And as a CPU fallback in userspace processing pipelines.

 *

 *   Similar to the motivation for kernel cpu access it is again important that

 *   the userspace code of a given importing subsystem can use the same

 *   interfaces with a imported dma-buf buffer object as with a native buffer

 *   object. This is especially important for drm where the userspace part of

 *   contemporary OpenGL, X, and other drivers is huge, and reworking them to

 *   use a different way to mmap a buffer rather invasive.

 *

 *   The assumption in the current dma-buf interfaces is that redirecting the

 *   initial mmap is all that's needed. A survey of some of the existing

 *   subsystems shows that no driver seems to do any nefarious thing like

 *   syncing up with outstanding asynchronous processing on the device or

 *   allocating special resources at fault time. So hopefully this is good

 *   enough, since adding interfaces to intercept pagefaults and allow pte

 *   shootdowns would increase the complexity quite a bit.

 *

 *   Interface::

 *

 *      int dma_buf_mmap(struct dma_buf \*, struct vm_area_struct \*,

 *		       unsigned long);

 *

 *   If the importing subsystem simply provides a special-purpose mmap call to

 *   set up a mapping in userspace, calling do_mmap with &dma_buf.file will

 *   equally achieve that for a dma-buf object.

 Wait on any implicit rendering fences */

/**

 * dma_buf_begin_cpu_access - Must be called before accessing a dma_buf from the

 * cpu in the kernel context. Calls begin_cpu_access to allow exporter-specific

 * preparations. Coherency is only guaranteed in the specified range for the

 * specified access direction.

 * @dmabuf:	[in]	buffer to prepare cpu access for.

 * @direction:	[in]	length of range for cpu access.

 *

 * After the cpu access is complete the caller should call

 * dma_buf_end_cpu_access(). Only when cpu access is braketed by both calls is

 * it guaranteed to be coherent with other DMA access.

 *

 * This function will also wait for any DMA transactions tracked through

 * implicit synchronization in &dma_buf.resv. For DMA transactions with explicit

 * synchronization this function will only ensure cache coherency, callers must

 * ensure synchronization with such DMA transactions on their own.

 *

 * Can return negative error values, returns 0 on success.

	/* Ensure that all fences are waited upon - but we first allow

	 * the native handler the chance to do so more efficiently if it

	 * chooses. A double invocation here will be reasonably cheap no-op.

/**

 * dma_buf_end_cpu_access - Must be called after accessing a dma_buf from the

 * cpu in the kernel context. Calls end_cpu_access to allow exporter-specific

 * actions. Coherency is only guaranteed in the specified range for the

 * specified access direction.

 * @dmabuf:	[in]	buffer to complete cpu access for.

 * @direction:	[in]	length of range for cpu access.

 *

 * This terminates CPU access started with dma_buf_begin_cpu_access().

 *

 * Can return negative error values, returns 0 on success.

/**

 * dma_buf_mmap - Setup up a userspace mmap with the given vma

 * @dmabuf:	[in]	buffer that should back the vma

 * @vma:	[in]	vma for the mmap

 * @pgoff:	[in]	offset in pages where this mmap should start within the

 *			dma-buf buffer.

 *

 * This function adjusts the passed in vma so that it points at the file of the

 * dma_buf operation. It also adjusts the starting pgoff and does bounds

 * checking on the size of the vma. Then it calls the exporters mmap function to

 * set up the mapping.

 *

 * Can return negative error values, returns 0 on success.

 check if buffer supports mmap */

 check for offset overflow */

 check for overflowing the buffer's size */

 readjust the vma */

/**

 * dma_buf_vmap - Create virtual mapping for the buffer object into kernel

 * address space. Same restrictions as for vmap and friends apply.

 * @dmabuf:	[in]	buffer to vmap

 * @map:	[out]	returns the vmap pointer

 *

 * This call may fail due to lack of virtual mapping address space.

 * These calls are optional in drivers. The intended use for them

 * is for mapping objects linear in kernel space for high use objects.

 *

 * To ensure coherency users must call dma_buf_begin_cpu_access() and

 * dma_buf_end_cpu_access() around any cpu access performed through this

 * mapping.

 *

 * Returns 0 on success, or a negative errno code otherwise.

/**

 * dma_buf_vunmap - Unmap a vmap obtained by dma_buf_vmap.

 * @dmabuf:	[in]	buffer to vunmap

 * @map:	[in]	vmap pointer to vunmap

 SPDX-License-Identifier: MIT */

/*

 * Copyright © 2019 Intel Corporation

 not our fault! */

 Now off to the races! */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DMA-BUF sysfs statistics.

 *

 * Copyright (C) 2021 Google LLC.

/**

 * DOC: overview

 *

 * ``/sys/kernel/debug/dma_buf/bufinfo`` provides an overview of every DMA-BUF

 * in the system. However, since debugfs is not safe to be mounted in

 * production, procfs and sysfs can be used to gather DMA-BUF statistics on

 * production systems.

 *

 * The ``/proc/<pid>/fdinfo/<fd>`` files in procfs can be used to gather

 * information about DMA-BUF fds. Detailed documentation about the interface

 * is present in Documentation/filesystems/proc.rst.

 *

 * Unfortunately, the existing procfs interfaces can only provide information

 * about the DMA-BUFs for which processes hold fds or have the buffers mmapped

 * into their address space. This necessitated the creation of the DMA-BUF sysfs

 * statistics interface to provide per-buffer information on production systems.

 *

 * The interface at ``/sys/kernel/dma-buf/buffers`` exposes information about

 * every DMA-BUF when ``CONFIG_DMABUF_SYSFS_STATS`` is enabled.

 *

 * The following stats are exposed by the interface:

 *

 * * ``/sys/kernel/dmabuf/buffers/<inode_number>/exporter_name``

 * * ``/sys/kernel/dmabuf/buffers/<inode_number>/size``

 *

 * The information in the interface can also be used to derive per-exporter

 * statistics. The data from the interface can be gathered on error conditions

 * or other important events to provide a snapshot of DMA-BUF usage.

 * It can also be collected periodically by telemetry to monitor various metrics.

 *

 * Detailed documentation about the interface is present in

 * Documentation/ABI/testing/sysfs-kernel-dmabuf-buffers.

 Statistics files do not need to send uevents. */

 create the directory for buffer stats */

 SPDX-License-Identifier: MIT

/*

 * Copyright © 2019 Intel Corporation

	/*

	 * We signaled the middle fence (2) of the 1-2-3 chain. The behavior

	 * of the dma-fence-chain is to make us wait for all the fences up to

	 * the point we want. Since fence 1 is still not signaled, this what

	 * we should get as fence to wait upon (fence 2 being garbage

	 * collected during the traversal of the chain).

		/*

		 * We can only find ourselves if we are on fence we were

		 * looking for.

 Fisher-Yates shuffle courtesy of Knuth */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * fence-chain: chain fences together in a timeline

 *

 * Copyright (C) 2018 Advanced Micro Devices, Inc.

 * Authors:

 *	Christian König <christian.koenig@amd.com>

/**

 * dma_fence_chain_get_prev - use RCU to get a reference to the previous fence

 * @chain: chain node to get the previous node from

 *

 * Use dma_fence_get_rcu_safe to get a reference to the previous fence of the

 * chain node.

/**

 * dma_fence_chain_walk - chain walking function

 * @fence: current chain node

 *

 * Walk the chain to the next node. Returns the next fence or NULL if we are at

 * the end of the chain. Garbage collects chain nodes which are already

 * signaled.

/**

 * dma_fence_chain_find_seqno - find fence chain node by seqno

 * @pfence: pointer to the chain node where to start

 * @seqno: the sequence number to search for

 *

 * Advance the fence pointer to the chain node which will signal this sequence

 * number. If no sequence number is provided then this is a no-op.

 *

 * Returns EINVAL if the fence is not a chain node or the sequence number has

 * not yet advanced far enough.

 Try to rearm the callback */

 Ok, we are done. No more unsignaled fences left */

	/* Manually unlink the chain as much as possible to avoid recursion

	 * and potential stack overflow.

		/* No need for atomic operations since we hold the last

		 * reference to prev_chain.

/**

 * dma_fence_chain_init - initialize a fence chain

 * @chain: the chain node to initialize

 * @prev: the previous fence

 * @fence: the current fence

 * @seqno: the sequence number to use for the fence chain

 *

 * Initialize a new chain node and either start a new chain or add the node to

 * the existing chain of the previous fence.

 Try to reuse the context of the previous chain node. */

 Make sure that we always have a valid sequence number. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/dma-buf/sync_file.c

 *

 * Copyright (C) 2012 Google, Inc.

/**

 * sync_file_create() - creates a sync file

 * @fence:	fence to add to the sync_fence

 *

 * Creates a sync_file containg @fence. This function acquires and additional

 * reference of @fence for the newly-created &sync_file, if it succeeds. The

 * sync_file can be released with fput(sync_file->file). Returns the

 * sync_file or NULL in case of error.

/**

 * sync_file_get_fence - get the fence related to the sync_file fd

 * @fd:		sync_file fd to get the fence from

 *

 * Ensures @fd references a valid sync_file and returns a fence that

 * represents all fence in the sync_file. On error NULL is returned.

/**

 * sync_file_get_name - get the name of the sync_file

 * @sync_file:		sync_file to get the fence from

 * @buf:		destination buffer to copy sync_file name into

 * @len:		available size of destination buffer.

 *

 * Each sync_file may have a name assigned either by the user (when merging

 * sync_files together) or created from the fence it contains. In the latter

 * case construction of the name is deferred until use, and so requires

 * sync_file_get_name().

 *

 * Returns: a string representing the name.

	/*

	 * The reference for the fences in the new sync_file and held

	 * in add_fence() during the merge procedure, so for num_fences == 1

	 * we already own a new reference to the fence. For num_fence > 1

	 * we own the reference of the dma_fence_array creation.

/**

 * sync_file_merge() - merge two sync_files

 * @name:	name of new fence

 * @a:		sync_file a

 * @b:		sync_file b

 *

 * Creates a new sync_file which contains copies of all the fences in both

 * @a and @b.  @a and @b remain valid, independent sync_file. Returns the

 * new merged sync_file or NULL in case of error.

	/*

	 * Assume sync_file a and b are both ordered and have no

	 * duplicates with the same context.

	 *

	 * If a sync_file can only be created with sync_file_merge

	 * and sync_file_create, this is a reasonable assumption.

	/*

	 * Passing num_fences = 0 means that userspace doesn't want to

	 * retrieve any sync_fence_info. If num_fences = 0 we skip filling

	 * sync_fence_info and return the actual number of fences on

	 * info->num_fences.

 SPDX-License-Identifier: MIT */

/*

 * Copyright © 2019 Intel Corporation

 Embed the line number into the parameter name so that we can order tests */

 Tests are listed in natural order in selftests.h */

 SPDX-License-Identifier: GPL-2.0

/*

 * DMABUF System heap exporter

 *

 * Copyright (C) 2011 Google, Inc.

 * Copyright (C) 2019, 2020 Linaro Ltd.

 *

 * Portions based off of Andrew Davis' SRAM heap:

 * Copyright (C) 2019 Texas Instruments Incorporated - http://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

/*

 * The selection of the orders used for allocation (1MB, 64K, 4K) is designed

 * to match with the sizes often found in IOMMUs. Using order 4 pages instead

 * of order 0 pages can significantly improve the performance of many IOMMUs

 * by reducing TLB pressure and time spent updating page tables.

		/*

		 * Avoid trying to allocate memory if the process

		 * has been killed by SIGKILL

 create the dmabuf */

 SPDX-License-Identifier: GPL-2.0

/*

 * DMABUF CMA heap exporter

 *

 * Copyright (C) 2012, 2019, 2020 Linaro Ltd.

 * Author: <benjamin.gaignard@linaro.org> for ST-Ericsson.

 *

 * Also utilizing parts of Andrew Davis' SRAM heap:

 * Copyright (C) 2019 Texas Instruments Incorporated - http://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

 free page list */

 release memory */

 Clear the cma pages */

			/*

			 * Avoid wasting time zeroing memory if the process

			 * has been killed by by SIGKILL

 create the dmabuf */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2010-2011, Code Aurora Forum. All rights reserved.

 *

 * Author: Stepan Moskovchenko <stepanm@codeaurora.org>

 bitmap of the page sizes currently supported */

 pagetable lock */

 Set VMID = 0 */

 Set the context number for that MID to this context */

 Set MID associated with this context bank to 0*/

 Set the ASID for TLB tagging for this context */

 Set security bit override to be Non-secure */

 Turn on TEX Remap */

 Set up HTW mode */

 TLB miss configuration: perform HTW on miss */

 V2P configuration: HTW for access */

 Set prrr and nmrr */

 Invalidate the TLB for this context */

 Set interrupt number to "secure" interrupt */

 Enable context fault interrupt */

 Stall access on a context fault and let the handler deal with it */

 Redirect all cacheable requests to L2 slave port. */

 Turn on BFB prefetch */

 Enable the MMU */

 Must be called under msm_iommu_lock */

 Invalidate context TLB */

 We are dealing with a supersection */

 Upper 20 bits from PAR, lower 12 from VA */

	/*

	 * Nothing is needed here, the barrier to guarantee

	 * completion of the tlb sync operation is implicitly

	 * taken care when the iommu client does a writel before

	 * kick starting the other master.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU API for MTK architected m4u v1 implementations

 *

 * Copyright (c) 2015-2016 MediaTek Inc.

 * Author: Honghui Zhang <honghui.zhang@mediatek.com>

 *

 * Based on driver/iommu/mtk_iommu.c

 MTK generation one iommu HW only support 4K size mapping */

/*

 * MTK m4u support 4GB iova address space, and only support 4K page

 * mapping. So the pagetable size should be exactly as 4M.

 lock for page table */

 Make sure the tlb flush all done */

 Clear the CPE status */

 Read error information from registers */

	/*

	 * MTK v1 iommu HW could not determine whether the fault is read or

	 * write fault, report as read fault.

 Interrupt clear */

 Only allow the domain created internally. */

/*

 * MTK generation one iommu HW only support one iommu domain, and all the client

 * sharing the same iova address space.

 Get the m4u device */

 MTK iommu support 4GB iova address space. */

 dev->iommu_fwspec might have changed */

 Not a iommu client device */

 protect memory,hw will write here while translation fault */

 Protect memory. HW will access here while translation fault.*/

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V stub IOMMU driver.

 *

 * Copyright (C) 2019, Microsoft, Inc.

 *

 * Author : Lan Tianyu <Tianyu.Lan@microsoft.com>

/*

 * According 82093AA IO-APIC spec , IO APIC has a 24-entry Interrupt

 * Redirection Table. Hyper-V exposes one single IO-APIC and so define

 * 24 IO APIC remmapping entries.

 Return error If new irq affinity is out of ioapic_max_cpumask. */

	/*

	 * Hypver-V IO APIC irq affinity should be in the scope of

	 * ioapic_max_cpumask because no irq remapping support.

 Claim the only I/O APIC emulated by Hyper-V */

 The rest is only relevant to guests */

	/*

	 * Hyper-V doesn't provide irq remapping function for

	 * IO-APIC and so IO-APIC only accepts 8-bit APIC ID.

	 * Cpu's APIC ID is read from ACPI MADT table and APIC IDs

	 * in the MADT table on Hyper-v are sorted monotonic increasingly.

	 * APIC ID reflects cpu topology. There maybe some APIC ID

	 * gaps when cpu number in a socket is not power of two. Prepare

	 * max cpu affinity for IOAPIC irqs. Scan cpu 0-255 and set cpu

	 * into ioapic_max_cpumask if its APIC ID is less than 256.

 IRQ remapping domain when Linux runs as the root partition */

 Invalid source */

 Turn it into an IO_APIC_route_entry, and generate MSI MSG. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2011,2016 Samsung Electronics Co., Ltd.

 *		http://www.samsung.com

 We do not consider super section mapping (16MB) */

/*

 * v1.x - v3.x SYSMMU supports 32bit physical and 32bit virtual address spaces

 * v5.0 introduced support for 36bit physical address space by shifting

 * all page entry values by 4 bits.

 * All SYSMMU controllers in the system support the address spaces of the same

 * size, so PG_ENT_SHIFT can be initialized on first SYSMMU probe to proper

 * value (0 or 4).

 no access */

 IOMMU_READ only */

 IOMMU_WRITE not supported, use read/write */

 IOMMU_READ | IOMMU_WRITE */

 no access */

 IOMMU_READ only */

 IOMMU_WRITE only */

 IOMMU_READ | IOMMU_WRITE */

 no access */

 IOMMU_READ only */

 IOMMU_WRITE not supported, use read/write */

 IOMMU_READ | IOMMU_WRITE */

 no access */

 IOMMU_READ only */

 IOMMU_WRITE only */

 IOMMU_READ | IOMMU_WRITE */

 System MMU 3.3 only */

 System MMU 3.2 only */

 System MMU 3.2+ only */

 common registers */

 11 bits */

 v1.x - v3.x registers */

 v5.x registers */

/*

 * IOMMU fault information register

 bit number in STATUS register */

 register to read VA fault address */

 human readable fault name */

 fault type for report_iommu_fault */

/*

 * This structure is attached to dev->iommu->priv of the master device

 * on device add, contains a list of SYSMMU controllers defined by device tree,

 * which are bound to given master device. It is usually referenced by 'owner'

 * pointer.

 list of sysmmu_drvdata.owner_node */

 domain this device is attached */

 for runtime pm of all sysmmus */

/*

 * This structure exynos specific generalization of struct iommu_domain.

 * It contains list of SYSMMU controllers from all master devices, which has

 * been attached to this domain and page tables of IO address space defined by

 * it. It is usually referenced by 'domain' pointer.

 list of sysmmu_drvdata.domain_node */

 lv1 page table, 16KB */

 free lv2 entry counter for each section */

 lock for modyfying list of clients */

 lock for modifying page table @ pgtable */

 generic domain data structure */

/*

 * This structure hold all data of a single SYSMMU controller, this includes

 * hw resources like registers and clocks, pointers and list nodes to connect

 * it to all other structures, internal state and parameters read from device

 * tree. It is usually referenced by 'data' pointer.

 SYSMMU controller device */

 master device (owner) */

 runtime PM link to master */

 our registers */

 SYSMMU's clock */

 SYSMMU's aclk clock */

 SYSMMU's pclk clock */

 master's device clock */

 lock for modyfying state */

 current status */

 domain we belong to */

 node for domain clients list */

 node for owner controllers list */

 assigned page table structure */

 our version */

 IOMMU core handle */

 controllers on some SoCs don't report proper version */

 SYSMMU is in blocked state when interrupt occurred. */

 unknown/unsupported fault */

 print debug message */

 fault is not recovered by fault handler */

 enable access protection bits check */

	/*

	 * SYSMMU driver keeps master's clock enabled only for the short

	 * time, while accessing the registers. For performing address

	 * translation during DMA transaction it relies on the client

	 * driver to enable it.

		/*

		 * L2TLB invalidation required

		 * 4KB page: 1 invalidation

		 * 64KB page: 16 invalidations

		 * 1MB page: 64 invalidations

		 * because it is set-associative TLB

		 * with 8-way and 64 sets.

		 * 1MB page can be cached in one of all sets.

		 * 64KB page can be one of 16 consecutive sets.

	/*

	 * use the first registered sysmmu device for performing

	 * dma mapping operations on iommu page tables (cpu cache flush)

 Check if correct PTE offsets are initialized */

 Workaround for System MMU v3.3 to prevent caching 1MiB mapping */

 For mapping page table entries we rely on dma == phys */

		/*

		 * If pre-fetched SLPD is a faulty SLPD in zero_l2_table,

		 * FLPD cache may cache the address of zero_l2_table. This

		 * function replaces the zero_l2_table with new L2 page table

		 * to write valid mappings.

		 * Accessing the valid area may cause page fault since FLPD

		 * cache may still cache zero_l2_table for the valid area

		 * instead of new L2 page table that has the mapping

		 * information of the valid area.

		 * Thus any replacement of zero_l2_table with other valid L2

		 * page table must involve FLPD cache invalidation for System

		 * MMU v3.3.

		 * FLPD cache invalidation is performed with TLB invalidation

		 * by VPN without blocking. It is safe to invalidate TLB without

		 * blocking because the target address of TLB invalidation is

		 * not currently mapped.

		/*

		 * Flushing FLPD cache in System MMU v3.3 that may cache a FLPD

		 * entry by speculative prefetch of SLPD which has no mapping.

 size == LPAGE_SIZE */

/*

 * *CAUTION* to the I/O virtual memory managers that support exynos-iommu:

 *

 * System MMU v3.x has advanced logic to improve address translation

 * performance with caching more page table entries by a page table walk.

 * However, the logic has a bug that while caching faulty page table entries,

 * System MMU reports page fault if the cached fault entry is hit even though

 * the fault entry is updated to a valid entry after the entry is cached.

 * To prevent caching faulty page table entries which may be updated to valid

 * entries later, the virtual memory manager should care about the workaround

 * for the problem. The following describes the workaround.

 *

 * Any two consecutive I/O virtual address regions must have a hole of 128KiB

 * at maximum to prevent misbehavior of System MMU 3.x (workaround for h/w bug).

 *

 * Precisely, any start address of I/O virtual region must be aligned with

 * the following sizes for System MMU v3.1 and v3.2.

 * System MMU v3.1: 128KiB

 * System MMU v3.2: 256KiB

 *

 * Because System MMU v3.3 caches page table entries more aggressively, it needs

 * more workarounds.

 * - Any two consecutive I/O virtual regions must have a hole of size larger

 *   than or equal to 128KiB.

 * - Start address of an I/O virtual region must be aligned by 128KiB.

 workaround for h/w bug in System MMU v3.3 */

 lv1ent_page(sent) == true here */

 lv1ent_large(ent) == true here */

		/*

		 * SYSMMU will be runtime activated via device link

		 * (dependency) to its master device, so there are no

		 * direct calls to pm_runtime_get/put in this driver.

 There is always at least one entry, see exynos_iommu_of_xlate() */

 SPDX-License-Identifier: GPL-2.0

/*

 * IOMMU API for s390 PCI devices

 *

 * Copyright IBM Corp. 2015

 * Author(s): Gerald Schaefer <gerald.schaefer@de.ibm.com>

/*

 * Physically contiguous memory regions can be mapped with 4 KiB alignment,

 * we allow all page sizes that are an order of 4KiB (no special large page

 * support so far).

 First device defines the DMA range limits */

 Allow only devices with identical DMA range limits */

	/*

	 * This is a workaround for a scenario where the IOMMU API common code

	 * "forgets" to call the detach_dev callback: After binding a device

	 * to vfio-pci and completing the VFIO_SET_IOMMU ioctl (which triggers

	 * the attach_dev), removing the device via

	 * "echo 1 > /sys/bus/pci/devices/.../remove" won't trigger detach_dev,

	 * only release_device will be called via the BUS_NOTIFY_REMOVED_DEVICE

	 * notifier.

	 *

	 * So let's call detach_dev from here if it hasn't been called before.

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * With interrupt-remapping, for now we will use virtual wire A

	 * mode, as virtual wire B is little complex (need to configure

	 * both IOAPIC RTE as well as interrupt-remapping table entry).

	 * As this gets called during crash dump, keep this simple for

	 * now.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007-2008 Advanced Micro Devices, Inc.

 * Author: Joerg Roedel <jroedel@suse.de>

/*

 * Use a function instead of an array here because the domain-type is a

 * bit-field, so an array would waste memory.

/**

 * iommu_device_register() - Register an IOMMU hardware instance

 * @iommu: IOMMU handle for the instance

 * @ops:   IOMMU ops to associate with the instance

 * @hwdev: (optional) actual instance device, used for fwnode lookup

 *

 * Return: 0 on success, or an error.

 We need to be able to take module references appropriately */

	/*

	 * Try to allocate a default domain - needs support from the

	 * IOMMU driver. There are still some drivers which don't

	 * support default domains, so the return value is not yet

	 * checked.

/**

 * iommu_insert_resv_region - Insert a new region in the

 * list of reserved regions.

 * @new: new region to insert

 * @regions: list of regions

 *

 * Elements are sorted by start address and overlapping segments

 * of the same type are merged.

 First add the new element based on start address sorting */

 Merge overlapping segments of type nr->type in @regions, if any */

 no merge needed on elements of different types than @new */

 look for the last stack element of same type as @iter */

/**

 * iommu_group_alloc - Allocate a new group

 *

 * This function is called by an iommu driver to allocate a new iommu

 * group.  The iommu group represents the minimum granularity of the iommu.

 * Upon successful return, the caller holds a reference to the supplied

 * group in order to hold the group until devices are added.  Use

 * iommu_group_put() to release this extra reference count, allowing the

 * group to be automatically reclaimed once it has no devices or external

 * references.

 triggers .release & free */

	/*

	 * The devices_kobj holds a reference on the group kobject, so

	 * as long as that exists so will the group.  We can therefore

	 * use the devices_kobj for reference counting.

/**

 * iommu_group_get_iommudata - retrieve iommu_data registered for a group

 * @group: the group

 *

 * iommu drivers can store data in the group for use when doing iommu

 * operations.  This function provides a way to retrieve it.  Caller

 * should hold a group reference.

/**

 * iommu_group_set_iommudata - set iommu_data for a group

 * @group: the group

 * @iommu_data: new data

 * @release: release function for iommu_data

 *

 * iommu drivers can store data in the group for use when doing iommu

 * operations.  This function provides a way to set the data after

 * the group has been allocated.  Caller should hold a group reference.

/**

 * iommu_group_set_name - set name for a group

 * @group: the group

 * @name: name

 *

 * Allow iommu driver to set a name for a group.  When set it will

 * appear in a name attribute file under the group in sysfs.

 We need to consider overlapping regions for different devices */

/**

 * iommu_group_add_device - add a device to an iommu group

 * @group: the group into which to add the device (reference should be held)

 * @dev: the device

 *

 * This function is called by an iommu driver to add a device into a

 * group.  Adding a device increments the group reference count.

			/*

			 * Account for the slim chance of collision

			 * and append an instance to the name.

 Notify any listeners about change to group. */

/**

 * iommu_group_remove_device - remove a device from it's current group

 * @dev: device to be removed

 *

 * This function is called by an iommu driver to remove the device from

 * it's current group.  This decrements the iommu group reference count.

 Pre-notify listeners that a device is being removed. */

/**

 * iommu_group_for_each_dev - iterate over each device in the group

 * @group: the group

 * @data: caller opaque data to be passed to callback function

 * @fn: caller supplied callback function

 *

 * This function is called by group users to iterate over group devices.

 * Callers should hold a reference count to the group during callback.

 * The group->mutex is held across callbacks, which will block calls to

 * iommu_group_add/remove_device.

/**

 * iommu_group_get - Return the group for a device and increment reference

 * @dev: get the group that this device belongs to

 *

 * This function is called by iommu drivers and users to get the group

 * for the specified device.  If found, the group is returned and the group

 * reference in incremented, else NULL.

/**

 * iommu_group_ref_get - Increment reference on a group

 * @group: the group to use, must not be NULL

 *

 * This function is called by iommu drivers to take additional references on an

 * existing group.  Returns the given group for convenience.

/**

 * iommu_group_put - Decrement group reference

 * @group: the group to use

 *

 * This function is called by iommu drivers and users to release the

 * iommu group.  Once the reference count is zero, the group is released.

/**

 * iommu_group_register_notifier - Register a notifier for group changes

 * @group: the group to watch

 * @nb: notifier block to signal

 *

 * This function allows iommu group users to track changes in a group.

 * See include/linux/iommu.h for actions sent via this notifier.  Caller

 * should hold a reference to the group throughout notifier registration.

/**

 * iommu_group_unregister_notifier - Unregister a notifier

 * @group: the group to watch

 * @nb: notifier block to signal

 *

 * Unregister a previously registered group notifier block.

/**

 * iommu_register_device_fault_handler() - Register a device fault handler

 * @dev: the device

 * @handler: the fault handler

 * @data: private data passed as argument to the handler

 *

 * When an IOMMU fault event is received, this handler gets called with the

 * fault event and data as argument. The handler should return 0 on success. If

 * the fault is recoverable (IOMMU_FAULT_PAGE_REQ), the consumer should also

 * complete the fault by calling iommu_page_response() with one of the following

 * response code:

 * - IOMMU_PAGE_RESP_SUCCESS: retry the translation

 * - IOMMU_PAGE_RESP_INVALID: terminate the fault

 * - IOMMU_PAGE_RESP_FAILURE: terminate the fault and stop reporting

 *   page faults if possible.

 *

 * Return 0 if the fault handler was installed successfully, or an error.

 Only allow one fault handler registered for each device */

/**

 * iommu_unregister_device_fault_handler() - Unregister the device fault handler

 * @dev: the device

 *

 * Remove the device fault handler installed with

 * iommu_register_device_fault_handler().

 *

 * Return 0 on success, or an error.

 we cannot unregister handler if there are pending faults */

/**

 * iommu_report_device_fault() - Report fault event to device driver

 * @dev: the device

 * @evt: fault event data

 *

 * Called by IOMMU drivers when a fault is detected, typically in a threaded IRQ

 * handler. When this function fails and the fault is recoverable, it is the

 * caller's responsibility to complete the fault.

 *

 * Return 0 on success, or an error.

 we only report device fault if there is a handler registered */

 Only send response if there is a fault report pending */

	/*

	 * Check if we have a matching page request pending to respond,

	 * otherwise return -EINVAL

		/*

		 * If the PASID is required, the corresponding request is

		 * matched using the group ID, the PASID valid bit and the PASID

		 * value. Otherwise only the group ID matches request and

		 * response.

 No big deal, just clear it. */

/**

 * iommu_group_id - Return ID for a group

 * @group: the group to ID

 *

 * Return the unique ID for the group matching the sysfs group number.

/*

 * To consider a PCI device isolated, we require ACS to support Source

 * Validation, Request Redirection, Completer Redirection, and Upstream

 * Forwarding.  This effectively means that devices cannot spoof their

 * requester ID, requests and completions cannot be redirected, and all

 * transactions are forwarded upstream, even as it passes through a

 * bridge where the target device is downstream.

/*

 * For multifunction devices which are not isolated from each other, find

 * all the other non-isolated functions and look for existing groups.  For

 * each function, we also need to look for aliases to or from other devices

 * that may already have a group.

/*

 * Look for aliases to or from the given device for existing groups. DMA

 * aliases are only supported on the same bus, therefore the search

 * space is quite small (especially since we're really only looking at pcie

 * device, and therefore only expect multiple slots on the root complex or

 * downstream switch ports).  It's conceivable though that a pair of

 * multifunction devices could have aliases between them that would cause a

 * loop.  To prevent this, we use a bitmap to track where we've been.

 We alias them or they alias us */

/*

 * DMA alias iterator callback, return the last seen device.  Stop and return

 * the IOMMU group if we find one along the way.

/*

 * Generic device_group call-back function. It just allocates one

 * iommu-group per device.

/*

 * Use standard PCI bus topology, isolation features, and DMA alias quirks

 * to find or create an IOMMU group for a device.

	/*

	 * Find the upstream DMA alias for the device.  A device must not

	 * be aliased due to topology in order to have its own IOMMU group.

	 * If we find an alias along the way that already belongs to a

	 * group, use it.

	/*

	 * Continue upstream from the point of minimum IOMMU granularity

	 * due to aliases to the point where devices are protected from

	 * peer-to-peer DMA by PCI ACS.  Again, if we find an existing

	 * group, use it.

	/*

	 * Look for existing groups on device aliases.  If we alias another

	 * device or another device aliases us, use the same group.

	/*

	 * Look for existing groups on non-isolated functions on the same

	 * slot and aliases of those funcions, if any.  No need to clear

	 * the search bitmap, the tested devfns are still valid.

 No shared group found, allocate new */

 Get the IOMMU group for device on fsl-mc bus */

/**

 * iommu_group_get_for_dev - Find or create the IOMMU group for a device

 * @dev: target device

 *

 * This function is intended to be called by IOMMU drivers and extended to

 * support common, bus-defined algorithms when determining or creating the

 * IOMMU group for a device.  On success, the caller will hold a reference

 * to the returned IOMMU group, which will already include the provided

 * device.  The reference should be released with iommu_group_put().

 Device is probed already if in a group */

	/*

	 * ADD/DEL call into iommu driver ops if provided, which may

	 * result in ADD/DEL notifiers to group->notifier

	/*

	 * Remaining BUS_NOTIFYs get filtered and republished to the

	 * group, if anyone is listening

 Ask for default domain requirements of all devices in the group */

	/*

	 * This code-path does not allocate the default domain when

	 * creating the iommu group, so do it after the groups are

	 * created.

 Remove item from the list */

 Try to allocate default domain */

 Clean up */

/**

 * bus_set_iommu - set iommu-callbacks for the bus

 * @bus: bus.

 * @ops: the callbacks provided by the iommu-driver

 *

 * This function is called by an iommu driver to set the iommu methods

 * used for a particular bus. Drivers for devices on that bus can use

 * the iommu-api after these ops are registered.

 * This special function is needed because IOMMUs are usually devices on

 * the bus itself, so the iommu drivers are not initialized when the bus

 * is set up. With this function the iommu-driver can set the iommu-ops

 * afterwards.

 Do IOMMU specific setup for this bus-type */

/**

 * iommu_set_fault_handler() - set a fault handler for an iommu domain

 * @domain: iommu domain

 * @handler: fault handler

 * @token: user data, will be passed back to the fault handler

 *

 * This function should be used by IOMMU users which want to be notified

 * whenever an IOMMU fault happens.

 *

 * The fault handler itself should return 0 on success, and an appropriate

 * error code otherwise.

 Assume all sizes by default; the driver may override this later */

	/*

	 * Lock the group to make sure the device-count doesn't

	 * change while we are attaching

/*

 * Check flags and other user provided data for valid combinations. We also

 * make sure no reserved fields or unused flags are set. This is to ensure

 * not breaking userspace in the future when these fields or flags are used.

 Check reserved padding fields */

	/*

	 * No new spaces can be added before the variable sized union, the

	 * minimum size is the offset to the union.

 Copy minsz from user to get flags and argsz */

 Fields before the variable size union are mandatory */

 PASID and address granu require additional info beyond minsz */

	/*

	 * User might be using a newer UAPI header which has a larger data

	 * size, we shall support the existing flags within the current

	 * size. Copy the remaining user data _after_ minsz but not more

	 * than the current kernel supported size.

 Now the argsz is validated, check the content */

 Check the range of supported formats */

 Check all flags */

 Check reserved padding fields */

	/*

	 * No new spaces can be added before the variable sized union, the

	 * minimum size is the offset to the union.

 Copy minsz from user to get flags and argsz */

 Fields before the variable size union are mandatory */

	/*

	 * User might be using a newer UAPI header, we shall let IOMMU vendor

	 * driver decide on what size it needs. Since the guest PASID bind data

	 * can be vendor specific, larger argsz could be the result of extension

	 * for one vendor but it should not affect another vendor.

	 * Copy the remaining user data _after_ minsz

/*

 * For IOMMU_DOMAIN_DMA implementations which already provide their own

 * guarantees that the group and its default domain are valid and correct.

/*

 * IOMMU groups are really the natural working unit of the IOMMU, but

 * the IOMMU API works on domains and devices.  Bridge that gap by

 * iterating over the devices in a group.  Ideally we'd have a single

 * device which represents the requestor ID of the group, but we also

 * allow IOMMU drivers to create policy defined minimum sets, where

 * the physical hardware may be able to distiguish members, but we

 * wish to group them at a higher level (ex. untrusted multi-function

 * PCI devices).  Thus we attach each device.

 Detach by re-attaching to the default domain */

 Page sizes supported by the hardware and small enough for @size */

 Constrain the page sizes further based on the maximum alignment */

 Make sure we have at least one suitable page size */

 Pick the biggest page size remaining */

 Find the next biggest support page size, if it exists */

	/*

	 * There's no point trying a bigger page size unless the virtual

	 * and physical addresses are similarly offset within the larger page.

 Calculate the offset to the next page size alignment boundary */

	/*

	 * If size is big enough to accommodate the larger page, reduce

	 * the number of smaller pages.

 find out the minimum page size supported */

	/*

	 * both the virtual address and the physical one, as well as

	 * the size of the mapping, must be aligned (at least) to the

	 * size of the smallest page supported by the hardware

		/*

		 * Some pages may have been mapped, even if an error occurred,

		 * so we should account for those so they can be unmapped.

 unroll mapping in case something went wrong */

 find out the minimum page size supported */

	/*

	 * The virtual address, as well as the size of the mapping, must be

	 * aligned (at least) to the size of the smallest page supported

	 * by the hardware

	/*

	 * Keep iterating until we either unmap 'size' bytes (or more)

	 * or we hit an area that isn't mapped.

 undo mappings already done */

/**

 * report_iommu_fault() - report about an IOMMU fault to the IOMMU framework

 * @domain: the iommu domain where the fault has happened

 * @dev: the device where the fault has happened

 * @iova: the faulting address

 * @flags: mmu fault flags (e.g. IOMMU_FAULT_READ/IOMMU_FAULT_WRITE/...)

 *

 * This function should be called by the low-level IOMMU implementations

 * whenever IOMMU faults happen, to allow high-level users, that are

 * interested in such events, to know about them.

 *

 * This event may be useful for several possible use cases:

 * - mere logging of the event

 * - dynamic TLB/PTE loading

 * - if restarting of the faulting device is required

 *

 * Returns 0 on success and an appropriate error code otherwise (if dynamic

 * PTE/TLB loading will one day be supported, implementations will be able

 * to tell whether it succeeded or not according to this return value).

 *

 * Specifically, -ENOSYS is returned if a fault handler isn't installed

 * (though fault handlers can also return -ENOSYS, in case they want to

 * elicit the default behavior of the IOMMU drivers).

	/*

	 * if upper layers showed interest and installed a fault handler,

	 * invoke it.

/**

 * generic_iommu_put_resv_regions - Reserved region driver helper

 * @dev: device for which to free reserved regions

 * @list: reserved region list for device

 *

 * IOMMU drivers can use this to implement their .put_resv_regions() callback

 * for simple reservations. Memory allocated for each reserved region will be

 * freed. If an IOMMU driver allocates additional resources per region, it is

 * going to have to implement a custom callback.

 Preallocate for the overwhelmingly common case of 1 ID */

/*

 * Per device IOMMU features.

/*

 * The device drivers should do the necessary cleanups before calling this.

 * For example, before disabling the aux-domain feature, the device driver

 * should detach all aux-domains. Otherwise, this will return -EBUSY.

/*

 * Aux-domain specific attach/detach.

 *

 * Only works if iommu_dev_feature_enabled(dev, IOMMU_DEV_FEAT_AUX) returns

 * true. Also, as long as domains are attached to a device through this

 * interface, any tries to call iommu_attach_device() should fail

 * (iommu_detach_device() can't fail, so we fail when trying to re-attach).

 * This should make us safe against a device being attached to a guest as a

 * whole while there are still pasid users on it (aux and sva).

/**

 * iommu_sva_bind_device() - Bind a process address space to a device

 * @dev: the device

 * @mm: the mm to bind, caller must hold a reference to it

 *

 * Create a bond between device and address space, allowing the device to access

 * the mm using the returned PASID. If a bond already exists between @device and

 * @mm, it is returned and an additional reference is taken. Caller must call

 * iommu_sva_unbind_device() to release each reference.

 *

 * iommu_dev_enable_feature(dev, IOMMU_DEV_FEAT_SVA) must be called first, to

 * initialize the required SVA features.

 *

 * On error, returns an ERR_PTR value.

 Ensure device count and domain don't change while we're binding */

	/*

	 * To keep things simple, SVA currently doesn't support IOMMU groups

	 * with more than one device. Existing SVA-capable systems are not

	 * affected by the problems that required IOMMU groups (lack of ACS

	 * isolation, device ID aliasing and other hardware issues).

/**

 * iommu_sva_unbind_device() - Remove a bond created with iommu_sva_bind_device

 * @handle: the handle returned by iommu_sva_bind_device()

 *

 * Put reference to a bond between device and address space. The device should

 * not be issuing any more transaction for this PASID. All outstanding page

 * requests for this PASID must have been flushed to the IOMMU.

/*

 * Changes the default domain of an iommu group that has *only* one device

 *

 * @group: The group for which the default domain should be changed

 * @prev_dev: The device in the group (this is used to make sure that the device

 *	 hasn't changed after the caller has called this function)

 * @type: The type of the new default domain that gets associated with the group

 *

 * Returns 0 on success and error code on failure

 *

 * Note:

 * 1. Presently, this function is called only when user requests to change the

 *    group's default domain type through /sys/kernel/iommu_groups/<grp_id>/type

 *    Please take a closer look if intended to use for other purposes.

	/*

	 * iommu group wasn't locked while acquiring device lock in

	 * iommu_group_store_type(). So, make sure that the device count hasn't

	 * changed while acquiring device lock.

	 *

	 * Changing default domain of an iommu group with two or more devices

	 * isn't supported because there could be a potential deadlock. Consider

	 * the following scenario. T1 is trying to acquire device locks of all

	 * the devices in the group and before it could acquire all of them,

	 * there could be another thread T2 (from different sub-system and use

	 * case) that has already acquired some of the device locks and might be

	 * waiting for T1 to release other device locks.

 Since group has only one device */

		/*

		 * If the user hasn't requested any specific type of domain and

		 * if the device supports both the domains, then default to the

		 * domain the device was booted with

	/*

	 * Switch to a new domain only if the requested domain type is different

	 * from the existing default domain type

 We can bring up a flush queue without tearing down the domain */

 Sets group->default_domain to the newly allocated domain */

	/*

	 * Release the mutex here because ops->probe_finalize() call-back of

	 * some vendor IOMMU drivers calls arm_iommu_attach_device() which

	 * in-turn might call back into IOMMU core code, where it tries to take

	 * group->mutex, resulting in a deadlock.

 Make sure dma_ops is appropriatley set */

/*

 * Changing the default domain through sysfs requires the users to unbind the

 * drivers from the devices in the iommu group, except for a DMA -> DMA-FQ

 * transition. Return failure if this isn't met.

 *

 * We need to consider the race between this and the device release path.

 * device_lock(dev) is used here to guarantee that the device release path

 * will not be entered at the same time.

	/*

	 * Lock/Unlock the group mutex here before device lock to

	 * 1. Make sure that the iommu group has only one device (this is a

	 *    prerequisite for step 2)

	 * 2. Get struct *dev which is needed to lock device

 Since group has only one device */

	/*

	 * Don't hold the group mutex because taking group mutex first and then

	 * the device lock could potentially cause a deadlock as below. Assume

	 * two threads T1 and T2. T1 is trying to change default domain of an

	 * iommu group and T2 is trying to hot unplug a device or release [1] VF

	 * of a PCIe device which is in the same iommu group. T1 takes group

	 * mutex and before it could take device lock assume T2 has taken device

	 * lock and is yet to take group mutex. Now, both the threads will be

	 * waiting for the other thread to release lock. Below, lock order was

	 * suggested.

	 * device_lock(dev);

	 *	mutex_lock(&group->mutex);

	 *		iommu_change_dev_def_domain();

	 *	mutex_unlock(&group->mutex);

	 * device_unlock(dev);

	 *

	 * [1] Typical device release path

	 * device_lock() from device/driver core code

	 *  -> bus_notifier()

	 *   -> iommu_bus_notifier()

	 *    -> iommu_release_device()

	 *     -> ops->release_device() vendor driver calls back iommu core code

	 *      -> mutex_lock() from iommu core code

 Check if the device in the group still has a driver bound to it */

 SPDX-License-Identifier: GPL-2.0

/*

 * Helpers for IOMMU drivers implementing SVA

/**

 * iommu_sva_alloc_pasid - Allocate a PASID for the mm

 * @mm: the mm

 * @min: minimum PASID value (inclusive)

 * @max: maximum PASID value (inclusive)

 *

 * Try to allocate a PASID for this mm, or take a reference to the existing one

 * provided it fits within the [@min, @max] range. On success the PASID is

 * available in mm->pasid, and must be released with iommu_sva_free_pasid().

 * @min must be greater than 0, because 0 indicates an unused mm->pasid.

 *

 * Returns 0 on success and < 0 on error.

/**

 * iommu_sva_free_pasid - Release the mm's PASID

 * @mm: the mm

 *

 * Drop one reference to a PASID allocated with iommu_sva_alloc_pasid()

 ioasid_find getter() requires a void * argument */

/**

 * iommu_sva_find() - Find mm associated to the given PASID

 * @pasid: Process Address Space ID assigned to the mm

 *

 * On success a reference to the mm is taken, and must be released with mmput().

 *

 * Returns the mm corresponding to this PASID, or an error if not found.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * OF helpers for IOMMU

 *

 * Copyright (c) 2012, NVIDIA CORPORATION.  All rights reserved.

	/*

	 * The otherwise-empty fwspec handily serves to indicate the specific

	 * IOMMU device we're waiting for, which will be useful if we ever get

	 * a proper probe-ordering dependency mechanism in future.

 In the deferred case, start again from scratch */

	/*

	 * We don't currently walk up the tree looking for a parent IOMMU.

	 * See the `Notes:' section of

	 * Documentation/devicetree/bindings/iommu/iommu.txt

	/*

	 * Two success conditions can be represented by non-negative err here:

	 * >0 : there is no IOMMU, or one was unavailable for non-fatal reasons

	 *  0 : we found an IOMMU, and dev->fwspec is initialised appropriately

	 * <0 : any actual error

 The fwspec pointer changed, read it again */

	/*

	 * If we have reason to believe the IOMMU driver missed the initial

	 * probe for dev, replay it to get things in order.

 Ignore all other errors apart from EPROBE_DEFER */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * CPU-agnostic ARM page table allocator.

 *

 * Copyright (C) 2014 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 Struct accessors */

/*

 * Calculate the right shift amount to get to the portion describing level l

 * in a virtual address mapped by the pagetable in d.

/*

 * Calculate the index at level l used to map virtual address a using the

 * pagetable in d.

 Calculate the block/page mapping size at level l for pagetable in d. */

 Page table bits */

 Ignore the contiguous bit for block splitting */

 Software bit for solving coherency races */

 Stage-1 PTE */

 Stage-2 PTE */

 Register bits */

 IOPTE accessors */

 Of the bits which overlap, either 51:48 or 15:12 are always RES0 */

 Rotate the packed high-order bits back to the top */

		/*

		 * We depend on the IOMMU being able to work with any physical

		 * address directly, so if the DMA layer suggests otherwise by

		 * translating or truncating them, that bodes very badly...

 We require an unmap first */

			/*

			 * We need to unmap and free the old table before

			 * overwriting it with a block entry.

	/*

	 * Ensure the table itself is visible before its PTE can be.

	 * Whilst we could get away with cmpxchg64_release below, this

	 * doesn't have any ordering semantics when !CONFIG_SMP.

 Even if it's not ours, there's no point waiting; just kick it */

 Find our entry at the current level */

 If we can install a leaf entry at this level, then do so */

 We can't allocate tables at the final level */

 Grab a pointer to the next level */

 We require an unmap first */

 Rinse, repeat */

	/*

	 * Note that this logic is structured to accommodate Mali LPAE

	 * having stage-1-like attributes but stage-2-like permissions.

	/*

	 * Also Mali has its own notions of shareability wherein its Inner

	 * domain covers the cores within the GPU, and its Outer domain is

	 * "outside the GPU" (i.e. either the Inner or System domain in CPU

	 * terms, depending on coherency).

 If no access, then nothing to do */

	/*

	 * Synchronise all PTE updates for the new mapping before there's

	 * a chance for anything to kick off a table walk for the new iova.

 Only leaf entries at the last level */

 Bytes unmapped */

 Unmap! */

		/*

		 * We may race against someone unmapping another part of this

		 * block, but anything else is invalid. We can't misinterpret

		 * a page entry here since we're never at the last level.

 Something went horribly wrong and we ran out of page table */

 If the size matches this level, we're in the right place */

 Also flush any partial walks */

		/*

		 * Insert a table at the next level to map the old region,

		 * minus the part we want to unmap

 Keep on walkin' */

 Valid IOPTE pointer? */

 Grab the IOPTE we're interested in */

 Valid entry? */

 Leaf entry? */

 Take it to the next level */

 Ran out of page tables to walk */

	/*

	 * We need to restrict the supported page sizes to match the

	 * translation regime for a particular granule. Aim to match

	 * the CPU page size if possible, otherwise prefer smaller sizes.

	 * While we're at it, restrict the block sizes to match the

	 * chosen granule.

 4TB */

 Calculate the actual size of our pgd (without concatenation) */

 TCR */

 MAIRs */

 Looking good; allocate a pgd */

 Ensure the empty pgd is visible before any actual TTBR write */

 TTBR */

 The NS quirk doesn't apply at stage 2 */

	/*

	 * Concatenate PGDs at level 1 if possible in order to reduce

	 * the depth of the stage-2 walk.

 VTCR */

 SL0 format is different for 4K granule size */

 Allocate pgd pages */

 Ensure the empty pgd is visible before any actual TTBR write */

 VTTBR */

 No quirks for Mali (hopefully) */

 Mali seems to need a full 4-level table regardless of IAS */

	/*

	 * MEMATTR: Mali has no actual notion of a non-cacheable type, so the

	 * best we can do is mimic the out-of-tree driver and hope that the

	 * "implementation-defined caching policy" is good enough. Similarly,

	 * we'll use it for the sake of a valid attribute for our 'device'

	 * index, although callers should never request that in practice.

 Ensure the empty pgd is visible before TRANSTAB can be written */

	/*

	 * The table format itself always uses two levels, but the total VA

	 * space is mapped by four separate tables, making the MMIO registers

	 * an effective "level 1". For simplicity, though, we treat this

	 * equivalently to LPAE stage 2 concatenation at level 2, with the

	 * additional TTBRs each just pointing at consecutive pages.

		/*

		 * Initial sanity checks.

		 * Empty page tables shouldn't provide any translations.

		/*

		 * Distinct mappings of different granule sizes.

 Overlapping mappings */

 Partial unmap */

 Remap of partial unmap */

 Full unmap */

 Remap full block */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic page table allocator for IOMMUs.

 *

 * Copyright (C) 2014 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

/*

 * It is the IOMMU driver's responsibility to ensure that the page table

 * is no longer accessible to the walker by this point.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU API for Graphics Address Relocation Table on Tegra20

 *

 * Copyright (c) 2010-2012, NVIDIA CORPORATION.  All rights reserved.

 *

 * Author: Hiroshi DOYU <hdoyu@nvidia.com>

 bitmap of the page sizes currently supported */

 offset to vmm_area start */

 offset to vmm_area end */

 for pagetable */

 for active domain */

 number of active devices */

 current active domain */

 IOMMU Core handle */

 unique for a system */

/*

 * Any interaction between any block on PPSB and a block on APB or AHB

 * must have these read-back to ensure the APB/AHB bus transaction is

 * complete before initiating activity on the PPSB block.

	/*

	 * All GART users shall be suspended at this point. Disable

	 * address translation to trap all GART accesses as invalid

	 * memory accesses.

 the GART memory aperture is required */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * omap iommu: debugfs interface

 *

 * Copyright (C) 2008-2009 Nokia Corporation

 *

 * Written by Hiroshi DOYU <Hiroshi.DOYU@nokia.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU sysfs class support

 *

 * Copyright (C) 2014 Red Hat, Inc.  All rights reserved.

 *     Author: Alex Williamson <alex.williamson@redhat.com>

/*

 * We provide a common class "devices" group which initially has no attributes.

 * As devices are added to the IOMMU, we'll add links to the group.

/*

 * Init the struct device for the IOMMU. IOMMU specific attributes can

 * be provided as an attribute group, allowing a unique namespace per

 * IOMMU type.

/*

 * IOMMU drivers can indicate a device is managed by a given IOMMU using

 * this interface.  A link to the device will be created in the "devices"

 * directory of the IOMMU device in sysfs and an "iommu" link will be

 * created under the linked device, pointing back at the IOMMU device.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright © 2006-2009, Intel Corporation.

 *

 * Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>

 The anchor node sits above the top of the usable address space */

	/*

	 * IOVA granularity will normally be equal to the smallest

	 * supported IOMMU page size; both *must* be capable of

	 * representing individual CPU pages exactly.

	/*

	 * Ideally what we'd like to judge here is whether limit_pfn is close

	 * enough to the highest-allocated IOVA that starting the allocation

	 * walk from the anchor node will be quicker than this initial work to

	 * find an exact starting point (especially if that ends up being the

	 * anchor node anyway). This is an incredibly crude approximation which

	 * only really helps the most likely case, but is at least trivially easy.

 Insert the iova into domain rbtree by holding writer lock */

 Figure out where to put new node */

 this should not happen */

 Add new node and rebalance tree. */

 Walk the tree backwards */

 pfn_lo will point to size aligned address if size_aligned is set */

 If we have 'prev', it's a valid place to start the insertion. */

/**

 * alloc_iova - allocates an iova

 * @iovad: - iova domain in question

 * @size: - size of page frames to allocate

 * @limit_pfn: - max limit address

 * @size_aligned: - set if size_aligned address range is required

 * This function allocates an iova in the range iovad->start_pfn to limit_pfn,

 * searching top-down from limit_pfn to iovad->start_pfn. If the size_aligned

 * flag is set then the allocated address iova->pfn_lo will be naturally

 * aligned on roundup_power_of_two(size).

 pfn falls within iova's range */

/**

 * find_iova - finds an iova for a given pfn

 * @iovad: - iova domain in question.

 * @pfn: - page frame number

 * This function finds and returns an iova belonging to the

 * given domain which matches the given pfn.

 Take the lock so that no other thread is manipulating the rbtree */

/**

 * __free_iova - frees the given iova

 * @iovad: iova domain in question.

 * @iova: iova in question.

 * Frees the given iova belonging to the giving domain

/**

 * free_iova - finds and frees the iova for a given pfn

 * @iovad: - iova domain in question.

 * @pfn: - pfn that is allocated previously

 * This functions finds an iova for a given pfn and then

 * frees the iova from that domain.

/**

 * alloc_iova_fast - allocates an iova from rcache

 * @iovad: - iova domain in question

 * @size: - size of page frames to allocate

 * @limit_pfn: - max limit address

 * @flush_rcache: - set to flush rcache on regular allocation failure

 * This function tries to satisfy an iova allocation from the rcache,

 * and falls back to regular allocation on failure. If regular allocation

 * fails too and the flush_rcache flag is set then the rcache will be flushed.

 Try replenishing IOVAs by flushing rcache. */

/**

 * free_iova_fast - free iova pfn range into rcache

 * @iovad: - iova domain in question.

 * @pfn: - pfn that is allocated previously

 * @size: - # of pages in range

 * This functions frees an iova range by trying to put it into the rcache,

 * falling back to regular iova deallocation via free_iova() if this fails.

	/*

	 * This code runs when the iova_domain is being detroyed, so don't

	 * bother to free iovas, just call the entry_dtor on all remaining

	 * entries.

	/*

	 * Order against the IOMMU driver's pagetable update from unmapping

	 * @pte, to guarantee that iova_domain_flush() observes that if called

	 * from a different CPU before we release the lock below. Full barrier

	 * so it also pairs with iommu_dma_init_fq() to avoid seeing partially

	 * written fq state here.

	/*

	 * First remove all entries from the flush queue that have already been

	 * flushed out on another CPU. This makes the fq_full() check below less

	 * likely to be true.

 Avoid false sharing as much as possible. */

/**

 * put_iova_domain - destroys the iova domain

 * @iovad: - iova domain in question.

 * All the iova's in that domain are destroyed.

/**

 * reserve_iova - reserves an iova in the given range

 * @iovad: - iova domain pointer

 * @pfn_lo: - lower page frame address

 * @pfn_hi:- higher pfn adderss

 * This function allocates reserves the address range from pfn_lo to pfn_hi so

 * that this address is not dished out as part of alloc_iova.

 Don't allow nonsensical pfns */

	/* We are here either because this is the first reserver node

	 * or need to insert remaining non overlap addr range

/*

 * Magazine caches for IOVA ranges.  For an introduction to magazines,

 * see the USENIX 2001 paper "Magazines and Vmem: Extending the Slab

 * Allocator to Many CPUs and Arbitrary Resources" by Bonwick and Adams.

 * For simplicity, we use a static magazine size and don't implement the

 * dynamic size tuning described in the paper.

 Only fall back to the rbtree if we have no suitable pfns at all */

 Swap it to pop it */

/*

 * Try inserting IOVA range starting with 'iova_pfn' into 'rcache', and

 * return true on success.  Can fail if rcache is full and we can't free

 * space, and free_iova() (our only caller) will then return the IOVA

 * range to the rbtree instead.

/*

 * Caller wants to allocate a new IOVA range from 'rcache'.  If we can

 * satisfy the request, return a matching non-NULL range and remove

 * it from the 'rcache'.

/*

 * Try to satisfy IOVA allocation range from rcache.  Fail if requested

 * size is too big or the DMA limit we are given isn't satisfied by the

 * top element in the magazine.

/*

 * free rcache data structures.

/*

 * free all the IOVA ranges cached by a cpu (used when cpu is unplugged)

/*

 * free all the IOVA ranges of global cache

 SPDX-License-Identifier: GPL-2.0-only

/*

 * omap iommu: tlb and pagetable primitives

 *

 * Copyright (C) 2008-2010 Nokia Corporation

 * Copyright (C) 2013-2017 Texas Instruments Incorporated - https://www.ti.com/

 *

 * Written by Hiroshi DOYU <Hiroshi.DOYU@nokia.com>,

 *		Paul Mundt and Toshihiro Kobayashi

 bitmap of the page sizes currently supported */

/**

 * to_omap_domain - Get struct omap_iommu_domain from generic iommu_domain

 * @dom:	generic iommu domain handle

/**

 * omap_iommu_save_ctx - Save registers for pm off-mode support

 * @dev:	client device

 *

 * This should be treated as an deprecated API. It is preserved only

 * to maintain existing functionality for OMAP3 ISP driver.

/**

 * omap_iommu_restore_ctx - Restore registers for pm off-mode support

 * @dev:	client device

 *

 * This should be treated as an deprecated API. It is preserved only

 * to maintain existing functionality for OMAP3 ISP driver.

/*

 *	TLB operations

 only used in iotlb iteration for-loop */

/**

 * load_iotlb_entry - Set an iommu tlb entry

 * @obj:	target iommu

 * @e:		an iommu tlb entry info

 increment victim for next tlb load */

 !PREFETCH_IOTLB */

 !PREFETCH_IOTLB */

/**

 * flush_iotlb_page - Clear an iommu tlb entry

 * @obj:	target iommu

 * @da:		iommu device virtual address

 *

 * Clear an iommu tlb entry which includes 'da' address.

/**

 * flush_iotlb_all - Clear all iommu tlb entries

 * @obj:	target iommu

/*

 *	H/W pagetable operations

 Note: freed iopte's must be clean ready for re-use */

 a table has already existed */

	/*

	 * do the allocation outside the page table lock

		/*

		 * we rely on dma address and the physical address to be

		 * the same for mapping the L2 table

 We raced, free the reduniovant table */

/**

 * omap_iopgtable_store_entry - Make an iommu pte entry

 * @obj:	target iommu

 * @e:		an iommu tlb entry info

/**

 * iopgtable_lookup_entry - Lookup an iommu pte entry

 * @obj:	target iommu

 * @da:		iommu device virtual address

 * @ppgd:	iommu pgd entry pointer to be returned

 * @ppte:	iommu pte entry pointer to be returned

 rewind to the 1st entry */

		/*

		 * do table walk to check if this table is necessary or not

 for the next L1 entry */

 rewind to the 1st entry */

/**

 * iopgtable_clear_entry - Remove an iommu pte entry

 * @obj:	target iommu

 * @da:		iommu device virtual address

/*

 *	Device IOMMU generic operations

 Fault callback or TLB/PTE Dynamic loading */

/**

 * omap_iommu_attach() - attach iommu device to an iommu domain

 * @obj:	target omap iommu device

 * @iopgd:	page table

/**

 * omap_iommu_detach - release iommu device

 * @obj:	target iommu

 check if there are any locked tlbs to save */

 no locked tlbs to restore */

/**

 * omap_iommu_domain_deactivate - deactivate attached iommu devices

 * @domain: iommu domain attached to the target iommu device

 *

 * This API allows the client devices of IOMMU devices to suspend

 * the IOMMUs they control at runtime, after they are idled and

 * suspended all activity. System Suspend will leverage the PM

 * driver late callbacks.

/**

 * omap_iommu_domain_activate - activate attached iommu devices

 * @domain: iommu domain attached to the target iommu device

 *

 * This API allows the client devices of IOMMU devices to resume the

 * IOMMUs they control at runtime, before they can resume operations.

 * System Resume will leverage the PM driver late callbacks.

/**

 * omap_iommu_runtime_suspend - disable an iommu device

 * @dev:	iommu device

 *

 * This function performs all that is necessary to disable an

 * IOMMU device, either during final detachment from a client

 * device, or during system/runtime suspend of the device. This

 * includes programming all the appropriate IOMMU registers, and

 * managing the associated omap_hwmod's state and the device's

 * reset line. This function also saves the context of any

 * locked TLBs if suspending.

 save the TLBs only during suspend, and not for power down */

/**

 * omap_iommu_runtime_resume - enable an iommu device

 * @dev:	iommu device

 *

 * This function performs all that is necessary to enable an

 * IOMMU device, either during initial attachment to a client

 * device, or during system/runtime resume of the device. This

 * includes programming all the appropriate IOMMU registers, and

 * managing the associated omap_hwmod's state and the device's

 * reset line. The function also restores any locked TLBs if

 * resuming after a suspend.

 restore the TLBs only during resume, and not for power up */

/**

 * omap_iommu_suspend_prepare - prepare() dev_pm_ops implementation

 * @dev:	iommu device

 *

 * This function performs the necessary checks to determine if the IOMMU

 * device needs suspending or not. The function checks if the runtime_pm

 * status of the device is suspended, and returns 1 in that case. This

 * results in the PM core to skip invoking any of the Sleep PM callbacks

 * (suspend, suspend_late, resume, resume_early etc).

	/*

	 * restrict IOMMU core registration only for processor-port MDMA MMUs

	 * on DRA7 DSPs

 can fail with -EPROBE_DEFER */

/*

 *	OMAP Device MMU(IOMMU) detection

	/*

	 * self-manage the ordering dependencies between omap_device_enable/idle

	 * and omap_device_assert/deassert_hardreset API

 Re-probe bus to probe device attached to this IOMMU */

	/*

	 * simplify return - we are only checking if any of the iommus

	 * reported an error, but not if all of them are unmapping the

	 * same number of entries. This should not occur due to the

	 * mirror programming.

 caller should call cleanup if this function fails */

		/*

		 * should never fail, but please keep this around to ensure

		 * we keep the hardware happy

 only a single client device can be attached to a domain */

 configure and enable the omap iommu */

 only a single device is supported per domain for now */

	/*

	 * cleanup in the reverse order of attachment - this addresses

	 * any h/w dependencies between multiple instances, if any

	/*

	 * An iommu device is still attached

	 * (currently, only one device can be attached) ?

	/*

	 * all the iommus within the domain will have identical programming,

	 * so perform the lookup using just the first iommu

	/*

	 * Allocate the per-device iommu structure for DT-based devices.

	 *

	 * TODO: Simplify this when removing non-DT support completely from the

	 * IOMMU users.

	/*

	 * retrieve the count of IOMMU nodes using phandle size as element size

	 * since #iommu-cells = 0 for OMAP

	/*

	 * use the first IOMMU alone for the sysfs device linking.

	 * TODO: Evaluate if a single iommu_group needs to be

	 * maintained for both IOMMUs

 L2 pagetable alignement */

 must be ready before omap3isp is probed */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU API for Rockchip

 *

 * Module Authors:	Simon Xue <xxm@rock-chips.com>

 *			Daniel Kurtz <djkurtz@chromium.org>

* MMU register offsets */

 Directory table address */

 IOVA of last page fault */

 Shootdown one IOTLB entry */

 IRQ status ignoring mask */

 Acknowledge and re-arm irq */

 IRQ enable */

 IRQ status after masking */

 RK_MMU_STATUS fields */

 RK_MMU_COMMAND command values */

 Enable memory translation */

 Disable memory translation */

 Stall paging to allow other cmds */

 Stop stall re-enables paging */

 Shoot down entire IOTLB */

 Clear page fault */

 Reset all registers */

 RK_MMU_INT_* register fields */

 page fault */

 bus read error */

 /*

  * Support mapping any size that fits in one page table:

  *   4 KiB to 4 MiB

 page directory table */

 lock for iommus list */

 lock for modifying page directory table */

 list of clocks required by IOMMU */

 entry in rk_iommu_domain.iommus */

 domain to which iommu is attached */

 runtime PM link from IOMMU to master */

 count of u32 entry */

/*

 * The Rockchip rk3288 iommu uses a 2-level page table.

 * The first level is the "Directory Table" (DT).

 * The DT consists of 1024 4-byte Directory Table Entries (DTEs), each pointing

 * to a "Page Table".

 * The second level is the 1024 Page Tables (PT).

 * Each PT consists of 1024 4-byte Page Table Entries (PTEs), each pointing to

 * a 4 KB page of physical memory.

 *

 * The DT and each PT fits in a single 4 KB page (4-bytes * 1024 entries).

 * Each iommu device has a MMU_DTE_ADDR register that contains the physical

 * address of the start of the DT page.

 *

 * The structure of the page table is as follows:

 *

 *                   DT

 * MMU_DTE_ADDR -> +-----+

 *                 |     |

 *                 +-----+     PT

 *                 | DTE | -> +-----+

 *                 +-----+    |     |     Memory

 *                 |     |    +-----+     Page

 *                 |     |    | PTE | -> +-----+

 *                 +-----+    +-----+    |     |

 *                            |     |    |     |

 *                            |     |    |     |

 *                            +-----+    |     |

 *                                       |     |

 *                                       |     |

 *                                       +-----+

/*

 * Each DTE has a PT address and a valid bit:

 * +---------------------+-----------+-+

 * | PT address          | Reserved  |V|

 * +---------------------+-----------+-+

 *  31:12 - PT address (PTs always starts on a 4 KB boundary)

 *  11: 1 - Reserved

 *      0 - 1 if PT @ PT address is valid

/*

 * In v2:

 * 31:12 - PT address bit 31:0

 * 11: 8 - PT address bit 35:32

 *  7: 4 - PT address bit 39:36

 *  3: 1 - Reserved

 *     0 - 1 if PT @ PT address is valid

 shift bit 8 to bit 32 */

 shift bit 4 to bit 36 */

/*

 * Each PTE has a Page address, some flags and a valid bit:

 * +---------------------+---+-------+-+

 * | Page address        |Rsv| Flags |V|

 * +---------------------+---+-------+-+

 *  31:12 - Page address (Pages always start on a 4 KB boundary)

 *  11: 9 - Reserved

 *   8: 1 - Flags

 *      8 - Read allocate - allocate cache space on read misses

 *      7 - Read cache - enable cache & prefetch of data

 *      6 - Write buffer - enable delaying writes on their way to memory

 *      5 - Write allocate - allocate cache space on write misses

 *      4 - Write cache - different writes can be merged together

 *      3 - Override cache attributes

 *          if 1, bits 4-8 control cache attributes

 *          if 0, the system bus defaults are used

 *      2 - Writable

 *      1 - Readable

 *      0 - 1 if Page @ Page address is valid

 TODO: set cache flags per prot IOMMU_CACHE */

/*

 * In v2:

 * 31:12 - Page address bit 31:0

 *  11:9 - Page address bit 34:32

 *   8:4 - Page address bit 39:35

 *     3 - Security

 *     2 - Readable

 *     1 - Writable

 *     0 - 1 if Page @ Page address is valid

/*

 * rk3288 iova (IOMMU Virtual Address) format

 *  31       22.21       12.11          0

 * +-----------+-----------+-------------+

 * | DTE index | PTE index | Page offset |

 * +-----------+-----------+-------------+

 *  31:22 - DTE index   - index of DTE in DT

 *  21:12 - PTE index   - index of PTE in PT @ DTE.pt_address

 *  11: 0 - Page offset - offset into page @ PTE.page_address

	/*

	 * TODO(djkurtz): Figure out when it is more efficient to shootdown the

	 * entire iotlb rather than iterate over individual iovas.

 Stall can only be enabled if paging is enabled */

	/*

	 * Check if register DTE_ADDR is working by writing DTE_ADDR_DUMMY

	 * and verifying that upper 5 nybbles are read back.

			/*

			 * Report page fault to any installed handlers.

			 * Ignore the return code, though, since we always zap cache

			 * and clear the page fault anyway.

 shootdown these iova from all iommus using this domain */

 Only zap TLBs of IOMMUs that are powered on. */

	/*

	 * Zap the first and last iova to evict from iotlb any previously

	 * mapped cachelines holding stale values for its dte and pte.

	 * We only zap the first and last iova, since only they could have

	 * dte or pte shared with an existing mapping.

 Unmap the range of iovas that we just mapped */

	/*

	 * pgsize_bitmap specifies iova sizes that fit in one page table

	 * (1024 4-KiB pages = 4 MiB).

	 * So, size will always be 4096 <= size <= 4194304.

	 * Since iommu_map() guarantees that both iova and size will be

	 * aligned, we will always only be mapping from a single dte here.

	/*

	 * pgsize_bitmap specifies iova sizes that fit in one page table

	 * (1024 4-KiB pages = 4 MiB).

	 * So, size will always be 4096 <= size <= 4194304.

	 * Since iommu_unmap() guarantees that both iova and size will be

	 * aligned, we will always only be unmapping from a single dte here.

 Just return 0 if iova is unmapped */

 Shootdown iotlb entries for iova range that was just unmapped */

 Must be called with iommu powered on and attached */

 Ignore error while disabling, just keep going */

 Must be called with iommu powered on and attached */

 Allow 'virtual devices' (eg drm) to detach from domain */

 iommu already detached */

	/*

	 * Allow 'virtual devices' (e.g., drm) to attach to domain.

	 * Such a device does not belong to an iommu group.

 iommu already attached */

	/*

	 * rk32xx iommus use a 2 level pagetable.

	 * Each level1 (dt) and level2 (pt) table has 1024 4-byte entries.

	 * Allocate one 4 KiB page for each table.

	/*

	 * That should not happen unless different versions of the

	 * hardware block are embedded the same SoC

	/*

	 * iommu clocks should be present for all new devices and devicetrees

	 * but there are older devicetrees without clocks out in the wild.

	 * So clocks as optional for the time being.

	/*

	 * Use the first registered IOMMU device for domain to use with DMA

	 * API, since a domain might not physically correspond to a single

	 * IOMMU device..

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * I/O Address Space ID allocator. There is one global IOASID space, split into

 * subsets. Users create a subset with DECLARE_IOASID_SET, then allocate and

 * free IOASIDs with ioasid_alloc and ioasid_put.

/*

 * struct ioasid_allocator_data - Internal data structure to hold information

 * about an allocator. There are two types of allocators:

 *

 * - Default allocator always has its own XArray to track the IOASIDs allocated.

 * - Custom allocators may share allocation helpers with different private data.

 *   Custom allocators that share the same helper functions also share the same

 *   XArray.

 * Rules:

 * 1. Default allocator is always available, not dynamically registered. This is

 *    to prevent race conditions with early boot code that want to register

 *    custom allocators or allocate IOASIDs.

 * 2. Custom allocators take precedence over the default allocator.

 * 3. When all custom allocators sharing the same helper functions are

 *    unregistered (e.g. due to hotplug), all outstanding IOASIDs must be

 *    freed. Otherwise, outstanding IOASIDs will be lost and orphaned.

 * 4. When switching between custom allocators sharing the same helper

 *    functions, outstanding IOASIDs are preserved.

 * 5. When switching between custom allocator and default allocator, all IOASIDs

 *    must be freed to ensure unadulterated space for the new allocator.

 *

 * @ops:	allocator helper functions and its data

 * @list:	registered custom allocators

 * @slist:	allocators share the same ops but different data

 * @flags:	attributes of the allocator

 * @xa:		xarray holds the IOASID space

 * @rcu:	used for kfree_rcu when unregistering allocator

 Needs framework to track results */

 Allocate and initialize a new custom allocator with its helper functions */

 For tracking custom allocators that share the same ops */

/**

 * ioasid_register_allocator - register a custom allocator

 * @ops: the custom allocator ops to be registered

 *

 * Custom allocators take precedence over the default xarray based allocator.

 * Private data associated with the IOASID allocated by the custom allocators

 * are managed by IOASID framework similar to data stored in xa by default

 * allocator.

 *

 * There can be multiple allocators registered but only one is active. In case

 * of runtime removal of a custom allocator, the next one is activated based

 * on the registration ordering.

 *

 * Multiple allocators can share the same alloc() function, in this case the

 * IOASID space is shared.

	/*

	 * No particular preference, we activate the first one and keep

	 * the later registered allocators in a list in case the first one gets

	 * removed due to hotplug.

 Use this new allocator if default is not active */

 Check if the allocator is already registered */

			/*

			 * If the new allocator shares the same ops,

			 * then they will share the same IOASID space.

			 * We should put them under the same xarray.

/**

 * ioasid_unregister_allocator - Remove a custom IOASID allocator ops

 * @ops: the custom allocator to be removed

 *

 * Remove an allocator from the list, activate the next allocator in

 * the order it was registered. Or revert to default allocator if all

 * custom allocators are unregistered without outstanding IOASIDs.

 No shared helper functions */

			/*

			 * All IOASIDs should have been freed before

			 * the last allocator that shares the same ops

			 * is unregistered.

		/*

		 * Find the matching shared ops to delete,

		 * but keep outstanding IOASIDs

/**

 * ioasid_set_data - Set private data for an allocated ioasid

 * @ioasid: the ID to set data

 * @data:   the private data

 *

 * For IOASID that is already allocated, private data can be set

 * via this API. Future lookup can be done via ioasid_find.

	/*

	 * Wait for readers to stop accessing the old private data, so the

	 * caller can free it.

/**

 * ioasid_alloc - Allocate an IOASID

 * @set: the IOASID set

 * @min: the minimum ID (inclusive)

 * @max: the maximum ID (inclusive)

 * @private: data private to the caller

 *

 * Allocate an ID between @min and @max. The @private pointer is stored

 * internally and can be retrieved with ioasid_find().

 *

 * Return: the allocated ID on success, or %INVALID_IOASID on failure.

	/*

	 * Custom allocator needs allocator data to perform platform specific

	 * operations.

 Custom allocator needs framework to store and track allocation results */

/**

 * ioasid_get - obtain a reference to the IOASID

/**

 * ioasid_put - Release a reference to an ioasid

 * @ioasid: the ID to remove

 *

 * Put a reference to the IOASID, free it when the number of references drops to

 * zero.

 *

 * Return: %true if the IOASID was freed, %false otherwise.

 Custom allocator needs additional steps to free the xa element */

/**

 * ioasid_find - Find IOASID data

 * @set: the IOASID set

 * @ioasid: the IOASID to find

 * @getter: function to call on the found object

 *

 * The optional getter function allows to take a reference to the found object

 * under the rcu lock. The function can also check if the object is still valid:

 * if @getter returns false, then the object is invalid and NULL is returned.

 *

 * If the IOASID exists, return the private pointer passed to ioasid_alloc.

 * Private data can be NULL if not set. Return an error if the IOASID is not

 * found, or if @set is not NULL and the IOASID does not belong to the set.

 data found but does not belong to the set */

 Now IOASID and its set is verified, we can return the private data */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * CPU-agnostic ARM page table allocator.

 *

 * ARMv7 Short-descriptor format, supporting

 * - Basic memory attributes

 * - Simplified access permissions (AP[2:1] model)

 * - Backwards-compatible TEX remap

 * - Large pages/supersections (if indicated by the caller)

 *

 * Not supporting:

 * - Legacy access permissions (AP[2:0] model)

 *

 * Almost certainly never supporting:

 * - PXN

 * - Domains

 *

 * Copyright (C) 2014-2015 ARM Limited

 * Copyright (c) 2014-2015 MediaTek Inc.

 Struct accessors */

/*

 * We have 32 bits total; 12 bits resolved at level 1, 8 bits at level 2,

 * and 12 bits in a page.

 * MediaTek extend 2 bits to reach 34bits, 14 bits at lvl1 and 8 bits at lvl2.

/*

 * Large page/supersection entries are effectively a block of 16 page/section

 * entries, along the lines of the LPAE contiguous hint, but all with the

 * same output address. For want of a better common name we'll call them

 * "contiguous" versions of their respective page/section entries here, but

 * noting the distinction (WRT to TLB maintenance) that they represent *one*

 * entry repeated 16 times, not 16 separate entries (as in the LPAE case).

 PTE type bits: these are all mixed up with XN/PXN bits in most cases */

 Page table bits */

/*

 * The attribute bits are consistently ordered*, but occupy bits [17:10] of

 * a level 1 PTE vs. bits [11:4] at level 2. Thus we define the individual

 * fields relative to that 8-bit block, plus a total shift relative to the PTE.

 MediaTek extend the bits below for PA 32bit/33bit/34bit */

 *well, except for TEX on level 2 large pages, of course :( */

 Simplified access permissions */

 Register bits */

 Doesn't fit in PTE */

		/*

		 * We depend on the IOMMU being able to work with any physical

		 * address directly, so if the DMA layer suggests otherwise by

		 * translating or truncating them, that bodes very badly...

			/*

			 * We need to unmap and free the old table before

			 * overwriting it with a block entry.

 We require an unmap first */

	/*

	 * Ensure the table itself is visible before its PTE can be.

	 * Whilst we could get away with cmpxchg64_release below, this

	 * doesn't have any ordering semantics when !CONFIG_SMP.

 Find our entry at the current level */

 If we can install a leaf entry at this level, then do so */

 We can't allocate tables at the final level */

 Grab a pointer to the next level */

 We've no easy way of knowing if it's synced yet, so... */

 We require an unmap first */

 Rinse, repeat */

 If no access, then nothing to do */

	/*

	 * Synchronise all PTE updates for the new mapping before there's

	 * a chance for anything to kick off a table walk for the new iova.

 Check that we didn't lose a race to get the lock */

 Bytes unmapped */

 Unmap! */

 Something went horribly wrong and we ran out of page table */

	/*

	 * If we've hit a contiguous 'large page' entry at this level, it

	 * needs splitting first, unless we're unmapping the whole lot.

	 *

	 * For splitting, we can't rewrite 16 PTEs atomically, and since we

	 * can't necessarily assume TEX remap we don't have a software bit to

	 * mark live entries being split. In practice (i.e. DMA API code), we

	 * will never be splitting large pages anyway, so just wrap this edge

	 * case in a lock for the sake of correctness and be done with it.

 If the size matches this level, we're in the right place */

 Also flush any partial walks */

		/*

		 * Insert a table at the next level to map the old region,

		 * minus the part we want to unmap

 Keep on walkin' */

 If ARM_MTK_4GB is enabled, the NO_PERMS is also expected. */

 We have to do this early for __arm_v7s_alloc_table to work... */

	/*

	 * Unless the IOMMU driver indicates supersection support by

	 * having SZ_16M set in the initial bitmap, they won't be used.

 TCR: T0SZ=0, EAE=0 (if applicable) */

	/*

	 * TEX remap: the indices used map to the closest equivalent types

	 * under the non-TEX-remap interpretation of those attribute bits,

	 * excepting various implementation-defined aspects of shareability.

 Looking good; allocate a pgd */

 Ensure the empty pgd is visible before any actual TTBR write */

 TTBR */

	/*

	 * Initial sanity checks.

	 * Empty page tables shouldn't provide any translations.

	/*

	 * Distinct mappings of different granule sizes.

 Overlapping mappings */

 Partial unmap */

 Remap of partial unmap */

 Full unmap */

 Remap full block */

 SPDX-License-Identifier: GPL-2.0

/*

 * iommu trace points

 *

 * Copyright (C) 2013 Shuah Khan <shuah.kh@samsung.com>

 *

 iommu_group_event */

 iommu_device_event */

 iommu_map_unmap */

 iommu_error */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2015-2016 MediaTek Inc.

 * Author: Yong Wu <yong.wu@mediatek.com>

 mmu0 | mmu1 */

 HW will use the EMI clock if there isn't the "bclk". */

/*

 * In M4U 4GB mode, the physical address is remapped as below:

 *

 * CPU Physical address:

 * ====================

 *

 * 0      1G       2G     3G       4G     5G

 * |---A---|---B---|---C---|---D---|---E---|

 * +--I/O--+------------Memory-------------+

 *

 * IOMMU output physical address:

 *  =============================

 *

 *                                 4G      5G     6G      7G      8G

 *                                 |---E---|---B---|---C---|---D---|

 *                                 +------------Memory-------------+

 *

 * The Region 'A'(I/O) can NOT be mapped by M4U; For Region 'B'/'C'/'D', the

 * bit32 of the CPU physical address always is needed to set, and for Region

 * 'E', the CPU physical address keep as is.

 * Additionally, The iommu consumers always use the CPU phyiscal address.

 List all the M4U HWs */

 disp: 0 ~ 4G */

 vdec: 4G ~ 8G */

 CAM/MDP: 8G ~ 12G */

 CCU0 */

 CCU1 */

/*

 * There may be 1 or 2 M4U HWs, But we always expect they are in the same domain

 * for the performance.

 *

 * Here always return the mtk_iommu_data of the first probed M4U where the

 * iommu domain information is recorded.

 Make sure the tlb flush all done */

 tlb sync */

 Clear the CPE status */

 Read error info from registers */

 Interrupt clear */

 Best fit. */

 ok if it is inside this region. */

 Use the exist domain as there is only one pgtable here. */

 Update our support page sizes bitmap */

 Update the iova region for this domain */

 Initialize the M4U HW */

 The "4GB mode" M4U physically can not use the lower remap of Dram. */

 Synchronize with the tlb_lock */

 Not a iommu client device */

 Get the m4u device */

 Only reserve when the region is inside the current domain */

		/*

		 * If 4GB mode is enabled, the validate PA range is from

		 * 0x1_0000_0000 to 0x1_ffff_ffff. here record bit[32:30].

 write command throttling mode */

 The register is called STANDARD_AXI_MODE in this case */

 Protect memory. HW will access here while translation fault.*/

 The id is consecutive if there is no this property */

 Get smi-common dev from the last larb. */

	/*

	 * Uppon first resume, only enable the clk and return, since the values of the

	 * registers are not yet set.

 Linear mapping. */

 Linear mapping. */

 SPDX-License-Identifier: GPL-2.0

/*

 * IOMMU debugfs core infrastructure

 *

 * Copyright (C) 2018 Advanced Micro Devices, Inc.

 *

 * Author: Gary R Hook <gary.hook@amd.com>

/**

 * iommu_debugfs_setup - create the top-level iommu directory in debugfs

 *

 * Provide base enablement for using debugfs to expose internal data of an

 * IOMMU driver. When called, this function creates the

 * /sys/kernel/debug/iommu directory.

 *

 * Emit a strong warning at boot time to indicate that this feature is

 * enabled.

 *

 * This function is called from iommu_init; drivers may then use

 * iommu_debugfs_dir to instantiate a vendor-specific directory to be used

 * to expose internal data.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 Freescale Semiconductor, Inc.

 * Author: Varun Sethi <varun.sethi@freescale.com>

/*

 * Global spinlock that needs to be held while

 * configuring PAMU.

 IOMMU core code handle */

 Set the geometry parameters for a LIODN */

	/*

	 * Configure the omi_index at the geometry setup time.

	 * This is a static value which depends on the type of

	 * device and would not change thereafter.

 Remove the device from the domain device list */

	/*

	 * Check here if the device is already attached to domain or not.

	 * If the device is already attached to a domain detach it.

	/*

	 * In case of devices with multiple LIODNs just store

	 * the info for the first LIODN as all

	 * LIODNs share the same domain

 remove all the devices from the device list */

 default geometry 64 GB i.e. maximum system address */

 Update stash destination for all LIODNs associated with the domain */

	/*

	 * Use LIODN of the PCI controller while attaching a

	 * PCI device.

		/*

		 * make dev point to pci controller device

		 * so we can get the LIODN programmed by

		 * u-boot.

 Ensure that LIODN value is valid */

	/*

	 * Use LIODN of the PCI controller while detaching a

	 * PCI device.

		/*

		 * make dev point to pci controller device

		 * so we can get the LIODN programmed by

		 * u-boot.

 Set the domain stash attribute */

 Check the PCI controller version number by readding BRR1 register */

 If PCI controller version is >= 0x204 we can partition endpoints */

 Get iommu group information from peer devices or devices on the parent bus */

	/*

	 * Traverese the pci bus device list to get

	 * the shared iommu group.

 We can partition PCIe devices so assign device group to the device */

		/*

		 * PCIe controller is not a paritionable entity

		 * free the controller device iommu_group.

		/*

		 * All devices connected to the controller will share the

		 * PCI controllers device group. If this is the first

		 * device to be probed for the pci controller, copy the

		 * device group information from the PCI controller device

		 * node and remove the PCI controller iommu group.

		 * For subsequent devices, the iommu group information can

		 * be obtained from sibling devices (i.e. from the bus_devices

		 * link list).

	/*

	 * For platform devices we allocate a separate group for

	 * each of the devices.

 SPDX-License-Identifier: GPL-2.0

/*

 * Handle device page faults

 *

 * Copyright (C) 2020 ARM Ltd.

/**

 * struct iopf_queue - IO Page Fault queue

 * @wq: the fault workqueue

 * @devices: devices attached to this queue

 * @lock: protects the device list

/**

 * struct iopf_device_param - IO Page Fault data attached to a device

 * @dev: the device that owns this param

 * @queue: IOPF queue

 * @queue_list: index into queue->devices

 * @partial: faults that are part of a Page Request Group for which the last

 *           request hasn't been submitted yet.

 Unmapped area */

 Access fault */

		/*

		 * For the moment, errors are sticky: don't handle subsequent

		 * faults in the group if there is an error.

/**

 * iommu_queue_iopf - IO Page Fault handler

 * @fault: fault event

 * @cookie: struct device, passed to iommu_register_device_fault_handler.

 *

 * Add a fault to the device workqueue, to be handled by mm.

 *

 * This module doesn't handle PCI PASID Stop Marker; IOMMU drivers must discard

 * them before reporting faults. A PASID Stop Marker (LRW = 0b100) doesn't

 * expect a response. It may be generated when disabling a PASID (issuing a

 * PASID stop request) by some PCI devices.

 *

 * The PASID stop request is issued by the device driver before unbind(). Once

 * it completes, no page request is generated for this PASID anymore and

 * outstanding ones have been pushed to the IOMMU (as per PCIe 4.0r1.0 - 6.20.1

 * and 10.4.1.2 - Managing PASID TLP Prefix Usage). Some PCI devices will wait

 * for all outstanding page requests to come back with a response before

 * completing the PASID stop request. Others do not wait for page responses, and

 * instead issue this Stop Marker that tells us when the PASID can be

 * reallocated.

 *

 * It is safe to discard the Stop Marker because it is an optimization.

 * a. Page requests, which are posted requests, have been flushed to the IOMMU

 *    when the stop request completes.

 * b. The IOMMU driver flushes all fault queues on unbind() before freeing the

 *    PASID.

 *

 * So even though the Stop Marker might be issued by the device *after* the stop

 * request completes, outstanding faults will have been dealt with by the time

 * the PASID is freed.

 *

 * Return: 0 on success and <0 on error.

 Not a recoverable page fault */

	/*

	 * As long as we're holding param->lock, the queue can't be unlinked

	 * from the device and therefore cannot disappear.

 Non-last request of a group. Postpone until the last one */

		/*

		 * The caller will send a response to the hardware. But we do

		 * need to clean up before leaving, otherwise partial faults

		 * will be stuck.

 See if we have partial faults for this group */

 Insert *before* the last fault */

/**

 * iopf_queue_flush_dev - Ensure that all queued faults have been processed

 * @dev: the endpoint whose faults need to be flushed.

 *

 * The IOMMU driver calls this before releasing a PASID, to ensure that all

 * pending faults for this PASID have been handled, and won't hit the address

 * space of the next process that uses this PASID. The driver must make sure

 * that no new fault is added to the queue. In particular it must flush its

 * low-level queue before calling this function.

 *

 * Return: 0 on success and <0 on error.

/**

 * iopf_queue_discard_partial - Remove all pending partial fault

 * @queue: the queue whose partial faults need to be discarded

 *

 * When the hardware queue overflows, last page faults in a group may have been

 * lost and the IOMMU driver calls this to discard all partial faults. The

 * driver shouldn't be adding new faults to this queue concurrently.

 *

 * Return: 0 on success and <0 on error.

/**

 * iopf_queue_add_device - Add producer to the fault queue

 * @queue: IOPF queue

 * @dev: device to add

 *

 * Return: 0 on success and <0 on error.

/**

 * iopf_queue_remove_device - Remove producer from fault queue

 * @queue: IOPF queue

 * @dev: device to remove

 *

 * Caller makes sure that no more faults are reported for this device.

 *

 * Return: 0 on success and <0 on error.

 Just in case some faults are still stuck */

/**

 * iopf_queue_alloc - Allocate and initialize a fault queue

 * @name: a unique string identifying the queue (for workqueue)

 *

 * Return: the queue on success and NULL on error.

	/*

	 * The WQ is unordered because the low-level handler enqueues faults by

	 * group. PRI requests within a group have to be ordered, but once

	 * that's dealt with, the high-level function can handle groups out of

	 * order.

/**

 * iopf_queue_free - Free IOPF queue

 * @queue: queue to free

 *

 * Counterpart to iopf_queue_alloc(). The driver must not be queuing faults or

 * adding/removing devices on this queue anymore.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Unisoc IOMMU driver

 *

 * Copyright (C) 2020 Unisoc, Inc.

 * Author: Chunyan Zhang <chunyan.zhang@unisoc.com>

/*

 * struct sprd_iommu_device - high-level sprd IOMMU device representation,

 * including hardware information and configuration, also driver data, etc

 *

 * @ver: sprd IOMMU IP version

 * @prot_page_va: protect page base virtual address

 * @prot_page_pa: protect page base physical address, data would be

 *		  written to here while translation fault

 * @base: mapped base address for accessing registers

 * @dev: pointer to basic device structure

 * @iommu: IOMMU core representation

 * @group: IOMMU group

 * @eb: gate clock which controls IOMMU access

 lock for page table */

 page table virtual address base */

 page table physical address base */

 clear IOMMU TLB buffer after page table updated */

/*

 * Clock is not required, access to some of IOMMUs is controlled by gate

 * clk, enabled clocks for that kind of IOMMUs before accessing.

 * Return 0 for success or no clocks found.

 All the client devices are in the same iommu-group */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A fairly generic DMA-API to IOMMU-API glue layer.

 *

 * Copyright (C) 2014-2015 ARM Ltd.

 *

 * based in part on arch/arm/mm/dma-mapping.c:

 * Copyright (C) 2000-2004 Russell King

 Full allocator for IOMMU_DMA_IOVA_COOKIE */

 Trivial linear page allocator for IOMMU_DMA_MSI_COOKIE */

 Domain for flush queue callback; NULL if flush queue not in use */

/**

 * iommu_get_dma_cookie - Acquire DMA-API resources for a domain

 * @domain: IOMMU domain to prepare for DMA-API usage

/**

 * iommu_get_msi_cookie - Acquire just MSI remapping resources

 * @domain: IOMMU domain to prepare

 * @base: Start address of IOVA region for MSI mappings

 *

 * Users who manage their own IOVA allocation and do not want DMA API support,

 * but would still like to take advantage of automatic MSI remapping, can use

 * this to initialise their own domain appropriately. Users should reserve a

 * contiguous IOVA region, starting at @base, large enough to accommodate the

 * number of PAGE_SIZE mappings necessary to cover every MSI doorbell address

 * used by the devices attached to @domain.

/**

 * iommu_put_dma_cookie - Release a domain's DMA mapping resources

 * @domain: IOMMU domain previously prepared by iommu_get_dma_cookie() or

 *          iommu_get_msi_cookie()

/**

 * iommu_dma_get_resv_regions - Reserved region driver helper

 * @dev: Device from iommu_get_resv_regions()

 * @list: Reserved region list from iommu_get_resv_regions()

 *

 * IOMMU drivers can use this to implement their .get_resv_regions callback

 * for general non-IOMMU-specific reservations. Currently, this covers GICv3

 * ITS region reservation on ACPI based ARM platforms that may require HW MSI

 * reservation.

 Get reserved DMA windows from host bridge */

 dma_ranges list should be sorted */

 If window is last entry */

 We ARE the software that manages these! */

 sysfs updates are serialised by the mutex of the group owning @domain */

	/*

	 * Prevent incomplete iovad->fq being observable. Pairs with path from

	 * __iommu_dma_unmap() through iommu_dma_free_iova() to queue_iova()

/**

 * iommu_dma_init_domain - Initialise a DMA mapping domain

 * @domain: IOMMU domain previously prepared by iommu_get_dma_cookie()

 * @base: IOVA at which the mappable address space starts

 * @limit: Last address of the IOVA space

 * @dev: Device the domain is being initialised for

 *

 * @base and @limit + 1 should be exact multiples of IOMMU page granularity to

 * avoid rounding surprises. If necessary, we reserve the page at address 0

 * to ensure it is an invalid IOVA. It is safe to reinitialise a domain, but

 * any change which could make prior IOVAs invalid will fail.

 Use the smallest supported page size for IOVA granularity */

 Check the domain allows at least some access to the device... */

 ...then finally give it a kicking to make sure it fits */

 start_pfn is always nonzero for an already-initialised domain */

 If the FQ fails we can simply fall back to strict mode */

/**

 * dma_info_to_prot - Translate DMA API directions and attributes to IOMMU API

 *                    page flags.

 * @dir: Direction of DMA transfer

 * @coherent: Is the DMA master cache-coherent?

 * @attrs: DMA attributes for the mapping

 *

 * Return: corresponding IOMMU API page protection flags

	/*

	 * Freeing non-power-of-two-sized allocations back into the IOVA caches

	 * will come back to bite us badly, so we have to waste a bit of space

	 * rounding up anything cacheable to make sure that can't happen. The

	 * order of the unadjusted size will still match upon freeing.

 Try to get PCI devices a SAC address */

 The MSI case is only ever cleaning up its most recent allocation */

 IOMMU can map any pages, so himem can also be used here */

 It makes no sense to muck about with huge pages */

		/*

		 * Higher-order allocations are a convenience rather

		 * than a necessity, hence using __GFP_NORETRY until

		 * falling back to minimum-order allocations.

/*

 * If size is less than PAGE_SIZE, then a full CPU page will be allocated,

 * but an IOMMU which supports smaller pages might not map the whole thing.

 CONFIG_DMA_REMAP */

	/*

	 * If both the physical buffer start address and size are

	 * page aligned, we don't need to use a bounce page.

 Cleanup the padding area. */

/*

 * Prepare a successfully-mapped scatterlist to give back to the caller.

 *

 * At this point the segments are already laid out by iommu_dma_map_sg() to

 * avoid individually crossing any boundaries, so we merely need to check a

 * segment's start address to avoid concatenating across one.

 Restore this segment's original unaligned fields first */

		/*

		 * Now fill in the real DMA data. If...

		 * - there is a valid output segment to append to

		 * - and this segment starts on an IOVA page boundary

		 * - but doesn't fall at a segment boundary

		 * - and wouldn't make the resulting output segment too long

 ...then concatenate it with the previous one */

 Otherwise start the next output segment */

/*

 * If mapping failed, then just restore the original list,

 * but making sure the DMA fields are invalidated.

/*

 * The DMA API client is passing in a scatterlist which could describe

 * any old buffer layout, but the IOMMU API requires everything to be

 * aligned to IOMMU pages. Hence the need for this complicated bit of

 * impedance-matching, to be able to hand off a suitably-aligned list,

 * but still preserve the original offsets and sizes for the caller.

	/*

	 * Work out how much IOVA space we need, and align the segments to

	 * IOVA granules for the IOMMU driver to handle. With some clever

	 * trickery we can modify the list in-place, but reversibly, by

	 * stashing the unaligned parts in the as-yet-unused DMA fields.

		/*

		 * Due to the alignment of our single IOVA allocation, we can

		 * depend on these assumptions about the segment boundary mask:

		 * - If mask size >= IOVA size, then the IOVA range cannot

		 *   possibly fall across a boundary, so we don't care.

		 * - If mask size < IOVA size, then the IOVA range must start

		 *   exactly on a boundary, therefore we can lay things out

		 *   based purely on segment lengths without needing to know

		 *   the actual addresses beforehand.

		 * - The mask must be a power of 2, so pad_len == 0 if

		 *   iova_len == 0, thus we cannot dereference prev the first

		 *   time through here (i.e. before it has a meaningful value).

	/*

	 * We'll leave any physical concatenation to the IOMMU driver's

	 * implementation - it knows better than we do.

	/*

	 * The scatterlist segments are mapped into a single

	 * contiguous IOVA allocation, so this is incredibly easy.

 Non-coherent atomic allocation? Easy */

		/*

		 * If it the address is remapped, then it's either non-coherent

		 * or highmem CMA, or an iommu_dma_alloc_remap() construction.

 Lowmem means a coherent atomic or CMA allocation */

/*

 * The IOMMU core code allocates the default DMA domain, which the underlying

 * IOMMU driver needs to support via the dma-iommu layer.

	/*

	 * The IOMMU core code allocates the default DMA domain, which the

	 * underlying IOMMU driver needs to support via the dma-iommu layer.

 see below */

	/*

	 * In fact the whole prepare operation should already be serialised by

	 * irq_domain_mutex further up the callchain, but that's pretty subtle

	 * on its own, so consider this locking as failsafe documentation...

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011-2014 NVIDIA CORPORATION.  All rights reserved.

 IOMMU Core code handle */

 per-SWGROUP SMMU_*_ASID register */

 page table definitions */

 setup aperture */

 TODO: free page directory and page tables */

 No point moving ahead if group was not found */

 We can't handle 64-bit DMA addresses */

 Set the page directory entry first */

 The flush the page directory entry from caches */

 And flush the iommu */

	/*

	 * When no entries in this page table are used anymore, return the

	 * memory page to the system.

 at first check whether allocation needs to be done at all */

	/*

	 * In order to prevent exhaustion of the atomic memory pool, we

	 * allocate page in a sleeping context if GFP flags permit. Hence

	 * spinlock needs to be unlocked and re-locked after allocation.

	/*

	 * In a case of blocking allocation, a concurrent mapping may win

	 * the PDE allocation. In this case the allocated page isn't needed

	 * if allocation succeeded and the allocation failure isn't fatal.

 If we aren't overwriting a pre-existing entry, increment use */

 Find group_soc associating with swgroup */

 Find existing iommu_group associating with swgroup or group_soc */

	/*

	 * Note: we are here releasing the reference of &iommu_pdev->dev, which

	 * is mc->dev. Although some functions in tegra_smmu_ops may keep using

	 * its private data beyond this point, it's still safe to do so because

	 * the SMMU parent device is the same as the MC, so the reference count

	 * isn't strictly necessary.

	/*

	 * This is a bit of a hack. Ideally we'd want to simply return this

	 * value. However the IOMMU registration process will attempt to add

	 * all devices to the IOMMU when bus_set_iommu() is called. In order

	 * not to rely on global variables to track the IOMMU instance, we

	 * set it here so that it can be looked up from the .probe_device()

	 * callback via the IOMMU device's .drvdata field.

 SPDX-License-Identifier: GPL-2.0

/*

 * IOMMU API for Renesas VMSA-compatible IPMMU

 * Author: Laurent Pinchart <laurent.pinchart@ideasonboard.com>

 *

 * Copyright (C) 2014-2020 Renesas Electronics Corporation

 Protects ctx and domains[] */

 Protects mappings */

 100us */

/* -----------------------------------------------------------------------------

 * Registers Definition

 MMU "context" registers */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2 only */

 R-Car Gen2 only */

 R-Car Gen2 only */

 R-Car Gen3 only */

 R-Car Gen2 only */

 R-Car Gen2 only */

 R-Car Gen2 only */

 R-Car Gen2 only */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3, IMEAR on R-Car Gen2 */

 R-Car Gen3 only */

 uTLB registers */

 R-Car Gen2/3 */

 R-Car Gen3 only */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen2/3 */

 R-Car Gen3 only */

/* -----------------------------------------------------------------------------

 * Root device handling

/* -----------------------------------------------------------------------------

 * Read/Write Access

/* -----------------------------------------------------------------------------

 * TLB and microTLB Management

 Wait for any pending TLB invalidations to complete */

/*

 * Enable MMU translation for the microTLB.

	/*

	 * TODO: Reference-count the microTLB as several bus masters can be

	 * connected to the same microTLB.

 TODO: What should we set the ASID to ? */

 TODO: Do we need to flush the microTLB ? */

/*

 * Disable MMU translation for the microTLB.

/* -----------------------------------------------------------------------------

 * Domain/Context Management

 TTBR0 */

	/*

	 * TTBCR

	 * We use long descriptors and allocate the whole 32-bit VA space to

	 * TTBR0.

 MAIR0 */

 IMBUSCR */

	/*

	 * IMSTR

	 * Clear all interrupt flags.

	/*

	 * IMCTR

	 * Enable the MMU and interrupt generation. The long-descriptor

	 * translation table format doesn't use TEX remapping. Don't enable AF

	 * software management as we have no use for it. Flush the TLB as

	 * required when modifying the context registers.

	/*

	 * Allocate the page table operations.

	 *

	 * VMSA states in section B3.6.3 "Control of Secure or Non-secure memory

	 * access, Long-descriptor format" that the NStable bit being set in a

	 * table descriptor will result in the NStable and NS bits of all child

	 * entries being ignored and considered as being set. The IPMMU seems

	 * not to comply with this, as it generates a secure access page fault

	 * if any of the NStable and NS bits isn't set when running in

	 * non-secure mode.

	/*

	 * TODO: Add support for coherent walk through CCI with DVM and remove

	 * cache handling. For now, delegate it to the io-pgtable code.

	/*

	 * Find an unused context.

	/*

	 * Disable the context. Flush the TLB as required when modifying the

	 * context registers.

	 *

	 * TODO: Is TLB flush really needed ?

/* -----------------------------------------------------------------------------

 * Fault Handling

	/*

	 * Clear the error status flags. Unlike traditional interrupt flag

	 * registers that must be cleared by writing 1, this status register

	 * seems to require 0. The error address register must be read before,

	 * otherwise its value will be 0.

 Log fatal errors. */

	/*

	 * Try to handle page faults and translation faults.

	 *

	 * TODO: We need to look up the faulty device based on the I/O VA. Use

	 * the IOMMU device for now.

	/*

	 * Check interrupts for all active contexts.

/* -----------------------------------------------------------------------------

 * IOMMU Operations

	/*

	 * Free the domain resources. We assume that all devices have already

	 * been detached.

 The domain hasn't been used yet, initialize it. */

		/*

		 * Something is wrong, we can't attach two devices using

		 * different IOMMUs to the same domain.

	/*

	 * TODO: Optimize by disabling the context when no device is attached.

 TODO: Is locking needed ? */

 sentinel */ }

 sentinel */ }

	/*

	 * R-Car Gen3 and RZ/G2 use the allow list to opt-in devices.

	 * For Other SoCs, this returns true anyway.

 Check whether this SoC can use the IPMMU correctly or not */

 Check whether this device can work with the IPMMU */

 Otherwise, do not allow use of IPMMU */

 Initialize once - xlate() will call multiple times */

	/*

	 * Create the ARM mapping, used by the ARM DMA mapping core to allocate

	 * VAs. This will allocate a corresponding IOMMU domain.

	 *

	 * TODO:

	 * - Create one mapping per context (TLB).

	 * - Make the mapping size configurable ? We currently use a 2GB mapping

	 *   at a 1GB offset to ensure that NULL VAs will fault.

 Attach the ARM VA mapping to the device. */

	/*

	 * Only let through devices that have been verified in xlate()

/* -----------------------------------------------------------------------------

 * Probe/remove and init

 Disable all contexts. */

 software only tested with one context */

 Terminator */

 Map I/O memory and request IRQ. */

	/*

	 * The IPMMU has two register banks, for secure and non-secure modes.

	 * The bank mapped at the beginning of the IPMMU address space

	 * corresponds to the running mode of the CPU. When running in secure

	 * mode the non-secure register bank is also available at an offset.

	 *

	 * Secure mode operation isn't clearly documented and is thus currently

	 * not implemented in the driver. Furthermore, preliminary tests of

	 * non-secure operation with the main register bank were not successful.

	 * Offset the registers base unconditionally to point to the non-secure

	 * alias space for now.

	/*

	 * Determine if this IPMMU instance is a root device by checking for

	 * the lack of has_cache_leaf_nodes flag or renesas,ipmmu-main property.

	/*

	 * Wait until the root device has been registered for sure.

 Root devices have mandatory IRQs */

	/*

	 * Register the IPMMU to the IOMMU subsystem in the following cases:

	 * - R-Car Gen2 IPMMU (all devices registered)

	 * - R-Car Gen3 IPMMU (leaf devices only - skip root IPMMU-MM device)

	/*

	 * We can't create the ARM mapping here as it requires the bus to have

	 * an IOMMU, which only happens when bus_set_iommu() is called in

	 * ipmmu_init() after the probe function returns.

 Reset root MMU and restore contexts */

 Re-enable active micro-TLBs */

 CONFIG_PM_SLEEP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Apple DART (Device Address Resolution Table) IOMMU driver

 *

 * Copyright (C) 2021 The Asahi Linux Contributors

 *

 * Based on arm/arm-smmu/arm-ssmu.c and arm/arm-smmu-v3/arm-smmu-v3.c

 *  Copyright (C) 2013 ARM Limited

 *  Copyright (C) 2015 ARM Limited

 * and on exynos-iommu.c

 *  Copyright (c) 2011,2016 Samsung Electronics Co., Ltd.

/*

 * Private structure associated with each DART device.

 *

 * @dev: device struct

 * @regs: mapped MMIO region

 * @irq: interrupt number, can be shared with other DARTs

 * @clks: clocks associated with this DART

 * @num_clks: number of @clks

 * @lock: lock for hardware operations involving this dart

 * @pgsize: pagesize supported by this DART

 * @supports_bypass: indicates if this DART supports bypass mode

 * @force_bypass: force bypass mode due to pagesize mismatch?

 * @sid2group: maps stream ids to iommu_groups

 * @iommu: iommu core device

/*

 * Convenience struct to identify streams.

 *

 * The normal variant is used inside apple_dart_master_cfg which isn't written

 * to concurrently.

 * The atomic variant is used inside apple_dart_domain where we have to guard

 * against races from potential parallel calls to attach/detach_device.

 * Note that even inside the atomic variant the apple_dart pointer is not

 * protected: This pointer is initialized once under the domain init mutex

 * and never changed again afterwards. Devices with different dart pointers

 * cannot be attached to the same domain.

 *

 * @dart dart pointer

 * @sid stream id bitmap

/*

 * This structure is attached to each iommu domain handled by a DART.

 *

 * @pgtbl_ops: pagetable ops allocated by io-pgtable

 * @finalized: true if the domain has been completely initialized

 * @init_lock: protects domain initialization

 * @stream_maps: streams attached to this domain (valid for DMA/UNMANAGED only)

 * @domain: core iommu domain pointer

/*

 * This structure is attached to devices with dev_iommu_priv_set() on of_xlate

 * and contains a list of streams bound to this device.

 * So far the worst case seen is a single device with two streams

 * from different darts, such that this simple static array is enough.

 *

 * @streams: streams for this device

/*

 * Helper macro to iterate over apple_dart_master_cfg.stream_maps and

 * apple_dart_domain.stream_maps

 *

 * @i int used as loop variable

 * @base pointer to base struct (apple_dart_master_cfg or apple_dart_domain)

 * @stream pointer to the apple_dart_streams struct for each loop iteration

 enable all streams globally since TCR is used to control isolation */

 clear any pending errors before the interrupt is unmasked */

 no need to allocate pgtbl_ops or do any other finalization steps */

 Keep things compiling when CONFIG_PCI_APPLE isn't selected */

 Restricted during dart probe */

 there should only be a single bit set but let's use == to be sure */

 SPDX-License-Identifier: GPL-2.0

/*

 * Virtio driver for the paravirtualized IOMMU

 *

 * Copyright (C) 2019 Arm Limited

 Device configuration */

 Supported MAP flags */

 protects viommu pointer */

/*

 * __viommu_sync_req - Complete all in-flight requests

 *

 * Wait for all added requests to complete. When this function returns, all

 * requests that were in-flight at the time of the call have completed.

/*

 * __viommu_add_request - Add one request to the queue

 * @buf: pointer to the request buffer

 * @len: length of the request buffer

 * @writeback: copy data back to the buffer when the request completes.

 *

 * Add a request to the queue. Only synchronize the queue if it's already full.

 * Otherwise don't kick the queue nor wait for requests to complete.

 *

 * When @writeback is true, data written by the device, including the request

 * status, is copied into @buf after the request completes. This is unsafe if

 * the caller allocates @buf on stack and drops the lock between add_req() and

 * sync_req().

 *

 * Return 0 if the request was successfully added to the queue.

 If the queue is full, sync and retry */

/*

 * Send a request and wait for it to complete. Return the request status (as an

 * errno)

 Fall-through (get the actual request status) */

/*

 * viommu_add_mapping - add a mapping to the internal tree

 *

 * On success, return the new mapping. Otherwise return NULL.

/*

 * viommu_del_mappings - remove mappings from the internal tree

 *

 * @vdomain: the domain

 * @iova: start of the range

 * @size: size of the range. A size of 0 corresponds to the entire address

 *	space.

 *

 * On success, returns the number of unmapped bytes (>= size)

 Trying to split a mapping? */

		/*

		 * Virtio-iommu doesn't allow UNMAP to split a mapping created

		 * with a single MAP request, so remove the full mapping.

/*

 * viommu_replay_mappings - re-send MAP requests

 *

 * When reattaching a domain that was previously detached from all endpoints,

 * mappings were deleted from the device. Re-create the mappings available in

 * the internal tree.

 Catch any overflow, including the unlikely end64 - start64 + 1 = 0 */

	/*

	 * For now, assume that properties of an endpoint that outputs multiple

	 * IDs are consistent. Only probe the first one.

 TODO: find EP by ID and report_iommu_fault */

 IOMMU API */

 Free all remaining mappings (size 2^64) */

		/*

		 * Properly initialize the domain now that we know which viommu

		 * owns it.

	/*

	 * In the virtio-iommu device, when attaching the endpoint to a new

	 * domain, it is detached from the old one and, if as as a result the

	 * old domain isn't attached to any endpoint, all mappings are removed

	 * from the old domain and it is freed.

	 *

	 * In the driver the old domain still exists, and its mappings will be

	 * recreated if it gets reattached to an endpoint. Otherwise it will be

	 * freed explicitly.

	 *

	 * vdev->vdomain is protected by group->mutex

		/*

		 * This endpoint is the first to be attached to the domain.

		 * Replay existing mappings (e.g. SW MSI).

 Device already removed all mappings after detach. */

	/*

	 * If the device didn't register any bypass MSI window, add a

	 * software-mapped region.

 Get additional information for this endpoint */

 First clear the DMA ops in case we're switching from a DMA domain */

 No async requests */

 Optional features */

 Populate the event queue with buffers */

 Stop all virtqueues */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 Freescale Semiconductor, Inc.

 define indexes for each operation mapping scenario */

 Base address of PAMU regs */

 The number of PAMUs */

 Has PAMU been probed? */

/*

 * Table for matching compatible strings, for device tree

 * guts node, for QorIQ SOCs.

 * "fsl,qoriq-device-config-2.0" corresponds to T4 & B4

 * SOCs. For the older SOCs "fsl,qoriq-device-config-1.0"

 * string would be used.

/*

 * Table for matching compatible strings, for device tree

 * L3 cache controller node.

 * "fsl,t4240-l3-cache-controller" corresponds to T4,

 * "fsl,b4860-l3-cache-controller" corresponds to B4 &

 * "fsl,p4080-l3-cache-controller" corresponds to other,

 * SOCs.

 maximum subwindows permitted per liodn */

/**

 * pamu_get_ppaace() - Return the primary PACCE

 * @liodn: liodn PAACT index for desired PAACE

 *

 * Returns the ppace pointer upon success else return

 * null.

/**

 * pamu_enable_liodn() - Set valid bit of PACCE

 * @liodn: liodn PAACT index for desired PAACE

 *

 * Returns 0 upon success else error code < 0 returned

 Ensure that all other stores to the ppaace complete first */

/**

 * pamu_disable_liodn() - Clears valid bit of PACCE

 * @liodn: liodn PAACT index for desired PAACE

 *

 * Returns 0 upon success else error code < 0 returned

 Derive the window size encoding for a particular PAACE entry */

 Bug if not a power of 2 */

 window size is 2^(WSE+1) bytes */

/*

 * Set the PAACE type as primary and set the coherency required domain

 * attribute

/*

 * Function used for updating stash destination for the coressponding

 * LIODN.

/**

 * pamu_config_paace() - Sets up PPAACE entry for specified liodn

 *

 * @liodn: Logical IO device number

 * @omi: Operation mapping index -- if ~omi == 0 then omi not defined

 * @stashid: cache stash id for associated cpu -- if ~stashid == 0 then

 *	     stashid not defined

 * @prot: window permissions

 *

 * Returns 0 upon success else error code < 0 returned

 window size is 2^(WSE+1) bytes */

 set up operation mapping if it's configured */

 configure stash id */

/**

 * get_ome_index() - Returns the index in the operation mapping table

 *                   for device.

 * @*omi_index: pointer for storing the index value

 *

/**

 * get_stash_id - Returns stash destination id corresponding to a

 *                cache type and vcpu.

 * @stash_dest_hint: L1, L2 or L3

 * @vcpu: vpcu target for a particular cache type.

 *

 * Returs stash on success or ~(u32)0 on failure.

 *

 Fastpath, exit early if L3/CPC cache is target for stashing */

 find the hwnode that represents the cache */

 can't traverse any further */

 advance to next node in cache hierarchy */

 Identify if the PAACT table entry belongs to QMAN, BMAN or QMAN Portal */

/**

 * Setup operation mapping and stash destinations for QMAN and QMAN portal.

 * Memory accesses to QMAN and BMAN private memory need not be coherent, so

 * clear the PAACE entry coherency attribute for them.

 setup QMAN Private data stashing for the L3 cache */

 Set DQRR and Frame stashing for the L3 cache */

/**

 * Setup the operation mapping table for various devices. This is a static

 * table where each table index corresponds to a particular device. PAMU uses

 * this table to translate device transaction to appropriate corenet

 * transaction.

 Configure OMI_QMAN */

 Configure OMI_FMAN */

 Configure OMI_QMAN private */

 Configure OMI_CAAM */

/*

 * Get the maximum number of PAACT table entries

 * and subwindows supported by PAMU

 Maximum number of subwindows per liodn */

 Setup PAMU registers pointing to PAACT, SPAACT and OMT */

 set up pointers to corenet control blocks */

	/*

	 * set PAMU enable bit,

	 * allow ppaact & omt to be cached

	 * & enable PAMU access violation interrupts.

 Enable all device LIODNS */

 window size is 2^(WSE+1) bytes */

 Assume that POEA points to a PAACE */

 Only the first four words are relevant */

 clear access violation condition */

 check if we got a violation for a disabled LIODN */

				/*

				 * As per hardware erratum A-003638, access

				 * violation can be reported for a disabled

				 * LIODN. If we hit that condition, disable

				 * access violation reporting.

 Disable the LIODN */

 LAWn base address high */

 LAWn base address low */

 LAWn attributes */

/*

 * Create a coherence subdomain for a given memory block.

 Local Access Control registers */

 LAW registers are at offset 0xC00 */

 The undocumented CSDID registers are at offset 0x600 */

 Find an unused coherence subdomain ID */

 Store the Port ID in the (undocumented) proper CIDMRxx register */

 Find the DDR LAW that maps to our buffer. */

 This should never happen */

 Find a free LAW entry */

 No higher priority LAW slots available */

/*

 * Table of SVRs and the corresponding PORT_ID values. Port ID corresponds to a

 * bit map of snoopers for a given range of memory mapped by a LAW.

 *

 * All future CoreNet-enabled SOCs will have this erratum(A-004510) fixed, so this

 * table should never need to be updated.  SVRs are guaranteed to be unique, so

 * there is no worry that a future SOC will inadvertently have one of these

 * values.

 P2040 1.0 */

 P2040 1.1 */

 P2041 1.0 */

 P2041 1.1 */

 P3041 1.0 */

 P3041 1.1 */

 P4040 2.0 */

 P4080 2.0 */

 P5010 1.0 */

 P5010 2.0 */

 P5020 1.0 */

 P5021 1.0 */

 P5040 1.0 */

 The Security (E) bit */

	/*

	 * enumerate all PAMUs and allocate and setup PAMU tables

	 * for each of them,

	 * NOTE : All PAMUs share the same LIODN tables.

 The ISR needs access to the regs, so we won't iounmap them */

 read in the PAMU capability registers */

	/*

	 * To simplify the allocation of a coherency domain, we allocate the

	 * PAACT and the OMT in the same memory buffer.  Unfortunately, this

	 * wastes more memory compared to allocating the buffers separately.

 Determine how much memory we need */

 Make sure the memory is naturally aligned */

 Check to see if we need to implement the work-around on this SOC */

 Determine the Port ID for our coherence subdomain */

 Disable PAMU bypass for this PAMU */

 Enable all relevant PAMU(s) */

 Enable DMA for the LIODNs in the device tree */

	/*

	 * The normal OF process calls the probe function at some

	 * indeterminate later time, after most drivers have loaded.  This is

	 * too late for us, because PAMU clients (like the Qman driver)

	 * depend on PAMU being initialized early.

	 *

	 * So instead, we "manually" call our probe function by creating the

	 * platform devices ourselves.

	/*

	 * We assume that there is only one PAMU node in the device tree.  A

	 * single PAMU node represents all of the PAMU devices in the SOC

	 * already.   Everything else already makes that assumption, and the

	 * binding for the PAMU nodes doesn't allow for any parent-child

	 * relationships anyway.  In other words, support for more than one

	 * PAMU node would require significant changes to a lot of code.

 SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)

 Copyright (C) 2016-2018, Allwinner Technology CO., LTD.

 Copyright (C) 2019-2020, Cerno

 Lock to modify the IOMMU registers */

 Number of devices attached to the domain */

 L1 Page Table */

/*

 * The Allwinner H6 IOMMU uses a 2-level page table.

 *

 * The first level is the usual Directory Table (DT), that consists of

 * 4096 4-bytes Directory Table Entries (DTE), each pointing to a Page

 * Table (PT).

 *

 * Each PT consits of 256 4-bytes Page Table Entries (PTE), each

 * pointing to a 4kB page of physical memory.

 *

 * The IOMMU supports a single DT, pointed by the IOMMU_TTB_REG

 * register that contains its physical address.

/*

 * Each Directory Table Entry has a Page Table address and a valid

 * bit:



 * +---------------------+-----------+-+

 * | PT address          | Reserved  |V|

 * +---------------------+-----------+-+

 *  31:10 - Page Table address

 *   9:2  - Reserved

 *   1:0  - 1 if the entry is valid

/*

 * Each PTE has a Page address, an authority index and a valid bit:

 *

 * +----------------+-----+-----+-----+---+-----+

 * | Page address   | Rsv | ACI | Rsv | V | Rsv |

 * +----------------+-----+-----+-----+---+-----+

 *  31:12 - Page address

 *  11:8  - Reserved

 *   7:4  - Authority Control Index

 *   3:2  - Reserved

 *     1  - 1 if the entry is valid

 *     0  - Reserved

 *

 * The way permissions work is that the IOMMU has 16 "domains" that

 * can be configured to give each masters either read or write

 * permissions through the IOMMU_DM_AUT_CTRL_REG registers. The domain

 * 0 seems like the default domain, and its permissions in the

 * IOMMU_DM_AUT_CTRL_REG are only read-only, so it's not really

 * useful to enforce any particular permission.

 *

 * Each page entry will then have a reference to the domain they are

 * affected to, so that we can actually enforce them on a per-page

 * basis.

 *

 * In order to make it work with the IOMMU framework, we will be using

 * 4 different domains, starting at 1: RD_WR, RD, WR and NONE

 * depending on the permission we want to enforce. Each domain will

 * have each master setup in the same way, since the IOMMU framework

 * doesn't seem to restrict page access on a per-device basis. And

 * then we will use the relevant domain index when generating the page

 * table entry depending on the permissions we want to be enforced.

	/*

	 * At boot, we'll have a first call into .flush_iotlb_all right after

	 * .probe_device, and since we link our (single) domain to our iommu in

	 * the .attach_device callback, we don't have that pointer set.

	 *

	 * It shouldn't really be any trouble to ignore it though since we flush

	 * all caches as part of the device powerup.

 We rely on the physical address and DMA address being the same */

	/*

	 * If the address is not in the page table, we can't get what

	 * operation triggered the fault. Assume it's a read

	 * operation.

		/*

		 * If we are in the read-only domain, then it means we

		 * tried to write.

		/*

		 * If we are in the write-only domain, then it means

		 * we tried to read.

		/*

		 * If we are in the domain without any permission, we

		 * can't really tell. Let's default to a read

		 * operation.

 WTF? */

	/*

	 * If the address is not in the page table, we can't get what

	 * operation triggered the fault. Assume it's a read

	 * operation.

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * perf.c - performance monitor

 *

 * Copyright (C) 2021 Intel Corporation

 *

 * Author: Lu Baolu <baolu.lu@linux.intel.com>

 *         Fenghua Yu <fenghua.yu@intel.com>

 SPDX-License-Identifier: GPL-2.0

 PCI bus number */

 PCI devfn number */

/*

 * Lock ordering:

 * ->dmar_global_lock

 *	->irq_2_ir_lock

 *		->qi->q_lock

 *	->iommu->register_lock

 * Note:

 * intel_irq_remap_ops.{supported,prepare,enable,disable,reenable} are called

 * in single-threaded environment with interrupt disabled, so no need to tabke

 * the dmar_global_lock.

		/*

		 * We use cmpxchg16 to atomically update the 128-bit IRTE,

		 * and it cannot be updated by the hardware or other processors

		 * behind us, so the return value of cmpxchg16 should be the

		 * same as the old value.

 Update iommu mode according to the IRTE mode */

/*

 * source validation type

 no verification is required */

 verify using SID and SQ fields */

 verify bus of request-id */

/*

 * source-id qualifier

 verify all 16 bits of request-id */

#define SQ_13_IGNORE_1	0x1  /* verify most significant 13 bits, ignore

			      * the third least significant bit

#define SQ_13_IGNORE_2	0x2  /* verify most significant 13 bits, ignore

			      * the second and third least significant bits

#define SQ_13_IGNORE_3	0x3  /* verify most significant 13 bits, ignore

			      * the least three significant bits

/*

 * set SVT, SQ and SID fields of irte to verify

 * source ids of interrupt requests

/*

 * Set an IRTE to match only the bus number. Interrupt requests that reference

 * this IRTE must have a requester-id whose bus number is between or equal

 * to the start_bus and end_bus arguments.

	/*

	 * Should really use SQ_ALL_16. Some platforms are broken.

	 * While we figure out the right quirks for these broken platforms, use

	 * SQ_13_IGNORE_3 for now.

	/*

	 * DMA alias provides us with a PCI device and alias.  The only case

	 * where the it will return an alias on a different bus than the

	 * device is the case of a PCIe-to-PCI bridge, where the alias is for

	 * the subordinate bus.  In this case we can only verify the bus.

	 *

	 * If there are multiple aliases, all with the same bus number,

	 * then all we can do is verify the bus. This is typical in NTB

	 * hardware which use proxy IDs where the device will generate traffic

	 * from multiple devfn numbers on the same bus.

	 *

	 * If the alias device is on a different bus than our source device

	 * then we have a topology based alias, use it.

	 *

	 * Otherwise, the alias is for a device DMA quirk and we cannot

	 * assume that MSI uses the same requester ID.  Therefore use the

	 * original device.

 Check whether the old ir-table has the same size as ours */

 Map the old IR table */

 Copy data over */

	/*

	 * Now check the table for used entries and mark those as

	 * allocated in the bitmap

 Set interrupt-remapping table pointer */

	/*

	 * Global invalidation of interrupt entry cache to make sure the

	 * hardware uses the new irq remapping table.

 Enable interrupt-remapping */

 Block compatibility-format MSIs */

	/*

	 * With CFI clear in the Global Command register, we should be

	 * protected from dangerous (i.e. compatibility) interrupts

	 * regardless of x2apic status.  Check just to be sure.

	/*

	 * If the queued invalidation is already initialized,

	 * shouldn't disable it.

		/*

		 * Clear previous faults.

/*

 * Disable Interrupt Remapping.

	/*

	 * global invalidation of interrupt entry cache before disabling

	 * interrupt-remapping.

 First make sure all IOMMUs support IRQ remapping */

 Detect remapping mode: lapic or x2apic */

 Do the initializations early */

/*

 * Set Posted-Interrupts capability.

		/*

		 * If IRTE is in posted format, the 'pda' field goes across the

		 * 64-bit boundary, we need use cmpxchg16b to atomically update

		 * it. We only expose posted-interrupt when X86_FEATURE_CX16

		 * is supported. Actually, hardware platforms supporting PI

		 * should have X86_FEATURE_CX16 support, this has been confirmed

		 * with Intel hardware guys.

	/*

	 * Setup Interrupt-remapping for all the DRHD's now.

		/*

		 * Access PCI directly due to the PCI

		 * subsystem isn't initialized yet.

		/*

		 * Access PCI directly due to the PCI

		 * subsystem isn't initialized yet.

/*

 * Finds the assocaition between IOAPIC's and its Interrupt-remapping

 * hardware unit.

	/*

	 * Disable Interrupt-remapping for all the DRHD's now.

	/*

	 * Clear Posted-Interrupts capability.

	/*

	 * Setup Interrupt-remapping for all the DRHD's now.

 Set up interrupt remapping for iommu.*/

	/*

	 * handle error condition gracefully here!

/*

 * Store the MSI remapping domain pointer in the device if enabled.

 *

 * This is called from dmar_pci_bus_add_dev() so it works even when DMA

 * remapping is disabled. Only update the pointer if the device is not

 * already handled by a non default PCI/MSI interrupt domain. This protects

 * e.g. VMD devices.

	/*

	 * Trigger mode in the IRTE will always be edge, and for IO-APIC, the

	 * actual level or edge trigger will be setup in the IO-APIC

	 * RTE. This will help simplify level triggered irq migration.

	 * For more details, see the comments (in io_apic.c) explainig IO-APIC

	 * irq migration in the presence of interrupt-remapping.

	/*

	 * Atomically updates the IRTE with the new destination, vector

	 * and flushes the interrupt entry cache.

 Update the hardware only if the interrupt is in remapped mode. */

/*

 * Migrate the IO-APIC irq in the presence of intr-remapping.

 *

 * For both level and edge triggered, irq migration is a simple atomic

 * update(of vector and cpu destination) of IRTE and flush the hardware cache.

 *

 * For level triggered, we eliminate the io-apic RTE modification (with the

 * updated vector information), by using a virtual vector (io-apic pin number).

 * Real vector that is used for interrupting cpu will be coming from

 * the interrupt-remapping table entry.

 *

 * As the migration is a simple atomic update of IRTE, the same mechanism

 * is used to migrate MSI irq's in the presence of interrupt-remapping.

	/*

	 * After this point, all the interrupts will start arriving

	 * at the new destination. So, time to cleanup the previous

	 * vector allocation.

 stop posting interrupts, back to remapping mode */

		/*

		 * We are not caching the posted interrupt entry. We

		 * copy the data from the remapped entry and modify

		 * the fields which are relevant for posted mode. The

		 * cached remapped entry is used for switching back to

		 * remapped mode.

 Update the posted mode fields */

 Set source-id of interrupt request */

	/*

	 * With IRQ remapping enabled, don't need contiguous CPU vectors

	 * to support multiple MSI interrupts.

 Initialize the common data */

/*

 * Support of Interrupt Remapping Unit Hotplug

 TODO: check all IOAPICs are covered by IOMMU */

 Setup Interrupt-remapping now. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright © 2006-2014 Intel Corporation.

 *

 * Authors: David Woodhouse <dwmw2@infradead.org>,

 *          Ashok Raj <ashok.raj@intel.com>,

 *          Shaohua Li <shaohua.li@intel.com>,

 *          Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>,

 *          Fenghua Yu <fenghua.yu@intel.com>

 *          Joerg Roedel <jroedel@suse.de>

/* We limit DOMAIN_MAX_PFN to fit in an unsigned long, and DOMAIN_MAX_ADDR

 IO virtual address start page frame number */

 page table handling */

/* VT-d pages must always be _smaller_ than MM pages. Otherwise things

 global iommu list, set NULL for ignored DMAR units */

/*

 * set to 1 to panic kernel if can't successfully enable VT-d

 * (used when kernel is launched w/ TXT)

/*

 * Take a root_entry and return the Lower Context Table Pointer (LCTP)

 * if marked present.

/*

 * Take a root_entry and return the Upper Context Table Pointer (UCTP)

 * if marked present.

/*

 * This domain is a statically identity mapping domain.

 *	1. This domain creats a static 1:1 mapping to all usable memory.

 * 	2. It maps to each iommu if successful.

 *	3. Each iommu mapps to this domain if successful.

 list of rmrr units	*/

 ACPI header		*/

 reserved base address*/

 reserved end address */

 target devices */

 target device count */

 list of ATSR units */

 ACPI header */

 target devices */

 target device count */

 include all ports */

 list of SATC units */

 ACPI header */

 target devices */

 the corresponding iommu */

 target device count */

 ATS is required */

 bitmap for indexing intel_iommus */

/*

 * Iterate over elements in device_domain_list and call the specified

 * callback @fn against each element.

/*

 * Calculate max SAGAW for each iommu.

/*

 * calculate agaw for each iommu.

 * "SAGAW" may be different across iommus, use a default agaw, and

 * get a supported less agaw for iommus that don't support the default agaw.

 This functionin only returns single iommu in a domain */

 si_domain and vm domain should not get here. */

 No hardware attached; use lowest common denominator */

			/*

			 * If the hardware is operating in the scalable mode,

			 * the snooping control is always supported since we

			 * always set PASID-table-entry.PGSNP bit if the domain

			 * is managed outside (UNMANAGED).

 set iommu_superpage to the smallest common denominator */

		/*

		 * There could possibly be multiple device numa nodes as devices

		 * within the same domain may sit behind different IOMMUs. There

		 * isn't perfect answer in such situation, so we select first

		 * come first served policy.

 Return the super pagesize bitmap if supported. */

	/*

	 * 1-level super page supports page size of 2MiB, 2-level super page

	 * supports page size of both 2MiB and 1GiB.

 Some capabilities may be different across iommus */

	/*

	 * If RHSA is missing, we should default to the device numa domain

	 * as fall back.

	/*

	 * First-level translation restricts the input-address to a

	 * canonical address (i.e., address bits 63:N have the same

	 * value as address bit [N-1], where N is 48-bits with 4-level

	 * paging and 57-bits with 5-level paging). Hence, skip bit

	 * [N-1].

/**

 * is_downstream_to_pci_bridge - test if a device belongs to the PCI

 *				 sub-hierarchy of a candidate PCI-PCI bridge

 * @dev: candidate PCI device belonging to @bridge PCI sub-hierarchy

 * @bridge: the candidate PCI-PCI bridge

 *

 * Return: true if @dev belongs to @bridge PCI sub-hierarchy, else false.

	/* We know that this device on this chipset has its own IOMMU.

	 * If we find it under a different IOMMU, then the BIOS is lying

	 * to us. Hope that the IOMMU for this device is actually

	 * disabled, and it needs no translation...

 "can't" happen */

 we know that the this iommu should be at offset 0xa000 from vtbar */

		/* VFs aren't listed in scope tables; we need to look up

				/* For a VF use its original BDF# not that of the PF

				 * which we used for the IOMMU lookup. Strictly speaking

				 * we could do this for all PCI devices; we only need to

 root entry dump */

 context entry dump */

 legacy mode does not require PASID entries */

 get the pointer to pasid directory entry */

 For request-without-pasid, get the pasid from context entry */

 get the pointer to the pasid table entry */

 Address beyond IOMMU's addressing capabilities. */

 Someone else set it while we were thinking; use theirs. */

 return address's pte at specific level */

 clear last level pte, a tlb flush should be followed */

 we don't need lock here; nobody else touches the iova range */

		/*

		 * Free the page table if we're below the level we want to

		 * retain and the range covers the entire table.

/*

 * clear last level (leaf) ptes and free page table pages below the

 * level we wish to keep intact.

 We don't need lock here; nobody else touches the iova range */

 free pgd */

/* When a page at a given level is being unlinked from its parent, we don't

   need to *modify* it at all. All we need to do is make a list of all the

   pages which can be freed just as soon as we've flushed the IOTLB and we

   know the hardware page-walk will no longer touch them.

   The 'pte' argument is the *parent* PTE, pointing to the page that is to

 If range covers entire pagetable, free it */

			/* These suborbinate page tables are going away entirely. Don't

 Recurse down into a level that isn't *entirely* obsolete */

/* We can't just free the pages because the IOMMU may still be walking

   the page tables, and may have cached the intermediate levels. The

 we don't need lock here; nobody else touches the iova range */

 free pgd */

 iommu handling */

 Make sure hardware complete it */

 Make sure hardware complete it */

 return value determine if we need a write buffer flush */

 Make sure hardware complete it */

 return value determine if we need a write buffer flush */

 global flush doesn't need set IVA_REG */

 IH bit is passed in as part of address */

 Note: set drain read/write */

	/*

	 * This is probably to be super secure.. Looks like we can

	 * ignore it without any impact.

 Note: Only uses first TLB reg currently */

 Make sure hardware complete it */

 check IOTLB invalidation granularity */

	/* For IOMMU that supports device IOTLB throttling (DIT), we assign

	 * PFSID to the invalidation desc of a VF such that IOMMU HW can gauge

	 * queue depth at PF level. If DIT is not set, PFSID will be treated as

	 * reserved, which should be set to 0.

 pdev will be returned if device is not a vf */

	/* The PCIe spec, in its wisdom, declares that the behaviour of

	   the device if you enable PASID support after ATS support is

	   undefined. So always enable PASID support on devices which

	   have it, even if we can't yet know if we're ever going to

		/*

		 * Fallback to domain selective flush if no PSI support or

		 * the size is too big. PSI requires page size to be 2 ^ x,

		 * and the base address is naturally aligned to the size.

	/*

	 * In caching mode, changes of pages from non-present to present require

	 * flush. However, device IOTLB doesn't need to be flushed in this case.

 Notification for newly created mappings */

	/*

	 * It's a non-present to present mapping. Only flush if caching mode

	 * and second level.

 wait for the protected region status bit to clear */

 Make sure hardware complete it */

 Make sure hardware complete it */

	/*

	 * If Caching mode is set, then invalid translations are tagged

	 * with domain-id 0, hence we need to pre-allocate it. We also

	 * use domain-id 0 as a marker for non-allocated domain-id, so

	 * make sure it is not used for a real domain.

	/*

	 * Vt-d spec rev3.0 (section 6.2.3.1) requires that each pasid

	 * entry for first-level or pass-through translation modes should

	 * be programmed with a domain id different from those used for

	 * second-level or nested translation. We reserve a domain id for

	 * this purpose.

 free context mapping */

/*

 * Check and return whether first level is used by default for

 * DMA translation.

 Only SL is available in legacy mode */

 Only level (either FL or SL) is available, just use it */

 Both levels are available, decide it based on domain type */

 Must be called with iommu->lock */

 Remove associated devices and clear attached or cached domains */

/*

 * Get the PASID directory size for scalable mode context entry.

 * Value of X in the PDTS field of a scalable mode context entry

 * indicates PASID directory with 2^(X + 7) entries.

/*

 * Set the RID_PASID field of a scalable mode context entry. The

 * IOMMU hardware will use the PASID value set in this field for

 * DMA translations of DMA requests without PASID.

/*

 * Set the DTE(Device-TLB Enable) field of a scalable mode context

 * entry.

/*

 * Set the PRE(Page Request Enable) field of a scalable mode context

 * entry.

 Convert value to context PASID directory size field coding. */

	/*

	 * For kdump cases, old valid entries may be cached due to the

	 * in-flight DMA and copied pgtable, but there is no unmapping

	 * behaviour for them, thus we need an explicit cache flush for

	 * the newly-mapped device. For kdump, at this point, the device

	 * is supposed to finish reset at its driver probe stage, so no

	 * in-flight DMA will exist, and we don't need to worry anymore

	 * hereafter.

 Setup the PASID DIR pointer: */

 Setup the RID_PASID field: */

		/*

		 * Setup the Device-TLB enable bit and Page request

		 * Enable bit:

			/*

			 * Skip top levels of page tables for iommu which has

			 * less agaw than default. Unnecessary for PT mode.

			/*

			 * In pass through mode, AW must be programmed to

			 * indicate the largest AGAW value supported by

			 * hardware. And ASR is ignored by hardware.

	/*

	 * It's a non-present to present mapping. If hardware doesn't cache

	 * non-present entry we only need to flush the write-buffer. If the

	 * _does_ cache non-present entries, then it does so in the special

	 * domain #0, which we have to flush:

 Returns a number of VTD pages, but aligned to MM page size */

 Return largest possible superpage level for a given mapping */

	/* To use a large page, the virtual *and* physical addresses

	   must be aligned to 2MiB/1GiB/etc. Lower bits set in either

	   of them will mean we have to use smaller pages. So just

/*

 * Ensure that old small page tables are removed to make room for superpage(s).

 * We're going to add new large pages, so make sure we don't remove their parent

 * tables. The IOTLB/devTLBs should be flushed if any PDE/PTEs are cleared.

 It is large page*/

		/* We don't need lock here, nobody else

		 * touches the iova range

		/* If the next PTE would be the first in a new page, then we

		 * need to flush the cache on the entries we've just written.

		 * And then we'll need to recalculate 'pte', so clear it and

		 * let it get set again in the if (!pte) block above.

		 *

		 * If we're done (!nr_pages) we need to flush the cache too.

		 *

		 * Also if we've been setting superpages, we may need to

		 * recalculate 'pte' and switch back to smaller pages for the

		 * end of the mapping, if the trailing size is not enough to

		 * use another superpage (i.e. nr_pages < lvl_pages).

 No lock here, assumes no domain exit in normal case */

	/*

	 * Skip top levels of page tables for iommu which has

	 * less agaw than default. Unnecessary for PT mode.

 Caller must free the original domain */

 PASID table is mandatory for a PCI device in scalable mode. */

 Setup the PASID entry for requests without PASID: */

	/*

	 * RMRR range might have overlap with physical memory range,

	 * clear it first

	/*

	 * Identity map the RMRRs so that devices with RMRRs could also use

	 * the si_domain.

		/*

		 * Return TRUE if this RMRR contains the device that

		 * is passed in.

/**

 * device_rmrr_is_relaxable - Test whether the RMRR of this device

 * is relaxable (ie. is allowed to be not enforced under some conditions)

 * @dev: device handle

 *

 * We assume that PCI USB devices with RMRRs have them largely

 * for historical reasons and that the RMRR space is not actively used post

 * boot.  This exclusion may change if vendors begin to abuse it.

 *

 * The same exception is made for graphics devices, with the requirement that

 * any use of the RMRR regions will be torn down before assigning the device

 * to a guest.

 *

 * Return: true if the RMRR is relaxable, false otherwise

/*

 * There are a couple cases where we need to restrict the functionality of

 * devices associated with RMRRs.  The first is when evaluating a device for

 * identity mapping because problems exist when devices are moved in and out

 * of domains and their respective RMRR information is lost.  This means that

 * a device with associated RMRRs will never be in a "passthrough" domain.

 * The second is use of the device through the IOMMU API.  This interface

 * expects to have full control of the IOVA space for the device.  We cannot

 * satisfy both the requirement that RMRR access is maintained and have an

 * unencumbered IOVA space.  We also have no ability to quiesce the device's

 * use of the RMRR space or even inform the IOMMU API user of the restriction.

 * We therefore prevent devices associated with an RMRR from participating in

 * the IOMMU API, which eliminates them from device assignment.

 *

 * In both cases, devices which have relaxable RMRRs are not concerned by this

 * restriction. See device_rmrr_is_relaxable comment.

/*

 * Return the required default domain type for a specific device.

 *

 * @dev: the device in query

 * @startup: true if this is during early boot

 *

 * Returns:

 *  - IOMMU_DOMAIN_DMA: device requires a dynamic mapping domain

 *  - IOMMU_DOMAIN_IDENTITY: device requires an identical mapping domain

 *  - 0: both identity and dynamic domains work for this device

	/*

	 * Start from the sane iommu hardware state.

	 * If the queued invalidation is already initialized by us

	 * (for example, while enabling interrupt-remapping) then

	 * we got the things already rolling from a sane state.

		/*

		 * Clear any previous faults.

		/*

		 * Disable queued invalidation if supported and already enabled

		 * before OS handover.

		/*

		 * Queued Invalidate not enabled, use Register Based Invalidate

 First calculate the correct index */

 First save what we may have and clean up */

 No LCTP, try UCTP */

 Now copy the context entry */

		/*

		 * We need a marker for copied context entries. This

		 * marker needs to work for the old format as well as

		 * for extended context entries.

		 *

		 * Bit 67 of the context entry is used. In the old

		 * format this bit is available to software, in the

		 * extended format it is the PGE bit, but PGE is ignored

		 * by HW if PASIDs are disabled (and thus still

		 * available).

		 *

		 * So disable PASIDs first and then mark the entry

		 * copied. This means that we don't copy PASID

		 * translations from the old kernel, but this is fine as

		 * faults there are not fatal.

	/*

	 * The RTT bit can only be changed when translation is disabled,

	 * but disabling translation means to open a window for data

	 * corruption. So bail out and don't copy anything if we would

	 * have to change the bit.

 This is too big for the stack - allocate it from slab */

 Context tables are copied, now write them to the root_entry table */

	/*

	 * VT-d virtual command interface always uses the full 20 bit

	 * PASID range. Host can partition guest PASID range based on

	 * policies but it is out of guest's control.

	/*

	 * Sanity check the ioasid owner is done at upper layer, e.g. VFIO

	 * We can only free the PASID when all the devices are unbound.

	/*

	 * If we are running in the host, no need for custom allocator

	 * in that PASIDs are allocated from the host system-wide.

	/*

	 * Register a custom PASID allocator if we are running in a guest,

	 * guest PASID must be obtained via virtual command interface.

	 * There can be multiple vIOMMUs in each guest but only one allocator

	 * is active. All vIOMMU allocators will eventually be calling the same

	 * host allocator.

		/*

		 * Disable scalable mode on this IOMMU if there

		 * is no custom allocator. Mixing SM capable vIOMMU

		 * and non-SM vIOMMU are not supported.

	/*

	 * for each drhd

	 *    allocate root

	 *    initialize and program root entry to not present

	 * endfor

		/*

		 * lock not needed as this is only incremented in the single

		 * threaded kernel __init code path all other access are read

		 * only

 Preallocate enough resources for IOMMU hot-addition */

		/*

		 * Find the max pasid size of all IOMMU's in the system.

		 * We need to ensure the system pasid table is no bigger

		 * than the smallest supported.

		/*

		 * TBD:

		 * we could share the same root & context tables

		 * among all IOMMU's. Need to Split it later.

				/*

				 * We found the IOMMU with translation

				 * enabled - but failed to copy over the

				 * old root-entry table. Try to proceed

				 * by disabling translation now and

				 * allocating a clean root-entry table.

				 * This might cause DMAR faults, but

				 * probably the dump will still succeed.

	/*

	 * Now that qi is enabled on all iommus, set the root entry and flush

	 * caches. This is required on some Intel X58 chipsets, otherwise the

	 * flush_context function will loop forever and the boot hangs.

	/*

	 * for each drhd

	 *   enable fault log

	 *   global invalidate context cache

	 *   global invalidate iotlb

	 *   enable translation

			/*

			 * we always have to disable PMRs or DMA may fail on

			 * this device

			/*

			 * Call dmar_alloc_hwirq() with dmar_global_lock held,

			 * could cause possible lock race condition.

 ignore DMAR unit if no devices exist */

		/* This IOMMU has *only* gfx devices. Either bypass it or

			/*

			 * we always have to disable PMRs or DMA may fail on

			 * this device

 CONFIG_PM */

	/*

	 * If memory is allocated from slab by ACPI _DSM method, we need to

	 * copy the memory content because the memory buffer will be freed

	 * on return.

	/*

	 * Disable translation if already enabled prior to OS handover.

		/*

		 * we always have to disable PMRs or DMA may fail on this device

 If it's an integrated device, allow ATS */

 Connected via non-PCIe: no ATS */

 If we found the root port, look it up in the ATSR */

 Disable PMRs explicitly here. */

 Make sure the IOMMUs are switched off */

	/*

	 * If Intel-IOMMU is disabled by default, we will apply identity

	 * map for all devices except those marked as being untrusted.

 To avoid a -Wunused-but-set-variable warning. */

	/*

	 * Intel IOMMU is required for a TXT/tboot launch or platform

	 * opt in, so enforce that.

	/*

	 * The bus notifier takes the dmar_global_lock, so lockdep will

	 * complain later when we register it under the lock.

		/*

		 * We exit the function here to ensure IOMMU's remapping and

		 * mempool aren't setup, which means that the IOMMU's PMRs

		 * won't be disabled via the call to init_dmars(). So disable

		 * it explicitly here. The PMRs were setup by tboot prior to

		 * calling SENTER, but the kernel is expected to reset/tear

		 * down the PMRs.

		/*

		 * Make sure the IOMMUs are switched off, even when we

		 * boot into a kexec kernel and the previous kernel left

		 * them enabled

		/*

		 * The flush queue implementation does not perform

		 * page-selective invalidations that are required for efficient

		 * TLB flushes in virtual environments.  The benefit of batching

		 * is likely to be much lower than the overhead of synchronizing

		 * the virtual and physical IOMMU page-tables.

 Finally, we enable the DMA remapping hardware. */

/*

 * NB - intel-iommu lacks any sort of reference counting for the users of

 * dependent devices.  If multiple endpoints have intersecting dependent

 * devices, unbinding the driver from any one of them will possibly leave

 * the others unable to operate.

 calculate AGAW */

 always allocate the top pgd */

/*

 * Check whether a @domain could be attached to the @dev through the

 * aux-domain attach/detach APIs.

 No private data needed for the default pasid */

	/*

	 * Subdevices from the same physical device can be attached to the

	 * same domain. For such cases, only the first subdevice attachment

	 * needs to go through the full steps in this function. So if ret >

	 * 1, just goto out.

	/*

	 * iommu->lock must be held to attach domain to iommu and setup the

	 * pasid entry for second level translation.

 Setup the PASID entry for mediated devices: */

 check if this iommu agaw is sufficient for max mapped address */

	/*

	 * Knock out extra levels of page tables if necessary

 normally dev is not mapped */

/*

 * 2D array for converting and sanitizing IOMMU generic TLB granularity to

 * VT-d granularity. Invalidation is typically included in the unmap operation

 * as a result of DMA or VFIO unmap. However, for assigned devices guest

 * owns the first level page tables. Invalidations of translation caches in the

 * guest are trapped and passed down to the host.

 *

 * vIOMMU in the guest will only expose first level page tables, therefore

 * we do not support IOTLB granularity for request without PASID (second level).

 *

 * For example, to find the VT-d granularity encoding for IOTLB

 * type and page selective granularity within PASID:

 * X: indexed by iommu cache type

 * Y: indexed by enum iommu_inv_granularity

 * [IOMMU_CACHE_INV_TYPE_IOTLB][IOMMU_INV_GRANU_ADDR]

	/*

	 * PASID based IOTLB invalidation: PASID selective (per PASID),

	 * page selective (address granularity)

 PASID based dev TLBs */

 PASID cache */

	/* VT-d size is encoded as 2^size of 4K pages, 0 for 4k, 9 for 2MB, etc.

	 * IOMMU cache invalidate API passes granu_size in bytes, and number of

	 * granu size in contiguous memory.

 Size is only valid in address selective invalidation */

		/*

		 * PASID is stored in different locations based on the

		 * granularity.

 HW will ignore LSB bits based on address mask */

			/*

			 * If granu is PASID-selective, address is ignored.

			 * We use npages = -1 to indicate that.

			/*

			 * Always flush device IOTLB if ATS is enabled. vIOMMU

			 * in the guest may assume IOTLB flush is inclusive,

			 * which is more efficient.

			/*

			 * PASID based device TLB invalidation does not support

			 * IOMMU_INV_GRANU_PASID granularity but only supports

			 * IOMMU_INV_GRANU_ADDR.

			 * The equivalent of that is we set the size to be the

			 * entire range of 64 bit. User only provides PASID info

			 * without address info. So we set addr to 0.

 check if minimum agaw is sufficient for mapped address */

	/* Round up size to next multiple of PAGE_SIZE, if it and

	/* Cope with horrid API which requires us to unmap more than the

 CONFIG_INTEL_IOMMU_FLOPPY_WA */

 Enable PASID support in the device, if it wasn't already */

/*

 * Check that the device does not live on an external facing PCI port that is

 * marked as untrusted. Such devices should not be able to apply quirks and

 * thus not be able to bypass the IOMMU restrictions.

 G4x/GM45 integrated gfx dmar support is totally busted. */

 Broadwell igfx malfunctions with dmar */

	/*

	 * Mobile 4 Series Chipset neglects to set RWBF capability,

	 * but needs it. Same seems to hold for the desktop versions.

 we have to ensure the gfx device is idle before we flush */

/* On Tylersburg chipsets, some BIOSes have been known to enable the

   ISOCH DMAR unit for the Azalia sound device, but not give it any

   TLB entries, which causes it to deadlock. Check for that.  We do

   this in a function called from init_dmars(), instead of in a PCI

   quirk, because we don't want to print the obnoxious "BIOS broken"

   message if VT-d is actually disabled.

 If there's no Azalia in the system anyway, forget it. */

	/* System Management Registers. Might be hidden, in which case

	   we can't do the sanity check. But that's OK, because the

 If Azalia DMA is routed to the non-isoch DMAR unit, fine. */

 Drop all bits other than the number of TLB entries */

 If we have the recommended number of TLB entries (16), fine. */

 Zero TLB entries? You get to ride the short bus to school. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright © 2015 Intel Corporation.

 *

 * Authors: David Woodhouse <dwmw2@infradead.org>

 Pages have been freed at this point */

	/* This might end up being called from exit_mmap(), *before* the page

	 * tables are cleared. And __mmu_notifier_release() will delete us from

	 * the list of notifiers so that our invalidate_range() callback doesn't

	 * get called when the page tables are cleared. So we need to protect

	 * against hardware accessing those page tables.

	 *

	 * We do it by clearing the entry in the PASID table and then flushing

	 * the IOTLB and the PASID table caches. This might upset hardware;

	 * perhaps we'll want to point the PASID to a dummy PGD (like the zero

	 * page) so that we end up taking a fault that the hardware really

	 * *has* to handle gracefully without affecting other processes.

 The caller should hold the pasid_mutex lock */

	/*

	 * If we found svm for the PASID, there must be at least one device

	 * bond.

 IOMMU core ensures argsz is more than the start of the union */

 Make sure no undefined flags are used in vendor data */

 VT-d supports devices with full 20 bit PASIDs only */

	/*

	 * We only check host PASID range, we have no knowledge to check

	 * guest PASID range.

		/*

		 * Do not allow multiple bindings of the same device-PASID since

		 * there is only one SL page tables per PASID. We may revisit

		 * once sharing PGD across domains are supported.

 We come here when PASID has never been bond to a device. */

		/* REVISIT: upper layer/VFIO can track host process that bind

		 * the PASID. ioasid_set = mm might be sufficient for vfio to

		 * check pasid VMM ownership. We can drop the following line

		 * once VFIO and IOASID set check is in place.

 Only count users if device has aux domains */

 Set up device context entry for PASID if not enabled already */

	/*

	 * PASID table is per device for better security. Therefore, for

	 * each bind of a new device even with an existing PASID, we need to

	 * call the nested mode setup function here.

		/*

		 * PASID entry should be in cleared state if nested mode

		 * set up failed. So we only need to clear IOASID tracking

		 * data such that free call will succeed.

				/*

				 * We do not free the IOASID here in that

				 * IOMMU driver did not allocate it.

				 * Unlike native SVM, IOASID for guest use was

				 * allocated prior to the bind call.

				 * In any case, if the free call comes before

				 * the unbind, IOMMU driver will get notified

				 * and perform cleanup.

 Find the matching device in svm list */

 Setup the pasid table: */

 Caller must hold pasid_mutex */

			/* Flush the PASID cache and IOTLB for this device.

			 * Note that we do depend on the hardware *not* using

			 * the PASID any more. Just as we depend on other

			 * devices never using PASIDs that they have no right

			 * to use. We have a *shared* PASID table, because it's

			 * large and has to be physically contiguous. So it's

				/* We mandate that no page faults may be outstanding

				 * for the PASID when intel_svm_unbind_mm() is called.

				 * If that is not obeyed, subtle errors will happen.

 Drop a PASID reference and free it if no reference. */

 Page request queue descriptor */

/**

 * intel_svm_drain_prq - Drain page requests and responses for a pasid

 * @dev: target device

 * @pasid: pasid for draining

 *

 * Drain all pending page requests and responses related to @pasid in both

 * software and hardware. This is supposed to be called after the device

 * driver has stopped DMA, the pasid entry has been cleared, and both IOTLB

 * and DevTLB have been invalidated.

 *

 * It waits until all pending page requests for @pasid in the page fault

 * queue are completed by the prq handling thread. Then follow the steps

 * described in VT-d spec CH7.10 to drain all page requests and page

 * responses pending in the hardware.

	/*

	 * Check and wait until all pending page requests in the queue are

	 * handled by the prq handling thread.

	/*

	 * A work in IO page fault workqueue may try to lock pasid_mutex now.

	 * Holding pasid_mutex while waiting in iopf_queue_flush_dev() for

	 * all works in the workqueue to finish may cause deadlock.

	 *

	 * It's unnecessary to hold pasid_mutex in iopf_queue_flush_dev().

	 * Unlock it to allow the works to be handled while waiting for

	 * them to finish.

	/*

	 * Perform steps described in VT-d spec CH7.10 to drain page

	 * requests and responses in hardware.

 Fill in event data for device specific processing */

		/*

		 * Set last page in group bit if private data is present,

		 * page response is required as it does for LPIG.

		 * iommu_report_device_fault() doesn't understand this vendor

		 * specific requirement thus we set last_page as a workaround.

		/*

		 * If the private data fields are not used by hardware, use it

		 * to monitor the prq handle latency.

	/*

	 * Per VT-d spec. v3.0 ch7.7, system software must

	 * respond with page group response if private data

	 * is present (PDP) or last page in group (LPIG) bit

	 * is set. This is an additional VT-d feature beyond

	 * PCI ATS spec.

	/*

	 * Clear PPR bit before reading head/tail registers, to ensure that

	 * we get a new interrupt if needed.

			/*

			 * It can't go away, because the driver is not permitted

			 * to unbind the mm while any page faults are outstanding.

		/*

		 * If prq is to be handled outside iommu driver via receiver of

		 * the fault notifiers, we skip the page response here.

	/*

	 * Clear the page request overflow bit and wake up all threads that

	 * are waiting for the completion of this handling.

	/*

	 * For responses from userspace, need to make sure that the

	 * pasid has been bound to its mm.

	/*

	 * Per VT-d spec. v3.0 ch7.7, system software must respond

	 * with page group response if private data is present (PDP)

	 * or last page in group (LPIG) bit is set. This is an

	 * additional VT-d requirement beyond PCI ATS spec.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright © 2018 Intel Corporation.

 *

 * Authors: Gayatri Kammela <gayatri.kammela@intel.com>

 *	    Sohil Mehta <sohil.mehta@intel.com>

 *	    Jacob Pan <jacob.jun.pan@linux.intel.com>

 *	    Lu Baolu <baolu.lu@linux.intel.com>

		/*

		 * Publish the contents of the 64-bit hardware registers

		 * by adding the offset to the pointer (virtual address).

	/*

	 * A legacy mode DMAR doesn't support PASID, hence default it to -1

	 * indicating that it's invalid. Also, default all PASID related fields

	 * to 0.

		/*

		 * Scalable mode root entry points to upper scalable mode

		 * context table and lower scalable mode context table. Each

		 * scalable mode context table has 128 context entries where as

		 * legacy mode context table has 256 context entries. So in

		 * scalable mode, the context entries for former 128 devices are

		 * in the lower scalable mode context table, while the latter

		 * 128 devices are in the upper scalable mode context table.

		 * In scalable mode, when devfn > 127, iommu_context_addr()

		 * automatically refers to upper scalable mode context table and

		 * hence the caller doesn't have to worry about differences

		 * between scalable mode and non scalable mode.

	/*

	 * No need to check if the root entry is present or not because

	 * iommu_context_addr() performs the same check before returning

	 * context entry.

/*

 * For active IOMMUs go through the Interrupt remapping

 * table and print valid entries in a table format for

 * Remapped and Posted Interrupts.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2006, Intel Corporation.

 *

 * Copyright (C) 2006-2008 Intel Corporation

 * Author: Ashok Raj <ashok.raj@intel.com>

 * Author: Shaohua Li <shaohua.li@intel.com>

 * Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>

 *

 * This file implements early detection/parsing of Remapping Devices

 * reported to OS through BIOS via DMA remapping reporting (DMAR) ACPI

 * tables.

 *

 * These routines are used by both DMA-remapping and Interrupt-remapping

/*

 * Assumptions:

 * 1) The hotplug framework guarentees that DMAR unit will be hot-added

 *    before IO devices managed by that unit.

 * 2) The hotplug framework guarantees that DMAR unit will be hot-removed

 *    after IO devices managed by that unit.

 * 3) Hotplug events are rare.

 *

 * Locking rules for DMA and interrupt remapping related global data structures:

 * 1) Use dmar_global_lock in process context

 * 2) Use RCU in interrupt context

	/*

	 * add INCLUDE_ALL at the tail, so scan the list will find it at

	 * the very end.

 Optimize out kzalloc()/kfree() for normal cases */

	/*

	 * Ignore devices that have a domain number higher than what can

	 * be looked up in DMAR, e.g. VMD subdevices with domain 0x10000

 Only generate path[] for device addition event */

 Return: > 0 if match found, 0 if no match found, < 0 if error happens */

		/*

		 * We expect devices with endpoint scope to have normal PCI

		 * headers, and devices with bridge scope to have bridge PCI

		 * headers.  However PCI NTB devices may be listed in the

		 * DMAR table with bridge scope, even though they have a

		 * normal PCI header.  NTB devices are identified by class

		 * "BRIDGE_OTHER" (0680h) - we don't declare a socpe mismatch

		 * for this special case.

	/* Only care about add/remove events for physical functions.

	 * For VFs we actually do the lookup based on the corresponding

		/*

		 * Ensure that the VF device inherits the irq domain of the

		 * PF device. Ideally the device would inherit the domain

		 * from the bus, but DMAR can have multiple units per bus

		 * which makes this impossible. The VF 'bus' could inherit

		 * from the PF device, but that's yet another x86'sism to

		 * inflict on everybody else.

/*

 * dmar_parse_one_drhd - parses exactly one DMA remapping hardware definition

 * structure which uniquely represent one DMA remapping hardware unit

 * present in the platform

	/*

	 * If header is allocated from slab by ACPI _DSM method, we need to

	 * copy the content because the memory buffer will be freed on return.

 BIT0: INCLUDE_ALL */

 Check for NUL termination within the designated length */

		/* We don't print this here because we need to sanity-check

/**

 * dmar_table_detect - checks to see if the platform supports DMAR devices

 if we could find DMAR table, then there are DMAR devices */

 Avoid looping forever on bad ACPI tables */

 Avoid passing table end */

 continue for forward compatibility */

/**

 * parse_dmar_table - parses the DMA reporting table

	/*

	 * Do it again, earlier dmar_tbl mapping could be mapped with

	 * fixed map.

	/*

	 * ACPI tables may not be DMA protected by tboot, so use DMAR copy

	 * SINIT saved in SinitMleData in TXT heap (which is DMA protected)

 Check our parent */

 Make sure ACS will be enabled */

/**

 * map_iommu: map the iommu's registers

 * @iommu: the iommu to map

 * @phys_addr: the physical address of the base resgister

 *

 * Memory map the iommu's registers.  Start w/ a single page, and

 * possibly expand if that turns out to be insufficent.

 the registers might be more than one page */

 Reflect status in gcmd */

	/*

	 * This is only for hotplug; at boot time intel_iommu_enabled won't

	 * be set yet. When intel_iommu_init() runs, it registers the units

	 * present at boot time, then sets intel_iommu_enabled.

/*

 * Reclaim all the submitted descriptors which have completed its work.

	/*

	 * If IQE happens, the head points to the descriptor associated

	 * with the error. No new descriptors are fetched until the IQE

	 * is cleared.

			/*

			 * desc->qw2 and desc->qw3 are either reserved or

			 * used by software as private data. We won't print

			 * out these two qw's for security consideration.

	/*

	 * If ITE happens, all pending wait_desc commands are aborted.

	 * No new descriptors are fetched until the ITE is cleared.

/*

 * Function to submit invalidation descriptors of all types to the queued

 * invalidation interface(QI). Multiple descriptors can be submitted at a

 * time, a wait descriptor will be appended to each submission to ensure

 * hardware has completed the invalidation before return. Wait descriptors

 * can be part of the submission but it will not be polled for completion.

	/*

	 * Check if we have enough empty slots in the queue to submit,

	 * the calculation is based on:

	 * # of desc + 1 wait desc + 1 space between head and tail

	/*

	 * update the HW tail register indicating the presence of

	 * new descriptors.

		/*

		 * We will leave the interrupts disabled, to prevent interrupt

		 * context to queue another cmd while a cmd is already submitted

		 * and waiting for completion on this cpu. This is to avoid

		 * a deadlock where the interrupt context can wait indefinitely

		 * for free slots in the queue.

/*

 * Flush the global interrupt entry cache.

 should never fail */

 PASID-based IOTLB invalidation */

	/*

	 * npages == -1 means a PASID-selective invalidation, otherwise,

	 * a positive value for Page-selective-within-PASID invalidation.

	 * 0 is not a valid input.

 PASID-based device IOTLB Invalidate */

	/*

	 * If S bit is 0, we only flush a single page. If S bit is set,

	 * The least significant zero bit indicates the invalidation address

	 * range. VT-d spec 6.5.2.6.

	 * e.g. address bit 12[0] indicates 8KB, 13[0] indicates 16KB.

	 * size order = 0 is PAGE_SIZE 4KB

	 * Max Invs Pending (MIP) is set to 0 for now until we have DIT in

	 * ECAP.

 Take page address */

		/*

		 * Existing 0s in address below size_order may be the least

		 * significant bit, we must set them to 1s to avoid having

		 * smaller size than desired.

 Clear size_order bit to indicate size */

 Set the S bit to indicate flushing more than 1 page */

/*

 * Disable Queued Invalidation interface.

	/*

	 * Give a chance to HW to complete the pending invalidation requests.

/*

 * Enable queued invalidation.

	/*

	 * Set DW=1 and QS=1 in IQA_REG when Scalable Mode capability

	 * is present.

 write zero to the tail reg */

 Make sure hardware complete it */

/*

 * Enable Queued Invalidation interface. This is a must to support

 * interrupt-remapping. Also used by DMA-remapping, which replaces

 * register based IOTLB invalidation.

	/*

	 * queued invalidation is already setup and enabled.

	/*

	 * Need two pages to accommodate 256 descriptors of 256 bits each

	 * if the remapping hardware supports scalable mode translation.

 iommu interrupt handling. Most stuff are MSI-like. */

 0x33-0x37 */

 0x3B-0x3F */

 0x49-0x4F */

 0x53-0x57 */

 0x5E-0x5F */

 0x60-0x67 */

 0x68-0x6F */

 0x7D-0x7F */

 0x88-0x8F */

 unmask it */

 Read a reg to force flush the post write */

 mask it */

 Read a reg to force flush the post write */

 TBD: ignore advanced fault log currently */

 Disable printing, simply clear the fault when ratelimited */

 highest 32 bits */

 clear the fault */

 Using pasid -1 if pasid is not present */

	/*

	 * Check if the fault interrupt is already initialized.

	/*

	 * Enable fault control interrupt.

		/*

		 * Clear any previous faults.

/*

 * Re-enable Queued Invalidation interface.

	/*

	 * First disable queued invalidation.

	/*

	 * Then enable queued invalidation again. Since there is no pending

	 * invalidation requests now, it's safe to re-enable queued

	 * invalidation.

/*

 * Check interrupt remapping support in DMAR table description.

 Check whether DMAR units are in use */

/*

 * DMAR Hotplug Support

 * For more details, please refer to Intel(R) Virtualization Technology

 * for Directed-IO Architecture Specifiction, Rev 2.2, Section 8.8

 * "Remapping Hardware Unit Hot Plug".

/*

 * Currently there's only one revision and BIOS will not check the revision id,

 * so use 0 for safety.

	/*

	 * All PCI devices managed by this unit should have been destroyed.

/*

 * dmar_platform_optin - Is %DMA_CTRL_PLATFORM_OPT_IN_FLAG set in DMAR table

 *

 * Returns true if the platform has %DMA_CTRL_PLATFORM_OPT_IN_FLAG set in

 * the ACPI DMAR table. This means that the platform boot firmware has made

 * sure no device can issue DMA outside of RMRR regions.

 SPDX-License-Identifier: GPL-2.0

/*

 * intel-pasid.c - PASID idr, table and entry manipulation

 *

 * Copyright (C) 2018 Intel Corporation

 *

 * Author: Lu Baolu <baolu.lu@linux.intel.com>

/*

 * Intel IOMMU system wide PASID name space:

/*

 * Per device pasid table management:

/*

 * Allocate a pasid table for @dev. It should be called in a

 * single-thread context.

 DMA alias device already has a pasid table, use it: */

 Free scalable mode PASID directory tables: */

		/*

		 * The pasid directory table entry won't be freed after

		 * allocation. No worry about the race with free and

		 * clear. However, this entry might be populated by others

		 * while we are preparing it. Use theirs with a retry.

/*

 * Interfaces for PASID table entry manipulation:

/*

 * Setup the DID(Domain Identifier) field (Bit 64~79) of scalable mode

 * PASID entry.

/*

 * Get domain ID value of a scalable mode PASID entry.

/*

 * Setup the SLPTPTR(Second Level Page Table Pointer) field (Bit 12~63)

 * of a scalable mode PASID entry.

/*

 * Setup the AW(Address Width) field (Bit 2~4) of a scalable mode PASID

 * entry.

/*

 * Setup the PGTT(PASID Granular Translation Type) field (Bit 6~8)

 * of a scalable mode PASID entry.

/*

 * Enable fault processing by clearing the FPD(Fault Processing

 * Disable) field (Bit 1) of a scalable mode PASID entry.

/*

 * Setup the SRE(Supervisor Request Enable) field (Bit 128) of a

 * scalable mode PASID entry.

/*

 * Setup the WPE(Write Protect Enable) field (Bit 132) of a

 * scalable mode PASID entry.

/*

 * Setup the P(Present) field (Bit 0) of a scalable mode PASID

 * entry.

/*

 * Setup Page Walk Snoop bit (Bit 87) of a scalable mode PASID

 * entry.

/*

 * Setup the Page Snoop (PGSNP) field (Bit 88) of a scalable mode

 * PASID entry.

/*

 * Setup the First Level Page table Pointer field (Bit 140~191)

 * of a scalable mode PASID entry.

/*

 * Setup the First Level Paging Mode field (Bit 130~131) of a

 * scalable mode PASID entry.

/*

 * Setup the Extended Access Flag Enable (EAFE) field (Bit 135)

 * of a scalable mode PASID entry.

	/*

	 * When PASID 0 is used, it indicates RID2PASID(DMA request w/o PASID),

	 * devTLB flush w/o PASID should be used. For non-zero PASID under

	 * SVA usage, device could do DMA with multiple PASIDs. It is more

	 * efficient to flush devTLB specific to the PASID.

 Device IOTLB doesn't need to be flushed in caching mode. */

/*

 * This function flushes cache for a newly setup pasid table entry.

 * Caller of it should not modify the in-use pasid table entries.

 CR0.WP is normally set but just to be sure */

/*

 * Set up the scalable mode pasid table entry for first only

 * translation type.

 Caller must ensure PASID entry is not in use. */

 Setup the first level page table pointer: */

 Setup Present and PASID Granular Transfer Type: */

/*

 * Skip top levels of page tables for iommu which has less agaw

 * than default. Unnecessary for PT mode.

/*

 * Set up the scalable mode pasid entry for second only translation type.

	/*

	 * If hardware advertises no support for second level

	 * translation, return directly.

 Caller must ensure PASID entry is not in use. */

	/*

	 * Since it is a second level only translation setup, we should

	 * set SRE bit as well (addresses are expected to be GPAs).

/*

 * Set up the scalable mode pasid entry for passthrough translation type.

 Caller must ensure PASID entry is not in use. */

	/*

	 * We should set SRE bit as well since the addresses are expected

	 * to be GPAs.

	/*

	 * Not all guest PASID table entry fields are passed down during bind,

	 * here we only set up the ones that are dependent on guest settings.

	 * Execution related bits such as NXE, SMEP are not supported.

	 * Other fields, such as snoop related, are set based on host needs

	 * regardless of guest settings.

 Enable write protect WP if guest requested */

	/*

	 * Memory type is only applicable to devices inside processor coherent

	 * domain. Will add MTS support once coherent devices are available.

/**

 * intel_pasid_setup_nested() - Set up PASID entry for nested translation.

 * This could be used for guest shared virtual address. In this case, the

 * first level page tables are used for GVA-GPA translation in the guest,

 * second level page tables are used for GPA-HPA translation.

 *

 * @iommu:      IOMMU which the device belong to

 * @dev:        Device to be set up for translation

 * @gpgd:       FLPTPTR: First Level Page translation pointer in GPA

 * @pasid:      PASID to be programmed in the device PASID table

 * @pasid_data: Additional PASID info from the guest bind request

 * @domain:     Domain info for setting up second level page tables

 * @addr_width: Address width of the first level (guest)

	/*

	 * Caller must ensure PASID entry is not in use, i.e. not bind the

	 * same PASID to the same device twice.

	/* Sanity checking performed by caller to make sure address

	 * width matching in two dimensions:

	 * 1. CPU vs. IOMMU

	 * 2. Guest vs. Host.

 First level PGD is in GPA, must be supported by the second level */

 Setup the second level based on the given domain */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel IOMMU trace support

 *

 * Copyright (C) 2019 Intel Corporation

 *

 * Author: Lu Baolu <baolu.lu@linux.intel.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * cap_audit.c - audit iommu capabilities for boot time and hot plug

 *

 * Copyright (C) 2021 Intel Corporation

 *

 * Author: Kyung Min Park <kyung.min.park@intel.com>

 *         Lu Baolu <baolu.lu@linux.intel.com>

 Abort hot plug if the hot plug iommu feature is smaller than global */

	/*

	 * If the system is sane to support scalable mode, either SL or FL

	 * should be sane.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007-2010 Advanced Micro Devices, Inc.

 * Author: Joerg Roedel <jroedel@suse.de>

 *         Leo Duran <leo.duran@amd.com>

 IO virtual address start page frame number */

 Reserved IOVA ranges */

 List of all available dev_data structures */

/*

 * Domain for untranslated devices - only allocated

 * if iommu=pt passed on kernel cmd line.

/*

 * general struct to manage commands send to an IOMMU

/****************************************************************************

 *

 * Helper functions

 *

	/*

	 * The IVRS alias stored in the alias table may not be

	 * part of the PCI DMA aliases if it's bus differs

	 * from the original device.

 For ACPI HID devices, there are no aliases */

	/*

	 * Add the IVRS alias to the pci aliases if it is on the same

	 * bus. The IVRS table may know about a quirk that we don't.

/*

* Find or create an IOMMU group for a acpihid device.

/*

 * This function checks if the driver got a valid device from the caller to

 * avoid dereferencing invalid pointers.

 Out of our scope? */

	/*

	 * By default we use passthrough mode for IOMMUv2 capable device.

	 * But if amd_iommu=force_isolation is set (e.g. to debug DMA to

	 * invalid address), we ignore the capability for the device so

	 * it'll be forced to go into translation mode.

	/*

	 * We keep dev_data around for unplugged devices and reuse it when the

	 * device is re-plugged - not doing so would introduce a ton of races.

/****************************************************************************

 *

 * Interrupt handling functions

 *

		/*

		 * If this is a DMA fault (for which the I(nterrupt)

		 * bit will be unset), allow report_iommu_fault() to

		 * prevent logging it.

 Did we hit the erratum? */

		/*

		 * Hardware bug: Interrupt may arrive before the entry is

		 * written to memory. If this happens we need to wait for the

		 * entry to arrive.

 Avoid memcpy function-call overhead */

		/*

		 * To detect the hardware bug we need to clear the entry

		 * back to zero.

 Update head pointer of hardware ring-buffer */

 Handle PPR entry */

 Refresh ring-buffer information */

 Avoid memcpy function-call overhead */

 Update head pointer of hardware ring-buffer */

 Handle GA entry */

 CONFIG_IRQ_REMAP */

 !CONFIG_IRQ_REMAP */

 Enable EVT and PPR and GA interrupts again */

		/*

		 * Hardware bug: ERBT1312

		 * When re-enabling interrupt (by writing 1

		 * to clear the bit), the hardware might also try to set

		 * the interrupt bit in the event status register.

		 * In this scenario, the bit will be set, and disable

		 * subsequent interrupts.

		 *

		 * Workaround: The IOMMU driver should read back the

		 * status register and check if the interrupt bits are cleared.

		 * If not, driver will need to go through the interrupt handler

		 * again and re-clear the bits

/****************************************************************************

 *

 * IOMMU command queuing functions

 *

 Copy command to buffer */

 Tell the IOMMU about it */

/*

 * Builds an invalidation address which is suitable for one page or multiple

 * pages. Sets the size bit (S) as needed is more than one page is flushed.

	/*

	 * msb_diff would hold the index of the most significant bit that

	 * flipped between the start and end.

	/*

	 * Bits 63:52 are sign extended. If for some reason bit 51 is different

	 * between the start and the end, invalidate everything.

		/*

		 * The msb-bit must be clear on the address. Just set all the

		 * lower bits.

 Clear bits 11:0 */

 Set the size bit - we flush more than one 4kb page */

 PDE bit - we want to flush everything, not only the PTEs */

/*

 * Writes the command to the IOMMUs command buffer and informs the

 * hardware about the new command.

 Skip udelay() the first time around */

 Update head and recheck remaining space */

 Do we need to make sure all commands are processed? */

/*

 * This function queues a completion wait command into the command

 * buffer of an IOMMU

/*

 * This function uses heavy locking and may disable irqs for some time. But

 * this is no issue because it is only called during resume.

/*

 * Command send function for flushing on-device TLB

/*

 * Command send function for invalidating a device table entry

/*

 * TLB invalidation function which is called from the mapping functions.

 * It invalidates a single PTE if the range to flush is within a single

 * page. Otherwise it flushes the whole TLB of the IOMMU.

		/*

		 * Devices of this domain are behind this IOMMU

		 * We need a TLB flush

	/*

	 * When NpCache is on, we infer that we run in a VM and use a vIOMMU.

	 * In such setups it is best to avoid flushes of ranges which are not

	 * naturally aligned, since it would lead to flushes of unmodified

	 * PTEs. Such flushes would require the hypervisor to do more work than

	 * necessary. Therefore, perform repeated flushes of aligned ranges

	 * until you cover the range. Each iteration flushes the smaller

	 * between the natural alignment of the address that we flush and the

	 * greatest naturally aligned region that fits in the range.

		/*

		 * size is always non-zero, but address might be zero, causing

		 * addr_alignment to be negative. As the casting of the

		 * argument in __ffs(address) to long might trim the high bits

		 * of the address on x86-32, cast to long when doing the check.

 Flush the whole IO/TLB for a given protection domain - including PDE */

		/*

		 * Devices of this domain are behind this IOMMU

		 * We need to wait for completion of all commands.

 Flush the not present cache if it exists */

/*

 * This function flushes the DTEs for all devices in domain

/****************************************************************************

 *

 * The next functions belong to the domain allocation. A domain is

 * allocated for every IOMMU as the default domain. If device isolation

 * is enabled, every device get its own domain. The most important thing

 * about domains is the page table mapping the DMA address space they

 * contain.

 *

 First mask out possible old values for GCR3 table */

 Encode GCR3 table into DTE */

	/*

	 * A kdump kernel might be replacing a domain ID that was copied from

	 * the previous kernel--if so, it needs to flush the translation cache

	 * entries for the old domain ID that is being overwritten

 remove entry from the device table seen by the hardware */

 Update data structures */

 Do reference counting */

 Update device table */

 Update data structures */

 Flush the DTE entry */

 Flush IOTLB */

 Wait for the flushes to finish */

 decrease reference counters - needs to happen after the flushes */

 Only allow access to user-accessible pages */

 First reset the PRI state of the device */

 Enable PRI */

 FIXME: Hardcode number of outstanding requests for now */

/*

 * If a device is not yet associated with a domain, this function makes the

 * device visible in the domain

	/*

	 * We might boot into a crash-kernel here. The crashed kernel

	 * left the caches in the IOMMU dirty. So we have to flush

	 * here to evict all dirty stuff.

/*

 * Removes a device from a protection domain (with devtable_lock held)

	/*

	 * First check if the device is still attached. It might already

	 * be detached from its domain because the generic

	 * iommu_detach_group code detached it and we try again here in

	 * our alias handling.

 Domains are initialized for this device - have a look what we ended up with */

/*****************************************************************************

 *

 * The next functions belong to the dma_ops mapping/unmapping code.

 *

 Update device table */

 Flush domain TLB(s) and wait for completion */

/*****************************************************************************

 *

 * The following functions belong to the exported interface of AMD IOMMU

 *

 * This interface allows access to lower level functions of the IOMMU

 * like protection domain handling and assignement of devices to domains

 * which is not possible with the dma_ops interface.

 *

	/*

	 * Force IOMMU v1 page table when iommu=pt and

	 * when allocating domain for pass-through devices.

	/*

	 * AMD's IOMMU can flush as many pages as necessary in a single flush.

	 * Unless we run in a virtual machine, which can be inferred according

	 * to whether "non-present cache" is on, it is probably best to prefer

	 * (potentially) too extensive TLB flushing (i.e., more misses) over

	 * mutliple TLB flushes (i.e., more flushes). For virtual machines the

	 * hypervisor needs to synchronize the host IOMMU PTEs with those of

	 * the guest, and the trade-off is different: unnecessary TLB flushes

	 * should be avoided.

 Exclusion range */

	/*

	 * Do not identity map IOMMUv2 capable devices when memory encryption is

	 * active, because some of those devices (AMD GPUs) don't have the

	 * encryption bit in their DMA-mask and require remapping.

/*****************************************************************************

 *

 * The next functions do a basic initialization of IOMMU for pass through

 * mode

 *

 * In passthrough mode the IOMMU is initialized and enabled but not used for

 * DMA-API translation.

 *

 IOMMUv2 specific functions */

 Number of GCR3 table levels required */

	/*

	 * Save us all sanity checks whether devices already in the

	 * domain support IOMMUv2. Just force that the domain has no

	 * devices attached when it is switched into IOMMUv2 mode.

	/*

	 * IOMMU TLB needs to be flushed before Device TLB to

	 * prevent device TLB refill from IOMMU TLB

 Wait until IOMMU TLB flushes are complete */

 Now flush device TLBs */

		/*

		   There might be non-IOMMUv2 capable devices in an IOMMUv2

		 * domain.

 Wait until all device TLBs are flushed */

/*****************************************************************************

 *

 * Interrupt Remapping Implementation

 *

 Nothing there yet, allocate new irq remapping table */

 Scan table for free entries */

	/*

	 * We use cmpxchg16 to atomically update the 128-bit IRTE,

	 * and it cannot be updated by the hardware or other processors

	 * behind us, so the return value of cmpxchg16 should be the

	 * same as the old value.

	/*

	 * With IRQ remapping enabled, don't need contiguous CPU vectors

	 * to support multiple MSI interrupts.

				/*

				 * Keep the first 32 indexes free for IOAPIC

				 * interrupts.

	/* Note:

	 * This device has never been set up for guest mode.

	 * we should not modify the IRTE

	/* Note:

	 * SVM tries to set up for VAPIC mode, but we are in

	 * legacy mode. So, we force legacy mode instead.

		/*

		 * This communicates the ga_tag back to the caller

		 * so that it can do all the necessary clean up.

	/*

	 * Atomically updates the IRTE with the new destination, vector

	 * and flushes the interrupt entry cache.

	/*

	 * After this point, all the interrupts will start arriving

	 * at the new destination. So, time to cleanup the previous

	 * vector allocation.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010-2012 Advanced Micro Devices, Inc.

 * Author: Joerg Roedel <jroedel@suse.de>

 For global state-list */

 Reference count */

	unsigned mmu_notifier_count;		/* Counting nested mmu_notifier

 mm_struct for the faults */

 mmu_notifier handle */

 PRI tag states */

 Link to our device_state */

 PASID index */

	bool invalid;				/* Used during setup and

	spinlock_t lock;			/* Protect pri_queues and

 To wait for count == 0 */

	/*

	 * First detach device from domain - No more PRI requests will arrive

	 * from that device after it is unbound from the IOMMUv2 domain.

 Everything is down now, free the IOMMUv2 domain */

 Finally get rid of the device-state */

 Must be called under dev_state->lock */

	/*

	 * Mark pasid_state as invalid, no more faults will we added to the

	 * work queue after this is visible everywhere.

 Make sure this is visible */

 After this the device/pasid can't access the mm anymore */

 Make sure no more pending faults are in the queue */

		/*

		 * This will call the mn_release function and

		 * unbind the PASID

		put_pasid_state_wait(pasid_state); /* Reference taken in

 Drop reference taken in amd_iommu_bind_pasid */

 failed to get a vma in the right range */

 Check if we have the right permissions on the vma */

 failed to service fault */

 In kdump kernel pci dev is not initialized yet -> send INVALID */

 We know the device but not the PASID -> send INVALID */

 We are OOM - send success and let the device re-fault */

	pasid_state->invalid      = true; /* Mark as valid only if we are

 Now we are ready to handle faults */

	/*

	 * Drop the reference to the mm_struct here. We rely on the

	 * mmu_notifier release call-back to inform us when the mm

	 * is going away.

	/*

	 * Drop reference taken here. We are safe because we still hold

	 * the reference taken in the amd_iommu_bind_pasid function.

 Clear the pasid state so that the pasid can be re-used */

	/*

	 * Call mmu_notifier_unregister to drop our reference

	 * to pasid_state->mm

	put_pasid_state_wait(pasid_state); /* Reference taken in

 Drop reference taken in this function */

 Drop reference taken in amd_iommu_bind_pasid */

	/*

	 * When memory encryption is active the device is likely not in a

	 * direct-mapped domain. Forbid using IOMMUv2 functionality for now.

 Get rid of any remaining pasid states */

	/*

	 * Wait until the last reference is dropped before freeing

	 * the device state.

		/*

		 * Load anyway to provide the symbols to other modules

		 * which may use AMD IOMMUv2 optionally.

	/*

	 * The loop below might call flush_workqueue(), so call

	 * destroy_workqueue() after it

 SPDX-License-Identifier: GPL-2.0-only */

/*

 * Quirks for AMD IOMMU

 *

 * Copyright (C) 2019 Kai-Heng Feng <kai.heng.feng@canonical.com>

 ivrs_ioapic[4]=00:14.0 ivrs_ioapic[5]=00:00.2 */

 ivrs_ioapic[4]=00:14.0 */

 ivrs_ioapic[32]=00:14.0 */

		/*

		 * Acer Aspire A315-41 requires the very same workaround as

		 * Dell Latitude 5495

 SPDX-License-Identifier: GPL-2.0

/*

 * AMD IOMMU driver

 *

 * Copyright (C) 2018 Advanced Micro Devices, Inc.

 *

 * Author: Gary R Hook <gary.hook@amd.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007-2010 Advanced Micro Devices, Inc.

 * Author: Joerg Roedel <jroedel@suse.de>

 *         Leo Duran <leo.duran@amd.com>

/*

 * definitions for the ACPI scanning code

/*

 * ACPI table definitions

 *

 * These data structures are laid over the table to parse the important values

 * out of it.

/*

 * structure describing one IOMMU in the ACPI table. Typically followed by one

 * or more ivhd_entrys.

 Following only valid on IVHD type 11h and 40h */

 Exact copy of MMIO_EXT_FEATURES */

/*

 * A device entry describing which devices a specific IOMMU translates and

 * which requestor ids they use.

/*

 * An AMD IOMMU memory definition structure. It defines things like exclusion

 * ranges for devices and regions that should be unity mapped.

u16 amd_iommu_last_bdf;			/* largest PCI device id we have

LIST_HEAD(amd_iommu_unity_map);		/* a list of required unity mappings

LIST_HEAD(amd_iommu_list);		/* list of all AMD IOMMUs in the

 Array to assign indices to IOMMUs*/

 Number of IOMMUs present in the system */

 IOMMUs have a non-present cache? */

/*

 * Pointer to the device table which is shared by all AMD IOMMUs

 * it is indexed by the PCI device id or the HT unit id and contains

 * information about the domain the device belongs to as well as the

 * page table root pointer.

/*

 * Pointer to a device table which the content of old device table

 * will be copied to. It's only be used in kdump kernel.

/*

 * The alias table is a driver specific data structure which contains the

 * mappings of the PCI device ids to the actual requestor ids on the IOMMU.

 * More than one device can share the same requestor id.

/*

 * The rlookup table is used to find the IOMMU which is responsible

 * for a specific device. It is also indexed by the PCI device id.

/*

 * This table is used to find the irq remapping table for a given device id

 * quickly.

/*

 * AMD IOMMU allows up to 2^16 different protection domains. This is a bitmap

 * to know which ones are already in use.

 size of the device table */

 size of the alias table */

 size if the rlookup table */

 Early ioapic and hpet maps from kernel command line */

/*

 * For IVHD type 0x11/0x40, EFR is also available via IVHD.

 * Default to IVHD EFR since it is available sooner

 * (i.e. before PCI init).

 Access to l1 and l2 indexed register spaces */

/****************************************************************************

 *

 * AMD IOMMU MMIO register space handling functions

 *

 * These functions are used to program the IOMMU device registers in

 * MMIO space required for that driver.

 *

/*

 * This function set the exclusion range in the IOMMU. DMA accesses to the

 * exclusion range are passed through untranslated

	/* Note:

	 * Re-purpose Exclusion base/limit registers for Completion wait

	 * write-back base/limit.

	/* Note:

	 * Default to 4 Kbytes, which can be specified by setting base

	 * address equal to the limit address.

 Programs the physical address of the device table into the IOMMU hardware */

 Generic functions to enable/disable certain features of the IOMMU. */

 Function to enable the hardware */

 Disable command buffer */

 Disable event logging and event interrupts */

 Disable IOMMU GA_LOG */

 Disable IOMMU hardware itself */

/*

 * mapping and unmapping functions for the IOMMU MMIO space. Each AMD IOMMU in

 * the system has one.

/****************************************************************************

 *

 * The functions below belong to the first pass of AMD IOMMU ACPI table

 * parsing. In this pass we try to find out the highest device id this

 * code has to handle. Upon this information the size of the shared data

 * structures is determined later.

 *

/*

 * This function calculates the length of a given IVHD entry

 For ACPI_HID, offset 21 is uid len */

/*

 * After reading the highest device id from the IOMMU PCI capability header

 * this function looks if there is a higher device id defined in the ACPI table

 Use maximum BDF value for DEV_ALL */

 all the above subfield types refer to device ids */

 ACPI table corrupt */

/*

 * Iterate over all IVHD entries in the ACPI table and find the highest device

 * id which we need to handle. This is the first of three functions which parse

 * the ACPI table. So we check the checksum here.

/****************************************************************************

 *

 * The following functions belong to the code path which parses the ACPI table

 * the second time. In this ACPI parsing iteration we allocate IOMMU specific

 * data structures, initialize the device/alias/rlookup table and also

 * basically initialize the hardware.

 *

/*

 * Allocates the command buffer. This buffer is per AMD IOMMU. We can

 * write commands to that buffer later and the IOMMU will execute them

 * asynchronously

/*

 * This function resets the command buffer if the IOMMU stopped fetching

 * commands from it.

/*

 * This function writes the command buffer address to the hardware and

 * enables it.

/*

 * This function disables the command buffer

 allocates the memory where the IOMMU will log its events to */

 set head and tail to zero manually */

/*

 * This function disables the event log buffer

 allocates the memory where the IOMMU will log its events to */

 set head and tail to zero manually */

 Check if already running */

 CONFIG_IRQ_REMAP */

 CONFIG_IRQ_REMAP */

	/*

	 * XT mode (32-bit APIC destination ID) requires

	 * GA mode (128-bit IRTE support) as a prerequisite.

 CONFIG_IRQ_REMAP */

 sets a specific bit in the device table entry. */

 All IOMMUs should use the same device table with the same size */

	/*

	 * When SME is enabled in the first kernel, the entry includes the

	 * memory encryption mask(sme_me_mask), we must remove the memory

	 * encryption mask to obtain the true physical address in kdump kernel.

 If gcr3 table existed, mask it out */

 Writes the specific IOMMU for a device into the rlookup table */

/*

 * This function takes the device specific flags read from the ACPI

 * table and sets up the device table entry with that information

/*

 * Takes a pointer to an AMD IOMMU entry in the ACPI table and

 * initializes the hardware and our data structures with it.

	/*

	 * First save the recommended feature enable bits from ACPI

	/*

	 * Done. Now parse the device entries

			/*

			 * add_special_device might update the devid in case a

			 * command-line override is present. So call

			 * set_dev_entry_from_acpi after add_special_device.

			/*

			 * add_special_device might update the devid in case a

			 * command-line override is present. So call

			 * set_dev_entry_from_acpi after add_special_device.

/*

 * Family15h Model 10h-1fh erratum 746 (IOMMU Logging May Stall Translations)

 * Workaround:

 *     BIOS should disable L2B micellaneous clock gating by setting

 *     L2_L2B_CK_GATE_CONTROL[CKGateL2BMiscDisable](D0F2xF4_x90[2]) = 1b

 Select NB indirect register 0x90 and enable writing */

 Clear the enable writing bit */

/*

 * Family15h Model 30h-3fh (IOMMU Mishandles ATS Write Permission)

 * Workaround:

 *     BIOS should enable ATS write permission check by setting

 *     L2_DEBUG_3[AtsIgnoreIWDis](D0F2xF4_x47[0]) = 1b

 Test L2_DEBUG_3[AtsIgnoreIWDis] == 1 */

 Set L2_DEBUG_3[AtsIgnoreIWDis] = 1 */

/*

 * This function clues the initialization function for one IOMMU

 * together and also allocates the command buffer and programs the

 * hardware. It does NOT enable the IOMMU. This is done afterwards.

 Add IOMMU to internal data structures */

 Index is fine - add IOMMU to the array */

	/*

	 * Copy data from ACPI table entry to the iommu struct

 Check if IVHD EFR contains proper max banks/counters */

		/*

		 * Note: GA (128-bit IRTE) mode requires cmpxchg16b supports.

		 * GAM also requires GA mode. Therefore, we need to

		 * check cmpxchg16b support before enabling it.

		/*

		 * Note: GA (128-bit IRTE) mode requires cmpxchg16b supports.

		 * XT, GAM also requires GA mode. Therefore, we need to

		 * check cmpxchg16b support before enabling them.

	/*

	 * Make sure IOMMU is not considered to translate itself. The IVRS

	 * table tells us so, but this is a lie!

/**

 * get_highest_supported_ivhd_type - Look up the appropriate IVHD type

 * @ivrs: Pointer to the IVRS header

 *

 * This function search through all IVDB of the maximum supported IVHD

/*

 * Iterates over all IOMMU entries in the ACPI table, allocates the

 * IOMMU structure and initializes it with init_iommu_one()

/*

 * Note: IVHD 0x11 and 0x40 also contains exact copy

 * of the IOMMU Extended Feature Register [MMIO Offset 0030h].

 * Default to EFR in IVHD since it is available sooner (i.e. before PCI init).

 read extended feature bits */

	/*

	 * Sanity check and warn if EFR values from

	 * IVHD and MMIO conflict.

 Prevent binding other PCI device drivers to IOMMU devices */

		/*

		 * Some rd890 systems may not be fully reconfigured by the

		 * BIOS, so it's necessary for us to store this information so

		 * it can be reprogrammed on resume

 Low bit locks writes to configuration space */

 Need to setup range after PCI init */

	/*

	 * Order is important here to make sure any unity map requirements are

	 * fulfilled. The unity mappings are created and written to the device

	 * table during the amd_iommu_init_api() call.

	 *

	 * After that we call init_device_table_dma() to make sure any

	 * uninitialized DTE will block DMA, and in the end we flush the caches

	 * of all IOMMUs to make sure the changes to the device table are

	 * active.

/****************************************************************************

 *

 * The following functions initialize the MSI interrupts for all IOMMUs

 * in the system. It's a bit challenging because there could be multiple

 * IOMMUs per PCI BDF but we can call pci_enable_msi(x) only once per

 * pci_dev.

 *

/*

 * There isn't really any need to mask/unmask at the irqchip level because

 * the 64-bit INTCAPXT registers can be updated atomically without tearing

 * when the affinity is being updated.

	/**

	 * Current IOMMU implemtation uses the same IRQ for all

	 * 3 IOMMU interrupts.

 No need for locking here (yet) as the init is single-threaded */

/****************************************************************************

 *

 * The next functions belong to the third pass of parsing the ACPI

 * table. In this last pass the memory mapping requirements are

 * gathered (like exclusion and unity mapping ranges).

 *

 called for unity map ACPI definition */

	/*

	 * Treat per-device exclusion ranges as r/w unity-mapped regions

	 * since some buggy BIOSes might lead to the overwritten exclusion

	 * range (exclusion_start and exclusion_length members). This

	 * happens when there are multiple exclusion ranges (IVMD entries)

	 * defined in ACPI table.

 iterates over all memory definitions we find in the ACPI table */

/*

 * Init the device table to not allow DMA access for devices

	/*

	 * make IOMMU memory accesses cache coherent

 Set IOTLB invalidation timeout to 1s */

 RD890 BIOSes may not have completely reconfigured the iommu */

	/*

	 * First, we need to ensure that the iommu is enabled. This is

	 * controlled by a register in the northbridge

 Select Northbridge indirect register 0x75 and enable writing */

 Enable the iommu */

 Restore the iommu BAR */

 Restore the l1 indirect regs for each of the 6 l1s */

 Restore the l2 indirect regs */

 Lock PCI setup registers */

/*

 * This function finally enables all IOMMUs found in the system after

 * they have been initialized.

 *

 * Or if in kdump kernel and IOMMUs are all pre-enabled, try to copy

 * the old content of device table entries. Not this case or copy failed,

 * just continue as normal kernel does.

		/*

		 * If come here because of failure in copying device table from old

		 * kernel with all IOMMUs enabled, print error message and try to

		 * free allocated old_dev_tbl_cpy.

	/*

	 * Note: We have already checked GASup from IVRS table.

	 *       Now, we need to make sure that GAMSup is set.

/*

 * Suspend/Resume support

 * disable suspend until real resume implemented

 re-load the hardware */

 disable IOMMUs to go out of the way for BIOS */

 SB IOAPIC is always on this device in AMD systems */

	/*

	 * If we have map overrides on the kernel command line the

	 * messages in this function might not describe firmware bugs

	 * anymore - so be careful

		/*

		 * We expect the SB IOAPIC to be listed in the IVRS

		 * table. The system timer is connected to the SB IOAPIC

		 * and if we don't have it in the list the system will

		 * panic at boot time.  This situation usually happens

		 * when the BIOS is buggy and provides us the wrong

		 * device id for the IOAPIC in the system.

/*

 * This is the hardware init function for AMD IOMMU in the system.

 * This function is called either from amd_iommu_init or from the interrupt

 * remapping setup code.

 *

 * This function basically parses the ACPI table for AMD IOMMU (IVRS)

 * four times:

 *

 *	1 pass) Discover the most comprehensive IVHD type to use.

 *

 *	2 pass) Find the highest PCI device id the driver has to handle.

 *		Upon this information the size of the data structures is

 *		determined that needs to be allocated.

 *

 *	3 pass) Initialize the data structures just allocated with the

 *		information in the ACPI table about available AMD IOMMUs

 *		in the system. It also maps the PCI devices in the

 *		system to specific IOMMUs

 *

 *	4 pass) After the basic data structures are allocated and

 *		initialized we update them with information about memory

 *		remapping requirements parsed out of the ACPI table in

 *		this last pass.

 *

 * After everything is set up the IOMMUs are enabled and the necessary

 * hotplug and suspend notifiers are registered.

	/*

	 * Validate checksum here so we don't need to do it when

	 * we actually parse the table

	/*

	 * First parse ACPI tables to find the largest Bus/Dev/Func

	 * we need to handle. Upon this information the shared data

	 * structures for the IOMMUs in the system will be allocated

 Device table - directly used by all IOMMUs */

	/*

	 * Alias table - map PCI Bus/Dev/Func to Bus/Dev/Func the

	 * IOMMU see for that device

 IOMMU rlookup table - find the IOMMU for a specific device */

	/*

	 * let all alias entries point to itself

	/*

	 * never allocate domain 0 because its used as the non-allocated and

	 * error value placeholder

	/*

	 * now the data structures are allocated and basically initialized

	 * start the real acpi table scan

 Disable any previously enabled IOMMUs */

		/*

		 * Interrupt remapping enabled, create kmem_cache for the

		 * remapping tables.

 init the device table */

 Don't leak any ACPI memory */

 Don't use IOMMU if there is Stoney Ridge graphics */

 Make sure ACS will be enabled during PCI probe */

/****************************************************************************

 *

 * AMD IOMMU Initialization State Machine

 *

 Nothing to do */

 Error states => do nothing */

 Unknown state */

 We enable MSI later when PCI is initialized */

/*

 * This is the core init function for AMD IOMMU hardware in the system.

 * This function is called from the generic x86 DMA layer initialization

 * code.

		/*

		 * We failed to initialize the AMD IOMMU - try fallback

		 * to GART if possible.

 For Fam17h, a specific level of support is required */

/****************************************************************************

 *

 * Early detect code. This code runs at IOMMU detection time in the DMA

 * layer. It just looks if there is an IVRS ACPI table to detect AMD

 * IOMMUs

 *

/****************************************************************************

 *

 * Parsing functions for the AMD IOMMU specific kernel command line

 * options.

 *

/****************************************************************************

 *

 * IOMMU EFR Performance Counter support functionality. This code allows

 * access to the IOMMU PC functionality.

 *

 Make sure the IOMMU PC resource is available */

 Check for valid iommu and pc register indexing */

 Limit the offset to the hw defined mmio region aperture */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * CPU-agnostic AMD IO page table allocator.

 *

 * Copyright (C) 2020 Advanced Micro Devices, Inc.

 * Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>

/*

 * Helper function to get the first pte of a large mapping

/****************************************************************************

 *

 * The functions below are used the create the page table mappings for

 * unity mapped regions.

 *

 PTE present? */						\

 Large PTE? */						\

 lowest 3 bits encode pgtable mode */

/*

 * This function is used to add another level to an IO page table. Adding

 * another level increases the size of the address space by 9 bits to a size up

 * to 64 bits.

	/*

	 * Device Table needs to be updated and flushed before the new root can

	 * be published.

		/*

		 * Return an error if there is no memory to update the

		 * page-table.

		/*

		 * If we replace a series of large PTEs, we need

		 * to tear down all of them.

			/*

			 * Unmap the replicated PTEs that still match the

			 * original large mapping

 pte could have been changed somewhere. */

 No level skipping support yet */

/*

 * This function checks if there is a PTE for a given dma address. If

 * there is one, it returns the pointer to it.

 Not Present */

 Large PTE */

 No level skipping support yet */

 Walk to the next level */

	/*

	 * If we have a series of large PTEs, make

	 * sure to return a pointer to the first one.

/*

 * Generic mapping functions. It maps a physical address into a DMA

 * address space. It allocates the page table pages if necessary.

 * In the future it can be extended to a generic mapping function

 * supporting all features of AMD IOMMU page tables like level skipping

 * and full 64 bit address spaces.

		/*

		 * Flush domain TLB(s) and wait for completion. Any Device-Table

		 * Updates and flushing already happened in

		 * increase_address_space().

 Everything flushed out, free pages now */

/*

 * ----------------------------------------------------

 Update data structure */

 Make changes visible to IOMMUs */

 Page-table is not visible to IOMMU anymore, so free it */

 SPDX-License-Identifier: GPL-2.0

/*

 * Implementation of the IOMMU SVA API for the ARM SMMUv3

/*

 * Check if the CPU ASID is available on the SMMU side. If a private context

 * descriptor is using it, try to replace it.

 All devices bound to this mm use the same cd struct. */

	/*

	 * Race with unmap: TLB invalidations will start targeting the new ASID,

	 * which isn't assigned yet. We'll do an invalidate-all on the old ASID

	 * later, so it doesn't matter.

	/*

	 * Update ASID and invalidate CD in all associated masters. There will

	 * be some overlap between use of both ASIDs, until we invalidate the

	 * TLB.

 Invalidate TLB entries previously associated with that context */

	/*

	 * MAIR value is pretty much constant and global, so we can just get it

	 * from the current CPU register

 Unpin ASID */

	/*

	 * DMA may still be running. Keep the cd valid to avoid C_BAD_CD events,

	 * but disable translation.

 Allocate or get existing MMU notifier for this {domain, mm} pair */

 Frees smmu_mn */

	/*

	 * If we went through clear(), we've already invalidated, and no

	 * new TLB entry can have been formed.

 Frees smmu_mn */

 If bind() was already called for this {dev, mm} pair, reuse it. */

 Allocate a PASID for this mm if necessary */

	/*

	 * Get the smallest PA size of all CPUs (sanitized by cpufeature). We're

	 * not even pretending to support AArch32 here. Abort if the MMU outputs

	 * addresses larger than what we support.

 We can support bigger ASIDs than the CPU, but not smaller */

	/*

	 * See max_pinned_asids in arch/arm64/mm/context.c. The following is

	 * generally the maximum number of bindable processes.

 We're not keeping track of SIDs in fault events */

 SSID support is mandatory for the moment */

	/*

	 * Drivers for devices supporting PRI or stall should enable IOPF first.

	 * Others have device-specific fault handlers and don't need IOPF.

	/*

	 * Some MMU notifiers may still be waiting to be freed, using

	 * arm_smmu_mmu_notifier_free(). Wait for them.

 SPDX-License-Identifier: GPL-2.0

/*

 * IOMMU API for ARM architected SMMUv3 implementations.

 *

 * Copyright (C) 2015 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 *

 * This driver is powered by bad coffee and bombay mix.

/*

 * Special value used by SVA when a process dies, to quiesce a CD without

 * disabling it.

 Low-level queue manipulation functions */

	/*

	 * Ensure that all CPU accesses (reads and writes) to the queue

	 * are complete before we update the cons pointer.

	/*

	 * We can't use the _relaxed() variant here, as we must prevent

	 * speculative reads of the queue before we have determined that

	 * prod has indeed moved.

 High-level queue accessors */

 Cover the entire SID range */

	/*

	 * Beware that Hi16xx adds an extra 32 bits of goodness to its MSI

	 * payload, so the write will zero the entire command on that platform.

		/*

		 * ATC Invalidation Completion timeout. CONS is still pointing

		 * at the CMD_SYNC. Attempt to complete other pending commands

		 * by repeating the CMD_SYNC, though we might well end up back

		 * here since the ATC invalidation may still be pending.

	/*

	 * We may have concurrent producers, so we need to be careful

	 * not to touch any of the shadow cmdq state.

 Convert the erroneous command into a CMD_SYNC */

/*

 * Command queue locking.

 * This is a form of bastardised rwlock with the following major changes:

 *

 * - The only LOCK routines are exclusive_trylock() and shared_lock().

 *   Neither have barrier semantics, and instead provide only a control

 *   dependency.

 *

 * - The UNLOCK routines are supplemented with shared_tryunlock(), which

 *   fails if the caller appears to be the last lock holder (yes, this is

 *   racy). All successful UNLOCK routines have RELEASE semantics.

	/*

	 * We can try to avoid the cmpxchg() loop by simply incrementing the

	 * lock counter. When held in exclusive state, the lock counter is set

	 * to INT_MIN so these increments won't hurt as the value will remain

	 * negative.

/*

 * Command queue insertion.

 * This is made fiddly by our attempts to achieve some sort of scalability

 * since there is one queue shared amongst all of the CPUs in the system.  If

 * you like mixed-size concurrency, dependency ordering and relaxed atomics,

 * then you'll *love* this monstrosity.

 *

 * The basic idea is to split the queue up into ranges of commands that are

 * owned by a given CPU; the owner may not have written all of the commands

 * itself, but is responsible for advancing the hardware prod pointer when

 * the time comes. The algorithm is roughly:

 *

 * 	1. Allocate some space in the queue. At this point we also discover

 *	   whether the head of the queue is currently owned by another CPU,

 *	   or whether we are the owner.

 *

 *	2. Write our commands into our allocated slots in the queue.

 *

 *	3. Mark our slots as valid in arm_smmu_cmdq.valid_map.

 *

 *	4. If we are an owner:

 *		a. Wait for the previous owner to finish.

 *		b. Mark the queue head as unowned, which tells us the range

 *		   that we are responsible for publishing.

 *		c. Wait for all commands in our owned range to become valid.

 *		d. Advance the hardware prod pointer.

 *		e. Tell the next owner we've finished.

 *

 *	5. If we are inserting a CMD_SYNC (we may or may not have been an

 *	   owner), then we need to stick around until it has completed:

 *		a. If we have MSIs, the SMMU can write back into the CMD_SYNC

 *		   to clear the first 4 bytes.

 *		b. Otherwise, we spin waiting for the hardware cons pointer to

 *		   advance past our command.

 *

 * The devil is in the details, particularly the use of locking for handling

 * SYNC completion and freeing up space in the queue before we think that it is

 * full.

		/*

		 * The valid bit is the inverse of the wrap bit. This means

		 * that a zero-initialised queue is invalid and, after marking

		 * all entries as valid, they become invalid again when we

		 * wrap.

 Poll */

 Mark all entries in the range [sprod, eprod) as valid */

 Wait for all entries in the range [sprod, eprod) to become valid */

 Wait for the command queue to become non-full */

	/*

	 * Try to update our copy of cons by grabbing exclusive cmdq access. If

	 * that fails, spin until somebody else updates it for us.

/*

 * Wait until the SMMU signals a CMD_SYNC completion MSI.

 * Must be called with the cmdq lock held in some capacity.

	/*

	 * The MSI won't generate an event, since it's being written back

	 * into the command queue.

/*

 * Wait until the SMMU cons index passes llq->prod.

 * Must be called with the cmdq lock held in some capacity.

		/*

		 * This needs to be a readl() so that our subsequent call

		 * to arm_smmu_cmdq_shared_tryunlock() can fail accurately.

		 *

		 * Specifically, we need to ensure that we observe all

		 * shared_lock()s by other CMD_SYNCs that share our owner,

		 * so that a failing call to tryunlock() means that we're

		 * the last one out and therefore we can safely advance

		 * cmdq->q.llq.cons. Roughly speaking:

		 *

		 * CPU 0		CPU1			CPU2 (us)

		 *

		 * if (sync)

		 * 	shared_lock();

		 *

		 * dma_wmb();

		 * set_valid_map();

		 *

		 * 			if (owner) {

		 *				poll_valid_map();

		 *				<control dependency>

		 *				writel(prod_reg);

		 *

		 *						readl(cons_reg);

		 *						tryunlock();

		 *

		 * Requires us to see CPU 0's shared_lock() acquisition.

/*

 * This is the actual insertion function, and provides the following

 * ordering guarantees to callers:

 *

 * - There is a dma_wmb() before publishing any commands to the queue.

 *   This can be relied upon to order prior writes to data structures

 *   in memory (such as a CD or an STE) before the command.

 *

 * - On completion of a CMD_SYNC, there is a control dependency.

 *   This can be relied upon to order subsequent writes to memory (e.g.

 *   freeing an IOVA) after completion of the CMD_SYNC.

 *

 * - Command insertion is totally ordered, so if two CPUs each race to

 *   insert their own list of commands then all of the commands from one

 *   CPU will appear before any of the commands from the other CPU.

 1. Allocate some space in the queue */

	/*

	 * 2. Write our commands into the queue

	 * Dependency ordering from the cmpxchg() loop above.

		/*

		 * In order to determine completion of our CMD_SYNC, we must

		 * ensure that the queue can't wrap twice without us noticing.

		 * We achieve that by taking the cmdq lock as shared before

		 * marking our slot as valid.

 3. Mark our slots as valid, ensuring commands are visible first */

 4. If we are the owner, take control of the SMMU hardware */

 a. Wait for previous owner to finish */

 b. Stop gathering work by clearing the owned flag */

		/*

		 * c. Wait for any gathered work to be written to the queue.

		 * Note that we read our own entries so that we have the control

		 * dependency required by (d).

		/*

		 * d. Advance the hardware prod pointer

		 * Control dependency ordering from the entries becoming valid.

		/*

		 * e. Tell the next owner we're done

		 * Make sure we've updated the hardware first, so that we don't

		 * race to update prod and potentially move it backwards.

 5. If we are inserting a CMD_SYNC, we must wait for it to complete */

		/*

		 * Try to unlock the cmdq lock. This will fail if we're the last

		 * reader, in which case we can safely update cmdq->q.llq.cons

	/*

	 * Don't send a SYNC, it doesn't do anything for RESUME or PRI_RESP.

	 * RESUME consumption guarantees that the stalled transaction will be

	 * terminated... at some point in the future. PRI_RESP is fire and

	 * forget.

 Context descriptor manipulation functions */

 See comment in arm_smmu_write_ctx_desc() */

 An invalid L1CD can be cached */

	/*

	 * This function handles the following cases:

	 *

	 * (1) Install primary CD, for normal DMA traffic (SSID = 0).

	 * (2) Install a secondary CD, for SID+SSID traffic.

	 * (3) Update ASID of a CD. Atomically write the first 64 bits of the

	 *     CD, then invalidate the old entry and mappings.

	 * (4) Quiesce the context without clearing the valid bit. Disable

	 *     translation, and ignore any translation fault.

	 * (5) Remove a secondary CD.

 (5) */

 (4) */

 (3) */

		/*

		 * Until CD+TLB invalidation, both ASIDs may be used for tagging

		 * this substream's traffic

 (1) and (2) */

		/*

		 * STE is live, and the SMMU might read dwords of this CD in any

		 * order. Ensure that it observes valid values before reading

		 * V=1.

	/*

	 * The SMMU accesses 64-bit values atomically. See IHI0070Ca 3.21.3

	 * "Configuration structures and configuration invalidation completion"

	 *

	 *   The size of single-copy atomic reads made by the SMMU is

	 *   IMPLEMENTATION DEFINED but must be at least 64 bits. Any single

	 *   field within an aligned 64-bit span of a structure can be altered

	 *   without first making the structure invalid.

 Stream table manipulation functions */

 See comment in arm_smmu_write_ctx_desc() */

	/*

	 * This is hideously complicated, but we only really care about

	 * three cases at the moment:

	 *

	 * 1. Invalid (all zero) -> bypass/fault (init)

	 * 2. Bypass/fault -> translation/bypass (attach)

	 * 3. Translation/bypass -> bypass/fault (detach)

	 *

	 * Given that we can't update the STE atomically and the SMMU

	 * doesn't read the thing in a defined order, that leaves us

	 * with the following maintenance requirements:

	 *

	 * 1. Update Config, return (init time STEs aren't live)

	 * 2. Write everything apart from dword 0, sync, write dword 0, sync

	 * 3. Update Config, sync

 STE corruption */

 Nuke the existing STE_0 value, as we're going to rewrite it */

 Bypass/fault */

 Nuke the VMID */

		/*

		 * The SMMU can perform negative caching, so we must sync

		 * the STE regardless of whether the old value was live.

 See comment in arm_smmu_write_ctx_desc() */

 It's likely that we'll want to use the new STE soon */

 IRQ and event handlers */

 Stage-2 is always pinned at the moment */

 Nobody cared, abort the access */

		/*

		 * Not much we can do on overflow, so scream and pretend we're

		 * trying harder.

 Sync our overflow flag, as we believe we're up to speed */

 Sync our overflow flag, as we believe we're up to speed */

 No errors pending */

 ATC invalidates are always on 4096-bytes pages */

	/*

	 * ATS and PASID:

	 *

	 * If substream_valid is clear, the PCIe TLP is sent without a PASID

	 * prefix. In that case all ATC entries within the address range are

	 * invalidated, including those that were requested with a PASID! There

	 * is no way to invalidate only entries without PASID.

	 *

	 * When using STRTAB_STE_1_S1DSS_SSID0 (reserving CD 0 for non-PASID

	 * traffic), translation requests without PASID create ATC entries

	 * without PASID, which must be invalidated with substream_valid clear.

	 * This has the unpleasant side-effect of invalidating all PASID-tagged

	 * ATC entries within the address range.

	/*

	 * In an ATS Invalidate Request, the address must be aligned on the

	 * range size, which must be a power of two number of page sizes. We

	 * thus have to choose between grossly over-invalidating the region, or

	 * splitting the invalidation into multiple commands. For simplicity

	 * we'll go with the first solution, but should refine it in the future

	 * if multiple commands are shown to be more efficient.

	 *

	 * Find the smallest power of two that covers the range. The most

	 * significant differing bit between the start and end addresses,

	 * fls(start ^ end), indicates the required span. For example:

	 *

	 * We want to invalidate pages [8; 11]. This is already the ideal range:

	 *		x = 0b1000 ^ 0b1011 = 0b11

	 *		span = 1 << fls(x) = 4

	 *

	 * To invalidate pages [7; 10], we need to invalidate [0; 15]:

	 *		x = 0b0111 ^ 0b1010 = 0b1101

	 *		span = 1 << fls(x) = 16

	/*

	 * Ensure that we've completed prior invalidation of the main TLBs

	 * before we read 'nr_ats_masters' in case of a concurrent call to

	 * arm_smmu_enable_ats():

	 *

	 *	// unmap()			// arm_smmu_enable_ats()

	 *	TLBI+SYNC			atomic_inc(&nr_ats_masters);

	 *	smp_mb();			[...]

	 *	atomic_read(&nr_ats_masters);	pci_enable_ats() // writel()

	 *

	 * Ensures that we always see the incremented 'nr_ats_masters' count if

	 * ATS was enabled at the PCI device before completion of the TLBI.

 IO_PGTABLE API */

	/*

	 * NOTE: when io-pgtable is in non-strict mode, we may get here with

	 * PTEs previously cleared by unmaps on the current CPU not yet visible

	 * to the SMMU. We are relying on the dma_wmb() implicit during cmd

	 * insertion to guarantee those are observed before the TLBI. Do be

	 * careful, 007.

 Get the leaf page size */

 Convert page size of 12,14,16 (log2) to 1,2,3 */

 Determine what level the granule is at */

			/*

			 * On each iteration of the loop, the range is 5 bits

			 * worth of the aligned size remaining.

			 * The range in pages is:

			 *

			 * range = (num_pages & (0x1f << __ffs(num_pages)))

 Determine the power of 2 multiple number of pages */

 Determine how many chunks of 2^scale size we have */

 range is num * 2^scale * pgsize */

 Clear out the lower order bits for the next iteration */

	/*

	 * Unfortunately, this can't be leaf-only since we may have

	 * zapped an entire table.

 IOMMU API */

	/*

	 * Allocate the domain and initialise some of its data structures.

	 * We can't really do anything meaningful until we've added a

	 * master.

 Free the CD and ASID, if we allocated them */

 Prevent SVA from touching the CD while we're freeing it */

 Prevent SVA from modifying the ASID until it is written to the CD */

	/*

	 * Note that this will end up calling arm_smmu_sync_cd() before

	 * the master has been added to the devices list for this domain.

	 * This isn't an issue because the STE hasn't been installed yet.

 Restrict the stage to what we can actually support */

 Two-level walk */

 Simple linear lookup */

 Bridged PCI devices may end up with duplicated IDs */

 Don't enable ATS at the endpoint if it's not enabled in the STE */

 Smallest Translation Unit: log2 of the smallest supported granule */

	/*

	 * Ensure ATS is disabled at the endpoint before we issue the

	 * ATC invalidation via the SMMU.

	/*

	 * Checking that SVA is disabled ensures that this device isn't bound to

	 * any mm, and can be safely detached from its old domain. Bonds cannot

	 * be removed concurrently since we're holding the group mutex.

		/*

		 * Check the SIDs are in range of the SMMU and our stream table

 Ensure l2 strtab is initialised */

 Insert into SID tree */

	/*

	 * Note that PASID must be enabled before, and disabled after ATS:

	 * PCI Express Base 4.0r1.0 - 10.5.1.3 ATS Control Register

	 *

	 *   Behavior is undefined if this bit is Set and the value of the PASID

	 *   Enable, Execute Requested Enable, or Privileged Mode Requested bits

	 *   are changed.

	/*

	 * We don't support devices sharing stream IDs other than PCI RID

	 * aliases, since the necessary ID-to-device lookup becomes rather

	 * impractical given a potential sparse 32-bit stream ID space.

 Restricted during device attach */

 Probing and initialisation functions */

 cmdq */

 evtq */

 priq */

 Calculate the L1 size, capped to the SIDSIZE. */

 Configure strtab_base_cfg for 2 levels */

 Configure strtab_base_cfg for a linear table covering all SIDs */

 Set the strtab base address */

 Allocate the first VMID for stage-2 bypass STEs */

 GBPA is "special" */

 Clear the MSI address regs */

 Allocate MSIs for evtq, gerror and priq. Ignore cmdq */

 Unknown */

 Add callback to free MSIs on teardown */

 Request interrupt lines */

 Disable IRQs first */

		/*

		 * Cavium ThunderX2 implementation doesn't support unique irq

		 * lines. Use a single irq line for all the SMMUv3 interrupts.

 Enable interrupt generation on the SMMU */

 Clear CR0 and sync (disables SMMU and queue processing) */

 CR1 (table and queue memory attributes) */

 CR2 (random crap) */

 Stream table */

 Command queue */

 Invalidate any cached configuration */

 Invalidate any stale TLB entries */

 Event queue */

 PRI queue */

 Enable the SMMU interface, or ensure bypass */

 IDR0 */

 2-level structures */

	/*

	 * Translation table endianness.

	 * We currently require the same endianness as the CPU, but this

	 * could be changed later by adding a new IO_PGTABLE_QUIRK.

 Boolean feature flags */

	/*

	 * The coherency feature as set by FW is used in preference to the ID

	 * register, but warn on mismatch.

 We only support the AArch64 table format at present */

 ASID/VMID sizes */

 IDR1 */

 Queue sizes, capped to ensure natural alignment */

		/*

		 * We don't support splitting up batches, so one batch of

		 * commands plus an extra sync needs to fit inside the command

		 * queue. There's also no way we can handle the weird alignment

		 * restrictions on the base pointer for a unit-length queue.

 SID/SSID sizes */

	/*

	 * If the SMMU supports fewer bits than would fill a single L2 stream

	 * table, use a linear table instead.

 IDR3 */

 IDR5 */

 Maximum number of outstanding stalls */

 Page sizes */

 Input address size */

 Output address size */

 4TB */

 Set the DMA mask for our table walker */

 Retrieve SMMUv3 specific data */

 Set bypass mode according to firmware probing result */

 Base address */

	/*

	 * Don't map the IMPLEMENTATION DEFINED regions, since they may contain

	 * the PMCG registers which are reserved by the PMU driver.

 Interrupt lines */

 Probe the h/w */

 Initialise in-memory data structures */

 Record our private device structure */

 Reset the device */

 And we're up. Go go go! */

 SPDX-License-Identifier: GPL-2.0-only

 Miscellaneous Arm SMMU implementation and integration quirks

 Copyright (C) 2019 Arm Limited

 Since we don't care for sGFAR, we can do without 64-bit accessors */

	/*

	 * Cavium CN88xx erratum #27704.

	 * Ensure ASID and VMID allocation is unique across all SMMUs in

	 * the system.

	/*

	 * On MMU-500 r2p0 onwards we need to clear ACR.CACHE_LOCK before

	 * writes to the context bank ACTLRs will stick. And we just hope that

	 * Secure has also cleared SACR.CACHE_LOCK for this to take effect...

	/*

	 * Allow unmatched Stream IDs to allocate bypass

	 * TLB entries for reduced latency.

	/*

	 * Disable MMU-500's not-particularly-beneficial next-page

	 * prefetcher for the sake of errata #841119 and #826419.

	/*

	 * Marvell Armada-AP806 erratum #582743.

	 * Split all the readq to double readl

	/*

	 * Marvell Armada-AP806 erratum #582743.

	 * Split all the writeq to double writel

	/*

	 * Armada-AP806 erratum #582743.

	 * Hide the SMMU_IDR2.PTFSv8 fields to sidestep the AArch64

	 * formats altogether and allow using 32 bits access on the

	 * interconnect.

	/*

	 * Set the impl for model-specific implementation quirks first,

	 * such that platform integration quirks can pick it up and

	 * inherit from it if necessary.

 This is implicitly MMU-400 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2019, The Linux Foundation. All rights reserved.

	/*

	 * On the GPU device we want to process subsequent transactions after a

	 * fault to keep the GPU from hanging

	/*

	 * The GPU will always use SID 0 so that is a handy way to uniquely

	 * identify it and configure it for per-instance pagetables

/*

 * Local implementation to configure TTBR0 with the specified pagetable config.

 * The GPU driver will call this to enable TTBR0 when per-instance pagetables

 * are active

 The domain must have split pagetables already enabled */

 If the pagetable config is NULL, disable TTBR0 */

 Do nothing if it is already disabled */

 Set TCR to the original configuration */

 Don't call this again if TTBR0 is already enabled */

	/*

	 * Assign context bank 0 to the GPU device so the GPU hardware can

	 * switch pagetables

 Only enable split pagetables for the GPU device (SID 0) */

	/*

	 * All targets that use the qcom,adreno-smmu compatible string *should*

	 * be AARCH64 stage 1 but double check because the arm-smmu code assumes

	 * that is the case when the TTBR1 quirk is enabled

	/*

	 * Initialize private interface with GPU:

	/*

	 * With some firmware versions writes to S2CR of type FAULT are

	 * ignored, and writing BYPASS will end up written as FAULT in the

	 * register. Perform a write to S2CR to detect if this is the case and

	 * if so reserve a context bank to emulate bypass streams.

 Ignore valid bit for SMR mask extraction. */

			/*

			 * Firmware with quirky S2CR handling will substitute

			 * BYPASS writes with FAULT, so point the stream to the

			 * reserved context bank and ask for translation on the

			 * stream

			/*

			 * Firmware with quirky S2CR handling will ignore FAULT

			 * writes, so trick it to write FAULT by asking for a

			 * BYPASS.

	/*

	 * To address performance degradation in non-real time clients,

	 * such as USB and UFS, turn off wait-for-safe on sdm845 based boards,

	 * such as MTP and db845, whose firmwares implement secure monitor

	 * call handlers to turn on/off the wait-for-safe logic.

 Check to make sure qcom_scm has finished probing */

 Match platform for ACPI boot */

	/*

	 * Do not change this order of implementation, i.e., first adreno

	 * smmu impl and then apss smmu since we can have both implementing

	 * arm,mmu-500 in which case we will miss setting adreno smmu specific

	 * features if the order is changed.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU API for ARM architected SMMU implementations.

 *

 * Copyright (C) 2013 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 *

 * This driver currently supports:

 *	- SMMUv1 and v2 implementations

 *	- Stream-matching and stream-indexing

 *	- v7/v8 long-descriptor format

 *	- Non-secure access to the SMMU

 *	- Context fault reporting

 *	- Extended Stream ID (16 bit)

/*

 * Apparently, some Qualcomm arm64 platforms which appear to expose their SMMU

 * global register space are still, in fact, using a hypervisor to mediate it

 * by trapping and emulating register accesses. Sadly, some deployed versions

 * of said trapping code have bugs wherein they go horribly wrong for stores

 * using r31 (i.e. XZR/WZR) as the source register.

 Continue walking */

 "mmu-masters" assumes Stream ID == Requester ID */

/*

 * With the legacy DT binding in play, we have no guarantees about

 * probe order, but then we're also not doing default domains, so we can

 * delay setting bus ops until we're sure every possible SMMU is ready,

 * and that way ensure that no probe_device() calls get missed.

 CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS */

 Wait for any pending TLB invalidations to complete */

	/*

	 * The TLBI write may be relaxed, so ensure that PTEs cleared by the

	 * current CPU are visible beforehand.

 See above */

/*

 * On MMU-401 at least, the cost of firing off multiple TLBIVMIDs appears

 * almost negligible, but the benefit of getting the first one in as far ahead

 * of the sync as possible is significant, hence we don't just make this a

 * no-op and call arm_smmu_tlb_inv_context_s2() from .iotlb_sync as you might

 * think.

 TCR */

 TTBRs */

 MAIRs (stage-1 only) */

 Unassigned context banks only need disabling */

 CBA2R */

 16-bit VMIDs live in CBA2R */

 CBAR */

	/*

	 * Use the weakest shareability/memory types, so they are

	 * overridden by the ttbcr/pte.

 8-bit VMIDs live in CBAR */

	/*

	 * TCR

	 * We must write this before the TTBRs, since it determines the

	 * access behaviour of some fields (in particular, ASID[15:8]).

 TTBRs */

 MAIRs (stage-1 only) */

 SCTLR */

	/*

	 * Mapping the requested stage onto what we support is surprisingly

	 * complicated, mainly because the spec allows S1+S2 SMMUs without

	 * support for nested translation. That means we end up with the

	 * following table:

	 *

	 * Requested        Supported        Actual

	 *     S1               N              S1

	 *     S1             S1+S2            S1

	 *     S1               S2             S2

	 *     S1               S1             S1

	 *     N                N              N

	 *     N              S1+S2            S2

	 *     N                S2             S2

	 *     N                S1             S1

	 *

	 * Note that you can't actually request stage-2 mappings.

	/*

	 * Choosing a suitable context format is even more fiddly. Until we

	 * grow some way for the caller to express a preference, and/or move

	 * the decision into the io-pgtable code where it arguably belongs,

	 * just aim for the closest thing to the rest of the system, and hope

	 * that the hardware isn't esoteric enough that we can't assume AArch64

	 * support to be a superset of AArch32 support...

		/*

		 * We will likely want to change this if/when KVM gets

		 * involved.

 Update the domain's page sizes to reflect the page table format */

 Initialise the context bank with our page table cfg */

	/*

	 * Request context fault interrupt. Do this last to avoid the

	 * handler seeing a half-initialised domain state.

 Publish page table ops for map/unmap */

	/*

	 * Disable the context bank and free the page tables before freeing

	 * it.

	/*

	 * Allocate the domain and initialise some of its data structures.

	 * We can't really do anything meaningful until we've added a

	 * master.

	/*

	 * Free the domain resources. We assume that all devices have

	 * already been detached.

/*

 * The width of SMR's mask field depends on sCR0_EXIDENABLE, so this function

 * should be called after sCR0 is written.

	/*

	 * If we've had to accommodate firmware memory regions, we may

	 * have live SMRs by now; tread carefully...

	 *

	 * Somewhat perversely, not having a free SMR for this test implies we

	 * can get away without it anyway, as we'll only be able to 'allocate'

	 * these SMRs for the ID/mask values we're already trusting to be OK.

	/*

	 * SMR.ID bits may not be preserved if the corresponding MASK

	 * bits are set, so check each one separately. We can reject

	 * masters later if they try to claim IDs outside these masks.

 Stream indexing is blissfully easy */

 Validating SMRs is... less so */

			/*

			 * Note the first free entry we come across, which

			 * we'll claim in the end if nothing else matches.

		/*

		 * If the new entry is _entirely_ matched by an existing entry,

		 * then reuse that, with the guarantee that there also cannot

		 * be any subsequent conflicting entries. In normal use we'd

		 * expect simply identical entries for this case, but there's

		 * no harm in accommodating the generalisation.

		/*

		 * If the new entry has any other overlap with an existing one,

		 * though, then there always exists at least one stream ID

		 * which would cause a conflict, and we can't allow that risk.

 Figure out a viable stream map entry allocation */

 It worked! Now, poke the actual hardware */

	/*

	 * FIXME: The arch/arm DMA API code tries to attach devices to its own

	 * domains between of_xlate() and probe_device() - we have no way to cope

	 * with that, so until ARM gets converted to rely on groups and default

	 * domains, just say no (but more politely than by dereferencing NULL).

	 * This should be at least a WARN_ON once that's sorted.

 Ensure that the domain is finalised */

	/*

	 * Sanity check the domain. We don't support domains across

	 * different SMMUs.

 Looks ok, so add the device to the domain */

	/*

	 * Setup an autosuspend delay to avoid bouncing runpm state.

	 * Otherwise, if a driver for a suspended consumer device

	 * unmaps buffers, it will runpm resume/suspend for each one.

	 *

	 * For example, when used by a GPU device, when an application

	 * or game exits, it can trigger unmapping 100s or 1000s of

	 * buffers.  With a runpm cycle for each buffer, that adds up

	 * to 5-10sec worth of reprogramming the context bank, while

	 * the system appears to be locked up to the user.

		/*

		 * Return true here as the SMMU can always send out coherent

		 * requests.

		/*

		 * If dev->iommu_fwspec is initally NULL, arm_smmu_register_legacy_master()

		 * will allocate/initialise a new one. Thus we need to update fwspec for

		 * later use.

 Remember group for faster lookups */

 Restricted during device attach */

 clear global FSR */

	/*

	 * Reset stream mapping groups: Initial values mark all SMRn as

	 * invalid and all S2CRn as bypass unless overridden.

 Make sure all context banks are disabled and clear CB_FSR  */

 Invalidate the TLB, just in case */

 Enable fault reporting */

 Disable TLB broadcasting. */

 Enable client access, handling unmatched streams as appropriate */

 Disable forced broadcasting */

 Don't upgrade barriers */

 Push the button */

 ID0 */

 Restrict available stages based on module parameter */

	/*

	 * In order for DMA API calls to work properly, we must defer to what

	 * the FW says about coherency, regardless of what the hardware claims.

	 * Fortunately, this also opens up a workaround for systems where the

	 * ID register value has ended up configured incorrectly.

 Max. number of entries we have for stream matching/indexing */

 Zero-initialised to mark as invalid */

 s2cr->type == 0 means translation, so initialise explicitly */

 ID1 */

 Check for size mismatch of SMMU address space from mapped region */

 Now properly encode NUMPAGE to subsequently derive SMMU_CB_BASE */

 ID2 */

 The output mask is also applied for bypass */

	/*

	 * What the page table walker can address actually depends on which

	 * descriptor format is in use, but since a) we don't know that yet,

	 * and b) it can vary per context bank, this will have to do...

 Now we've corralled the various formats, what'll it do? */

 Retrieve SMMU1/2 specific data */

 Ignore the configuration access interrupt */

 Oh, for a proper bus abstraction */

	/*

	 * The resource size should effectively match the value of SMMU_TOP;

	 * stash that temporarily until we know PAGESIZE to validate it with.

 Ignore superfluous interrupts */

	/*

	 * We want to avoid touching dev->power.lock in fastpaths unless

	 * it's really going to do something useful - pm_runtime_enabled()

	 * can serve as an ideal proxy for that decision. So, conditionally

	 * enable pm_runtime.

	/*

	 * For ACPI and generic DT bindings, an SMMU will be probed before

	 * any device which might need it, so we want the bus ops in place

	 * ready to handle default domain setup as soon as any SMMU exists.

 Turn the thing off */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (C) 2019-2020 NVIDIA CORPORATION.  All rights reserved.

/*

 * Tegra194 has three ARM MMU-500 Instances.

 * Two of them are used together and must be programmed identically for

 * interleaved IOVA accesses across them and translates accesses from

 * non-isochronous HW devices.

 * Third one is used for translating accesses from isochronous HW devices.

 *

 * In addition, the SMMU driver needs to coordinate with the memory controller

 * driver to ensure that the right SID override is programmed for any given

 * memory client. This is necessary to allow for use-case such as seamlessly

 * handing over the display controller configuration from the firmware to the

 * kernel.

 *

 * This implementation supports programming of the two instances that must

 * be programmed identically and takes care of invoking the memory controller

 * driver for SID override programming after devices have been attached to an

 * SMMU instance.

 clear global FSR */

		/*

		 * Interrupt line is shared between all contexts.

		 * Check for faults across all contexts.

 Instance 0 is ioremapped by arm-smmu.c. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IOMMU API for QCOM secure IOMMUs.  Somewhat based on arm-smmu.c

 *

 * Copyright (C) 2013 ARM Limited

 * Copyright (C) 2017 Red Hat

 IOMMU core code handle */

 indexed by asid-1 */

 asid and ctx bank # are 1:1 */

 Protects iommu pointer */

 Update the domain's page sizes to reflect the page table format */

 TTBRs */

 TCR */

 MAIRs (stage-1 only) */

 SCTLR */

 Publish page table ops for map/unmap */

	/*

	 * Allocate the domain and initialise some of its data structures.

	 * We can't really do anything meaningful until we've added a

	 * master.

		/*

		 * NOTE: unmap can be called after client device is powered

		 * off, for example, with GPUs or anything involving dma-buf.

		 * So we cannot rely on the device_link.  Make sure the IOMMU

		 * is on to avoid unclocked accesses in the TLB inv path:

 Ensure that the domain is finalized */

	/*

	 * Sanity check the domain. We don't support domains across

	 * different IOMMUs.

 Disable the context bank: */

	/* NOTE: unmap can be called after client device is powered off,

	 * for example, with GPUs or anything involving dma-buf.  So we

	 * cannot rely on the device_link.  Make sure the IOMMU is on to

	 * avoid unclocked accesses in the TLB inv path:

		/*

		 * Return true here as the SMMU can always send out coherent

		 * requests.

	/*

	 * Establish the link between iommu and master, so that the

	 * iommu gets runtime enabled/disabled as per the master's

	 * needs.

	/* make sure the asid specified in dt is valid, so we don't have

	 * to sanity check this elsewhere, since 'asid - 1' is used to

	 * index into qcom_iommu->ctxs:

		/* make sure devices iommus dt node isn't referring to

		 * multiple different iommu devices.  Multiple context

		 * banks are ok, but multiple devices are not:

	/* read the "reg" property directly to get the relative address

	 * of the context bank, and calculate the asid from that:

 context banks are 0x1000 apart */

	/* clear IRQs before registering fault handler, just in case the

	 * boot-loader left us a surprise:

 sentinel */ }

	/* find the max asid (which is 1:1 to ctx bank idx), so we know how

	 * many child ctx devices we have:

 register context bank devices, which are child nodes: */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ACPI support for Intel Lynxpoint LPSS.

 *

 * Copyright (C) 2013, Intel Corporation

 * Authors: Mika Westerberg <mika.westerberg@linux.intel.com>

 *          Rafael J. Wysocki <rafael.j.wysocki@intel.com>

 Offsets relative to LPSS_PRIVATE_OFFSET */

 LPSS Flags */

/*

 * For some devices the DSDT AML code for another device turns off the device

 * before our suspend handler runs, causing us to read/save all 1-s (0xffffffff)

 * as ctx register values.

 * Luckily these devices always use the same ctx register values, so we can

 * work around this by saving the ctx registers once on activation.

 Devices which need to be in D3 before lpss_iosf_enter_d3_state() proceeds */

 LPSS run time quirks */

/*

 * LPSS_QUIRK_ALWAYS_POWER_ON: override power state for LPSS DMA device.

 *

 * The LPSS DMA controller has neither _PS0 nor _PS3 method. Moreover

 * it can be powered off automatically whenever the last LPSS device goes down.

 * In case of no power any access to the DMA controller will hang the system.

 * The behaviour is reproduced on some HP laptops based on Intel BayTrail as

 * well as on ASuS T100TA transformer.

 *

 * This quirk overrides power state of entire LPSS island to keep DMA powered

 * on whenever we have at least one other device in use.

 UART Component Parameter Register */

/*

 * BYT PWM used for backlight control by the i915 driver on systems without

 * the Crystal Cove PMIC.

 Only call pwm_add_table for the first PWM controller */

 Expected to always be true, but better safe then sorry */

 Detect I2C bus shared with PUNIT and ignore its d3 status */

 BSW PWM used for backlight control by the i915 driver */

 Only call pwm_add_table for the first PWM controller */

 CONFIG_X86_INTEL_LPSS */

 Generic LPSS devices */

 Lynxpoint LPSS devices */

 BayTrail LPSS devices */

 Braswell LPSS devices */

 Broadwell LPSS devices */

 Wildcat Point LPSS devices */

 LPSS main clock device. */

 Prevent division by zero */

 Please keep this list sorted alphabetically by vendor and model */

/*

 * The _DEP method is used to identify dependencies but instead of creating

 * device links for every handle in _DEP, only links in the following list are

 * created. That is necessary because, in the general case, _DEP can refer to

 * devices that might not have drivers, or that are on different buses, or where

 * the supplier is not enumerated until after the consumer is probed.

 CHT External sdcard slot controller depends on PMIC I2C ctrl */

 CHT iGPU depends on PMIC I2C controller */

 BYT iGPU depends on the Embedded Controller I2C controller (UID 1) */

 BYT CR iGPU depends on PMIC I2C controller (UID 5 on CR) */

 BYT iGPU depends on PMIC I2C controller (UID 7 on non CR) */

 Avoid acpi_bus_attach() instantiating a pdev for this dev. */

 Skip the device, but continue the namespace scan. */

 Skip the device, but continue the namespace scan. */

	/*

	 * This works around a known issue in ACPI tables where LPSS devices

	 * have _PS0 and _PS3 without _PSC (and no power resources), so

	 * acpi_bus_init_power() will assume that the BIOS has put them into D0.

/**

 * acpi_lpss_save_ctx() - Save the private registers of LPSS device

 * @dev: LPSS device

 * @pdata: pointer to the private data of the LPSS device

 *

 * Most LPSS devices have private registers which may loose their context when

 * the device is powered down. acpi_lpss_save_ctx() saves those registers into

 * prv_reg_ctx array.

/**

 * acpi_lpss_restore_ctx() - Restore the private registers of LPSS device

 * @dev: LPSS device

 * @pdata: pointer to the private data of the LPSS device

 *

 * Restores the registers that were previously stored with acpi_lpss_save_ctx().

	/*

	 * The following delay is needed or the subsequent write operations may

	 * fail. The LPSS devices are actually PCI devices and the PCI spec

	 * expects 10ms delay before the device can be accessed after D3 to D0

	 * transition. However some platforms like BSW does not need this delay.

 default 10ms delay */

	/*

	 * This is called only on ->probe() stage where a device is either in

	 * known state defined by BIOS or most likely powered off. Due to this

	 * we have to deassert reset line to be sure that ->probe() will

	 * recognize the device.

 IOSF SB for LPSS island */

	/*

	 * PMC provides an information about actual status of the LPSS devices.

	 * Here we read the values related to LPSS power island, i.e. LPSS

	 * devices, excluding both LPSS DMA controllers, along with SCC domain.

	/*

	 * Get the status of entire LPSS power island per device basis.

	 * Shutdown both LPSS DMA controllers if and only if all other devices

	 * are already in D3hot.

	/*

	 * This call must be last in the sequence, otherwise PMC will return

	 * wrong status for devices being about to be powered off. See

	 * lpss_iosf_enter_d3_state() for further information.

	/*

	 * This call is kept first to be in symmetry with

	 * acpi_lpss_runtime_suspend() one.

		/*

		 * The driver's ->suspend_late callback will be invoked by

		 * acpi_lpss_do_suspend_late(), with the assumption that the

		 * driver really wanted to run that code in ->suspend_noirq, but

		 * it could not run after acpi_dev_suspend() and the driver

		 * expected the latter to be called in the "late" phase.

 Follow acpi_subsys_resume_noirq(). */

	/*

	 * The driver's ->resume_early callback will be invoked by

	 * acpi_lpss_do_resume_early(), with the assumption that the driver

	 * really wanted to run that code in ->resume_noirq, but it could not

	 * run before acpi_dev_resume() and the driver expected the latter to be

	 * called in the "early" phase.

 This is analogous to what happens in acpi_lpss_resume_noirq(). */

 This is analogous to the acpi_lpss_suspend_noirq() case. */

 CONFIG_PM_SLEEP */

 CONFIG_PM */

 CONFIG_X86_INTEL_LPSS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2012, Intel Corporation

 * Copyright (c) 2015, Red Hat, Inc.

 * Copyright (c) 2015, 2016 Linaro Ltd.

/*

 * Erratum 44 for QDF2432v1 and QDF2400v1 SoCs describes the BUSY bit as

 * occasionally getting stuck as 1. To avoid the potential for a hang, check

 * TXFE == 0 instead of BUSY == 1. This may not be suitable for all UART

 * implementations, so only do so if an affected platform is detected in

 * acpi_parse_spcr().

/*

 * Some Qualcomm Datacenter Technologies SoCs have a defective UART BUSY bit.

 * Detect them by examining the OEM fields in the SPCR header, similar to PCI

 * quirk detection in pci_mcfg.c.

/*

 * APM X-Gene v1 and v2 UART hardware is an 16550 like device but has its

 * register aligned to 32-bit. In addition, the BIOS also encoded the

 * access width to be 8 bits. This function detects this errata condition.

/**

 * acpi_parse_spcr() - parse ACPI SPCR table and add preferred console

 *

 * @enable_earlycon: set up earlycon for the console specified by the table

 * @enable_console: setup the console specified by the table.

 *

 * For the architectures with support for ACPI, CONFIG_ACPI_SPCR_TABLE may be

 * defined to parse ACPI SPCR table.  As a result of the parsing preferred

 * console is registered and if @enable_earlycon is true, earlycon is set up.

 * If @enable_console is true the system console is also configured.

 *

 * When CONFIG_ACPI_SPCR_TABLE is defined, this function should be called

 * from arch initialization code as soon as the DT/ACPI decision is made.

 *

		/*

		 * SPCR 1.04 defines 0 as a preconfigured state of UART.

		 * Assume firmware or bootloader configures console correctly.

	/*

	 * If the E44 erratum is required, then we need to tell the pl011

	 * driver to implement the work-around.

	 *

	 * The global variable is used by the probe function when it

	 * creates the UARTs, whether or not they're used as a console.

	 *

	 * If the user specifies "traditional" earlycon, the qdf2400_e44

	 * console name matches the EARLYCON_DECLARE() statement, and

	 * SPCR is not used.  Parameter "earlycon" is false.

	 *

	 * If the user specifies "SPCR" earlycon, then we need to update

	 * the console name so that it also says "qdf2400_e44".  Parameter

	 * "earlycon" is true.

	 *

	 * For consistency, if we change the console name, then we do it

	 * for everyone, not just earlycon.

		/* for xgene v1 and v2 we don't know the clock rate of the

		 * UART so don't attempt to change to the baud rate state

		 * in the table because driver cannot calculate the dividers

 SPDX-License-Identifier: GPL-2.0-or-later

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ACPI support for platform bus type.

 *

 * Copyright (C) 2012, Intel Corporation

 * Authors: Mika Westerberg <mika.westerberg@linux.intel.com>

 *          Mathias Nyman <mathias.nyman@linux.intel.com>

 *          Rafael J. Wysocki <rafael.j.wysocki@intel.com>

 PIC */

 Timer */

 AT DMA Controller */

 IOxAPIC */

 IOAPIC */

 ACPI SMBUS virtual device */

 Nothing to do here */

	/*

	 * If the device has parent we need to take its resources into

	 * account as well because this device might consume part of those.

/**

 * acpi_create_platform_device - Create platform device for ACPI device node

 * @adev: ACPI device node to create a platform device for.

 * @properties: Optional collection of build-in properties.

 *

 * Check if the given @adev can be represented as a platform device and, if

 * that's the case, create and register a platform device, populate its common

 * resources and returns a pointer to it.  Otherwise, return %NULL.

 *

 * Name of the platform device will be the same as @adev's.

 If the ACPI node already has a physical device attached, skip it. */

	/*

	 * If the ACPI node has a parent and that parent has a physical device

	 * attached to it, that physical device should be the parent of the

	 * platform device we are about to create.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * acpi_pad.c ACPI Processor Aggregator Driver

 *

 * Copyright (c) 2009, Intel Corporation.

		/*

		 * AMD Fam10h TSC will tick in all

		 * C/P/S0/S1 states when this bit is set.

 TSC could halt in idle */

 avoid HT sibilings if possible */

 percentage */

 second */

 round robin to cpus */

 TSC could halt in idle, so notify users */

		/*

		 * current sched_rt has threshold for rt task running time.

		 * When a rt task uses 95% CPU time, the rt thread will be

		 * scheduled out for 5% CPU time to not starve other tasks. But

		 * the mechanism only works when all CPUs have RT task running,

		 * as if one CPU hasn't RT task, RT task from other CPUs will

		 * borrow CPU time from this CPU and cause RT task use > 95%

		 * CPU time. To make 'avoid starvation' work, takes a nap here.

		/* If an external event has set the need_resched flag, then

		 * we need to deal with it, or this loop will continue to

		 * spin without calling __mwait().

/*

 * Query firmware how many CPUs should be idle

 * return -1 on failure

 rev 1 */

 Xen ACPI PAD is used when running as Xen Dom0. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * acpi_lpit.c - LPIT table processing functions

 *

 * Copyright (C) 2017 Intel Corporation. All rights reserved.

 Storage for an memory mapped and FFH based entries */

 Silently fail, if cpuidle attribute group is not present */

 Silently fail, if cpuidle attribute group is not present */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AMD ACPI support for ACPI2platform device.

 *

 * Copyright (c) 2014,2015 AMD Corporation.

 * Authors: Ken Xue <Ken.Xue@amd.com>

 *	Wu, Jeff <Jeff.Wu@amd.com>

/**

 * struct apd_device_desc - a descriptor for apd device

 * @fixed_clk_rate: fixed rate input clock source for acpi device;

 *			0 means no fixed rate input clock source

 * @properties: build-in properties of the device such as UART

 * @setup: a hook routine to set device resource during create platform device

 *

 * Device description defined as acpi_device_id.driver_data

 CONFIG_X86_AMD_PLATFORM_DEVICE */

 CONFIG_ARM64 */

/*

 * Create platform device during acpi scan attach handle.

 * Return value > 0 on success of creating device.

 Generic apd devices */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  ec.c - ACPI Embedded Controller Driver (v3)

 *

 *  Copyright (C) 2001-2015 Intel Corporation

 *    Author: 2014, 2015 Lv Zheng <lv.zheng@intel.com>

 *            2006, 2007 Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>

 *            2006       Denis Sadykov <denis.m.sadykov@intel.com>

 *            2004       Luming Yu <luming.yu@intel.com>

 *            2001, 2002 Andy Grover <andrew.grover@intel.com>

 *            2001, 2002 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>

 *  Copyright (C) 2008      Alexey Starikovskiy <astarikovskiy@suse.de>

 Uncomment next line to get verbose printout */

 #define DEBUG */

 EC status register */

 Output buffer full */

 Input buffer full */

 Input buffer contains a command */

 burst mode */

 EC-SCI occurred */

/*

 * The SCI_EVT clearing timing is not defined by the ACPI specification.

 * This leads to lots of practical timing issues for the host EC driver.

 * The following variations are defined (from the target EC firmware's

 * perspective):

 * STATUS: After indicating SCI_EVT edge triggered IRQ to the host, the

 *         target can clear SCI_EVT at any time so long as the host can see

 *         the indication by reading the status register (EC_SC). So the

 *         host should re-check SCI_EVT after the first time the SCI_EVT

 *         indication is seen, which is the same time the query request

 *         (QR_EC) is written to the command register (EC_CMD). SCI_EVT set

 *         at any later time could indicate another event. Normally such

 *         kind of EC firmware has implemented an event queue and will

 *         return 0x00 to indicate "no outstanding event".

 * QUERY: After seeing the query request (QR_EC) written to the command

 *        register (EC_CMD) by the host and having prepared the responding

 *        event value in the data register (EC_DATA), the target can safely

 *        clear SCI_EVT because the target can confirm that the current

 *        event is being handled by the host. The host then should check

 *        SCI_EVT right after reading the event response from the data

 *        register (EC_DATA).

 * EVENT: After seeing the event response read from the data register

 *        (EC_DATA) by the host, the target can clear SCI_EVT. As the

 *        target requires time to notice the change in the data register

 *        (EC_DATA), the host may be required to wait additional guarding

 *        time before checking the SCI_EVT again. Such guarding may not be

 *        necessary if the host is notified via another IRQ.

 EC commands */

 Wait 500ms max. during EC ops */

 Wait 1ms max. to get global lock */

 Wait 1ms for EC transaction polling */

#define ACPI_EC_CLEAR_MAX	100	/* Maximum number of events to query

 Maximum number of parallel queries */

 Query is enabled */

 Query is pending */

 Guard for SCI_EVT check */

 Event handler installed */

 OpReg handler installed */

 _Qxx handlers installed */

 Driver is started */

 Driver is stopped */

 Events masked */

 Available for command byte */

 Completed last byte */

 ec.c is compiled in acpi namespace so this shows up as acpi.ec_delay param */

/*

 * If the number of false interrupts per one transaction exceeds

 * this threshold, will think there is a GPE storm happened and

 * will disable the GPE for normal transaction.

 Needs ECDT port address correction */

 Needs ECDT GPE as correction setting */

 Needs DSDT GPE as correction setting */

 Needs acpi_ec_clear() on boot/resume */

/* --------------------------------------------------------------------------

 *                           Logging/Debugging

/*

 * Splitters used by the developers to track the boundary of the EC

 * handling processes.

/* --------------------------------------------------------------------------

 *                           Device Flags

	/*

	 * There is an OSPM early stage logic. During the early stages

	 * (boot/resume), OSPMs shouldn't enable the event handling, only

	 * the EC transactions are allowed to be performed.

	/*

	 * However, disabling the event handling is experimental for late

	 * stage (suspend), and is controlled by the boot parameter of

	 * "ec_freeze_events":

	 * 1. true:  The EC event handling is disabled before entering

	 *           the noirq stage.

	 * 2. false: The EC event handling is automatically disabled as

	 *           soon as the EC driver is stopped.

/* --------------------------------------------------------------------------

 *                           EC Registers

/* --------------------------------------------------------------------------

 *                           GPE Registers

		/*

		 * On some platforms, EN=1 writes cannot trigger GPE. So

		 * software need to manually trigger a pseudo GPE event on

		 * EN=1 writes.

/* --------------------------------------------------------------------------

 *                           Transaction Management

/*

 * acpi_ec_submit_flushable_request() - Increase the reference count unless

 *                                      the flush operation is not in

 *                                      progress

 * @ec: the EC device

 *

 * This function must be used before taking a new action that should hold

 * the reference count.  If this function returns false, then the action

 * must be discarded or it will prevent the flush operation from being

 * completed.

	/*

	 * Unconditionally invoke this once after enabling the event

	 * handling mechanism to detect the pending events.

/*

 * Process _Q events that might have accumulated in the EC.

 * Run with locked ec mutex.

 Drain additional events if hardware requires that */

 flush ec->work */

 flush queries */

	/*

	 * When ec_freeze_events is true, we need to flush events in

	 * the proper position before entering the noirq stage.

 Without ec_wq there is nothing to flush. */

 CONFIG_PM_SLEEP */

	/*

	 * If firmware SCI_EVT clearing timing is "event", we actually

	 * don't know when the SCI_EVT will be cleared by firmware after

	 * evaluating _Qxx, so we need to re-check SCI_EVT after waiting an

	 * acceptable period.

	 *

	 * The guarding period begins when EC_FLAGS_QUERY_PENDING is

	 * flagged, which means SCI_EVT check has just been performed.

	 * But if the current transaction is ACPI_EC_COMMAND_QUERY, the

	 * guarding should have already been performed (via

	 * EC_FLAGS_QUERY_GUARDING) and should not be applied so that the

	 * ACPI_EC_COMMAND_QUERY transaction can be transitioned into

	 * ACPI_EC_COMMAND_POLL state immediately.

 Trigger if the threshold is 0 too. */

	/*

	 * Clear GPE_STS upfront to allow subsequent hardware GPE_STS 0->1

	 * changes to always trigger a GPE interrupt.

	 *

	 * GPE STS is a W1C register, which means:

	 *

	 * 1. Software can clear it without worrying about clearing the other

	 *    GPEs' STS bits when the hardware sets them in parallel.

	 *

	 * 2. As long as software can ensure only clearing it when it is set,

	 *    hardware won't set it in parallel.

	/*

	 * Another IRQ or a guarded polling mode advancement is detected,

	 * the next QR_EC submission is then allowed.

 Ensure guarding period before polling EC status */

 Perform busy polling */

			/*

			 * Perform wait polling

			 * 1. Wait the transaction to be completed by the

			 *    GPE handler after the transaction enters

			 *    ACPI_EC_COMMAND_POLL state.

			 * 2. A special guarding logic is also required

			 *    for event clearing mode "event" before the

			 *    transaction enters ACPI_EC_COMMAND_POLL

			 *    state.

 number of command restarts */

 start transaction */

 Enable GPE for command processing (IBF=0/OBF=1) */

 following two actions should be kept atomic */

 Disable GPE for command processing (IBF=0/OBF=1) */

 Get the handle to the EC device */

 Enable GPE for event processing (SCI_EVT=1) */

 Disable GPE for event processing (SCI_EVT=1) */

 Prevent transactions from being carried out */

	/*

	 * Allow transactions to happen again (this function is called from

	 * atomic context during wakeup, so we don't need to acquire the mutex).

/* --------------------------------------------------------------------------

                                Event Management

	/*

	 * Query the EC to find out which _Qxx method we need to evaluate.

	 * Note that successful completion of the query causes the ACPI_EC_SCI

	 * bit to be cleared (and thus clearing the interrupt source).

	/*

	 * It is reported that _Qxx are evaluated in a parallel way on

	 * Windows:

	 * https://bugzilla.kernel.org/show_bug.cgi?id=94411

	 *

	 * Put this log entry before schedule_work() in order to make

	 * it appearing before any other log entries occurred during the

	 * work queue execution.

			/*

			 * Take care of the SCI_EVT unless no one else is

			 * taking care of it.

		/*

		 * Before exit, make sure that this work item can be

		 * scheduled again. There might be QR_EC failures, leaving

		 * EC_FLAGS_QUERY_PENDING uncleared and preventing this work

		 * item from being scheduled again.

/* --------------------------------------------------------------------------

 *                           Address Space Management

/* --------------------------------------------------------------------------

 *                             Driver Interface

 clear addr values, ec_parse_io_ports depend on it */

		/*

		 * Always inherit the GPE number setting from the ECDT

		 * EC.

 Get GPE bit assignment (EC events). */

 TODO: Add support for _GPE returning a package */

		/*

		 * Errors are non-fatal, allowing for ACPI Reduced Hardware

		 * platforms which use GpioInt instead of GPE.

 Use the global lock for all EC transactions? */

/**

 * ec_install_handlers - Install service callbacks and register query methods.

 * @ec: Target EC.

 * @device: ACPI device object corresponding to @ec.

 *

 * Install a handler for the EC address space type unless it has been installed

 * already.  If @device is not NULL, also look for EC query methods in the

 * namespace and register them, and install an event (either GPE or GPIO IRQ)

 * handler for the EC, if possible.

 *

 * Return:

 * -ENODEV if the address space handler cannot be installed, which means

 *  "unable to handle transactions",

 * -EPROBE_DEFER if GPIO IRQ acquisition needs to be deferred,

 * or 0 (success) otherwise.

 ACPI reduced hardware platforms use a GpioInt from _CRS. */

		/*

		 * Bail out right away for deferred probing or complete the

		 * initialization regardless of any other errors.

 Find and register all query methods */

		/*

		 * Failures to install an event handler are not fatal, because

		 * the EC can be polled for events.

 EC is fully operational, allow queries */

	/*

	 * Stops handling the EC transactions after removing the operation

	 * region handler. This is required because _REG(DISCONNECT)

	 * invoked during the removal can result in new EC transactions.

	 *

	 * Flushes the EC requests and thus disables the GPE before

	 * removing the GPE handler. This is required by the current ACPICA

	 * GPE core. ACPICA GPE core will automatically disable a GPE when

	 * it is indicated but there is no way to handle it. So the drivers

	 * must disable the GPEs prior to removing the GPE handlers.

 First EC capable of handling transactions */

 Fast path: this device corresponds to the boot EC. */

			/*

			 * Trust PNP0C09 namespace location rather than

			 * ECDT ID. But trust ECDT GPE rather than _GPE

			 * because of ASUS quirks, so do not change

			 * boot_ec->gpe to ec->gpe.

 Reprobe devices depending on the EC */

	/*

	 * The first address region returned is the data port, and

	 * the second address region returned is the status/command

	 * port.

/*

 * This function is not Windows-compatible as Windows never enumerates the

 * namespace EC before the main ACPI device enumeration process. It is

 * retained for historical reason and will be deprecated in the future.

	/*

	 * If a platform has ECDT, there is no need to proceed as the

	 * following probe is not a part of the ACPI device enumeration,

	 * executing _STA is not safe, and thus this probe may risk of

	 * picking up an invalid EC device.

	/*

	 * At this point, the namespace is initialized, so start to find

	 * the namespace objects.

	/*

	 * When the DSDT EC is available, always re-configure boot EC to

	 * have _REG evaluated. _REG can only be evaluated after the

	 * namespace initialization.

	 * At this point, the GPE is not fully initialized, so do not to

	 * handle the events.

/*

 * acpi_ec_ecdt_start - Finalize the boot ECDT EC initialization.

 *

 * First, look for an ACPI handle for the boot ECDT EC if acpi_ec_add() has not

 * found a matching object in the namespace.

 *

 * Next, in case the DSDT EC is not functioning, it is still necessary to

 * provide a functional ECDT EC to handle events, so add an extra device object

 * to represent it (see https://bugzilla.kernel.org/show_bug.cgi?id=115021).

 *

 * This is useful on platforms with valid ECDT and invalid DSDT EC settings,

 * like ASUS X550ZE (see https://bugzilla.kernel.org/show_bug.cgi?id=196847).

 Bail out if a matching EC has been found in the namespace. */

 Look up the object pointed to from the ECDT in the namespace. */

 Add a special ACPI device object to represent the boot EC. */

/*

 * On some hardware it is necessary to clear events accumulated by the EC during

 * sleep. These ECs stop reporting GPEs until they are manually polled, if too

 * many events are accumulated. (e.g. Samsung Series 5/9 notebooks)

 *

 * https://bugzilla.kernel.org/show_bug.cgi?id=44161

 *

 * Ideally, the EC should also be instructed NOT to accumulate events during

 * sleep (which Windows seems to do somehow), but the interface to control this

 * behaviour is not known at this time.

 *

 * Models known to be affected are Samsung 530Uxx/535Uxx/540Uxx/550Pxx/900Xxx,

 * however it is very likely that other Samsung models are affected.

 *

 * On systems which don't accumulate _Q events during sleep, this extra check

 * should be harmless.

/*

 * Some ECDTs contain wrong register addresses.

 * MSI MS-171F

 * https://bugzilla.kernel.org/show_bug.cgi?id=12461

/*

 * Some ECDTs contain wrong GPE setting, but they share the same port addresses

 * with DSDT EC, don't duplicate the DSDT EC with ECDT EC in this case.

 * https://bugzilla.kernel.org/show_bug.cgi?id=209989

/*

 * Some DSDTs contain wrong GPE setting.

 * Asus FX502VD/VE, GL702VMK, X550VXK, X580VD

 * https://bugzilla.kernel.org/show_bug.cgi?id=195651

bugzilla.kernel.org/show_bug.cgi?id=209989 */

 Generate a boot ec context. */

		/*

		 * Asus X50GL:

		 * https://bugzilla.kernel.org/show_bug.cgi?id=11880

	/*

	 * Ignore the GPE value on Reduced Hardware platforms.

	 * Some products have this set to an erroneous value.

	/*

	 * At this point, the namespace is not initialized, so do not find

	 * the namespace objects, or handle the events.

	/*

	 * The SCI handler doesn't run at this point, so the GPE can be

	 * masked at the low level without side effects.

	/*

	 * Report wakeup if the status bit is set for any enabled GPE other

	 * than the EC one.

	/*

	 * Dispatch the EC GPE in-band, but do not report wakeup in any case

	 * to allow the caller to process events properly after that.

 Flush the event and query workqueues. */

 CONFIG_PM_SLEEP */

	/*

	 * Disable EC wakeup on following systems to prevent periodic

	 * wakeup from EC GPE.

 Driver must be registered after acpi_ec_init_workqueues(). */

 EC driver currently not unloadable */

 0 */

